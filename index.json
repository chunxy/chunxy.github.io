[{"authors":null,"categories":null,"content":"Articles contains the notes where I combined my own elaboration with other blogs, forum discussions, lecture notes, Wiki, etc. According to the subject, they are further grouped into:\n Information Theory Machine Learning Mathematics Optimization  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"32f1ae15161db38a68d36a5b05971087","permalink":"https://chunxy.github.io/notes/articles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/","section":"notes","summary":"Articles contains the notes where I combined my own elaboration with other blogs, forum discussions, lecture notes, Wiki, etc. According to the subject, they are further grouped into:\n Information Theory Machine Learning Mathematics Optimization  ","tags":null,"title":"Articles","type":"book"},{"authors":null,"categories":null,"content":"Books contains the notes mostly extracted from many classical textbooks. These notes are just grouped according to the book title:\n Information Theory, Inference and Learning Algorithms Linear Algebra and Its Applications 概率论与数理统计  Nowadays ebooks are more convenient and available than traditional paper-based books. However, you cannot flip through an ebook as easily as in a paper-based book to locate the content you want to recap. So if you are also an ebook-fan like me, I believe it is a beneficial habit to turn your reading scratches to systematic notes like above.\n","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"33c9e341aa36b46c894c7dbd62dc3b5f","permalink":"https://chunxy.github.io/notes/books/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/","section":"notes","summary":"Books contains the notes mostly extracted from many classical textbooks. These notes are just grouped according to the book title: Information Theory, Inference and Learning Algorithms Linear Algebra and Its","tags":null,"title":"Books","type":"book"},{"authors":null,"categories":null,"content":"  Jacobian Matrix Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .\n  Spherical Coordinates The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be described as below:\n  Lipschitz Continuity For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"e7ed34de58dff49e385796884a33c783","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/","section":"notes","summary":"Jacobian Matrix Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .","tags":null,"title":"Calculus","type":"book"},{"authors":null,"categories":null,"content":"  Entropy The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy of continuous distribution \\(q\\) (probability density function) is usually similarly defined as \\[ H(q) = -\\E_{x\\sim q}\\log q(x) = -\\int q(x)\\log q(x)\\d x \\] This is actually the differential entropy introduced by Shannon.\n  Conditional Entropy The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known.\n  Cross Entropy The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).\n  Mutual Information Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable.\n  KL-divergence KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\). In information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).\n  f-divergence \\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function.\n  Jenson-Shannon Divergence Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric.\n  Overview Distribution or Random Variable? Both Cross Entropy and KL-divergence describe the relationship between two distributions. Both Conditional Entropy and Mutual Information describe the relationship between two random variables. Since the notion of entropy is the basis of all other concepts in this section and the entropy is more well-defined on discrete random variable, they are more meaningful to be applied in discrete case, though they are usually easily extended to continuous case.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"21c069359db95b8f5e32ca83dd5fa9b8","permalink":"https://chunxy.github.io/notes/articles/information-theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/","section":"notes","summary":"Entropy The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy of continuous distribution \\(q\\) (probability density function) is usually similarly defined as \\[ H(q) = -\\E_{x\\sim q}\\log q(x) = -\\int q(x)\\log q(x)\\d x \\] This is actually the differential entropy introduced by Shannon.","tags":null,"title":"Information Theory","type":"book"},{"authors":null,"categories":null,"content":"  Source Coding Theory Notations and Concepts An ensemble \\(X\\) is a triple \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(x\\) is the outcome of a random variable, which takes on values from\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"648da0d35c916615b2e059c81f3e0f9d","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/","section":"notes","summary":"Source Coding Theory Notations and Concepts An ensemble \\(X\\) is a triple \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(x\\) is the outcome of a random variable, which takes on values from","tags":null,"title":"Information Theory, Inference and Learning Algorithms","type":"book"},{"authors":null,"categories":null,"content":"  Eigenvectors and Eigenvalues Eigenvectors and Eigenvalues Definition An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\).\n  Real Symmetric Matrix Assume \\(A\\) is a \\(n \\times n\\) real-valued symmetric matrix. We have its properties as following. Real-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued. Suppose by contradiction that \\(A\\) has some imaginary eigenvalue \\(\\lambda\\) and the corresponding imaginary eigenvector \\(x\\).\n  Singular Value Decomposition Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gather} \\text{$U$ is $m \\times m$, $\\Sigma$ is diagonal, $V$ is $n \\times n$} \\\\\n  Matrix Identity A useful matrix identity \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026amp;= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026amp;= A(I_M+BA)^{-1} \\\\ \\iff A \u0026amp;= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A \\end{aligned} \\]\n  Difference Equation Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form: \\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]\n  Metrics Spectral Normalization Spectral Normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), \\(\\sigma_{\\max}(A)\\) is \\(A\\)’s largest singular value.\n   Laplace Expansion Also known as Cofactor Expansion, Laplace Expansion is an expression of an $n \\times n$ matrix as the weighted sum of determinants of some $(n-1) \\times (n-1)$ sub-matrices.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"6e77d541ab651be2804d292bd4482e95","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/","section":"notes","summary":"Eigenvectors and Eigenvalues Eigenvectors and Eigenvalues Definition An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\).","tags":null,"title":"Linear Algebra","type":"book"},{"authors":null,"categories":null,"content":"  Gram-Schmidt Orthogonalization Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\). Given a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define\n  Least Squares Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\).\n  Orthogonality and Projection If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other. Let \\(\\{u_1, u_2, ..., u_k\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\R^n\\).\n  Coordinate System and Change of Basis Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, .\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"2c296658266b2ff674bc5d326158b641","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/","section":"notes","summary":"Gram-Schmidt Orthogonalization Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\). Given a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define","tags":null,"title":"Linear Algebra and Its Applications","type":"book"},{"authors":null,"categories":null,"content":"  Machine Learning Bullet Points This post lists out various topics under the machine learning subject. Data data modalities numbers texts images videos audios data sampling bootstrap aggregation data cleaning imbalanced data data augmentation data splitting feature extraction/engineering domain expertise statistics feature normalization Model supervised vs.\n  Linear Discriminant Analysis Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is.\n  Logistic Regression Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\).\n  Support Vector Machine Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary.\n  Linear Regression Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx\n  Non-linear Regression The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space. Polynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\).\n  Clustering Given dataset \\(\\mathcal D = \\{x^{(i)}, i=1,\\dots,M\\}\\), a clustering \\(\\mathcal C\\) of the \\(M\\) points into \\(K (K \\le M)\\) clusters is a partition of \\(\\mathcal D\\) into \\(K\\) disjoint groups \\(\\{C_1,\\dots,C_K\\}\\).\n  Dimension Reduction Unsupervised Dimension Reduction Dimensionality reduction reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity. Compared to feature selection,\n  Principal Component Analysis Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the\n  Eckart-Young-Mirsky Theorem Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, .\n  Independent Component Analysis Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in R^{N \\times 1}\\), find the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\).\n  RANSAC Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to\n  Fisher\u0026#39;s Linear Discriminant Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)).\n  Bias-variance Decomposition The notation used is as follows: Symbol Notation \\(\\mathcal D\\) the dataset \\(x\\) the sample \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise \\(y\\) the real value of \\(x\\) \\(\\bar y\\) the mean of the real values \\(f\\) the model learned with \\(\\mathcal D\\) \\(f(x)\\) the prediction of \\(f\\) with \\(x\\) \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\) \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026amp;[l(f(x), y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026amp;\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar …","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"4a28399944c3ca08382a34ee9f39e2d2","permalink":"https://chunxy.github.io/notes/articles/machine-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/","section":"notes","summary":"Machine Learning Bullet Points This post lists out various topics under the machine learning subject. Data data modalities numbers texts images videos audios data sampling bootstrap aggregation data cleaning imbalanced data data augmentation data splitting feature extraction/engineering domain expertise statistics feature normalization Model supervised vs.","tags":null,"title":"Machine Learning","type":"book"},{"authors":null,"categories":null,"content":"Mathematics is itself a huge topic. So I further group the Mathematics subsection into sub-subsections:\n Calculus Linear Algebra Numerical Analysis Probability and Statistics  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"caabf42bcf0fbef2c6df7dcee4f1ce10","permalink":"https://chunxy.github.io/notes/articles/mathematics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/","section":"notes","summary":"Mathematics is itself a huge topic. So I further group the Mathematics subsection into sub-subsections:\n Calculus Linear Algebra Numerical Analysis Probability and Statistics  ","tags":null,"title":"Mathematics","type":"book"},{"authors":null,"categories":null,"content":"The “Notes” section contains the notes I took from various sources, e.g. blogs, forum discussions, books, papers, etc, which I gave credit informally by simply putting the source address in my posts.\nI generally group my notes into Articles, Books and Papers, according to the source format, under which more topics and subtopics are further defined for better categorization. These notes are thus more organized and structured than Blogs.\nHowever, they are more of a tedious manual than an inviting roadmap. I try to put rigorous (within my command) definitions and proofs in them, just for fast recap, as well as being my second brain. But these notes may lack of the justification for why the definitions are necessary and why the proofs should be done in this way, which I believe instead is more vital in learning.\n","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"ab7a472e5d2a0dba3f39440bfb9334ba","permalink":"https://chunxy.github.io/notes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/","section":"notes","summary":"The “Notes” section contains the notes I took from various sources, e.g. blogs, forum discussions, books, papers, etc, which I gave credit informally by simply putting the source address in my posts.","tags":null,"title":"Chunxy' Notes","type":"book"},{"authors":null,"categories":null,"content":"  Fourier Transform Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two\n  Stirling\u0026#39;s Approximation Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"0bb8d49c4270a2e9cbca37fbd9592b1d","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/","section":"notes","summary":"Fourier Transform Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two","tags":null,"title":"Numerical Analysis","type":"book"},{"authors":null,"categories":null,"content":"  Lagrange Multiplier Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026amp;\\inf f_0(x) \\\\ s.t.\\quad \u0026amp; f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp; h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).\n  Convex Optimization Definitions Convex Set A set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).\n  Gradient Descent If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent. \\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner.\n  Coordinate Descent Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima?\n  Expectation Maximization If a probabilistic model contains only observable variables, Maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables).\n  Subgradient Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\}\n  Least Angle Regression Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.\n   Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"9c9979febb0e862d1e8d3228769675eb","permalink":"https://chunxy.github.io/notes/articles/optimization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/","section":"notes","summary":"Lagrange Multiplier Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026\\inf f_0(x) \\\\ s.t.\\quad \u0026 f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026 h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).","tags":null,"title":"Optimization","type":"book"},{"authors":null,"categories":null,"content":"Papers, as the name suggests, contains the notes I took from the academic papers I read:\n Bounding Mutual Information Contrastive Predictive Coding Noise Contrastive Estimation  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"76727f9a2d5c315359826ee4597017e2","permalink":"https://chunxy.github.io/notes/papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/","section":"notes","summary":"Papers, as the name suggests, contains the notes I took from the academic papers I read:\n Bounding Mutual Information Contrastive Predictive Coding Noise Contrastive Estimation  ","tags":null,"title":"Papers","type":"book"},{"authors":null,"categories":null,"content":"  Gaussian Distribution Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\] To verify that it integrates to \\(1\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} p(x) \\d x \u0026amp;= \\sqrt{(\\int_{-\\infty}^{+\\infty} p(x) \\d x) \\cdot (\\int_{-\\infty}^{+\\infty} p(y) \\d y)} \\\\ \u0026amp;= \\sqrt {\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y} \\\\ \\end{aligned} \\]\n  Unconscious Statistics Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).\n  随机变量的收敛 依概率收敛（convergence in probability）\n  特征函数 定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),\n  Whitening Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"c69caf43e206cc97cb8f8ad1a813d6ef","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/","section":"notes","summary":"Gaussian Distribution Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\] To verify that it integrates to \\(1\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} p(x) \\d x \u0026= \\sqrt{(\\int_{-\\infty}^{+\\infty} p(x) \\d x) \\cdot (\\int_{-\\infty}^{+\\infty} p(y) \\d y)} \\\\ \u0026= \\sqrt {\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y} \\\\ \\end{aligned} \\]","tags":null,"title":"Probability and Statistics","type":"book"},{"authors":null,"categories":null,"content":"  总览 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）\n  事件与概率 事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\\n  常见分布 离散型 二项分布（binomial distribution） 如\n  协方差与相关系数 以下以二维随机变量为例，展示协方差以及相关系数的概念。 协方差\n  随机变量的函数 设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，\n  大数定律和中心极限定理 前置知识 Chebyshev不等式 定理 设随机变量\\(X\\)的期\n  三大分布与正态总体的抽样分布 \\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由\n  统计量 定义 设\\((X_1,\\dots,X_n)\\)为取自总体的一组\n  参数估计 点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知\n  假设检验 假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间\n  贝叶斯推断 贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j) P(A|B_j)} \\] 这\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"9dbea9556e81ba7c3226a6643554efbc","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","section":"notes","summary":"总览 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子","tags":null,"title":"概率论与数理统计","type":"book"},{"authors":null,"categories":null,"content":"  Spectral Normalization Spectral Normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), \\(\\sigma_{\\max}(A)\\) is \\(A\\)’s largest singular value.\n  Frobenius Normalization Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F = \\sqrt{\\sum_{ij}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 \u0026amp;= \\sum_{ij}A_{ij}^2 \\\\ \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026amp;= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026amp;= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026amp;= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026amp;= tr(AA^T) = tr(A^TA) \\end{aligned} \\]\n  Chebyshev Distance Discrete form Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ \\begin{aligned} d_p(x, x^\\prime) \u0026amp;= ||x - x^\\prime||_p \\\\ \u0026amp;= (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\text{, where $p \\to\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"c8fe1129e63cb1356c7400a85d34b5af","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/","section":"notes","summary":"Spectral Normalization Spectral Normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), \\(\\sigma_{\\max}(A)\\) is \\(A\\)’s largest singular value.","tags":null,"title":"Metrics","type":"book"},{"authors":null,"categories":null,"content":" 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）到数字的映射，是一个抽象事件数字化的过程。概率论和统计的一个共同的主要话题就是随机变量，可以说它们像是随机变量的一体两面。\n概率论更加注重随机变量取值集合相对应事件的概率。为此，概率论需要讨论事件所有可能的试验结果、事件的运算、事件的独立性、随机变量的取值范围、随机变量的概率分布等话题。\n统计中的随机变量来自于对总体的随机抽样，这个过程中，我们会得到一组样本，每个样本在被观测之前，都是服从总体分布的随机变量；观测（observation）之后，它们便有了一个具体的观测值（realization）。我们可以将观测行为类比概率论中的试验，而这种观测行为将会导致一个随机变量坍缩成为一个具体的观测值。\n可以这样理解概率论与统计：概率论是根据事件总体的自身属性，正向推导所有事件对应概率分布的分布函数；统计是根据某个概率分布（即总体分布）采样得到的结果，反向推导该分布的分布函数。\n  概率论 统计    概率空间（事件总体） 样本空间（总体）  试验 样本  随机变量的数字特征 样本的数字特征（统计量）  渐进理论 统计推段    ","date":1667927521,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667927521,"objectID":"d457d492963dc7122080eed24322b500","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/","section":"notes","summary":"概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）","tags":null,"title":"总览","type":"book"},{"authors":null,"categories":null,"content":" Covariance matrix of a random vector can usually be deduced from the distribution’s property, or estimated from samples. But how to generate an arbitrary covariance matrix? How should we populate the entries in a square matrix so that it makes a legitimate covariance matrix?\nFirst Method In general, we construct the target covariance matrix \\(\\Sigma\\) by giving its eigenvalues and its orthonormal eigenvectors (any real symmetric matrix, including the covariance matrix of course, can be constructed in this way). A diagonal matrix of eigenvalues (denoted as \\(D\\)) are easy to synthesize. It remains that how to synthesize a square matrix that has orthonormal column vectors.\nLet the dimension of the target covariance matrix be \\(n\\). Given an arbitrary square matrix \\(M\\) of dimension \\(n\\), we can decompose \\(M M^T\\), which is a real symmetric matrix, into \\(U \\Lambda U^T\\), where \\(U\\) is the orthonormal matrix consisting of \\(M M^T\\)’s eigenvectors and \\(\\Lambda\\) is the diagonal matrix populated with \\(M M^T\\)’s eigenvalues, due to the property of real symmetric matrix.\nThen we define \\[ \\begin{align} (M M^T)^{1/2} \u0026amp;\\coloneqq U \\Lambda^{1/2} U^T \\\\ (M M^T)^{-1/2} \u0026amp;\\coloneqq U \\Lambda^{-1/2} U^T \\\\ \\end{align} \\] We take \\(E = (M M^T)^{-1/2} M\\). Now \\(E\\) will contain the orthonormal column vectors as expected. To verify, \\[ \\begin{aligned} \u0026amp;E E^T = \\left[ (M M^T)^{-1/2} M \\right] \\left[ M^T ((M M^T)^{-1/2})^T \\right] \\\\ \u0026amp;= \\left[ (M M^T)^{-1/2} \\right] \\left[ M M^T \\right] \\left[ ((M M^T)^{-1/2})^T \\right] \\\\ \u0026amp;= \\left[ U \\Lambda^{-1/2} U^T \\right] \\left[ U \\Lambda U^T \\right] \\left[ U \\Lambda^{-1/2} U^T \\right] \\\\ \u0026amp;= U \\Lambda^{-1/2} \\underbrace{\\left[ U^T U \\right]}_{I} \\Lambda \\underbrace{\\left[ U^T U \\right]}_{I} \\Lambda^{-1/2} U^T \\\\ \u0026amp;= U \\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} U^T = U U^T = I \\end{aligned} \\] Thus, the targeting covariance matrix can be constructed as \\(\\Sigma = E D E^T\\).\nSecond Method The easiest way to generate a legitimate covariance matrix would be to arbitrarily synthesize a square matrix \\(A\\) of dimension \\(n\\), and take \\(\\Sigma = A A^T\\) (see here).\nBy doing so, we can obtain a covariance matrix very fast. But you lose the control over it. Since \\(A\\) is totally arbitrary, you can tell little about \\(\\Sigma\\)’ s eigenvalues, eigenvectors, etc.\n","date":1660389801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660389801,"objectID":"5bf21f5bc7eafa0b686f7b485a307a68","permalink":"https://chunxy.github.io/blogs/generating-covariance-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/generating-covariance-matrix/","section":"blogs","summary":"Covariance matrix of a random vector can usually be deduced from the distribution’s property, or estimated from samples. But how to generate an arbitrary covariance matrix? How should we populate the entries in a square matrix so that it makes a legitimate covariance matrix?","tags":null,"title":"Generating Covariance Matrix","type":"blogs"},{"authors":null,"categories":null,"content":"本文为Elliptic Curve Cryptography系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。\nTable of Contents  加密算法分支 椭圆曲线与群  椭圆曲线 群（Group） 椭圆曲线上的群 标量积（Scalar Multiplication） 对数运算（Logarithm）   椭圆曲线与有限域  有限域（Finite Field） 有限域上的椭圆曲线 再看群 标量积与子群 离散对数运算（Discrete Logarithm）   椭圆曲线加密算法  Elliptic Curve Diffie-Hellman Elliptic Curve Digital Signature Algorithm 再看离散对数运算 ECC与RSA      加密算法分支  基于椭圆曲线\n基于椭圆曲线的加密算法包括ECC（Elliptic Curve Cryptography）、ECDH和ECDSA。ECDH与ECDSA是基于ECC发展而来。\n 基于模余运算\n基于模余运算的加密算法包括RSA、DSA、DH以及其他衍生算法。\n  椭圆曲线与群 椭圆曲线 一条椭圆曲线就是一组满足\\(y^2 = x^3 + ax + b\\)且\\(4a^3 + 27b^2 \\ne 0\\)的二维平面点集。\\(4a^3 + 27b^2 \\ne 0\\)的条件是为了保证曲线不存在奇点（singularity）；\\(y^2 = x^3 + ax + b\\)又被称作椭圆曲线的Weierstrass normal form。\n除了这条曲线上的点，我们还需要一个无穷远处的点，我们用\\(0\\)这个特殊的符号来表示这个点，所以椭圆曲线更准确的表达式为 \\[ \\{(x,y) \\in \\R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \\ne 0\\} \\cup \\{0\\} \\] 椭圆曲线的一条显而易见的性质是，它是关于\\(x\\)轴对称的。\n群（Group） 一个集合\\(G\\)加上一个二元运算\\(\\oplus\\)，若满足以下条件，就构成了数学上的一个群：\n 封闭性（closure）：\\(a \\in G, b \\in G \\to a \\oplus b \\in G\\)； 结合律（associativity）：\\((a + b) + c = a + (b + c)\\)； 存在一个单位元（identity element）\\(0\\)，使得\\(a + 0 = 0 + a = a\\)，即单位元与任何元素进行运算，不改变该元素的值； 每个数都存在一个逆元（inverse）。  若该群进一步满足\n 交换律（commutativity）：\\(a + b = b + a\\)。  则称该群为阿贝尔群（Abelian group）。\n椭圆曲线上的群 对于我们定义的椭圆曲线集合，我们\n 定义无穷远处的\\(0\\)为单位元；\n 定义逆元为该点关于\\(x\\)轴另一侧的对称点；\n 定义二元运算\\(\\oplus\\)如下：\n若一条直线与椭圆曲线的三个交点分别为\\(P,Q,R\\)，则\\(P \\oplus Q \\oplus R = 0\\)，我们称这三个点是对齐的（aligned）。在此处我们没有规定三个点之间的顺序，即三个点之间可以任意交换位置，也就是说我们的定义的二元运算是满足交换律的，我们定义的群是一个阿贝尔群。\n给定两个非零、非对称的点\\(P = (x_P, y_p), Q = (x_q, y_Q)\\)，我们可以很轻松地找到\\(R = P \\oplus Q\\): \\[ \\begin{align} x_R \u0026amp;= m^2 - x_P - x_Q \\\\ y_R \u0026amp;= y_P + m(x_R - x_P) \\\\ \u0026amp;= y_Q + m(x_R - x_Q) \\end{align} \\] 其中： \\[ m = \\begin{cases} \\frac{y_P - y_Q}{x_P - xQ}, \u0026amp; P \\ne Q \\\\ \\frac{3x_P^2 + a}{2y_P}, \u0026amp; P = Q \\end{cases} \\]\n  标量积（Scalar Multiplication） 给定之前的二元加法运算，我们可以定义出相应的群中元素与标量之间的乘法运算： \\[ n P = \\underbrace{P + \\dots + P}_{n \\text{ times}} \\] 这样的乘法运算可以在\\(\\Omicron(\\log n)\\)时间内完成。\n对数运算（Logarithm） 给定\\(n\\)和\\(P\\)，我们可以很高效地完成标量积运算\\(Q = nP\\)；但如果给定\\(Q\\)和\\(P\\)，我们如何计算出对数运算（虽然这里是除法，但是为了和密码学中的标记保持一致，这里使用了对数）\\(n = Q \\div P\\)呢？\n椭圆曲线与有限域 有限域（Finite Field） 有限域首先是一系列元素的集合，比如说由整数模余某个质数\\(p\\)得到的集合（通常表示为\\(\\Z/p\\)或\\(\\newcommand{F}{\\mathbb F} \\F_p\\)）；有限域还定义了两种二元运算：加法和乘法，且这两种运算应该满足如下条件：\n 在有限域上都是封闭的、满足结合律以及交换律的； 存在单位元； 每个元素都存在相应的逆元。  除此之外，乘法运算还应该满足分配律（distributive）：\\(x \\cdot (y + z) = x \\cdot y + x \\cdot z\\)。\n\\(\\F_p\\)包含了从\\(0\\)到\\(p-1\\)的所有整数，而加法、乘法操作之后要追加模余（除数为\\(p\\)）操作。\n 若\\(a + b = 0 \\pmod p\\)，则\\(a\\)，\\(b\\)互为加法逆元（additive inverse），\\(a=-b, b=-a\\)； 若\\(ab = 1 \\pmod o\\)，则\\(a\\)，\\(b\\)互为乘法逆元（multiplicative inverse），\\(a=b^{-1},b=a^{-1}\\)；\\(xy^{-1}\\)有时也表示为\\(x/y\\)；\\(n\\)的乘法逆元可以通过Extended Euclidean Algorithm，其时间复杂度为\\(\\Omicron(\\log n)\\)。  可以证明，\\(\\F_p\\)也是一个阿贝尔群。\n有限域上的椭圆曲线 椭圆曲线本身的定义为： \\[ \\{(x,y) \\in \\R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \\ne 0\\} \\cup \\{0\\} \\] 加上有限域的限制之后，变为 \\[ \\{(x,y) \\in \\F^2 | y^2 = x^3 + ax + b \\pmod p, 4a^3 + 27b^2 \\ne 0 \\pmod p, a, b \\in \\F_p \\} \\cup \\{0\\} \\] 由于有限域的限制，此时所有的点全部出现第一象限。该图像关于\\(y = p / 2\\)对称，因为若\\(y_1 + y_2 = p\\)， \\[ \\begin{aligned} y_1^2 \u0026amp;= (p - y_2)^2 \\\\ \u0026amp;= p^2 - 2py_2 + y_2^2 \\\\ \u0026amp;= y_2^2 \\pmod p \\end{aligned} \\]\n再看群  对于一个点\\(Q = (x_Q, y_Q)\\)，其逆元\\(-Q\\)定义为\\(-Q = (x_Q, -y_Q \\mod p)\\)；\n 我们这样定义有限域上椭圆曲线上的点之间的二元运算\\(\\oplus\\)，同之前一样，三个对齐的点（aligned points）\\(P,Q,R\\)满足 \\[ P \\oplus Q \\oplus R = 0 \\] 只不过这里“对齐”的含义与之前有所不同，之前的对齐指的是几何上的共线，即三个点满足\\(ax + by + c = 0\\)；而这里的对齐指的是： \\[ ax + by + c = 0 \\pmod p \\] 有趣的是，计算加法的公式和之前没有发生太大变化（证明）。给定两个非零、非对称的点\\(P = (x_P, y_p), Q = (x_q, y_Q)\\)，我们可以很轻松地找到\\(R = P \\oplus Q\\)： \\[ \\begin{align} x_R \u0026amp;= (m^2 - x_P - x_Q) \\mod p \\\\ y_R \u0026amp;= (y_P + m(x_R - x_P)) \\mod p \\\\ \u0026amp;= (y_Q + m(x_R - x_Q)) \\mod p \\end{align} \\]\n其中： \\[ m = \\begin{cases} (y_P - y_R)(x_P - x_R)^{-1} \\mod p, \u0026amp; P \\ne Q \\\\ (3x_P^2 + a)(2y_P)^{-1} \\mod p, \u0026amp; P = Q \\end{cases} \\]\n  群中元素的个数叫做群的秩（order），可以通过Schoof’s algorithm计算求得。\n标量积与子群 标量积依旧遵循之前的定义，给定正整数\\(n\\)和群中的点\\(P\\)， \\[ nP = \\underbrace{P + \\dots + P}_{n \\text{ times}} \\] 标量积其实就是对某个点\\(P\\)不断做加法，其中一个有趣的性质是，\\(0P, 1P, 2P, \\dots\\)的结果会以某个最小正周期周期\\(k\\)循环（证明）。这也就意味着，群中对加法\\(P\\)的倍数是关于加法封闭的（closed under addition），它们又构成了一个循环子群（cyclic subgroup），\\(P\\)又称作这个循环子群的基点（base point/generator），\\(k\\)是这个循环子群的秩（subgroup order）。\n根据Lagrange’s theorem，子群的秩是其父群的秩的约数。\n寻找基点 在ECC算法中，我们一般会先计算父群的秩\\(N\\)，找出它一个比较大的约数\\(n\\)，让\\(n\\)作为子群的秩，\\(h = N / n\\)称作这个子群的余因子（cofactor），再根据这个子群的秩去找这个子群的基点。一般来说，\\(n\\)会从\\(N\\)的质因子中选取，基本算法如下：\n 计算父群的秩\\(N\\)；\n 计算\\(N\\)的质因子\\(n\\)，从大到小排列进行试验：\n计算余因子\\(h = N / n\\)；\n 随机选择椭圆曲线上的一点\\(P\\)；\n 计算\\(G = hP\\)；\n 如果\\(G\\)为\\(0\\)，则重新选择\\(P\\)进行试验；否则这意味着\\(G\\)就是秩为\\(n\\)的子群的基点。\n   需要注意的是，ECC算法能够运行的前提是，\\(n\\)必须是\\(N\\)的质因子。\n离散对数运算（Discrete Logarithm） 现在我们解答之前提出的对数运算问题，给定\\(Q\\)和\\(P\\)，目前没有算法能够在多项式时间之内求解满足\\(Q = kP\\)的\\(k\\)。这个问题有点类似于给定整数\\(a\\)和\\(b\\)，如何求解满足\\(b = a^k \\pmod p\\)的\\(k\\)？这两个问题目前都没有算法能在多项式时间之内求解，这也是ECC算法安全的根本。\n椭圆曲线加密算法 寻找到之前秩为\\(n\\)、基点为\\(G\\)的子群后，我们就可以生成私钥和公钥了：\n 私钥是从\\(\\{1,\\dots,n-1\\}\\)中随机抽取的数字\\(d\\)； 公钥是点\\(H = dG\\)。  下面介绍两个基于ECC的公钥加密算法。\nElliptic Curve Diffie-Hellman ECDH是DH算法在椭圆曲线中的变体，它实际上是一种密钥交换算法，而不是加密算法。它的大致流程如下：\n Alice和Bob各自随机生成私钥和公钥：\\(H_A = d_A G, H_B = d_B G\\)，注意，Alice和Bob使用了相同的基点； Alice和Bob在非安全信道上交换各自的公钥，即使中间人拦截到 …","date":1655996601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655996601,"objectID":"921d4559d7269b7450dea97b0144cd64","permalink":"https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","section":"blogs","summary":"本文为Elliptic Curve Cryptography系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。\n","tags":null,"title":"椭圆曲线加密算法","type":"blogs"},{"authors":null,"categories":null,"content":" Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\]\nTo verify that it integrates to \\(1\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} p(x) \\d x \u0026amp;= \\sqrt{(\\int_{-\\infty}^{+\\infty} p(x) \\d x) \\cdot (\\int_{-\\infty}^{+\\infty} p(y) \\d y)} \\\\ \u0026amp;= \\sqrt {\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y} \\\\ \\end{aligned} \\]\nLet \\(I^2 = \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y\\), \\[ \\begin{gather} \\int_{-\\infty}^{+\\infty} p(x) \\d x = \\sqrt{I^2} \\\\ I^2 = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{y - \\mu}{\\sigma})^2} \\d x \\d y \\end{gather} \\] Let \\(u = \\frac{x-\\mu}{\\sigma}, v = \\frac{y-\\mu}{\\sigma}\\), \\[ \\begin{aligned} I^2 \u0026amp;= \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}u^2}\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}v^2} \\d u \\d v \\\\ \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(u^2 + v^2)} \\d u \\d v \\end{aligned} \\] Let \\(u = r \\sin \\theta, v = r \\cos \\theta\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(u^2+v^2)} \\d u \\d v \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} e^{-\\frac{1}{2} (r^2 \\sin^2\\theta + r^2 \\cos^2\\theta)} r \\d r \\d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} -e^{-\\frac{1}{2} r^2} \\d(-\\frac{1}{2} r^2) \\d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi}-e^{t}\\Big|_{t=0}^{t=-\\infty}d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi}d\\theta \\\\ \u0026amp;= 2\\pi \\end{aligned} \\] Therefore, \\(I^2 = \\frac{1}{2\\pi}2\\pi = 1\\), \\(\\int_{-\\infty}^{+\\infty}p(x)dx = \\sqrt{I^2} = 1\\)\nIndependent standard \\(n\\)-dimensional Let \\(Z = [Z_1, Z_2, ..., Z_n]^T\\), suppose \\(Z_i, Z_j (i,j=1,...,n \\and i \\ne j)\\) are independent and \\(Z_i (i=1,...,n)\\) observes standard Gaussian distribution, we can derive the joint distribution density function for random variable \\(Z\\) to be \\[ \\begin{aligned} \\newcommand{z}{\\mathrm{z}} p(\\z) \u0026amp;= p(z_1, z_2, ..., z_n) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z_i^2} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2}\\z^T\\z} \\\\ \\end{aligned} \\]\nFirst-order correlated \\(n\\)-dimensional We have given the joint distribution function of independent \\(n\\)-dimensional standard Gaussian distribution. What if \\(n\\) dimensions are not standard, are not independent with each other, but are correlated only in first order?\nFormally, \\(Z = [Z_1, Z_2, ..., Z_n]^T\\). Suppose \\(Z_i, Z_j (i,j=1,...,n \\and i \\ne j)\\) are not independent and \\(Z_i (i=1,...,n)\\) observes \\(N(\\mu_i, \\sigma_i^2)\\). We would like to linearly transform \\(Z\\) into \\(X\\) such that \\(X_i, X_j (i,j=1,...,n \\and i \\ne j)\\) are independent and \\(X_i (i=1,...,n)\\) observes standard Gaussian distribution.\nDenote \\(Z\\)’s covariance matrix as \\(\\Sigma_Z\\). \\(\\Sigma_Z\\) is real-symmetric and thus can be diagonalized into \\(U\\Lambda U^T\\). There is an invertible transformation matrix \\(B^{-1} = \\Lambda^{-\\frac{1}{2}} U^T\\) such that \\[ X = B^{-1}(Z - \\mu) \\sim \\mathcal{N}(0, I) \\] That is \\[ \\begin{aligned} p_X(\\x) \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} \\x^T\\x} \\end{aligned} \\]\nSuppose \\(Z\\) is to take on values in \\(\\mathcal{Z}\\), which is a subset of \\(\\R^{n}\\), \\[ \\newcommand{z}{\\mathrm{z}} P_Z(Z \\in \\mathcal{Z}) = \\int_\\mathcal{Z} p_Z(\\z) \\d \\z \\] \\(Z = f(X) = BX + \\mu\\). Since \\(B\\) is invertible, the mapping \\(X \\to Z\\) is one-to-one, therefore the multivariate Jacobian transformation: \\[ J(X \\to Z) = B^{-1} \\\\ \\] with its determinant \\(J = |J(X \\to Z)| = |B^{-1}| = |B|^{-1}\\). Note that \\[ \\begin{aligned} |J| \u0026amp;= \\sqrt{|B|^{-1}|B|^{-1}} \\\\ \u0026amp;= \\sqrt{|B|^{-1}|B^T|^{-1}} \\\\ \u0026amp;= \\sqrt{|BB^T|^{-1}} \\\\ \u0026amp;= |BB^T|^{-\\frac{1}{2}} \\end{aligned} \\] Therefore, \\[ \\begin{aligned} P_Z (Z \\in \\mathcal{Z}) \u0026amp;= P_X (X \\in f^{-1}(\\mathcal{Z})) \\\\ \u0026amp;= \\int_{f^{-1}(S)} p_X(\\x) \\d \\x \\\\ \u0026amp;= [\\int_\\mathcal{Z} p_X (f^{-1}(\\z)) |J| \\d \\z]_{\\x = f^{-1}(\\z)} \\\\ \u0026amp;= \\int_\\mathcal{Z} p_X (f^{-1}(\\z)) |J| \\d \\z \\\\ \\int_\\mathcal{Z} p_Z (\\z) \\d \\z \u0026amp;= \\int_\\mathcal{Z} p_X (f^{-1}(\\z)) |J| \\d \\z \\\\ p_Z (\\z) \u0026amp;= p_X (f^{-1}(\\z))|J| = p_X (B^{-1} (\\z - \\mu) |J| \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T(B^{-1})^T B^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T (B^T)^{-1} B^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T (BB^T)^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}|BB^T|^\\frac{1}{2}} e^{-\\frac{1}{2} (\\z-\\mu)^T (BB^T)^{-1} (\\z-\\mu)} \\\\ \\end{aligned} \\]\nAlso note that \\[ \\begin{aligned} \\Sigma_Z \u0026amp;= E[(\\z-\\mu) (\\z-\\mu)^T] \\\\ \u0026amp;= E[B X X^T B^T] \\\\ \u0026amp;= B E[X X^T] B^T \\\\ \u0026amp;= B I B^T \\\\ \u0026amp;= B B^T \\end{aligned} \\]\nThen, \\[ p_Z(\\z) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma_Z|}} e^{-\\frac{1}{2} (\\z-\\mu)^T …","date":1652036982,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652036982,"objectID":"6fb6c01d5200030f5c4ab1f8ad913498","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/","section":"notes","summary":"Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\]\nTo verify that it integrates to \\(1\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} p(x) \\d x \u0026= \\sqrt{(\\int_{-\\infty}^{+\\infty} p(x) \\d x) \\cdot (\\int_{-\\infty}^{+\\infty} p(y) \\d y)} \\\\ \u0026= \\sqrt {\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y} \\\\ \\end{aligned} \\]","tags":null,"title":"Gaussian Distribution","type":"book"},{"authors":null,"categories":null,"content":" The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy of continuous distribution \\(q\\) (probability density function) is usually similarly defined as \\[ H(q) = -\\E_{x\\sim q}\\log q(x) = -\\int q(x)\\log q(x)\\d x \\] This is actually the differential entropy introduced by Shannon. In fact, it is not a good continuous analog of discrete entropy and it was not rigorously derived. For example, this formula can be negative. Therefore, in the case of entropy, the random variable had better be discrete, despite the wide usage of differential entropy.\nGaussian Case The entropy of a \\(n\\)-dimensional Gaussian distribution \\(p(x) = \\frac{e^{-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1}(x-\\mu)}}{\\sqrt{|2\\pi\\Sigma|}}\\) can be derived as follows: \\[ \\begin{aligned} H(p) \u0026amp;\\triangleq -\\int p(x) \\log p(x) \\d x = -\\int p(x) [-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1} (x-\\mu) - \\frac 1 2 \\log |2\\pi\\Sigma|] \\d x \\\\ \u0026amp;= \\frac 1 2 \\int p(x) (x-\\mu)^T \\Sigma^{-1}(x-\\mu) \\d x + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= \\frac 1 2 \\int p(x) x^T \\Sigma^{-1} x \\d x + \\frac 1 2 \\int p(x) \\mu^T \\Sigma^{-1} \\mu \\d x \\\\ \u0026amp;\\quad - \\frac 1 2 \\int p(x) \\mu^T \\Sigma^{-1} x \\d x - \\frac 1 2 \\int p(x) x^T \\Sigma^{-1} \\mu \\d x \\\\ \u0026amp;\\quad + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= [\\frac 1 2 \\tr(\\Sigma^{-1} \\Sigma) + \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu] + \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu \\\\ \u0026amp;\\quad - \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu - \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu \\\\ \u0026amp;\\quad + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= \\frac 1 2 n + \\frac 1 2 \\log |2\\pi\\Sigma| \\end{aligned} \\]\n","date":1651186036,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651186036,"objectID":"79348391dc77fdceb73b7108eee60233","permalink":"https://chunxy.github.io/notes/articles/information-theory/entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/entropy/","section":"notes","summary":"The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy of continuous distribution \\(q\\) (probability density function) is usually similarly defined as \\[ H(q) = -\\E_{x\\sim q}\\log q(x) = -\\int q(x)\\log q(x)\\d x \\] This is actually the differential entropy introduced by Shannon.","tags":null,"title":"Entropy","type":"book"},{"authors":null,"categories":null,"content":" Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026amp;\\inf f_0(x) \\\\ s.t.\\quad \u0026amp; f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp; h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\). Then the Lagrangian function is \\(L:\\R^n \\times \\R^r \\times \\R^s \\mapsto \\R\\) (there is an implicit constraint that the variables must reside in the natural domain of the functions):\n\\[ L(x, \\lambda, \\mu) = f_0(x) + \\sum_{i=1}^r\\lambda_if_i(x) + \\sum_{i=1}^s\\mu_ih_i(x) \\] Define the function \\(P: \\mathcal X \\mapsto \\R\\) as \\[ P(x) = \\sup\\limits_{\\lambda,\\mu;\\lambda_i \\ge 0}L(x,\\lambda,\\mu) \\] The primal problem is \\[ \\inf_{x \\in \\mathcal X} P(x) \\label{primal} \\] It is easy to have \\(P(x) =f_0(x)\\) on \\(\\mathcal X\\). Thus the primal problem is equivalent to the original problem. Denote primal problem \\(\\eqref{primal}\\)’s optimal value as \\(p^\\star\\).\nDefine the Lagrangian dual function \\(D: \\R^r \\times \\R^s \\mapsto \\R\\) as \\[ D(\\lambda, \\mu) = \\inf_{x \\in \\mathcal \\R^n}L(x, \\lambda, \\mu) \\] The Lagrangian dual problem is \\[ \\sup_{\\lambda,\\mu;\\lambda_i \\ge 0} D(\\lambda, \\mu) \\label{dual} \\] Denote dual problem \\(\\eqref{dual}\\)’s optimal value as \\(d^\\star\\). We always have \\(p^\\star \\ge d^\\star\\).\n Proof\nFor any feasible \\(x \\in \\mathcal X, (\\lambda, \\mu) \\in {\\R^r}^+ \\times \\R^s\\), \\[ P(x) = \\sup\\limits_{\\lambda\u0026#39;,\\mu\u0026#39;;\\lambda\u0026#39;_i \\ge 0}L(x,\\lambda\u0026#39;,\\mu\u0026#39;) \\ge L(x,\\lambda,\\mu) \\ge \\inf_{x\u0026#39; \\in \\R^n}L(x\u0026#39;, \\lambda, \\mu) =D (\\lambda, \\mu) \\] Therefore \\(p^\\star \\ge d^\\star\\).\n  Strong Duality \\(p^\\star \\ge d^\\star\\) is called weak duality because it always holds. \\(p^\\star = d^\\star\\) is called strong duality because it does not hold in general. Assume, though, a strong duality holds, let \\(x^\\star\\) be the primal optima, \\((\\lambda^\\star, \\mu^\\star)\\) be the dual optima, then\n\\[ f_0(x^\\star) = P(x^\\star) = D(\\lambda^\\star, \\mu^\\star) \\\\ \\]\n Proof \\[ \\begin{gathered} f_0(x^\\star) = p^\\star = d^\\star = D(\\lambda^\\star, \\mu^\\star) = \\inf_{x \\in \\R^n}L(x,\\lambda^\\star,\\mu^\\star) \\le L(x^\\star,\\lambda^\\star,\\mu^\\star) \\\\ f_0(x^\\star) = p^\\star = P(x^\\star) = \\sup\\limits_{\\lambda,\\mu;\\lambda_i \\ge 0}L(x^\\star,\\lambda,\\mu) \\ge L(x^\\star,\\lambda^\\star,\\mu^\\star) \\\\ L(x^\\star,\\lambda^\\star,\\mu^\\star) \\le f_0(x^\\star) \\le L(x^\\star,\\lambda^\\star,\\mu^\\star) \\end{gathered} \\]\nTherefore, \\[ f_0(x^\\star) = P(x^\\star) = D(\\lambda^\\star, \\mu^\\star) = L(x^\\star,\\lambda^\\star,\\mu^\\star) \\]\n  If the strong duality holds and \\((x^\\star,\\lambda^\\star,\\mu^\\star)\\) is the optima, they must satisfy the KKT conditions, and we can leverage the KKT conditions to solve the optima and optimal value:\nKarush-Kuhn-Tucker Conditions In standard optimization problem, KKT conditions refer to the below four:\n primal constraint: \\(f_1(x) \\dots,f_r(x) \\le 0, h_1(x),\\dots,h_s(x) = 0\\)\n dual constraint: \\(\\lambda_1,\\dots,\\lambda_r \\ge 0\\)\n complementary slackness: \\(\\lambda_1f_1(x),\\dots,\\lambda_rf_r(x) = 0\\)\n vanishing gradient of Lagrangian w.r.t. \\(x\\) : \\[ \\nabla f_0(x) + \\sum_{i=1}^r\\lambda_i\\nabla f_i(x) + \\sum_{i=1}^s\\mu_i\\nabla h_i(x) = 0 \\]\n  Note that solutions satisfying KKT conditions do not imply a strong duality or an optimal point. For a better discussion between strong duality and KKT conditions, please go to this discussion.\nWhen-to-apply Slater Condition Strong duality does not hold generally. But it does hold in standard convex optimization problem. In such case, KKT conditions are also sufficient for strong duality provided that \\(x^\\star\\) is an interior point of the feasible region.\nGeneral Case In cases where we cannot tell strong duality directly, we may still try to apply Lagrangian multiplier to convert the primal problem to the less-constrained dual problem (\\(\\lambda \\ge 0\\) is much looser than the constraints in the original problem). That is, we solve \\(d^\\star\\) first, and then check if \\(d^\\star = p^\\star\\). We do so with the following process:\n firstly take derivative of \\(L\\) w.r.t. the unconstrained \\(x\\) and make it zero (vanishing gradient) to obtain the closed-form expression of \\(D(\\lambda, \\mu)\\), find \\(\\lambda^\\star \\ge 0\\) (dual constraint) and \\(\\mu^\\star\\) that maximizes the \\(D(\\lambda, \\mu)\\) and solve \\(x^\\star\\) w.r.t. \\(\\lambda^\\star\\) and \\(\\mu^\\star\\), finally verify that \\(x^\\star\\) satisfies the constraints (primal constraint and the implied complementary slackness) in the original problem and \\(f_0(x^\\star) = d^\\star\\) (strong duality).  If we can successfully go through the above process, we can still solve the problem with Lagrangian multiplier.\n","date":1641560406,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560406,"objectID":"bb7039f27464630ccfa027fb1b9a7f87","permalink":"https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/lagrange-multiplier/","section":"notes","summary":"Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026\\inf f_0(x) \\\\ s.t.\\quad \u0026 f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026 h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).","tags":null,"title":"Lagrange Multiplier","type":"book"},{"authors":null,"categories":null,"content":" Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\).\nGiven a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define\n\\[ \\begin{aligned} \\newcommand{\\v}{\\mathrm{v}} \\v_1 \u0026amp;= \\x_1 \\\\ \\v_2 \u0026amp;= \\x_2 - \\frac{\\x_2 \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 \\\\ \\v_3 \u0026amp;= \\x_3 - \\frac{\\x_3 \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 - \\frac{\\x_3 \\cdot \\v_2}{\\v_2 \\cdot \\v_2} \\v_2 \\\\ \u0026amp;\\vdots \\\\ \\v_p \u0026amp;= \\x_p - \\frac{\\x_p \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 - \\frac{\\x_p \\cdot \\v_{p-1}}{\\v_{p-1} \\cdot \\v_{p-1}} \\v_{p-1} \\end{aligned} \\] Then \\(\\{ \\v_1, \\dots, \\v_p \\}\\) is an orthogonal basis for \\(W\\). In addition, \\[ \\newcommand{\\span}[1]{\\mathrm{Span}\\{#1\\}} \\span{\\v_1, \\dots,\\v_k} = \\span{\\x_1, \\dots, \\x_k} \\text{\\quad for $1 \\le k \\le p$} \\]\nQR Factorization If \\(A\\) is an \\(m \\times n\\) matrix with linearly independent columns, then \\(A\\) can be factored as \\(A = QR\\), where \\(Q\\) is an \\(m \\times n\\) matrix whose columns form an orthonormal basis for \\(\\mathop{\\mathrm{Col}}A\\) and \\(R\\) is an \\(n \\times n\\) upper triangular invertible matrix with positive entries on its diagonal.\n","date":1641560025,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560025,"objectID":"8d5e4410527b7138a117b40c7e3e0044","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/","section":"notes","summary":"Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\).\nGiven a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define","tags":null,"title":"Gram-Schmidt Orthogonalization","type":"book"},{"authors":null,"categories":null,"content":" Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\). By best approximation we mean to minimize the \\(||Ax - b||\\) over all \\(x \\in \\R^n\\).\nThe best approximation can be achieved when \\(Ax = \\hat b = \\mathop{proj}_{Col(A)}b\\).\nInstead of finding a orthogonal basis for \\(A\\), computing \\(\\hat b\\) and then solving \\(Ax = \\hat b\\), we can derive \\(x\\) in this way: \\[ \\begin{gather} (b - \\hat b) \\perp Col(A) \\iff (b - \\hat b) \\in Nul(A^T)\\iff A^T(b - \\hat b) = 0 \\\\ A^T(b - Ax) = 0 \\\\ A^TAx = A^Tb \\label{solution} \\end{gather} \\]\nWe will show that if columns of \\(A\\) are independent, then the least-square solution \\(\\hat x\\) is uniquely given by \\((A^TA)^{-1}A^Tb\\)\nFirstly, \\(Nul(A) = Nul(A^TA)\\). This is because \\[ \\begin{gather} Ax = 0 \\Rightarrow A^TAx = A^T0 = 0 \\\\ A^TAx = 0 \\iff x^TA^TAx = 0 \\iff (Ax)^TAx = 0 \\Rightarrow Ax = 0 \\end{gather} \\] When columns of \\(A\\) are independent, \\(Nul(A) = 0\\) so that \\(Nul(A^TA) = 0\\), which indicates that equation \\(\\eqref{solution}\\) has the unique solution. Conversely, as an aside, when \\(A^TA\\) is invertible, \\(Nul(A^TA) = 0\\) so that \\(Nul(A) = 0\\), which indicates that columns of \\(A\\) are independent.\n","date":1641560025,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560025,"objectID":"53c0e3d273d6c8d7d6004f30e0a2c177","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/least-squares/","section":"notes","summary":"Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\).","tags":null,"title":"Least Squares","type":"book"},{"authors":null,"categories":null,"content":" Eigenvectors and Eigenvalues Definition An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\). This scalar \\(\\lambda\\) is the corresponding eigenvalue. Note that by definition, \\((\\lambda, \\vec 0)\\) is a pair of eigenvalue and eigenvector of any square matrix. However, \\(\\vec 0\\) is just too trivial an eigenvector that people exclude it from the eigen discussion.\nNote, though, \\(0\\) can be an eigenvalue. Also note that if \\((\\lambda, v)\\) is a pair of eigen of matrix \\(A\\), then \\((\\lambda, kv)\\) is also a pair of eigen.\nSimilarity If \\(A\\) and \\(B\\) are \\(n \\times n\\) matrices, then \\(A\\) is similar to \\(B\\) if there is an invertible matrix \\(P\\) such that \\(PAP^{-1} = B\\), or equivalently \\(P^{-1}BP = A\\).\nIf \\(A\\) and \\(B\\) are similar, then they have the same characteristic polynomial and hence then the same eigenvalues: \\[ B - \\lambda I = PAP^{-1} - \\lambda PP^{-1} = P(A - \\lambda I)P^{-1} \\\\ \\]\n\\[ \\begin{aligned} \u0026amp;\\det(B - \\lambda I) = \\det(P(A - \\lambda I)P^{-1}) \\\\ \u0026amp;= \\det(P) \\cdot \\det(A - \\lambda I) \\cdot \\det(P^{-1}) \\\\ \u0026amp;= \\det(A - \\lambda I) \\end{aligned} \\]\nIndependence between Eigenvectors  Theorem\nSuppose \\(v_1, v_2, ... , v_r\\) are the eigenvectors corresponding to the distinct eigenvalues \\(\\lambda_1, \\lambda_2, ..., \\lambda_r\\) of an \\(n \\times n\\) matrix \\(A\\) (\\(v_1, v_2, ... , v_r\\) are also called eigenvectors from different eigenspaces), then \\(v_1, v_2, ..., v_r\\) are linearly independent.\n Proof\nSuppose instead these vectors are dependent. Let \\(p\\) be the least index such that \\(v_{p+1}\\) is a linear combination of previous vectors. Then there exists scalars \\(c_1, c_2, ..., c_p\\) such that \\[ c_1v_1 + c_2v_2 + \\cdots + c_pv_p = v_{p+1} \\label{lincom} \\] Multiply both sides with \\(A\\) to obtain \\[ c_1\\lambda_1v_1 + c_2\\lambda_2v_2 + \\cdots + c_p\\lambda_pv_p = \\lambda_{p+1}v_{p+1} \\label{eq1} \\] Multiply both sides of \\(\\eqref{lincom}\\) with \\(\\lambda_{p+1}\\) to give \\[ c_1\\lambda_{p+1}v_1 + c_2\\lambda_{p+1}v_2 + \\cdots + c_p\\lambda_{p+1}v_p = \\lambda_{p+1}v_{p+1} \\label{eq2} \\] Subtract \\(\\eqref{eq2}\\) from \\(\\eqref{eq1}\\) to give \\[ c_1(\\lambda_1 - \\lambda_{p+1})v_1 + c_2(\\lambda_2 - \\lambda_{p+1})v_2 + \\cdots + c_p(\\lambda_p - \\lambda_{p+1})v_p = 0 \\label{diff} \\] Since \\(v_1, v_2, ..., v_p\\) are independent, the weights in \\(\\eqref{diff}\\) are all zeros. None of \\((\\lambda_i - \\lambda_{p+1})\\) is zero, so \\(c_i = 0\\) for all \\(i = 1,...p\\). Then \\(\\eqref{lincom}\\) says \\(v_{p+1}\\) is \\(0\\), which is impossible.\n Note\n\\(v_1\\), as the eigenvector, is nonzero so that the conclusion can hold even for \\(p=1\\).\n  Diagonalization  Theorem\nAn \\(n \\times n\\) matrix \\(A\\) is diagonalizable if and only if \\(A\\) has \\(n\\) independent eigenvectors.\n Proof\n Necessity\nSuppose \\(A = PDP^{-1}\\), then \\(AP = PD\\). Let \\(\\newcommand{\\p}{\\mathrm{p}} P = [\\p_1, \\p_2, ..., \\p_n]\\). Since \\(D\\) is a diagonal matrix, \\[ AP = [A\\p_1, A\\p_2, ..., A\\p_n] = [D_{11} \\p_1, D_{22} \\p_2, ..., D_{nn} \\p_n] \\] Because \\(P\\) is invertible, \\(\\p_i\\)’s are independent, which indicates that \\(\\mathrm p_i\\)’s are \\(A\\)’s \\(n\\) independent eigenvectors.\nFrom this, we could also see that if \\(A\\) is diagonalizable, then \\(P\\) must be the matrix concatenated with \\(A\\)’s eigenvectors and \\(D\\) must be the diagonal matrix filled with corresponding eigenvalues.\n Sufficiency\nEasy to show.\n   Eigenvalues: Rank, Trace and Determinant Since the characteristic equation of an \\(n \\times n\\) matrix involves a polynomial of degree \\(n\\), the equation always has exactly \\(n\\) roots, counting multiplicities, including complex roots. There are some relations between eigenvalues and matrix’s rank, trace and determinant: \\[ \\begin{gather} \\rank = \\text{the number of (non-zero) real eigenvalues, including multiplicities} \\\\ \\tr = \\text{sum of the eigenvalues} \\\\ \\det = \\text{product of the eigenvalues} \\end{gather} \\]\nExternal Materials Eigen Decomposition.pdf\n","date":1640462913,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640462913,"objectID":"afba1d485102ac3a34eb2677daabbc0f","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/","section":"notes","summary":"Eigenvectors and Eigenvalues Definition An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\).","tags":null,"title":"Eigenvectors and Eigenvalues","type":"book"},{"authors":null,"categories":null,"content":" This post lists out various topics under the machine learning subject.\nData  data modalities  numbers texts images videos audios  data sampling  bootstrap aggregation  data cleaning imbalanced data data augmentation data splitting feature extraction/engineering  domain expertise statistics  feature normalization  Model  supervised vs. unsupervised  unsupervised learning  discovers inherent properties (latent variables) in the data includes:  clustering dimension reduction manifold embedding anomaly detection    discriminative vs. generative classification vs. regression  from classification model to regression model from binary classification to multi-class classification  linear vs. non-linear parametric vs. non-parametric boosting method  ensemble method Adaboost  regularization and overfitting  Evaluation  training criterion\n mean squared error cross entropy impurity  Gini index entropy   testing criterion\n confusion matrix\n  Real/Pred Positive Negative    Positive TP FN  Negative FP TN     accuracy precision recall F-score    ","date":1639922389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639922389,"objectID":"d49e44baf2be0a6818a6dae89804b186","permalink":"https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/machine-learning-bullet-points/","section":"notes","summary":"This post lists out various topics under the machine learning subject.\nData  data modalities  numbers texts images videos audios  data sampling  bootstrap aggregation  data cleaning imbalanced data data augmentation data splitting feature extraction/engineering  domain expertise statistics  feature normalization  Model  supervised vs.","tags":null,"title":"Machine Learning Bullet Points","type":"book"},{"authors":null,"categories":null,"content":"其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。\n当然，减少严谨的符号运算，并不意味着完全不出现符号，因为算法本身就是对问题的抽象，剥掉这层抽象，就没办法进行架构在抽象之上的信息传递了。\nTable of Contents  问题描述及定义 解决思路 正文之前  负权边 负权环路 非负权环路 放缩操作   解决方法  Bellman-Ford算法 Dijkstra算法 有向无环图中的最短路径 对比      问题描述及定义 单源最短路径问题，旨在求解带权有向图（weighted directed graph）中1，从某个点（vertex）出发，到图中任意一点的最短距离，某些情况下，还需要找出这一条最短距离的路径，称之为最短路径，若无特殊指明且不致歧义，以下最短路径问题均指代单源最短路径问题。\n更严格一些，设\\(G(V, E)\\)表示带权有向图，\\(w : E \\to \\mathbb{R}\\)表示权重，路径\\(p = \\left \u0026lt;v_0, v_1, ... v_k\\right \u0026gt;\\)的距离定义为： \\[ W(p) = \\sum\\limits_{i = 1}^k w(v_{k-1}, v_k) \\] 其中， \\[ \\begin{gather*} \\forall i \\in [0, k], v_i \\in V \\\\ \\forall i \\in [1, k], (v_{i-1}, v_i) \\in E \\end{gather*} \\] 我们从某一点\\(s\\)（叫作起点，或者源点）出发，记其到图中任意一点\\(v\\)的最短路径距离为\\(\\delta(v)\\)，单源最短路径求解的就是任意一条从\\(s\\)到\\(v\\)的路径\\(p\\)，使得\\(W(p) = \\delta(v)\\)。\n为了方便，我们为每一个点\\(v \\in V\\)设立一个中间变量\\(d\\)，用\\(v.d\\)来表示求解过程中的最短距离的可行上界，也就是说始终有\\(\\delta(v) \\leq v.d\\)，算法初始化时，\\(v.d = +\\infty\\)，算法运行过程中，我们通过寻找路径使\\(v.d\\)这个上界不断减小，直到\\(v.d = \\delta(v)\\)。\n解决思路 最短路径问题（包括多源最短路径问题）都隐含着一个最优子结构（optimal substructure），即：\n 如果\\(p\\)是一条连接两个点的最短路径，那么\\(p\\)的任意一条子路径，一定也是连接其两个端点的最短路径。\n 这条性质可由反证法轻松得到，也将是后续寻找最短路径所需要理解的一个重要概念。\n正文之前 负权边 负权边指的是图中某些边的权重为负。虽然负权边不会对最短路径的最优子结构性质产生任何影响，但是后面我们会看到，负权边会导致Dijkstra算法失效。\n负权环路 负权环路指的是图中某些边构成一条环路（loop），并且这条环路上的所有边的权重相加结果为负。\n一旦从起点可以到达这个环路上的点，那么最短路径问题就变得没有意义了：我们可以不断重复地走这条环路，然后 “拐出” 环路，到达目标点，使得到达目标点的路径的权重变得任意小（arbitrarily short），所以也就不存在什么 “最短路径” 了。\n一个成熟的算法应当能够检测出图中是否有可以由\\(s\\)到达的负权环路，如果没有，则算法照常进行；如果有，应予以通报。\n非负权环路 非负权环路指的是图中某些边构成一条环路，并且这条环路上的所有边的权重相加的结果大于等于0。\n负权环路会使最短路径问题没有意义，那么非负权环路呢？或者说，最短路径是否包含非负权环路呢？\n答案是否，如果一条最短路径包含了非负权环路，我们大可将这段环路从路径中 “拿掉”，得到的路径和原路径可以达到同样的终点，并且新路径的权重不大于原路径的权重。\n放缩操作 放缩操作的对象是边，对于边\\((u,v)\\)，放缩操作将检测能否优化点\\(v\\)的上界：\nRELAX(u, v, w): if v.d \u0026gt; u.d + w(u, v): v.d = u.d + w(u, v) v.predecessor = u 即如果路径\\(s \\sim u \\to v\\)的长度小于当前\\(v\\)的上界，我们便可以借此优化\\(v\\)的上界，并同时通过将的\\(v\\)前继设为\\(u\\)来记录这一次优化。\n解决方法 Bellman-Ford算法 Bellman-Ford算法是最短路径问题中最为robust的一种了，能处理负权边、能检测负权环路、不要求当前图为有向无环图（directed acyclic graph）。Bellman-Ford算法基本框架如下：\n  原图中存在可由起点抵达的负权环路，返回 false，用以告知存在负权环路，最短路径问题无意义 原图中不存在可由起点抵达的负权环路，返回 true，用以告知最短路径问题已解决，并将结果蕴含在相应的数据结构中   // 算法主体 for i = 1 to |V| - 1 for each edge (u, v) in E RELAX(u, v, w) // 检测是否存在负权回路 for each edge (u, v) in E if v.d \u0026gt; u.d + w(u, v) return false return true  算法主体\n我们不妨先假设原图中不存在负权环路，先思考Bellman-Ford在解决最短路径问题时的正确性。\n根据以上的讨论，任意一点的最短路径中不存在环，故任意一点\\(t\\)的最短路径最多由\\(|V|-1\\)条边，\\(|V|\\)个点构成。设： \\[ p=\\left \u0026lt;v_0,v_1, ... v_k\\right \u0026gt;，其中v_0 = s，v_k = t \\] 在寻找\\(t\\)的最短路径时，任一\\(v \\in \\{u | (u,t) \\in E, u.d = \\delta(s, u)\\}\\)（即此时\\(v\\)的最短路径已找到，且点\\(v\\)有一条连向点\\(t\\)的边） ，都是\\(v_{k-1}\\)（也就是\\(t\\)在其最短路径中的前继）的一个候选，我们需要证明的是，\\(t\\)的实际前继能够在Bellman-Ford算法运行之下被发现，从而被真正地选为\\(t\\)的前继。\n根据前面的讨论，路径\\(p\\)的前缀\\(\\left \u0026lt;v_0, v_1\\right \u0026gt;, \\left \u0026lt; v_0, v_1, v_2\\right \u0026gt;...\\)分别是\\(v_1, v_2, ...\\)的最短路径，在外侧第一轮for-loop后，边\\((v_0, v_1)\\)一定会被放缩，而由于\\(\\left \u0026lt;v_0, v_1\\right\u0026gt;\\)实际是最短路径，故放缩之后，\\(v_1.d = \\delta(v_1)\\)且将不再变化（因为这已经是最小）；在外侧第二轮for-loop后，边\\((v_1, v_2)\\)一定会被放缩，而由于\\(\\left \u0026lt;v_0, v_1, v_2\\right\u0026gt;\\)实际是最短路径，故放缩之后，\\(v_2.d = \\delta(v_2)\\)且将不再变化（因为这已经是最小）\\(\\dots\\)如此放缩\\(k\\)轮后，我们便寻找到了\\(v_1, ... v_k\\)一众节点的最短路径以及其最短路径的前继。\n 负权回路检测\n至于负权回路检测部分的正确性，则不得不引入一些公式，但其实并不复杂。\n假设原图存在可由到达的负权回路\\(c = \\left \u0026lt;v_0, v_1, ... v_k\\right \u0026gt;, v_0 = v_k\\)，其中，\\(W(c) = \\sum_{i=0}^{k-1}w(v_i, v_{i+1}) \u0026lt; 0\\)。运用反证法，即假设最终不存在点\\(u,v\\)，使得\\(v.d \u0026gt; u.d + w(u, v)\\)，则有： \\[ \\begin{aligned} v_i.d \u0026amp;\\leq v_{i-1}.d + w(v_{i-1}, v_i), \\forall i \\in [1, k] \\Rightarrow \\\\ \\sum_{i=1}^k v_i.d \u0026amp;\\leq \\sum_{i=1}^k (v_{i-1}.d + w(v_{i-1},v_i)) \\\\ v_k.d + \\sum_{i=1}^{k-1} v_i.d \u0026amp;\\leq v_0.d + \\sum_{i=1}^{k-1} v_{i}.d + \\sum_{i=1}^k w(v_{i-1},v_i) \\\\ 0 \u0026amp;\\leq \\sum_{i=1}^k w(v_{i-1},v_i) = W(c) \\end{aligned} \\] 与\\(W(c) \u0026lt; 0\\)矛盾，故得证。\n  Dijkstra算法 Dijkstra算法相对于Bellman-Ford算法来说，可以在时间复杂度上有所优化，但是能够处理的情形也就少了一些：Dijkstra算法不能处理负权边（所以更不用提负权环路了）。Dijkstra算法基本框架如下：\n  维持一个点集\\(S\\)，点集\\(S\\)由最短路径已确定的点构成； 不断向中加入能够确定最短路径的点，直到所有中的点都被加入。   S = {} Q = G.V while Q is not empty u = EXTRACT-MIN(Q) add u to S for each v in G.adj[u] RELAX(u, v, w) 当然，难点在于如何根据\\(S\\)找出能够确定最短路径的点。寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集\\(S\\)，正是一个最短路径已经确定的点的集合。\n\\(\\forall u \\in S\\)且\\((u, t) \\in E\\)，对\\((u, t)\\)进行放缩后得到的值\\(t.d\\)，都是\\(\\delta(t)\\)的一个备选，\\(S\\)中每加入一个点\\(v\\)（非\\(t\\)的点），若边\\((v,t)\\)存在，对该边进行放缩后，\\(\\delta(t)\\)的备选（也就是放缩后的\\(t.d\\)）就会多一个，而\\(\\delta(t)\\)自然是这些备选中最小的那个。而当\\(t\\)的最短路径确定后，便可以将\\(t\\)加入到点集\\(S\\)中，\\(S\\)不断扩展，直至最终包含整个点集\\(V\\)，也就是所有点的最短路径都被找到。\n之前提到过，Dijkstra算法不能处理负权边的情况，但上述 Dijkstra算法的讨论中似乎也没有涉及到负权边，为什么它就不能处理了呢？并且，我们只知道放缩后\\(t.d\\)的是\\(\\delta(t)\\)的备选，那么对\\(t.d\\)的放缩要进行到什么时候，才能确认\\(t.d=\\delta(t)\\)呢？Dijkstra算法告诉我们，\\(\\forall u \\in V - S\\)，有\\(t.d \\leq u.d\\)时，\\(\\delta(t) = t.d\\)，也就是当\\(t\\)的上界小于所有待确认点\\((V-S)\\)的上界时，我们就能确定\\(t\\)的最短路径，也就能够将点\\(t\\)加入到\\(S\\)中。\n为什么？如果没有负权边，我们可以会发现，\\(\\forall u \\in V - S\\)，其上界\\(u.d\\)总是由放缩操作得到的，所以在算法运行过程中，它必然是单调递减的，而且它代表了一条具体的到达\\(u\\)的路径。但为什么\\(V - S\\)中的所有点的上界的最小值，却能够成为某个特定点\\(t\\)的最短路径呢？\n我们来看看\\(t\\)的上界成为最小上界的之前之后都发生了什么，换言之，在此之前，或者在此之后，\\(t.d\\)有没有可能更小？之前是不会更小了，因为我们说过，\\(t.d\\)是单调递减的；那么之后呢？如果在\\(t\\)的上界成为\\((V-S)\\)中的最小上界、从而被加入\\(S\\)之后，我们在新的某一轮中选取另外一个点\\(u \\in V - S\\)，作为此轮加 …","date":1624267992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624267992,"objectID":"b2158400d728ad52bd17a581d7d97410","permalink":"https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","section":"blogs","summary":"其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。\n","tags":null,"title":"单源最短路径问题","type":"blogs"},{"authors":null,"categories":null,"content":" Notations and Concepts An ensemble \\(X\\) is a triple \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(x\\) is the outcome of a random variable, which takes on values from \\(\\A_X = \\{a_1, a_1, \\dots \\}\\), that has probability \\(\\P_X = \\{p_1, p_2, \\dots \\}\\).\nThe raw bit content of \\(X\\) is \\[ H_0(X) = \\log |\\mathcal A_X| \\] The smallest \\(\\delta\\)-sufficient subset \\(S_\\delta\\) is the smallest subset of \\(\\mathcal A_X\\) satisfying \\[ P(X \\in S_\\delta) \\ge 1 - \\delta \\] The essential bit content of \\(X\\) is \\[ H_\\delta(X) = \\log |S_\\delta| \\]\nShannon’s source coding theorem  Let \\(X\\) be an ensemble with entropy \\(H(X) = H\\) bits, and \\(X^N\\) be the ensemble composed of \\(N\\) i.i.d. such random variables. Given \\(\\epsilon \u0026gt; 0\\) and \\(0 \u0026lt; \\delta \u0026lt; 1\\), there exists a positive integer \\(N_0\\) such that for \\(N \u0026gt; N_0\\), \\[ |\\frac{1}{N} H_\\delta (X^N) - H| \u0026lt; \\epsilon \\]\n Or put it in a verbal way,\n \\(N\\) i.i.d. random variables each with entropy \\(H(X) = H\\) can be compressed into more than \\(NH\\) bits with negligible information loss as \\(N \\to \\infty\\); conversely if they are compressed fewer than \\(NH\\) bits, it is virtually certain that there is information loss.\n  Proof: The random variable \\(\\frac 1 N \\log \\frac 1 {P(Y)}\\) is defined for the ensemble \\(Y = X^N\\) which composes of \\(N\\) i.i.d. \\(X_1 \\dots X_N\\). \\(\\frac 1 N \\log \\frac 1 {P(Y)}\\) can be re-written as the average of \\(N\\) information contents \\(h_i = \\log \\frac 1 {P(X_i)}\\): \\[ \\frac 1 N \\log \\frac 1 {P(Y)} = \\frac 1 N \\log \\frac 1 {P(X_1) \\dots P(X_N)} = \\frac 1 N (\\log \\frac{1}{P(X_1)} + \\dots + \\log \\frac{1}{P(X_N)}) \\] Each of these information contents is in turn a random variable with mean \\(\\bar h_i = H(X) = H\\) and variance \\(\\sigma_{h_i}^2 = \\sigma^2\\). The typical set with parameters \\(N\\) and \\(\\beta\\) is defined as \\[ T_{N \\beta} = \\{y \\in \\mathcal A_X^N: [\\frac 1 N \\log \\frac{1}{P(y)} - H]^2 \u0026lt; \\beta^2 \\} \\] By the Weak Law of Large Numbers, \\(P((\\frac 1 N \\log \\frac{1}{P(y)} - H)^2 \\ge \\beta^2) = \\frac{\\sigma^2}{\\beta^2 N}\\), and thus \\[ P(y \\in T_{N \\beta}) \\ge 1 - \\frac{\\sigma^2}{\\beta^2 N} \\] As \\(N\\) increases, the probability that \\(y\\) falls in \\(T_{N \\beta}\\) draws near to \\(1\\). We need to relate this to the theorem that for any given \\(\\epsilon, \\delta\\), there is a sufficiently-large \\(N\\) such that \\(H_\\delta(X^N) \\simeq NH\\).\n\\(H_\\delta(X^N) \u0026lt; N(H + \\epsilon)\\)\nThe set \\(T_{N \\beta}\\) is not the best sufficient subset for compression. Therefore, \\(\\log |T_{N \\beta}|\\) upper-bounds the \\(H_\\delta(X^N)\\). On the other hand, for all \\(y \\in T_{N \\beta}\\), \\(2^{-N(H - \\beta)} \u0026lt; P(y) \u0026lt; 2^{-N(H + \\beta)}\\), \\[ \\begin{align*} |T_{N \\beta}| 2^{-N(H + \\beta)} \u0026amp;\u0026lt; 1 \\\\ |T_{N \\beta}| \u0026amp;\u0026lt; 2^{N(H + \\beta)} \\\\ \\end{align*} \\] If we set \\(\\beta = \\epsilon\\) and \\(N = N_0\\) in a way such that \\(\\frac{\\sigma^2}{\\epsilon^2 N_0} \\le \\delta\\) and thus \\(P(y \\in T_{N_0 \\epsilon}) \\ge 1 - \\delta\\), \\(T_{N_0 \\epsilon}\\) is a \\(\\delta\\)-sufficient subset. Then, \\(H_\\delta(X^N) \\le \\log |T_{N_0 \\epsilon}| \\le N_0(H + \\epsilon)\\) holds for any \\(N \\ge N_0\\).\n \\(H_\\delta(X^N) \u0026gt; N(H - \\epsilon)\\)\nThis part is reached by contradiction. Suppose instead there exists a \\(\\delta\u0026#39;\\) such that there exists a sufficiently large \\(N_0\\) which results in \\(H_\\delta(X^{N_0}) \\le N_0(H - \\epsilon)\\) for arbitrary \\(\\beta\\). When \\(\\beta = \\epsilon/2\\), we have \\[ H_\\delta(X^N) \\le N_0(H - 2\\beta) \\] Denote the associated subset by \\(S\u0026#39;\\). We are to disprove \\(S\u0026#39;\\) with \\(|S\u0026#39;| \\le 2^{N(H - 2\\beta)}\\) can achieve \\(P(y \\in S\u0026#39;) \\ge 1 - \\delta\\). \\[ P(y \\in S\u0026#39;) = P(y \\in S\u0026#39; \\cap T_{N \\beta}) + P(y \\in S\u0026#39; \\cap \\overline{T_{N \\beta}}) \\] \\(|S\u0026#39; \\cap T_{N \\beta}| \\le |S\u0026#39;| \\le 2^{N(H - 2\\beta)}\\). The maximum value of the first term is obtained when \\(S\u0026#39; \\cap T_{N \\beta}\\) contains \\(2^{N(H - 2\\beta)}\\) outcomes all with probability \\(2^{-N(H - \\beta)}\\). \\(S\u0026#39; \\cap \\overline{T_{N \\beta}} \\subseteq \\overline{T_{N \\beta}}\\), \\(P(y \\in S\u0026#39; \\cap \\overline{T_{N \\beta}}) \\le P(y \\in \\overline{T_{N \\beta}}) \u0026lt; \\frac{\\sigma^2}{\\beta^2 N}\\). Therefore, \\[ P(y \\in S\u0026#39;) \\le 2^{N(H - 2\\beta)} 2^{-N(H - \\beta)} + \\frac{\\sigma^2}{\\beta^2 N} = 2^{-N \\beta} + \\frac{\\sigma^2}{\\beta^2 N} \\] For arbitrary \\(\\delta\\) and a sufficiently-large \\(N_0\\), we can have \\(P(y \\in S\u0026#39;) \\le 1 - \\delta\\) instead of \\(P(y \\in S\u0026#39;) \\ge 1 - \\delta\\). We shall conclude that \\(S\u0026#39;\\) with \\(|S\u0026#39;| \\le 2^{N(H - 2\\beta)}\\) cannot achieve \\(P(y \\in S\u0026#39;) \\ge 1 - \\delta\\) and thus \\(H_\\delta(X^N) \u0026gt; N(H - \\epsilon)\\).\n   Source coding theorem for symbol codes Kraft-McMillan inequality Denote the length of each symbol code as \\(l_i\\) and there are \\(I\\) symbols. If \\[ \\sum_{i=1}^I 2^{-l_i} \\le 1 \\] there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes’ lengths.\n Proof: The proof is done by construction. The number of codes of length \\(l\\) should be less than \\(2^{l+1}\\), or else the above condition will be violated. Therefore we can loosely …","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"fd99477a66067050df822acb9bb2f88b","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/","section":"notes","summary":"Notations and Concepts An ensemble \\(X\\) is a triple \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(x\\) is the outcome of a random variable, which takes on values from","tags":null,"title":"Source Coding Theory","type":"book"},{"authors":null,"categories":null,"content":" Three Perspectives on NCE Non-parametric estimation The traditional log-likelihood function will be \\(\\ell = \\sum_x \\ln p_\\theta(x)\\). In NCE, we learn \\[ p_\\theta(1|x) = \\sigma(G(x;\\theta) - \\gamma) = \\frac{1}{1 + e^{-G(x;\\theta) + \\gamma}} \\] And corresponding loss function becomes \\[ \\begin{aligned} \\mathcal {L} \u0026amp;= -\\E_{x \\sim \\tilde p(x)} \\ln p_\\theta(1|x) - \\E_{x \\sim q(x)} \\ln p_\\theta(0|x) \\\\ \u0026amp;= -\\int \\tilde p(x) \\ln p_\\theta(1|x) dx - \\int q(x) \\ln p_\\theta(0|x) dx \\\\ \u0026amp;= - \\int [\\tilde p(x) + q(x)] [\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(1|x) + \\frac{q(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(0|x)]dx \\end{aligned} \\]\nLet \\(P(y|x) = \\begin{cases}\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)}, y=1 \\\\\\frac{q(x)}{\\tilde p(x) + q(x)},y=0\\end{cases}\\), \\[ \\label{loss} \\begin{aligned} \\arg \\min_{\\theta, \\gamma} \\mathcal{L} \u0026amp;= \\arg \\min_{\\theta, \\gamma} -\\int [\\tilde p(x) + q(x)][\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(1|x) + \\int \\frac{q(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(0|x)]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} -\\int [\\tilde p(x) + q(x)][P(1|x) \\ln p_\\theta(1|x) + P(0|x)\\ln p_\\theta(0|x)]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} \\int [\\tilde p(x) + q(x)][P(1|x) \\ln \\frac{1}{p_\\theta(1|x)} + P(0|x)\\ln \\frac{1}{p_\\theta(0|x)}]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} \\int [\\tilde p(x) + q(x)] H[P(y|x)||p_\\theta(y|x)]dx \\end{aligned} \\]\nSince cross entropy \\(H(p||q) \\ge H(p)\\) and the minimum is reached only when \\(p = q\\), the global minimum for equation \\(\\eqref{loss}\\) is reached when \\(p_\\theta(y|x) = P(y|x)\\). Therefore, \\[ \\begin{aligned} p_\\theta(1|x) = \\frac{1}{1 + e^{-G(x;\\theta) + \\gamma}} \u0026amp;= \\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} = P(1|x) \\\\ \\frac{q(x)}{\\tilde p(x)} \u0026amp;= e^{-G(x;\\theta) + \\gamma} \\\\ \\tilde p(x) \u0026amp;= \\frac{q(x) e^{G(x;\\theta)}}{e^\\gamma} \\end{aligned} \\] \\(\\theta\\) and \\(\\gamma\\) are learnt so that \\(q(x) e^{G(x;\\theta) - \\gamma}\\) fit the real distribution. It becomes more intuitive when \\(q\\) is a uniform distribution and \\(q(x)\\) is a constant \\(U\\), where \\(e^\\gamma\\) will be the learnt normalizing factor.\nhttps://kexue.fm/archives/5617/comment-page-1\nMaximum likelihood estimation (the original paper’s view) The model is defined as \\(\\ln p_\\theta(x) = \\ln p^0_\\alpha(x) + c\\). The MLE will maximize the objective function \\[ \\begin{aligned} J_T(\\theta) \u0026amp;= \\frac{1}{T_d} [\\sum_{i=1}^{T_d \\\\} \\ln h(x_i;\\theta) + \\sum_{i=1}^{T_n} \\ln (1 - h(y_i;\\theta)) \\text{, ($x_i$\u0026#39;s are samples, $y_i$\u0026#39;s are noises, $T_n = \\nu T_d$)} \\\\ \u0026amp;\\stackrel{P}\\to J(\\theta) \\triangleq \\E_{x \\sim \\tilde p} \\ln h(x;\\theta) + \\nu \\E_{x \\sim q} \\ln (1 - h(x;\\theta)) \\\\ \\end{aligned} \\] where \\[ \\begin{gather} r_\\nu(u) = \\frac{1}{1 + \\nu e^{-u}} \\\\ G(x; \\theta) = \\ln p_\\theta(x) - \\ln q(x) \\\\ h(x;\\theta) = r_\\nu(G(x;\\theta)) = \\frac{1}{1 + \\nu e^{-G(x;\\theta)}} \\\\ \\end{gather} \\] Denote by \\(\\tilde J\\) the objective function \\(J\\) seen as a function of \\(f(.) = \\ln p_\\theta(.)\\), \\[ \\tilde J(f) = \\E_{x \\sim \\tilde p} \\ln r_\\nu(f(x) - \\ln q(x)) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) - q(x)]) \\] For \\(\\epsilon \u0026gt; 0\\) and \\(\\phi(x)\\) a perturbation of \\(f\\), \\[ \\tilde J(f + \\epsilon \\phi) = \\E_{x \\sim \\tilde p} \\ln r_\\nu(f(x) + \\epsilon \\phi(x) - \\ln q(x)) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) + \\epsilon \\phi - q(x)]) \\]\nBy Taylor’s expansion,\n\\[ \\begin{aligned} \\ln r_\\nu(u + \\epsilon u_1 + \\epsilon^2 u_2) \u0026amp;\\approx \\ln r_\\nu (u) + r_{\\frac{1}{\\nu}}(-u)(\\epsilon u_1 + \\epsilon^2 u_2) - \\frac{r_\\nu (u)r_\\frac{1}{\\nu}(-u)}{2}(\\epsilon u_1 + \\epsilon^2 u_2)^2 + \\Omicron \\big((\\epsilon u_1 + \\epsilon^2 u_2)^3 \\big) \\\\ \u0026amp;= \\ln r_\\nu (u) + \\epsilon u_1r_{\\frac{1}{\\nu}}(-u) + \\epsilon^2(u_2r_{\\frac{1}{\\nu}}(-u) - \\frac{1}{2}u_1^2 r_{\\frac{1}{\\nu}}(-u)r_\\nu (u)) + \\Omicron \\big(\\epsilon^3 \\big) \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} \\ln \\big(1 - r_v(u + \\epsilon u_1 + \\epsilon^2 u_2) \\big) \u0026amp;\\approx \\ln \\big(1 - r_v(u) \\big) - r_v(u)(\\epsilon u_1 + \\epsilon^2 u_2) - \\frac{r_{\\frac{1}{\\nu}}(-u) r_\\nu(u)}{2} (\\epsilon u_1 + \\epsilon^2 u_2)^2 + \\Omicron \\big((\\epsilon u_1 + \\epsilon^2 u_2)^3 \\big) \\\\ \u0026amp;= \\ln(1 - r_v(u)) - \\epsilon u_1 r_v(u) - \\epsilon^2 \\big( u_2 r_v(u) + \\frac{1}{2} u_1^2 r_{\\frac{1}{\\nu}}(-u) r_\\nu(u) \\big) + \\Omicron(\\epsilon^3) \\end{aligned} \\]\nSubstitute \\(u\\) with \\(f(x)\\), \\(u_1\\) with \\(\\phi(x)\\), \\(u_2\\) with \\(0\\) to give \\[ \\begin{aligned} \\tilde J(f + \\epsilon \\phi) \\approx \u0026amp;\\E_{x \\sim \\tilde p} \\ln r_\\nu \\big(f(x) + \\epsilon \\phi(x) - \\ln q(x) \\big) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) + \\epsilon \\phi - q(x)]) \\\\ = \u0026amp;\\E_{x \\sim \\tilde p} \\{\\ln r_\\nu \\big(f(x) - \\ln q(x) \\big) + \\epsilon \\phi(x) r_{\\frac{1}{\\nu}} \\big(\\ln q(x) - f(x) \\big) \\} \\\\ \u0026amp;+\\nu \\E_{x \\sim q} \\{ \\ln \\big(1 - r_\\nu[f(x) -\\ln q(x)] \\big) - \\epsilon \\phi(x) r_\\nu \\big( f(x) - \\ln q(x) \\big) \\} + \\Omicron(\\epsilon^2) \\\\ = \u0026amp;\\tilde J(f) + \\epsilon \\int \\phi(x) \\big(r_\\frac{1}{\\nu} [\\ln q(x) - f(x)] \\tilde p(x) - \\nu r_\\nu[f(x) - \\ln q(x)] q(x) \\big) + \\Omicron(\\epsilon^2) \\end{aligned} \\] The above equation attains the local …","date":1652354761,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652354761,"objectID":"c29c64575064d59ab58ff2d26987147a","permalink":"https://chunxy.github.io/notes/papers/noise-contrastive-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/noise-contrastive-estimation/","section":"notes","summary":"Three Perspectives on NCE Non-parametric estimation The traditional log-likelihood function will be \\(\\ell = \\sum_x \\ln p_\\theta(x)\\). In NCE, we learn \\[ p_\\theta(1|x) = \\sigma(G(x;\\theta) - \\gamma) = \\frac{1}{1 +","tags":null,"title":"Noise Contrastive Estimation","type":"book"},{"authors":null,"categories":null,"content":" Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, ..., x_n) \\\\ ... \\\\ y_m = f_m(x_1, x_2, ..., x_n) \\\\ \\end{cases} \\] Then Jacobian matrix is \\(m \\times n\\): \\[ \\begin{aligned} J \u0026amp;= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \u0026amp; \\frac{\\partial f}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial x_n} \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\frac{\\partial f_m}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_m}{\\partial x_n} \\\\ \\end{bmatrix} \\end{aligned} \\] When \\(f\\) is a linear transformation, i.e., \\(f\\) is a \\(m \\times n\\) matrix \\(T\\), \\(y = Tx\\), then, \\[ J = T \\]\nWhen \\(f\\) is a linear transformation and \\(n = m\\), i.e., \\(f\\) is a \\(n \\times n\\) square matrix \\(T\\), \\[ \\begin{bmatrix} dy_1 \\\\ dy_2 \\\\ \\vdots \\\\ dy_n \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} \u0026amp; \\frac{\\partial f_n}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_n}{\\partial x_n} \\\\ \\end{bmatrix} \\begin{bmatrix} dx_1 \\\\ dx_2 \\\\ \\vdots \\\\ dx_n \\\\ \\end{bmatrix} \\] That is, \\[ \\underbrace{ \\begin{bmatrix} dy_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; dy_2 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; dy_n \\end{bmatrix}}_A \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ \\end{bmatrix} = \\underbrace{ \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} \u0026amp; \\frac{\\partial f_n}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_n}{\\partial x_n} \\\\ \\end{bmatrix}}_J \\underbrace{ \\begin{bmatrix} dx_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; dx_2 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; dx_n \\end{bmatrix}}_B \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ \\end{bmatrix} \\] \\(A\\) and \\(JB\\) are both diagonal. From above equation we can find that \\(A = JB\\). Therefore, \\[ \\begin{aligned} |A| \u0026amp;= |JB| \\\\ dy_1 \\dots dy_n \u0026amp;= |J|dx_1 \\dots dx_n \\\\ \\end{aligned} \\]\n","date":1652134140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652134140,"objectID":"b4e8ede93b88935cf7e33406e7b28137","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/jacobian-matrix/","section":"notes","summary":"Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .","tags":null,"title":"Jacobian Matrix","type":"book"},{"authors":null,"categories":null,"content":" The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be described as below:\n Cartesian-to-spherical \\[ \\begin{alignat}{2} r \u0026amp;= \\sqrt{x_1^2 + \\dots + x_k^2} \u0026amp;\u0026amp; \\\\ \\varphi_1 \u0026amp;= \\arccot \\frac{x_1} {\\sqrt{x_k^2 + \\dots + x_2^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_1} {\\sqrt{x_k^2 + \\dots + x_1^2}} \\\\ \\varphi_2 \u0026amp;= \\arccot \\frac{x_2} {\\sqrt{x_k^2 + \\dots + x_3^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_2} {\\sqrt{x_k^2 + \\dots + x_2^2}} \\\\ \u0026amp; \\vdots \u0026amp;\u0026amp;\\vdots\\\\ \\varphi_{k-2} \u0026amp;= \\arccot \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_{k-2}} {\\sqrt{x_k^2 + x_{k-1}^2 + x_{k-2}^2}} \\\\ \\varphi_{k-1} \u0026amp;= 2 \\arccot \\frac{x_{k-1} + \\sqrt{x_k^2 + x_{k-1}^2}}{x_k} \u0026amp;\u0026amp;= \\begin{cases} \\arccos \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}}, \u0026amp;x_n \\ge 0 \\\\ 2\\pi - \\arccos \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}}, \u0026amp;x_n \u0026gt; 0\\\\ \\end{cases} \\end{alignat} \\]\n Spherical-to-Cartesian \\[ \\begin{align} x_1 \u0026amp;= r \\cos(\\varphi_1) \\\\ x_2 \u0026amp;= r \\sin(\\varphi_1) \\cos(\\varphi_2) \\\\ \\notag \u0026amp;\\vdots \\\\ x_{k-1} \u0026amp;= r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\\\ x_k \u0026amp;= r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\\\ \\end{align} \\] The corresponding Jacobian Matrix is \\[ J^{(k)} = \\left[ \\begin{array}{ccccc|c} \\cos (\\varphi_1) \u0026amp; -r \\sin(\\varphi_1) \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\sin(\\varphi_1) \\cos(\\varphi_2) \u0026amp; r \\cos(\\varphi_1) \\cos(\\varphi_2) \u0026amp; -r \\sin(\\varphi_1) \\sin(\\varphi_2) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\hline \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\ \u0026amp; \\ \u0026amp; -r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\\\ \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\ \u0026amp; \\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\end{array} \\right] \\] \\(|J^{(2)}|\\) can be easily derived as \\(r\\) and \\(|J^{(k)}|\\) can be constructed from \\(|J^{(k-1)}|\\). Comparing \\(J^{(k)}\\) and \\(J^{(k-1)}\\),\n\\(J^{(k)}\\) has an extra column \\(k\\) and an extra row \\(k\\), On row \\(k-1\\), before column \\(k\\), \\(J^{(k)}\\) has an extra \\(\\cos(\\varphi_{k-1})\\) term in each element than the elements of \\(J^{(k-1)}\\) on row \\(k-1\\), On row \\(k\\), before column \\(k\\), \\(J^{(k)}\\) has an extra \\(\\sin(\\varphi_{k-1})\\) term in each element than the elements of \\(J^{(k-1)}\\) on row \\(k-1\\), \\(J^{(k)}\\) and \\(J^{(k-1)}\\) are totally the same on the region delimited by row \\(1\\), row \\(k-2\\), column \\(1\\), column \\(k-1\\).  Apply the Laplace Expansion along column \\(k\\) and combine the property of determinant to give \\[ \\begin{aligned} |J^{(k)}| =\\ \u0026amp; \\underbrace{0 + \\dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\sin(\\varphi_{k-1}) \\big( \\sin(\\varphi_{k-1}) |J^{(k-1)}| \\big)] + \\\\ \u0026amp; (-1)^{n+n} [r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\big( \\cos(\\varphi_{k-1}) |J^{(k-1)}| \\big)] \\\\ =\\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\big( \\sin_{\\varphi_{k-1}}^2 + \\cos{\\varphi_{k-1}}^2 \\big) |J^{(k-1)}| \\\\ =\\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) |J^{(k-1)}| \\\\ \\end{aligned} \\] By induction, \\[ |J^{(k)}| = r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\] Therefore when changing basis from orthogonal coordinate system to polar coordinate system, \\[ \\d x_1 \\dots \\d x_k = r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\d r \\d \\varphi_1 \\dots \\d \\varphi_{k-1} \\]\n  Wiki|3d Case Blog 1|3d Case Blog 2\n","date":1652127973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652127973,"objectID":"2a6309977dd0e92cdd0292fd696fa72d","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/spherical-coordinates/","section":"notes","summary":"The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be described as below:","tags":null,"title":"Spherical Coordinates","type":"book"},{"authors":null,"categories":null,"content":" Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two basis is zero. Inner product can also be defined on some common interval between two functions, and thus the orthogonality.\nFrequency Domain It is intuitive to model after the inner product between vectors. Function (signal) on its domain can be viewed as an “infinite-dimension” vector. We represent this infinity in the definition of function inner product by integration. In particular, given two functions \\(s\\) and \\(g\\), an interval \\(I\\), the inner product is \\[ \\int\\limits_{x \\in I}s(x)g(x)dx \\] \\(s\\) and \\(g\\) are orthogonal on interval \\(I\\) if their inner product \\(\\int_{x \\in I}s(x)g(x)dx = 0\\).\nA set of basis of \\(\\R^N\\) Euclidean space contains \\(N\\) independent orthogonal basis. For an “infinite-dimension” function space, there are an infinite number of basis, among which a group of sine and cosine functions satisfy. For integer \\(k\\) and positive integers \\(m, n\\), \\[ \\begin{aligned} \\left\\{ \\begin{array} \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx = \\pi, m = n, m, n \\ge 1 \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx = 0, m \\ne n, m, n \\ge 1 \\\\ \\end{array} \\right. \\\\ \\left\\{ \\begin{array} \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx = \\pi, m = n, m, n \\ge 1 \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx = 0, m \\ne n, m, n \\ge 1 \\\\ \\end{array} \\right. \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx = 0, m, n \\ge 1 \\end{aligned} \\]\n Proof\n\\(m = n\\) \\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos^2(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{1 - \\cos(2nx)}{2}dx \\\\ \u0026amp;= \\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin^2(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{1 + \\cos(2nx)}{2}dx \\\\ \u0026amp;= \\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(nx) \\cos(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\sin(2nx)}{2}dx \\\\ \u0026amp;= 0 \\end{aligned} \\]\n \\(m \\ne n\\) \\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\cos((m+n)x) + \\cos((m-n)x)}{2}dx \\\\ \u0026amp;= \\frac{\\frac{\\sin((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} + \\frac{\\sin((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\cos((m+n)x) - \\cos((m-n)x)}{2}dx \\\\ \u0026amp;= \\frac{\\frac{\\sin((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} - \\frac{\\sin((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\sin((n+m)x) + \\sin((n-m)x)}{2}dx \\\\ \u0026amp;= -\\frac{\\frac{\\cos((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} + \\frac{\\cos((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n   In other words, we can use the linear combination of \\(1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x), \\dots\\) to fit any continuous function on interval \\([-\\pi, \\pi]\\). Or use \\(1, \\cos(2\\pi fx), \\sin(2\\pi fx), \\cos(2\\pi f2x), \\sin(2\\pi f2x), \\dots\\) to fit any function on interval \\([\\frac{-1}{2f} + \\frac{k}{f}, \\frac{1}{2f} + \\frac{k}{f}]\\), which can be any interval by choosing the value of \\(f\\) (frequency) and the integer \\(k\\).\nA continuous function \\(s\\) approximated with such series up to level \\(N\\) can be written as: \\[ \\begin{align} \u0026amp;\\begin{split} s_N(x) \u0026amp;= a_0 + \\sum_{i=1}^N \\big( \\underbrace{a_n}_{A_n\\sin(\\varphi_n)} \\cos(2\\pi fnx) + \\underbrace{b_n}_{A_n\\cos(\\varphi_n)} \\sin(2\\pi fnx) \\big) \\\\ \u0026amp;= a_0 + \\sum_{n=1}^N \\bigg( A_n\\sin(2\\pi fnx + \\varphi_n) \\bigg) \\text{, where} \\\\ \\end{split} \\\\ \\notag \u0026amp;A_n = \\sqrt{a_n^2 + b_n^2}, \\sin(\\varphi_n) = \\frac{a_n}{\\sqrt{a_n^2 + b_n^2}}, \\cos(\\varphi_n) = \\frac{b_n}{\\sqrt{a_n^2 + b_n^2}} \\end{align} \\] \\(A_n\\) can be interpreted as the amplitude, \\(\\varphi_n\\) as the phase, \\(nf\\) as the frequency.\nComplex Frequency Domain By Euler’s Formula we have \\[ e^{ix} = \\cos x + i\\sin x \\] Thus \\(s_N(x)\\) can be re-written as \\[ \\begin{alignat}{2} s_N(x) \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\bigg( \\underbrace{a_n}_{A_n\\cos \\phi_n} \\cos(2\\pi fnx) + \\underbrace{b_n}_{A_n\\sin \\phi_n} \\sin(2\\pi fnx) \\bigg) \\\\ \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\bigg( A_n\\cos(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\frac{A_n}{2} \\bigg( \\cos(2\\pi fnx - \\phi_n) + i\\sin(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp; \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\frac{A_n}{2} \\bigg( \\cos(2\\pi fnx - \\phi_n) - i\\sin(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp; \u0026amp;\u0026amp;\\Downarrow_\\text{by multiplication rule between complex numbers in polar form} \\\\ \u0026amp;= \u0026amp;\u0026amp; …","date":1652125189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652125189,"objectID":"4b793860ef0dddc5cda7df9cda2ef02b","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/fourier-transform/","section":"notes","summary":"Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two","tags":null,"title":"Fourier Transform","type":"book"},{"authors":null,"categories":null,"content":" Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).\nIf the probability mass function is known, \\[ \\E[g(X)] = \\sum_x g(x) p(x) \\] If the probability density function is known, \\[ \\E[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)p(x)\\ \\d x \\] If the cumulative distribution function is known, \\[ \\E[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)\\ \\d F(x) \\]\nWiki\nMarginal Expectation If the joint distribution of two random variables \\(X\\) and \\(Y\\) is known, then the expectation of one component can be calculated as \\[ \\E[X] = \\int_{-\\infty}^{+\\infty} x p_X(x)\\; \\d x = \\int_{-\\infty}^{+\\infty} x \\int_{-\\infty}^{+\\infty} p(x,y)\\; \\d y\\; \\d x = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} xp(x,y)\\ \\d y\\ \\d x \\] On the other hand, \\[ \\E [X] = \\E_{y \\sim p_Y} [\\E_{x \\sim p(X|Y=y)}] = \\int_{-\\infty}^{+\\infty} p(y) \\bigg( \\int_{-\\infty}^{+\\infty} x p(x|y)\\ \\d x \\bigg) \\d y \\] StackExchange Discussion\nExpectation of Non-negative Random Variables If \\(X\\) is a random variable whose value is non-negative, and its expectation exists, and\n when \\(X\\) is continuous, \\[ \\begin{aligned}[b] \\E (X) \u0026amp;= \\int_{0}^{+\\infty} x p(x)\\ \\d x = \\int_{0}^{+\\infty} x\\ \\d \\big( P(x) - 1 \\big) \\\\ \u0026amp;= [x \\big( P(x) - 1 \\big)]\\bigg|_{x=0}^{+\\infty} - \\int_0^{+\\infty} \\big( P(x) - 1 \\big)\\ \\d x \\end{aligned} \\] Because the expectation exists, the above expression and especially the \\([x \\big( P(x) - 1 \\big)]\\bigg|_{x=0}^{+\\infty}\\) term must converge: \\[ \\begin{gather} [x \\big( P(x) - 1 \\big)]\\bigg|_{x=0} = 0 \\\\ [P(x) - 1]\\bigg|_{x \\to +\\infty} = 0 \\Rightarrow [x \\big( P(x) - 1 \\big)]\\bigg|_{x \\to +\\infty} = 0 \\end{gather} \\] Therefore, \\[ \\E(X) = \\int_{0}^{+\\infty} \\big (1 - P(x) \\big)\\ \\d x \\]\n when \\(X\\) is discrete and \\(X\\) only takes on integer values, supposing the max value of \\(X\\) is \\(N\\), \\[ \\begin{aligned} \u0026amp;\\E(X) = \\sum_{k=0}^{N} [k P(X = k)] \\\\ \u0026amp;= \\sum_{k=0}^{N} [(\\sum_{j=0}^{k-1} 1) P(X = k)] \\\\ \u0026amp;= \\sum_{k=0}^{N} [\\sum_{j=0}^{k-1} P(X = k)] \\\\ \u0026amp;= \\sum_{j=0}^{N-1} [\\sum_{k=j+1}^{N} P(X = k)] \\\\ \u0026amp;= \\sum_{j=0}^{N-1} P(X \u0026gt; j) \\end{aligned} \\]\n  StackExchange Discussion || Summation by Parts\nExpectation and Quantile Function Let \\(f\\) be the PDF and \\(F\\) be the CDF of a random variable \\(X\\). Let \\(Q = F^{-1}\\) be the inverse of \\(F\\). \\(Q\\) is called the quantile function of \\(X\\), and \\[ \\int_0^1 Q(p)\\ \\d p \\stackrel{p=F(x)}{\\Longrightarrow} = \\int_{-\\infty}^{+\\infty} x f(x)\\ \\d x = \\E(X) \\] StackExchange Answer\n","date":1651575054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651575054,"objectID":"98772df8c803c49e6d7e354aae83a4a2","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/","section":"notes","summary":"Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).","tags":null,"title":"Unconscious Statistics","type":"book"},{"authors":null,"categories":null,"content":" Rationale CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the original data. The original data can either be the data before the encoding, or the future data after various steps.\nCPC learns the representation by minimizing the following loss function: \\[ \\newcommand{\\c}{\\mathrm{c}} \\label{loss} \\mathcal L_N = -\\E_{t \\sim \\Phi} \\log \\frac{f_\\theta(\\x_l,\\c)} {\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} = -\\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{f_\\theta(\\x_l,\\c)} {\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} \\] \\(t = (\\x_1, \\dots, \\x_{N+1}, \\c^*; \\ell)\\) is a tuple of random variables and \\(\\Phi\\) is the distribution from which \\(t\\) is drawn. \\((\\x:\\c)_{1:N+1}\\) are drawn from the joint distribution \\(\\tilde p(\\x,\\c)\\). All \\(\\c_i\\)’s but one randomly-chosen \\(\\c^*\\) are trimmed from the original samples. \\(\\c^*\\) is known but it is unknown which sample it is associated with. \\(\\ell\\) denotes the index of this unique sample we are trying to predict. In essence, \\(\\theta\\) is parameterizing the representation \\(c\\). The formal score function \\(f_\\theta\\) is simply a deterministic cosine similarity between \\(\\x\\) and \\(c\\).\n\\(\\x_{1:N+1}\\) consists of one positive sample \\(\\x^*\\) that is matched with \\(\\c^*\\) and more other independent negative (noise) samples \\(\\x_i\\)’s that are not matched with \\(\\c\\). \\(N\\) is the fixed ratio of the number of negative samples to the number of positive samples. Let \\(P(\\ell=i|\\x_{1:N+1},\\c^*)\\) represent the probability that \\(\\x_i\\) is the positive sample given \\(\\x_1, \\dots x_{N+1}\\) and \\(\\c^*\\). \\[ \\begin{aligned} P(\\ell=i|\\x_{1:N+1},\\c^*) \u0026amp;= \\frac{P(\\ell=i, \\x_{1:N+1},\\c^*)}{\\sum_{j=1}^{N+1} P(\\ell=j, \\x_{1:N+1},\\c^*)} \\\\ \\end{aligned} \\]\nSubstitute \\(P(\\ell=i,\\x_{1:N+1},\\c^*) = P(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} P(\\x_j)\\) to give \\[ \\begin{aligned} P(\\ell=i|\\x_{1:N+1},\\c^*) \u0026amp;= \\frac{P(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} P(\\x_j)}{\\sum_{j=1}^{N+1} [P(\\x_j,\\c^*) \\prod_{k=1,k \\ne j}^{N+1} P(\\x_k)]} \\\\ \u0026amp;= \\frac{\\tilde p(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} \\tilde p_X(\\x_j)} {\\sum_{j=1}^{N+1} [\\tilde p(\\x_j,\\c^*) \\prod_{k=1,k \\ne j}^{N+1} \\tilde p_X(\\x_j)]} \\\\ \u0026amp;= \\frac{\\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} } {\\sum_{j=1}^{N+1} \\frac{\\tilde p(\\x_j,\\c^*)}{\\tilde p_X(\\x_j)}} \\\\ \\end{aligned} \\] The loss function \\(\\eqref{loss}\\) is in fact the expectation (the outer \\(\\E\\)) of the categorical cross entropy (the inner \\(\\E\\)) of identifying the sample as positive or negative. The minimum of loss function is thus reached when the two categorical distributions are identical. That is, \\[ \\begin{gather} P_\\theta(l = i|x_{1:N+1},\\c^*) = \\frac{f_\\theta(\\x_i,\\c)}{\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} = \\frac{\\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} } {\\sum_{j=1}^{N+1} \\frac{\\tilde p(\\x_j,\\c^*)}{\\tilde p_X(\\x_j)}} = P(\\ell=i|\\x_{1:N+1},\\c^*) \\\\ f_\\theta(\\x_i,\\c) = \\frac{\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)}{\\sum_{\\x^\\prime \\in X} \\frac{\\tilde p(\\x^\\prime|\\c)}{\\tilde p_X(\\x^\\prime)}} \\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} \\\\ f_\\theta(\\x_i,\\c) \\propto \\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} \\end{gather} \\]\nBounding the Mutual Information CPC helps estimate the lower bound of the mutual information between the encoded representation and the original data when optimizing the InfoNCE loss: \\[ \\begin{aligned} \u0026amp;\\mathcal L_N^{\\text{opt}} = -\\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)}} {\\sum_{\\x\u0026#39; \\in X} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)}} \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)} + \\sum_{\\x\u0026#39; \\in X, \\x\u0026#39; \\ne x_l} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)}} {\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)}} \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} \\sum_{\\x\u0026#39; \\in X, \\x\u0026#39; \\ne x_l} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)} \\big) \\\\ \u0026amp;\\approx \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\E_{\\tilde p_X(\\x\u0026#39;)} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)} \\big) \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\big) \\\\ \u0026amp;\\ge \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\big) \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} [\\log \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)}] + \\log (N - 1) \\\\ \u0026amp;= -I(\\x;\\c^*) + \\log (N - 1) \\end{aligned} \\] Therefore, \\(I(\\x;\\c^\\star) \\ge \\log(N-1) - \\mathcal L^{\\mathrm{opt}}_{N}\\).\nExternal Materials Paper Review || CPC Formulation || NCE and InfoNCE || Demo of Bounding Mutual Information\n","date":1651231922,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651231922,"objectID":"58805b758267fe4a8cc5f2347a8ec405","permalink":"https://chunxy.github.io/notes/papers/contrastive-predictive-coding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/contrastive-predictive-coding/","section":"notes","summary":"Rationale CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the original data. The original data can either be the data before the encoding, or the future data after various steps.","tags":null,"title":"Contrastive Predictive Coding","type":"book"},{"authors":null,"categories":null,"content":" The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known. The entropy of \\(Y\\) conditioned on \\(X\\) is defined as \\[ H(Y|X) = -\\sum_{(x,y) \\in X:Y} p_{(X,Y)}(x,y) \\log \\frac{p_{(X,Y)}(x,y)}{p_X(x)} \\]\n","date":1649862201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649862201,"objectID":"87fa413dc1946d1c48b838a54f76f550","permalink":"https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/conditional-entropy/","section":"notes","summary":"The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known.","tags":null,"title":"Conditional Entropy","type":"book"},{"authors":null,"categories":null,"content":" For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.\nLipschitz Constant If \\(K\\) is the minimum number to make the above condition hold, then \\(K\\) is called the Lipschitz constant.\nThe Lipschitz constant for a general differentiable function \\(f\\) will be the maximum spectral norm of its gradient \\(\\nabla f\\) over its domain. \\[ ||f||_{Lip} = \\sup_x \\sigma[\\nabla f(x)] = \\sup_x \\sup_{||v||=1} \\nabla f(x) \\cdot v \\] where \\(\\sigma[\\nabla f(x)]\\) denotes the spectral norm of \\(f\\)’s gradient at \\(x\\).\n The Lipschitz constant for a matrix transformation will be the matrix’s spectral norm. The Lipschitz constant for a \\(\\R \\mapsto \\R\\) function will be its largest subgradient over its domain  Composition of Functions Suppose two functions \\(f\\) and \\(g\\) are Lipschitz continuous respectively. Then, \\[ \\nabla (f \\circ g)(x) = \\nabla f [g(x)] \\nabla g(x) \\] by the chain rule of derivatives. \\(f \\circ g\\)’s Lipschitz constant will be \\[ \\begin{aligned} \\sigma(\\nabla (f \\circ g)(x)) \u0026amp;= \\sup_{||v|| = 1} ||\\{\\nabla f[g(x)] \\nabla g(x)\\} v|| \\\\ \u0026amp;= \\sup_{||v|| = 1} \\{||\\nabla g(x) v||\\} \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;\\le \\sup_{||v|| = 1} \\sigma[\\nabla g(x)] \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;= \\sigma[\\nabla g(x)] \\sup_{||v|| = 1} \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;= \\sigma[\\nabla g(x)] \\cdot \\sigma\\{\\nabla f[g(x)]\\} \\end{aligned} \\] In other words, \\(||f \\circ g||_{Lip} = ||f||_{Lip} \\cdot ||g||_{Lip}\\)\nLipschitz Continuity, convexity, subgradients – Marco Tulio Ribeiro – (washington.edu) Lipschitz continuous gradient · Xingyu Zhou’s blog\n","date":1643587341,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643587341,"objectID":"a018239acbdc9ed4220798346aa0dcb0","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/lipschitz-continuity/","section":"notes","summary":"For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.","tags":null,"title":"Lipschitz Continuity","type":"book"},{"authors":null,"categories":null,"content":" Assume \\(A\\) is a \\(n \\times n\\) real-valued symmetric matrix. We have its properties as following.\nReal-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued. Suppose by contradiction that \\(A\\) has some imaginary eigenvalue \\(\\lambda\\) and the corresponding imaginary eigenvector \\(x\\). We have \\[ \\begin{aligned} Ax \u0026amp;= \\lambda x \\\\ A(x_{real} + x_{img}) \u0026amp;= (\\lambda_{real} + \\lambda_{img})(x_{real} + x_{img}) \\\\ Ax_{real} + Ax_{img} \u0026amp;= (\\lambda_{real}x_{real} + \\lambda_{img}x_{img}) + (\\lambda_{real}x_{real} + \\lambda_{img}x_{real}) \\\\ \\end{aligned} \\] Denoting \\(\\lambda\\)’s and \\(x\\)’s complex conjugate by \\(\\bar \\lambda\\) and \\(\\bar x\\) respectively, we have \\[ \\begin{gather} \\left. \\begin{aligned} \u0026amp;A\\bar x = Ax_{real} - Ax_{img} \\\\ \u0026amp;= (\\lambda_{real}x_{real} + \\lambda_{img}x_{img}) \\\\ \u0026amp;\\quad- (\\lambda_{real}x_{real} + \\lambda_{img}x_{real}) \\\\ \u0026amp;= \\bar \\lambda \\bar x \\end{aligned} \\right\\} \\Rightarrow \\begin{aligned} (A\\bar x)^T \u0026amp;= (\\bar \\lambda \\bar x)^T \\\\ \\bar x^TA^T \u0026amp;= \\bar \\lambda \\bar x^T \\\\ \\bar x^TA \u0026amp;= \\bar \\lambda \\bar x^T \\end{aligned} \\end{gather} \\] Left-multiply \\(\\bar x^T\\) on both sides of \\(Ax = \\lambda x\\) to give: \\[ \\bar x^TAx = \\bar x^T \\lambda x = \\lambda \\bar x^T x \\] Right-multiply \\(x\\) on both side of \\(\\bar x^TA = \\bar \\lambda \\bar x^T\\) to give: \\[ \\bar x^TAx = \\bar \\lambda \\bar x^T x \\] Therefore \\(\\bar \\lambda \\bar x^T x = \\lambda \\bar x^T x\\). Since \\(\\bar x^Tx\\) is real-value, \\(\\bar \\lambda = \\lambda\\). In other words, \\(\\lambda\\) is real-valued and thus so is \\(x\\).\nOrthogonal Eigenvectors Its eigenvectors corresponding to different eigenvalues are orthogonal. Arbitrarily taking \\(A\\)’ s two eigenvectors \\(v_1, v_2\\) and their eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_1 \\ne \\lambda_2\\), we have\n\\[ \\begin{align} \u0026amp;\\begin{aligned} (Av_1)^Tv_2 \u0026amp;= v_1^TA^Tv_2 \\\\ \u0026amp;= v_1^TAv_2 \\\\ \u0026amp;= v_1^T \\lambda_2v_2 \\\\ \u0026amp;= \\lambda_2v_1^Tv_2 \\\\ \\end{aligned} \\\\ \u0026amp;\\begin{aligned} (Av_1)^Tv_2 \u0026amp;= \\lambda_1v_1^Tv_2 \\end{aligned} \\end{align} \\]\nTherefore, \\[ \\begin{aligned} \\lambda_1v_1^Tv_2 \u0026amp;= \\lambda_2v_1^Tv_2 \\\\ (\\lambda_1 - \\lambda_2)v_1^Tv_2 \u0026amp;= 0 \\end{aligned} \\]\nSince \\(\\lambda_1 \\ne \\lambda_2\\), we have \\(v_1^Tv_2 = 0\\) and thus \\(v_1\\) and \\(v_2\\) are orthogonal.\nDiagonalizable It has \\(n\\) independent eigenvectors and thus diagonalizable.\n“Easily Invertible” Further on the diagonalizable property, suppose \\(A = P\\Lambda P^{-1}\\). If \\(A\\) is not invertible, by the relation between the matrix rank and the eigenvalues, some of \\(\\Lambda\\)’s entries on the diagonal are zero. By adding \\(A\\) with \\(\\lambda I\\), we have \\[ \\begin{aligned} A^\\prime \u0026amp;= P\\Lambda P^{-1} + \\lambda PIP^{-1} \\\\ \u0026amp;= P(\\Lambda + \\lambda I)P^{-1} \\end{aligned} \\] where \\(\\lambda I\\) complements all the zero entries on the diagonal of \\(\\Lambda\\). Thus \\(A^\\prime\\) has \\(n\\) non-zero eigenvalues and is invertible. A singular symmetric matrix \\(A\\) becomes invertible by adding \\(\\lambda I\\).\nOrthogonally Diagonalizable Its diagonalization can be in the form of \\(A = P\\Lambda P^T\\), where \\(P^TP = I\\), by properly selecting the orthonormal eigenvectors.\nEigenvectors from different eigenspaces are already orthogonal. Eigenvectors from the same eigenspace are independent but not necessarily orthogonal. However, the linear combination of these homo-spatial independent eigenvectors is still an eigenvector. Thus we can apply the Gram-Schmidt process to these eigenvectors and obtain the orthogonal basis for this eigenspace.\nFinally, we pull together all the orthogonal eigenvectors, normalize them to unit vector, and get the orthonormal matrix \\(P\\).\nIn fact, an \\(n \\times n\\) matrix \\(A\\) is orthogonally diagonalizable if and only if \\(A\\) is a symmetric matrix.\nEigenvalues and Positive Definiteness A real symmetric matrix is positive (semi-)definite if and only if its eigenvalues are (non-)negative.\nCovariance Matrix Covariance matrix is a special kind of real symmetric matrix. It is positive semi-definite and thus its eigenvalues are non-negative.\nExternal Material real-valued eigenvalues and orthogonal eigenvectors\n","date":1643550372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643550372,"objectID":"d8eac528181390c4eaaa3a11fc1441e9","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/","section":"notes","summary":"Assume \\(A\\) is a \\(n \\times n\\) real-valued symmetric matrix. We have its properties as following.\nReal-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued. Suppose by contradiction that \\(A\\) has some imaginary eigenvalue \\(\\lambda\\) and the corresponding imaginary eigenvector \\(x\\).","tags":null,"title":"Real Symmetric Matrix","type":"book"},{"authors":null,"categories":null,"content":" Spectral Normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), \\(\\sigma_{\\max}(A)\\) is \\(A\\)’s largest singular value.\nThe equation on the right can be proved in this way: \\[ \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} \\iff \\max_{\\mathrm z} \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} \\] We may force a constraint on \\(\\mathrm z\\) such that \\(||\\mathrm z||^2_2 = 1\\). This is because \\[ \\frac{||Ac\\mathrm z||^2_2}{||c\\mathrm z||^2_2} = \\frac{c^2||A\\mathrm z||^2_2}{c^2||\\mathrm z||^2_2} = \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} \\] The problem becomes \\[ \\begin{gather} \\max_{\\mathrm z} \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} = ||A\\mathrm z||^2_2 = \\mathrm z^TA^TA\\mathrm z \\\\ s.t. ||\\mathrm z||^2_2 = 1 \\end{gather} \\] This can be solved by Lagrange Multiplier, where the Lagrangian function will be \\[ L(\\mathrm z, \\lambda) = \\mathrm z^TA^TA\\mathrm z + \\lambda(||\\mathrm z||^2_2 - 1) \\]\nThe extrapolation of the Spectral Normalization will be related to Rayleigh Quotient.\nPower Iteration We can apply SVD to obtain a matrix \\(A\\)’s Spectral Norm, i.e. the square root of \\(A^TA\\)’s largest eigenvalue. There is also an iterative method to do so.\nBegin with an arbitrary vector \\(v_0 = v \\in \\R^N\\), the iteration rule will be \\[ v_{t+1} = A^TAv_t \\] Unroll \\(v_{t}\\) to get \\(v_t = (A^TA)^tv\\). Because \\(A^TA\\) is real symmetric, \\(v\\) can be written as a linear combination of \\(A^TA\\)’s orthonormal eigenvectors: \\[ v = \\sum_{i=1}^N \\alpha_i u_i \\] Let \\(u_i\\) be arranged by corresponding eigenvector from large to small, then \\[ \\begin{aligned} v_t \u0026amp;= (A^TA)^t \\sum_{i=1}^N \\alpha_i u_i \\\\ \u0026amp;= \\sum_{i=1}^N \\alpha_i \\lambda_i^t u_i \\\\ \u0026amp;= \\alpha_1 \\lambda_1^t \\sum_{i=1}^N \\frac{\\alpha_i}{\\alpha_1} (\\frac{\\lambda_i}{\\lambda_1})^t u_i \\\\ \u0026amp;\\to \\alpha_1 \\lambda_1^t u_1 \\end{aligned} \\] Thus \\[ ||A^TA \\frac{v_t}{||v_t||}|| = ||A^TAu_1|| = ||\\lambda_1 u_1|| = \\lambda_1 \\]\nmatrices - Why is the maximum Rayleigh quotient equal to the maximum eigenvalue? - Mathematics Stack Exchange\n","date":1643491438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643491438,"objectID":"daf295077a13c1a7211087e16860920e","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/","section":"notes","summary":"Spectral Normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), \\(\\sigma_{\\max}(A)\\) is \\(A\\)’s largest singular value.","tags":null,"title":"Spectral Normalization","type":"book"},{"authors":null,"categories":null,"content":" Definitions  Convex Set\nA set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).\n Convex Function\nA function \\(f: \\R^N \\mapsto \\R\\) is convex if \\(dom(f)\\) is a convex set and \\[ f(\\alpha x^{(1)} + (1 - \\alpha)x^{(2)}) \\le \\alpha f(x^{(1)}) + (1 - \\alpha)f(x^{(2)}), \\forall x^{(1)}, x^{(2)} \\in dom(f), \\alpha \\in [0,1] \\] \\(f\\) is concave if \\(-f\\) is convex.\n  Differentiable function \\(f\\) with a convex domain is convex if and only if \\[ f(y) \\ge f(x) + \\nabla^T f(x)(y - x), \\forall x, y \\in dom(f) \\] \\(f\\) is twice differentiable if \\(\\nabla^2f(x)_{ij} = \\frac{\\partial^2f(x)}{\\partial x_ix_j}\\) exists at each \\(x \\in dom(f)\\). \\(f\\) is convex if and only if\n\\[ \\nabla^2f(x) \\succeq 0 \\]\nTheorem Any local minima of a convex function \\(f: \\R^N \\mapsto R\\) is also a global minima.\nProof Suppose \\(y\\) is a local minima. Then there exists a number \\(r \u0026gt; 0\\) such that \\(\\forall x \\in dom(f) \\land ||x-y||_2 \\le r \\Rightarrow f(x) \\ge f(y)\\). Suppose instead there exists a global minima \\(z\\). Then it must hold that \\[ \\begin{gather} ||y-z||_2 \u0026gt; r \\\\ 0 \u0026lt; \\frac{r}{||y-z||_2} \u0026lt; 1 \\end{gather} \\] There exists some \\(0 \u0026lt; \\epsilon \u0026lt; r\\) such that \\(0 \u0026lt; \\theta = 1 - \\frac{r - \\epsilon}{||y-z||_2} \u0026lt; 1\\). Then the distance from \\(y\\) to point \\((\\theta y + (1 - \\theta)z)\\) is \\[ ||y - (\\theta y + (1 - \\theta)z)||_2 = (1 - \\theta)||y-z||_2 = \\frac{r - \\epsilon}{||y-z||_2}||y-z||_2 \u0026lt; r \\] That is to say \\[ f(\\theta y + (1 - \\theta)z) \\ge f(y) \\] However, since \\(f\\) is convex and \\(0 \u0026lt; \\theta \u0026lt; 1\\), \\[ f(\\theta y + (1 - \\theta)z) \\le \\theta f(y) + (1 - \\theta)f(z) \\] Since \\(z\\) is the global minima, \\[ f(\\theta y + (1 - \\theta)z) \\le \\theta f(y) + (1 - \\theta)f(z) \u0026lt; \\theta f(y) + (1 - \\theta)f(y) = f(y) \\] which contradicts that \\(f(\\theta y + (1 - \\theta)z) \\ge f(y)\\) derived. In other words, \\(y\\) is a global minima.\nStandard Form of the Convex Optimization Problem Standard form of the optimization problem is \\[ \\begin{aligned} \u0026amp;\\min f_0(x) \\\\ s.t.\\quad \u0026amp;f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp;h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Standard form of the convex optimization problem is \\[ \\begin{aligned} \u0026amp;\\min f_0(x) \\\\ s.t.\\quad \u0026amp;f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp;a_i^Tx = b_i (Ax = b), i = 1,\\dots,s \\\\ \u0026amp;\\text{$f_0,\\dots,f_r$ are convex, and $a_0,\\dots,a_s \\in \\R^N$} \\end{aligned} \\]\n","date":1641560217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560217,"objectID":"c358464778e629d4dbcdff3a6c72282a","permalink":"https://chunxy.github.io/notes/articles/optimization/convex-optimization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/convex-optimization/","section":"notes","summary":"Definitions  Convex Set\nA set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).","tags":null,"title":"Convex Optimization","type":"book"},{"authors":null,"categories":null,"content":" Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gather} \\text{$U$ is $m \\times m$, $\\Sigma$ is diagonal, $V$ is $n \\times n$} \\\\ UU^T = I \\\\ VV^T = I \\\\ \\Sigma = diag_{m \\times n}(\\sigma_1, \\sigma_2, ..., \\sigma_p) \\\\ \\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_p \\ge 0 \\\\ p = min(m, n) \\end{gather} \\]\nProof This is shown by construction.\nConstructing \\(V\\) and \\(\\Sigma\\)\n\\(A^TA\\) is an \\(n \\times n\\) real symmetric matrix, so it can be diagonalized by an orthonormal matrix. Let \\(V\\) be this matrix: \\[ V^{-1}(A^TA)V = \\Lambda \\text{, where $\\Lambda$ is the diagonal matrix consisting of $A^TA$\u0026#39;s eigenvalues} \\] Because \\(V\\) consists of orthonormal basis, we have \\(V^TV = I\\). Therefore, \\[ V^{T}(A^TA)V = \\Lambda \\] Note that \\(A^TA\\)’s eigenvalues are non-negative. To show that, let \\((\\lambda, x)\\) be one pair of its eigen, \\[ ||Ax||^2 = (Ax)^TAx = x^T(A^TA)x = x^T\\lambda x = \\lambda||x||^2 \\ge 0 \\] Let \\(V\\)’s columns be permuted such that their corresponding eigenvalues are ordered in \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_n \\ge 0\\).\nLet \\(\\sigma_i = \\sqrt{\\lambda_i}, i = 1, 2, ..., n\\) (which are defined as \\(A\\)’s the singular values).\nLet \\(\\rank(A) = r\\), then \\(\\rank(A^TA) = r\\). And because \\(A^TA\\) is symmetric, \\(r\\) is the number of its positive eigenvalues: \\[ \\begin{gather} \\lambda_1, \\lambda_2, ..., \\lambda_r \\ge 0, \\lambda_r, \\lambda_{r+1}, ..., \\lambda_{n} = 0 \\\\ \\sigma_1, \\sigma_2, ..., \\sigma_r \\ge 0, \\sigma_r, \\sigma_{r+1}, ..., \\sigma_{n} = 0 \\end{gather} \\] Let \\(V_1 = [v_1, v_2, ..., v_r], V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\\).\nAlso let \\[ \\Sigma_1 = \\begin{bmatrix} \\sigma_1 \\\\ \u0026amp; \\sigma_2 \\\\ \u0026amp; \u0026amp; \\ddots \\\\ \u0026amp; \u0026amp; \u0026amp; \\sigma_r \\end{bmatrix} \\] Complement it with \\(0\\) to get the \\(m \\times n\\) matrix: \\[ \\Sigma = \\begin{bmatrix} \\Sigma_1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Because \\(V_2\\) consists of \\(A^TA\\)’s eigenvectors whose eigenvalues are \\(0\\), \\[ A^TAV_2 = 0 \\] \\(\\rank(A^TA) = r\\) and \\(V_2\\) has \\(n - r\\) independent columns. Therefore, \\(V_2\\) spans the \\(n(A^TA) = n(A)\\): \\[ \\begin{gather} I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1V_1^T + V_2V_2^T \\\\ A = AI = AV_1V_1^T + AV_2V_2^T = AV_1V_1^T \\end{gather} \\]\n Constructing \\(U\\)\nLet \\[ \\begin{equation} u_i = \\frac{1}{\\sigma_i}Av_i, i = 1, 2, ..., r \\end{equation} \\] \\[ U_1 = [u_1, u_2, ..., u_r] \\] We have \\(AV_1 = U_1\\Sigma_1\\): \\[ \\begin{aligned} u_i^Tu_j \u0026amp;= (\\frac{1}{\\sigma_i}Av_i)^T(\\frac{1}{\\sigma_j}Av_j) \\\\ \u0026amp;= \\frac{1}{\\sigma_i\\sigma_j}v_i^TA^TAv_j \\\\ \u0026amp;= \\frac{1}{\\sigma_i\\sigma_j}v_i^T\\lambda_jv_j \\\\ \u0026amp;= \\frac{\\sigma_j}{\\sigma_i}v_i^Tv_j \\\\ \u0026amp;= \\begin{cases} 0\u0026amp; i \\ne j \\\\ 1\u0026amp; i = j \\end{cases} \\end{aligned} \\]\nSince \\(u_i, i = 1, 2, ..., r\\) is from \\(Col(A)\\) and they are orthogonal as shown, and \\(\\rank(A) = r\\), they form the \\(Col(A)\\). Let \\(Col(A)^\\perp\\) be \\(Col(A)\\)’s complement. we have \\(Col(A)^\\perp = n(A^T)\\). Let \\(\\{u_{r+1}, u_{r+2}, ..., u_{m}\\}\\) be an orthonormal basis of \\(n(A^T)\\). They will be orthogonal to \\(U_1\\)’s column vectors.\nLet \\[ U_2 = [u_{r+1}, u_{r+2}, ..., u_{m}] \\\\ U = [U_1, U_2] \\]\n Prove that \\(U\\Sigma V^T = A\\) \\[ \\begin{aligned} U\\Sigma V^T \u0026amp;= [U_1, U_2] \\begin{bmatrix} \\Sigma_1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} V_1^T \\\\ V_2^T \\\\ \\end{bmatrix} \\\\ \u0026amp;= [U_1\\Sigma_1, 0] \\begin{bmatrix} V_1^T \\\\ V_2^T \\\\ \\end{bmatrix} \\\\ \u0026amp;= U_1\\Sigma_1V_1^T \\\\ \u0026amp;= AV_1V_1^T \\\\ \u0026amp;= A \\end{aligned} \\]\n  Note There is another view on \\(U\\): \\[ AA^TU = [AA^TU_1, AA^TU_2] \\] Breaking it into two parts, \\(\\forall i = 1, 2, ..., r\\), \\[ \\begin{aligned} A A^T u_i \u0026amp;= A A^T \\frac{1}{\\sigma_i} A v_i \\\\ \u0026amp;= \\frac{1}{\\sigma_i}A (A^T A) v_i \\\\ \u0026amp;= \\frac{1}{\\sigma_i}A \\lambda_i v_i \\\\ \u0026amp;= \\lambda_i(\\frac{1}{\\sigma_i} A v_i) \\\\ \u0026amp;= \\lambda_i u_i \\end{aligned} \\] \\(U_1\\) is \\(AA^T\\)’s concatenated eigenvectors, ordered by corresponding eigenvalues. \\[ AA^TU_2 = 0 = 0U_2 \\] \\(U_2\\) is the corresponding eigenvectors with eigenvalues being \\(0\\).\nIn all, \\(U\\) is the concatenated orthonormal eigenvectors of \\(AA^T\\). It doesn’t matter how \\(U_2\\) is permuted.\nExternal materials 奇异值分解(SVD)原理与在降维中的应用\n奇异值的物理意义是什么？ - 知乎 (zhihu.com)\nSingular Value Decomposition.pdf\n","date":1641560132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560132,"objectID":"5d1c7485522774dc14e6a6b95481f2e1","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/singular-value-decomposition/","section":"notes","summary":"Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gather} \\text{$U$ is $m \\times m$, $\\Sigma$ is diagonal, $V$ is $n \\times n$} \\\\","tags":null,"title":"Singular Value Decomposition","type":"book"},{"authors":null,"categories":null,"content":" Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is. Take the Gaussian distribution as an example: \\[ \\begin{aligned} p(x|y=C_j) \u0026amp;= \\mathcal{N}(x;\\mu_j, \\Sigma) \\\\ \u0026amp;= \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}e^{-\\frac{1}{2}(x-\\mu_j)^T\\Sigma^{-1}(x-\\mu_j)} \\end{aligned} \\] Then the classification function will be \\(f_{LDA} = \\arg \\max\\limits_{j}p(x|y=C_j)p(y=C_j)\\).\nLDA’s parameters \\(\\theta = (\\mu, \\Sigma, \\varphi)\\) is also learned with Maximum Likelihood Estimation: \\[ \\begin{aligned} L(\\theta) \u0026amp;= \\prod_{i=1}^mp(x^{(i)}, y^{(i)};\\theta) \\\\ \u0026amp;= \\prod_{i=1}^mp(x^{(i)}|y^{(i)};\\theta)p(y^{(i)}) \\end{aligned} \\] The log-likelihood function will be \\[ \\begin{aligned} l(\\theta) \u0026amp;= \\log L(\\theta) \\\\ \u0026amp;= \\sum_{i=1}^m\\log p(x^{(i)}|y^{(i)};\\mu,\\Sigma) + \\sum_{i=1}^m\\log p(y^{(i)}, \\varphi) \\\\ \\end{aligned} \\] We can maximize above two summations separately.\nLet \\[ \\begin{aligned} l_1(\\mu,\\Sigma) \u0026amp;= \\sum_{i=1}^m\\log p(x^{(i)}|y^{(i)};\\mu,\\Sigma) \\\\ \u0026amp;= -\\frac{1}{2}\\sum_{i=1}^m(x^{(i)}-\\mu_{j|y^{(i)}})^T\\Sigma^{-1}(x^{(i)}-\\mu_{j|y^{(i)}}) + \\log|\\Sigma| + n\\log2\\pi\\\\ \\end{aligned} \\] Take derivative w.r.t. \\(\\mu_j\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial l_1}{\\partial \\mu_j} = \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)\\Sigma^{-1}(x^{(i)} - \\mu_j) \\\\ \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)\\Sigma^{-1}(x^{(i)} - \\mu_j) \u0026amp;= 0 \\\\ \\Sigma^{-1}\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \\mu_j) \u0026amp;= 0 \\\\ \\end{aligned} \\] Since the covariance matrix \\(\\Sigma\\) is real-symmetric and invertible and thus its nullspace is \\(0\\), \\[ \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \\mu_j) = 0 \\\\ \\mu_j = \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)} \\] Take derivative w.r.t. \\(\\Sigma\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial l_1}{\\partial \\Sigma} = -\\frac{1}{2}\\sum_{i=1}^m(\\Sigma^{-1} -\\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T\\Sigma^{-1}) \\\\ \\sum_{i=1}^m(\\Sigma^{-1} -\\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T\\Sigma^{-1}) = 0 \\\\ \\Sigma^{-1}\\sum_{i=1}^m(I - \\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T) = 0 \\\\ \\sum_{i=1}^m(I - \\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T) = 0 \\\\ \\sum_{i=1}^mI = \\Sigma^{-1}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\sum_{i=1}^m\\Sigma = \\Sigma\\Sigma^{-1}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\end{aligned} \\] Let \\[ \\begin{aligned} l_2(\\varphi) \u0026amp;= \\sum_{i=1}^m\\log p(y^{(i)}; \\varphi) \\\\ \u0026amp;= \\sum_{i=1}^m\\log\\prod_{j=1}^c\\varphi_j^{\\mathbb{I}(y^{(i)}=C_j)} \\\\ \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\log \\varphi_j \\end{aligned} \\] Note that \\(p(y^{(i)};\\varphi) = \\prod_{j=1}^c\\varphi_j^{\\mathbb{I}(y^{(i)}=C_j)} = \\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\varphi_j\\). We chose the product notation because it could be easily “logged”.\nThere is also a constraint in maximization of \\(l_2\\): \\(\\sum_{j=1}^c\\varphi_j = 1\\). We can form the corresponding Lagrangian function: \\[ J(\\varphi, \\lambda) = \\sum_{i=1}^m\\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\log \\varphi_j + \\lambda(-1 + \\sum_{j=1}^c\\varphi_j) \\] Take derivative w.r.t. \\(\\varphi_j\\) and make it zero to give \\[ \\begin{gather} \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\varphi_j} + \\lambda = 0 \\\\ \\varphi_j = -\\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\lambda} \\end{gather} \\] Because \\(\\sum_{j=1}^c\\varphi_j = 1\\), we have \\[ \\begin{gather} -\\frac{\\sum_{j=1}^c\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\lambda} = 1 \\\\ \\lambda = -M \\end{gather} \\] Then, \\[ \\varphi_j = \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{M} \\]\n","date":1641559342,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559342,"objectID":"28e9e3528c82215d54ec265584617cc9","permalink":"https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/linear-discriminant-analysis/","section":"notes","summary":"Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is.","tags":null,"title":"Linear Discriminant Analysis","type":"book"},{"authors":null,"categories":null,"content":" A useful matrix identity \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026amp;= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026amp;= A(I_M+BA)^{-1} \\\\ \\iff A \u0026amp;= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A \\end{aligned} \\]\n","date":1640188302,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640188302,"objectID":"571a787a063764a5f4b1c74a1ec25dbe","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/matrix-identity/","section":"notes","summary":"A useful matrix identity \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026= A(I_M+BA)^{-1} \\\\ \\iff A \u0026= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026= A \\end{aligned} \\]","tags":null,"title":"Matrix Identity","type":"book"},{"authors":null,"categories":null,"content":" Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F = \\sqrt{\\sum_{ij}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 \u0026amp;= \\sum_{ij}A_{ij}^2 \\\\ \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026amp;= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026amp;= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026amp;= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026amp;= tr(AA^T) = tr(A^TA) \\end{aligned} \\]\n","date":1640015034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640015034,"objectID":"7a68b4d1cd7bdb941faed2c7b109ee09","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/","section":"notes","summary":"Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F = \\sqrt{\\sum_{ij}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 \u0026= \\sum_{ij}A_{ij}^2 \\\\ \u0026= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026= tr(AA^T) = tr(A^TA) \\end{aligned} \\]","tags":null,"title":"Frobenius Normalization","type":"book"},{"authors":null,"categories":null,"content":" If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other.\nLet \\(\\{u_1, u_2, ..., u_k\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\R^n\\). Then for each \\(y\\) in \\(W\\), the weights in the linear combination \\[ y = c_1u_1 + c_2u_2 + ... + c_ku_k \\] are given by \\[ c_i = \\frac{y \\cdot u_i}{u_i \\cdot u_i} \\]\nGiven a nonzero vector \\(u\\) in \\(\\R^n\\) and another vector \\(y\\) in \\(\\R^n\\), we wish to decompose \\(y\\) such that \\[ y = \\hat y + z \\] where \\(\\hat y = \\alpha u\\) for some scalar \\(\\alpha\\) and \\(z\\) is some vector orthogonal to \\(u\\). \\[ \\begin{aligned} z \u0026amp;= y - \\hat y \\\\ z \\cdot u \u0026amp;= (y - \\alpha u) \\cdot u \\\\ y \\cdot u - \\alpha u \\cdot u \u0026amp;= 0 \\\\ \\alpha \u0026amp;= \\frac{y \\cdot u}{u \\cdot u} \\end{aligned} \\] \\(\\hat y = \\frac{y \\cdot u}{u \\cdot u}u\\) is called the orthogonal projection of \\(y\\) onto \\(u\\), \\(z = y - \\hat y\\) is called the component of \\(y\\) orthogonal to \\(u\\).\nThe projection of \\(y\\) onto \\(cu\\) for any scalar \\(c\\) is the same as that onto \\(u\\). Therefore \\(\\hat y\\) is the projection onto the subspace \\(L\\) spanned by \\(u\\). In this sense, \\(\\hat y\\) is also denoted as \\(\\mathop{proj}_Ly\\).\nLet \\(W\\) be a subspace of \\(\\R^n\\), then each \\(y\\) in \\(\\R^n\\) can be uniquely written in the form: \\[ y = \\hat y + z \\] where \\(\\hat y\\) is in \\(W\\) and \\(z\\) is in \\(W^\\perp\\). \\(\\hat y\\) is called the orthogonal projection of \\(y\\) onto \\(W\\), denoted as \\(\\mathop{proj}_Wy\\).\nThis projection can be found by finding an arbitrary orthogonal basis of \\(W\\) and then computing weights using equation (2).\n\\(\\hat y\\) is also the closest point in \\(W\\) to \\(y\\), in the sense that: \\[ ||y - \\hat y|| \u0026lt; ||y - v||, \\forall v \\in W \\text{ and } v \\ne \\hat y \\] In this case, \\(\\hat y\\) is called the best approximation to \\(y\\) by elements of \\(W\\).\nAbove can be seen by \\[ y - v = (y - \\hat y) + (\\hat y - v) \\] which gives \\[ \\begin{aligned} \u0026amp;||y - v||^2 = ||(y - \\hat y) + (\\hat y - v)|| \\\\ \u0026amp;= ||(y - \\hat y)||^2 + ||(\\hat y - v)||^2 + 2(y - \\hat y) \\cdot (\\hat y - v) \\\\ \u0026amp;= ||(y - \\hat y)||^2 + ||(\\hat y - v)||^2 \\text{($y - \\hat y$ is in $W^\\perp$, $\\hat y - v$ is in $W$)} \\\\ \u0026amp;\u0026gt; ||(y - \\hat y)||^2 \\end{aligned} \\]\nAn \\(m \\times n\\) matrix \\(U\\) has orthonormal columns if and only if \\(U^TU = I\\).\nAn orthogonal matrix is a square invertible matrix \\(U\\) such that \\(U^{-1} = U^T\\). By its definition, it has orthonormal columns and orthonormal rows.\n","date":1639995546,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639995546,"objectID":"97f37144a26b5ac4d5786f81f4626f9e","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/","section":"notes","summary":"If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other.\nLet \\(\\{u_1, u_2, ..., u_k\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\R^n\\).","tags":null,"title":"Orthogonality and Projection","type":"book"},{"authors":null,"categories":null,"content":" Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, ..., p_n\\) such that: \\[ x = q_1b_1 + q_2b_2 + ... + q_nb_n \\] These scalars are called the coordinates of \\(x\\) relative to the basis \\(\\mathcal{B}\\). \\[ [x]_\\mathcal{B} = \\begin{bmatrix} q_1 \\cdots q_n \\end{bmatrix}^T \\] is the coordinate vectors of \\(x\\) relative to \\(\\mathcal{B}\\). The mapping \\(x \\mapsto [x]_\\mathcal{B}\\) is called the coordinate mapping determined by \\(\\mathcal{B}\\).\nLet \\(P_\\mathcal{B} = [b_1, b_2, ..., b_n]\\), then \\(x = P_\\mathcal{B}[x]_\\mathcal{B}\\)\nLet \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) both be a basis for an n-dimensional vector space \\(\\R^n\\). Then there is a unique \\(n \\times n\\) matrix \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}\\) such that: \\[ [x]_\\mathcal{C} = \\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\] The columns of \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}\\) are the \\(\\mathcal{C}\\)-coordinate vectors of the vectors in the basis \\(\\mathcal{B}\\): \\[ \\mathop{P}\\limits_\\mathcal{C \\leftarrow B} = [[b_1]_\\mathcal{C}, [b_2]_\\mathcal{C}, ..., [b_n]_\\mathcal{C}] \\]\n\\[ \\begin{aligned} \u0026amp;P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} = P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B} \\begin{bmatrix} q_1 \\cdots q_n \\end{bmatrix}^T \\\\ \u0026amp;= P_\\mathcal{C}(q_1[b_1]_\\mathcal{C} + q_2[b_2]_\\mathcal{C} + ... + q_n[b_n]_\\mathcal{C}) \\\\ \u0026amp;= q_1P_\\mathcal{C}[b_1]_\\mathcal{C} + q_2P_\\mathcal{C}[b_2]_\\mathcal{C} + ... + q_nP_\\mathcal{C}[b_n]_\\mathcal{C} \\\\ \u0026amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\\\ \u0026amp;= x \\end{aligned} \\]\nSpecifically, when \\(\\mathcal{B} = \\mathcal{I}\\) is the standard basis, \\[ \\begin{aligned} \\mathop{P}\\limits_\\mathcal{C \\leftarrow I} \u0026amp;= [[e_1]_\\mathcal{C}, [e_2]_\\mathcal{C}, ..., [e_n]_\\mathcal{C}] \\\\ P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow I} \u0026amp;= P_\\mathcal{C}[[e_1]_\\mathcal{C}, [e_2]_\\mathcal{C}, ..., [e_n]_\\mathcal{C}] \\\\ \u0026amp;= [e_1, e_2, ..., e_n] \\\\ \u0026amp;= I \\end{aligned} \\] Since \\(P_\\mathcal{C}\\) is invertible, \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow I} = P_\\mathcal{C}^{-1}\\). Therefore, \\([x]_\\mathcal{B} = P_\\mathcal{B}^{-1}x\\) in equation (2)\n\\[ \\begin{aligned} \\\\ [x]_\\mathcal{C} \u0026amp;= \\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\\\ (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}[x]_\\mathcal{C} \u0026amp;= (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\\\ [x]_\\mathcal{B} \u0026amp;= (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}[x]_\\mathcal{C} \\\\ \\end{aligned} \\] In other words, \\(\\mathop{P}\\limits_\\mathcal{B \\leftarrow C} = (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}, \\mathop{P}\\limits_\\mathcal{B \\leftarrow C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B} = I\\)\n","date":1639860457,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639860457,"objectID":"75f9386bf8aae53c2959ba86f33e7c1e","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/","section":"notes","summary":"Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, .","tags":null,"title":"Coordinate System and Change of Basis","type":"book"},{"authors":null,"categories":null,"content":" Discrete form Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ \\begin{aligned} d_p(x, x^\\prime) \u0026amp;= ||x - x^\\prime||_p \\\\ \u0026amp;= (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\text{, where $p \\to \\infty$ } \\end{aligned} \\] Chebyshev distance is in effect \\(\\max\\limits_i (|x_i - x^\\prime_i|)\\).\nProof Let \\(a_i = |x_i - x^\\prime_i|\\) and without loss of generality let \\(a_1 = \\max\\limits_ia_i = \\max\\limits_i|x_i - x^\\prime_i|\\). \\[ \\begin{aligned} d_p(x, x^\\prime) \u0026amp;= \\lim_{p \\to \\infty}(\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(\\sum_{i=1}^na_i^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(a_1^p\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(a_1^p)^{1/p} \\cdot \\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \u0026amp;= a_1 \\cdot \\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \\end{aligned} \\] Because \\(\\forall i, a_1 \u0026gt; a_i\\), then \\(\\frac{a_i}{a_1} \\le 1\\), and \\[ \\begin{aligned} 1 \\le \u0026amp;\\sum_{i=1}^n(\\frac{a_i}{a_1})^p \\le n \\\\ 1^{1/p} \\le \u0026amp;(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le n^{1/p} \\text{, where $p \u0026gt; 1$} \\\\ \\lim_{p \\to \\infty}1^{1/p} \\le \u0026amp;\\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le \\lim_{p \\to \\infty}n^{1/p} \\\\ 1 \\le \u0026amp;\\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le 1 \\\\ \\end{aligned} \\]\nTherefore, \\(d_p(x, x^\\prime) = a_1 \\cdot 1 = a_1 = \\max\\limits_i|x_i - x^\\prime_i|\\).\nContinuous form Let \\(f(x)\\) be continuous and bounded on interval \\((a, b)\\), then, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} = \\sup\\limits_{x \\in (a,b)}|f(x)| \\]\nProof Let \\(S = \\sup\\limits_{x \\in (a,b)}|f(x)|\\), then \\(\\forall\\varepsilon \u0026gt; 0, \\exists x_0 \\in (a,b), |f(x_0)| \u0026gt; S - \\varepsilon\\). By continuity, \\(\\lim\\limits_{x \\to x_0}|f(x)| \u0026gt; S - \\varepsilon\\), then \\(\\exists\\delta \u0026gt; 0, \\forall x \\in U(x_0, \\delta), ||f(x)| - \\lim\\limits_{x \\to x_0}|f(x)|| \u0026lt; \\varepsilon \\rightarrow |f(x)| \u0026gt; S - 2\\varepsilon\\). \\[ \\begin{aligned} (\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \u0026amp;\\ge (\\int_{x_0 - \\delta}^{x_0 + \\delta} (S - 2\\varepsilon)^pdx)^{1/p} \\\\ \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \u0026amp;\\ge \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} (S - 2\\varepsilon)^pdx)^{1/p} \\\\ \u0026amp;= \\lim\\limits_{p \\to +\\infty}(2\\delta(S - 2\\varepsilon)^p)^{1/p} \\\\ \u0026amp;= S - 2\\varepsilon \\end{aligned} \\] Since \\(U(x_0, \\delta)\\) in within the interval \\((a,b)\\) and \\(|f(x)|^p \\ge 0\\), then, \\[ \\begin{aligned} (\\int_a^b |f(x)|^pdx)^{1/p} \u0026amp;\\ge (\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \\\\ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \u0026amp;\\ge \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \\\\ \u0026amp;\\ge S - 2\\varepsilon \\end{aligned} \\] Because \\(\\varepsilon\\) is arbitrarily and positively valued, \\(\\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \\ge S\\)\nOn the other hand, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \\le \\lim\\limits_{p \\to +\\infty}(\\int_a^b S^pdx)^{1/p} = \\lim\\limits_{p \\to +\\infty}((b - a)S^p)^{1/p} = S \\] then, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} = S = \\sup\\limits_{x \\in (a,b)}|f(x)| \\]\np范数的极限（无穷范数）为什么是极大值范数？ - 知乎 (zhihu.com)\n","date":1639859304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639859304,"objectID":"100621809c5d4a049b152158436e8417","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/","section":"notes","summary":"Discrete form Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ \\begin{aligned} d_p(x, x^\\prime) \u0026= ||x - x^\\prime||_p \\\\ \u0026= (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\text{, where $p \\to","tags":null,"title":"Chebyshev Distance","type":"book"},{"authors":null,"categories":null,"content":" 依概率收敛（convergence in probability） 随机变量序列即是由随机变量构成的序列。对于一个普通数列\\(\\{x_n\\}\\)来说，若其收敛于\\(c\\)，则当\\(n\\)充分大时，\\(x_n\\)和\\(c\\)的距离可以达到任意小。而随机变量序列\\(X_1, X_2, \\dots\\)的极限却不能按照这样定义，因为\\(X_n\\)取值不确定，不可能总和某个数字\\(c\\)的距离任意小。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \u0026lt; \\epsilon) = 1\\)，抑或是，对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \\ge \\epsilon) = 0\\)），则称该随机变量序列依概率收敛于\\(c\\)，记作\\(X_n \\stackrel{P}{\\to} c\\)。\n换言之，对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)，都存在\\(N \u0026gt; 0\\)，使得\\(n \u0026gt; N\\)时，始终有 \\[ 1 - \\delta \u0026lt; P(|X_n - c| \u0026lt; \\epsilon) \\le 1 \\]\n  依概率收敛的一个例子便是Bernoulli大数定律，即当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。\n几乎必然收敛（almost-sure convergence） 在某些情况下，若随机变量序列能够和某个数字\\(c\\)几乎接近，我们说它几乎必然收敛。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得\\(P(\\lim_{n \\to \\infty} X_n = c) = 1\\)，则称该随机变量序列几乎必然收敛于\\(c\\)，记作\\(X_n \\stackrel{a.s.}{\\to} c\\)。\n换言之，对于任意\\(\\epsilon \u0026gt; 0\\)，都存在\\(N \u0026gt; 0\\)，使得\\(n \u0026gt; N\\)时，始终有 \\[ P(|X_n - c| \u0026lt; \\epsilon) = 1 \\]\n  需要注意的是，几乎必然收敛和依概率收敛是不等价的，因为\\(\\lim_{n \\to \\infty} f(x_n)\\)中的极限符号不总是能够交换到函数\\(f\\)内部，举个简单的例子： \\[ \\begin{gathered} \\{ x_n \\} = -\\frac{1}{n}, \\ f(x) = \\begin{cases} x^2 - 1, \u0026amp; -1 \\le x \u0026lt; 0 \\\\ x, \u0026amp; x \\ge 0 \\end{cases} \\\\ \\lim_{n \\to \\infty} f(x_n) = \\lim_{n \\to \\infty}(\\frac{1}{n^2}-1) = -1 \\ne f(\\lim_{n \\to \\infty} x_n) = f(0) = 0 \\end{gathered} \\] 注意\\(f\\)是右连续的，这也意味着，我们可以找到类似的右连续的分布函数\\(P\\)，使得极限符号不能被移至\\(P\\)内部。也就是说，几乎必然收敛和依概率收敛是不等价的，而显然，几乎必然收敛是强于依概率收敛的。\n\\(L_p\\)收敛（convergence in \\(L_p\\)）  定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，对于某个\\(p \u0026gt; 0\\)，如果存在一个常数\\(c\\)，使得\\(\\lim_{n \\to \\infty} \\E(|| X_n - c||_p^p) = 0\\)，则称该随机变量序列\\(L_p\\)收敛于\\(c\\)，记作\\(X_n \\stackrel{L_p}{\\to} c\\)。\n  均方收敛 当\\(p=2\\)时，\\(L_p\\)收敛又称作均方收敛。根据Chebyshev不等式， \\[ P(|X_n-\\E(X_n)| \\ge \\epsilon) \\le \\frac{\\Var(X_n)}{\\epsilon^2} = \\frac{\\E[(X_n - \\E(X_n))^2]}{\\epsilon^2} \\] 在两边取\\(n \\to \\infty\\)可以得到 \\[ \\lim_{n \\to \\infty} P(|X_n-\\E(X_n)| \\ge \\epsilon) \\le \\lim_{n \\to \\infty} \\frac{\\E[(X_n - \\E(X_n))^2]}{\\epsilon^2} = 0 \\] 即均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也强于依概率收敛；但它和几乎必然收敛之间并没有推导关系。\n依分布收敛（convergence in distribution） 前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，让\\(F_n\\)表示\\(X_n\\)的分布函数，如果存在一个分布函数\\(F\\)，使得\\(\\lim_{n \\to \\infty} F_n(x) = F(x)\\)，则称该随机变量序列依分布收敛于\\(F\\)，记作\\(X_n \\stackrel{d}{\\to} F\\)。\n  参考资料 随机变量的收敛\n","date":1657723265,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657723265,"objectID":"cea954f6f69fb48b3f027376f5dd667e","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/","section":"notes","summary":"依概率收敛（convergence in probability）","tags":null,"title":"随机变量的收敛","type":"book"},{"authors":null,"categories":null,"content":" Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\). It takes a probabilistic approach and maps \\(f(x)\\) to a probability value between \\(0\\) and \\(1\\) by applying a sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\). That is \\[ \\begin{aligned} p(y = +1|x) \u0026amp;= \\sigma(f(x)) \\\\ p(y = -1|x) \u0026amp;= 1 - \\sigma(f(x)) = \\sigma(-f(x)) \\\\ \\end{aligned} \\] Or equivalently, \\(p(y|x) = \\sigma(yf(x))\\). Then, \\(f_{LR}(x) = \\arg \\max\\limits_{y \\in \\{-1, 1\\}} \\sigma(y(f(x)))\\).\nLogistic Regression is a discriminative classifier because we are directly modelling \\(p(y|x)\\), with no intermediate step on \\(p(x|y)\\).\nGiven dataset \\(\\mathcal{D} = {x^{(i)}, y^{(i)}, i=1, \\dots, M}\\), Logistic Regression is learning by maximizing the conditional log-likelihood of \\(\\mathcal{D}\\). \\[ l(w, b) = \\log L(w, b) = \\log \\prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) = \\sum_{i=1}^M\\log p(y^{(i)}|x^{(i)};w,b) \\]\n$$ \\[\\begin{aligned} \u0026amp;(w^\\star, b^\\star) = \\arg \\max\\limits_{w, b}l(w, b) \\\\ \u0026amp;= \\arg \\max\\limits_{w, b}\\sum_{i=1}^M\\log p(y^{(i)}|x^{(i)};w,b) \\\\ \u0026amp;= \\arg \\max\\limits_{w, b}\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\\\ \u0026amp;= \\arg \\min\\limits_{w, b}-\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\\\ \\end{aligned}\\] \\[ Let $J(w, b) = -\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b))$ be the target function. There is no closed-form solution to this optimization problem. Rather, it is to be solved by some iterative algorithm, e.g. gradient descent. For each iteration, parameters are updated by \\] \\[\\begin{gather} w \\leftarrow w - \\eta\\frac{\\partial J}{\\partial w} \\\\ b \\leftarrow b - \\eta\\frac{\\partial J}{\\partial b} \\end{gather}\\] \\[ Specifically, \\] \\[\\begin{aligned} \\frac{\\partial J}{\\partial w} \u0026amp;= -\\sum_{i=1}^M\\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial w} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial \\sigma(y^{(i)}f(x^{(i)}))} \\frac{\\partial \\sigma(y^{(i)}f(x^{(i)}))}{\\partial (y^{(i)}f(x^{(i)}))} \\frac{\\partial (y^{(i)}f(x^{(i)}))}{\\partial w} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{1}{\\sigma(y^{(i)}f(x^{(i)}))} \\sigma(y^{(i)}f(x^{(i)}))(1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\\\ \u0026amp;= -\\sum_{i=1}^M (1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\\\ \\end{aligned}\\] $$\n\\[ \\begin{aligned} \\frac{\\partial J}{\\partial b} \u0026amp;= -\\sum_{i=1}^M\\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial b} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial \\sigma(y^{(i)}f(x^{(i)}))} \\frac{\\partial \\sigma(y^{(i)}f(x^{(i)}))}{\\partial (y^{(i)}f(x^{(i)}))} \\frac{\\partial (y^{(i)}f(x^{(i)}))}{\\partial b} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{1}{\\sigma(y^{(i)}f(x^{(i)}))} \\sigma(y^{(i)}f(x^{(i)}))(1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\\\ \u0026amp;= -\\sum_{i=1}^M (1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\\\ \\end{aligned} \\]\nNote that the posterior obtained by binary Linear Discriminant Analysis also has the form of \\[ p(y|x) = \\frac{1}{1 + e^{-(w^Tx + b)}} \\] This is not to say LDA and LR are the same in binary case. LDA is a generative model which learns \\(p(x|y)\\) and \\(p(y)\\), LR is a discriminative model which only learns \\(p(y|x)\\). One can discriminate with a generative model, but not reversely.\nMulti-class Logistic Regression One way to train use Logistic Regression in multi-class classification in to for each class \\(i = {1, \\dots, C}\\), assign a weight vector \\(w^{(i)}\\), the probability is define by the softmax function: \\[ p(y = c|x) =\\frac{exp(w^{(c)}\\cdot x + b^{(c)})}{\\sum_{i=1}^Cexp(w^{(i)} \\cdot x + b^{(i)})} \\] Then \\(f_{LR_{multi-class}}(x) = \\arg \\max\\limits_{y \\in \\{1,\\dots,C\\}}p(y|x)\\)\nTo prevent overfitting, a prior distribution is usually added to \\(w\\). Suppose each dimension of \\(w\\) is independently sampled from a Gaussian distribution \\(\\mathcal N(0, \\frac{C}{2})\\), then, \\[ p(w) \\propto \\exp(-\\frac{1}{C}w^Tw) \\]\n","date":1655497979,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655497979,"objectID":"2daad7c28250e9d73e1b3180fce1e548","permalink":"https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/logistic-regression/","section":"notes","summary":"Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\).","tags":null,"title":"Logistic Regression","type":"book"},{"authors":null,"categories":null,"content":" \\(I_\\text{BA}\\) The very basic bound on the Mutual Information is based on the non-negativity of KL-divergence. \\[ \\begin{aligned} I(X;Y) = \u0026amp;\\E_{p(x,y)} \\log \\frac{p(x|y)}{p(x)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} + \\\\ \u0026amp;\\underbrace{\\E_{p(x,y)} \\log \\frac{p(x|y)}{q(x|y)}}_{\\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\\\ \\ge \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log q(x|y) + H(X) \\triangleq I_\\text{BA} \\end{aligned} \\] This bound is not usually tractable since \\(p(x)\\) and thus \\(H(X)\\) has no closed-form expression.\nThis bound is tight when \\(q(x|y) = p(x|y)\\).\n\\(I_\\text{UBA}\\) When \\(q(x|y)\\) is replaced with an unnormalized model, that is \\[ q(x|y) = \\frac{p(x)}{Z(y)} e^{f(x,y)}, \\text{where } Z(y) = \\E_{p(x)} e^{f(x,y)} \\] Substituting this back to the \\(I_\\text{BA}\\) will give \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} \\log \\frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\triangleq I_\\text{UBA} \\end{aligned} \\]\nNote that by scaling \\(q(x|y)\\) with \\(p(x)\\), the \\(p(x)\\) term is canceled.\nThis bound is tight when \\(q(x|y) = \\frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\\). That is \\[ \\begin{aligned} e^{f(x,y)} \u0026amp;= \\frac{p(x, y)}{p(x)p(y)} Z(y) \\\\ e^{f(x,y)} \u0026amp;= \\frac{Z(y)}{p(y)} p(y|x) \\\\ f(x,y) \u0026amp;= \\ln p(y|x) + \\underbrace{\\ln \\frac{Z(y)}{p(y)}}_{c(y)} \\end{aligned} \\]\n\\(I_\\text{DV}\\) By further applying Jensen’s inequality to the \\(\\E_{p(y)} Z(y)\\) term in \\(I_\\text{UBA}\\), we get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\log\\E_{p(y)} [Z(y)] \\triangleq I_\\text{DV} \\end{aligned} \\]\nThe original \\(I_\\text{DV}\\) bound is in effect derived from the variational lower bound of KL-divergence: \\[ \\begin{aligned} I\u0026amp;(X;Y) = D_\\text{KL}(p_{(x,y)} || p(x) \\otimes p(y)) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\log [\\E_{p(x) \\otimes p(y)} f(x,y)] \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\log [\\E_{p(y)} \\E_{p(x)} e^{f(x,y)}] \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\log \\E_{p(y)} Z(y) \\\\ \\end{aligned} \\]\n\\(I_\\text{TUBA}\\) \\(\\log\\) function also has the following property: \\[ \\begin{aligned} \\forall x,a\u0026gt;0,\\log(x) \u0026amp;\\le \\frac{x}{a} + \\log(a) - 1 \u0026amp;\\iff \\\\ a + a\\log(x) \u0026amp;\\le x + a\\log(a) \u0026amp;\\iff \\\\ a\\log(x) - x \u0026amp;\\le a\\log(a) - a \\\\ \\end{aligned} \\] Therefore, we can insert the inequality \\(\\log Z(y) \\le \\frac{Z(y)}{a(y)} + \\log a(y) - 1\\) into the \\(I_\\text{UBA}\\) to get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} [\\frac{Z(y)}{a(y)} + \\log a(y) - 1 \\big)] \\triangleq I_\\text{TUBA} \\end{aligned} \\]\nThis bound is tight when\nthe tight condition of \\(I_\\text{UBA}\\) holds: \\(f(x,y) = \\log p(y|x) + \\underbrace{\\log \\frac{Z(y)}{p(y)}}_{c(y)}\\), and the tight condition of \\(I_\\text{TUBA}\\) holds: \\(a(y) = Z(y)\\).  \\(I_\\text{NWJ}\\) By setting \\(a(y) = e\\) in \\(I_\\text{TUBA}\\), we get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} [\\frac{Z(y)}{a(y)} + \\log a(y) - 1 \\big)] \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - e^{-1}\\E_{p(y)} Z(y)\\triangleq I_\\text{NWJ} \\end{aligned} \\] Since \\(I_\\text{NWJ}\\) is a special case of \\(I_\\text{TUBA}\\), its bound is tight when \\(Z(y)\\) self-normalizes to \\(e\\). In this case \\[ \\begin{aligned} f(x,y) \u0026amp;= \\log p(y|x) + \\log \\frac{e}{p(y)} \\\\ \u0026amp;= 1 + \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= 1 + \\log \\frac{p(x|y)}{p(x)} \\end{aligned} \\]\n\\(I_\\text{NCE}\\) Our goal is to estimate the \\(I(X_1;Y)\\) given one sample from \\(p(x_1)p(y|x_1)\\) and \\(K-1\\) additional independent samples \\(x_{2:K} \\sim p^{K-1}(x_{2:K})\\). For any random variable \\(Z\\) that is independent from \\(X_1\\) and \\(Y\\), \\[ \\begin{aligned} I(X_1,Z;Y) \u0026amp;= \\E_{p(x_1,z,y)} \\frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\\\ \u0026amp;= \\E_{p(x_1,z,y)} \\frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\\\ \u0026amp;= I(X_1;Y) \\end{aligned} \\] Therefore, \\(I(X_1;Y) = I(X_1,X_{2:K};Y)\\). Bounding \\(I(X_1;Y)\\) becomes bounding \\(I(X_1,X_{2:K};Y)\\), which can be estimated using any of the preceding methods.\nSet the critic to \\(f(x_{1:K},y) = 1 + \\overbrace{\\log \\frac{e^{g(x,y)}} {a(y;x_{1:K})}}^{h(x_{1:K},y)}\\), where \\(x\\) is the sample among \\(x_{1:K}\\) that is paired with \\(y\\). Usually \\((x,y)_{1:K}\\) are sampled from the same marginal distribution of \\(\\tilde p(x,y)\\). \\(y\\) is then uniformly drawn among \\(y_{1:K}\\). Substitute these to the \\(I_\\text{NWJ}\\), we have \\[ \\label{infonce} \\begin{aligned} I(X_{1:K};Y) \u0026amp;\\ge \\E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\\E_{p(y)} Z(y) \\\\ \u0026amp;= \\E_{p(x_{1:K},y)} [1 + \\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \\E_{p(y)} [\\E_{p(x_{1:K})} e^{1 + \\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\\\ \u0026amp;= 1 + \\E_{p(x_{1:K})p(y|x_{1:K})} [\\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \\E_{p(y)} [e\\E_{p(x_{1:K})} \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\\\ \u0026amp;= 1 + \\E_{p(x_{1:K})p(y|x_{1:K})} [\\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \\E_{p(x_{1:K})p(y)} \\frac{e^{g(x,y)}}{a(y,x_{1:K})} \\\\ \\end{aligned} \\] Further set \\(a(y;x_{1:K}) = \\frac{1}{K} \\sum_{i=1}^K e^{g(x_i, y)}\\). Substitute this into the last term in equation …","date":1654178480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654178480,"objectID":"0d74aa602870085f61f54726c27c2b64","permalink":"https://chunxy.github.io/notes/papers/bounding-mutual-information/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/bounding-mutual-information/","section":"notes","summary":"\\(I_\\text{BA}\\) The very basic bound on the Mutual Information is based on the non-negativity of KL-divergence. \\[ \\begin{aligned} I(X;Y) = \u0026\\E_{p(x,y)} \\log \\frac{p(x|y)}{p(x)} \\\\ = \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\\\ = \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} + \\\\ \u0026\\underbrace{\\E_{p(x,y)} \\log \\frac{p(x|y)}{q(x|y)}}_{\\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\\\ \\ge \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} \\\\ = \u0026\\E_{p(x,y)} \\log q(x|y) + H(X) \\triangleq I_\\text{BA} \\end{aligned} \\] This bound is not usually tractable since \\(p(x)\\) and thus \\(H(X)\\) has no closed-form expression.","tags":null,"title":"Bounding Mutual Information","type":"book"},{"authors":null,"categories":null,"content":" The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).\nThe Cross Entropy of distribution \\(q\\) relative to a distribution \\(p\\) is defined as \\[ H(p||q) = -\\mathrm{E}_{x \\sim p} \\log q(x) \\]\n","date":1650904786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650904786,"objectID":"4beb35e5c9dceb8cc4a3971a051aded0","permalink":"https://chunxy.github.io/notes/articles/information-theory/cross-entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/cross-entropy/","section":"notes","summary":"The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).","tags":null,"title":"Cross Entropy","type":"book"},{"authors":null,"categories":null,"content":" If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent.\n\\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner. Quantitatively, smoothness assumes that \\(f\\) has a Lipschitz gradient. That means there exists an \\(L \u0026gt; 0\\) such that \\[ ||\\nabla f(x) - \\nabla f(y)||_2 \\le L||x - y||_2, \\forall x,y \\in \\mathrm{dom}(f) \\] We will say that such function is \\(L\\)-smooth or strongly smooth. \\(L\\)-smoothness is equivalent to \\[ f(y) \\le f(x) + (y - x)^T \\nabla f(x) + \\frac{L}{2}||y - x||^2_2 \\]\nProof  Further, \\(L\\)-smoothness is equivalent to \\(\\frac{L}{2}||x||^2_2 - f(x)\\) being convex.\nWith the smoothness, or the Lipschitz gradient, we can bound \\(f(y)\\) from above.\nWe will assume \\(L\\)-smoothness in all cases below. Meanwhile, the local minima (in convex case, the global minima) is denoted as \\(x^\\star\\).\nConvergence with Convexity With convexity, we can bound \\(f(y)\\) from below with linear approximation: \\[ f(y) \\ge f(x) + (y - x)^T \\nabla f(x) \\]\nConvergence Rate Fixed Step Size Now consider a \\(L\\)-smooth function \\(f\\). We adopt a fixed step size \\(\\alpha = \\frac{1}{L}\\) so that the update in each iteration will be \\[ x_{k+1} = x_k - \\frac{1}{L}\\nabla f(x) \\] By the definition of \\(L\\)-smoothness, \\[ \\label{diff} \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x_k) + (-\\frac{1}{L}\\nabla f(x_k))^T \\nabla f(x_k) + \\frac{L}{2}||-\\frac{1}{L}\\nabla f(x_k)||^2_2 \\\\ \u0026amp;= f(x_k) - \\frac{1}{2L}||\\nabla f(x_k)||^2_2 \\end{aligned} \\] \\(f(x_{k+1}) \\le f(x_k)\\) holds in each iteration.\nBy the convexity of \\(f\\), \\[ \\begin{gather} f(x^\\star) \\ge f(x_k) + (x^\\star - x_k)^T \\nabla f(x_k) \\\\ \\label{convexity} f(x_k) \\le f(x^\\star) + (x_k - x^\\star)^T \\nabla f(x_k) \\end{gather} \\] Substituting the result back to the right-hand side of equation \\(\\eqref{diff}\\), \\[ \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x^\\star) + (x_k - x^\\star)^T \\nabla f(x_k) - \\frac{1}{2M}||\\nabla f(x_k)||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \u0026amp;\u0026lt; (x_k - x^\\star)^T L(x_k - x_{k+1}) - \\frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \u0026amp;\u0026lt; \\frac{L}{2}(2(x_k - x^\\star)^T(x_k - x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\\\ \\end{aligned} \\] By using the fact that \\[ \\begin{aligned} ||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\\\ 2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2 \\end{aligned} \\] We have \\[ \\label{closer} f(x_{k+1}) - f(x^\\star) \\le \\frac{L}{2}(||x_k - x^\\star||^2_2 - ||x_{k+1} - x^\\star||^2_2) \\]\nThis indicates that \\(x_{k+1}\\) is closer to the global minima \\(x^\\star\\) than \\(x_k\\). In addition, \\[ \\begin{aligned} \\sum_{i=0}^k [f(x_{i+1}) - f(x^\\star)] \u0026amp;\\le \\sum_{i=0}^k \\frac{L}{2}(||x_i - x^\\star||^2_2 - ||x_{i+1} - x^\\star||^2_2) \\\\ \u0026amp;= \\frac{L}{2}(||x_0 - x^\\star||^2_2 - ||x_{k+1} - x^\\star||^2_2) \\\\ \u0026amp;\\le \\frac{L}{2}||x_0 - x^\\star||^2_2 \\end{aligned} \\]\nBecause \\(f(x_{i+1}) \\le f(x_i), \\forall i=0,\\dots,k\\), \\(f(x_{k+1}) \\le f(x_i), \\forall i=0,\\dots,k\\), then \\[ \\begin{aligned} k(f(x_{k+1}) - f(x^\\star)) \\le \\frac{L}{2}||x_0 - x^\\star||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \\le \\frac{L}{2k}||x_0 - x^\\star||^2_2 \\end{aligned} \\]\nVariant Step Size In real application scenario, it may be difficult to know \\(L\\) exactly. However, as long as we choose \\(\\alpha_k \\le \\frac{1}{L}\\) in each iteration, the convergence and the rate still holds. \\[ \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x_k) + (-\\alpha_k \\nabla f(x_k))^T \\nabla f(x_k) + \\frac{L}{2}||-\\alpha_k \\nabla f(x_k)||^2_2 \\\\ \u0026amp;= f(x_k) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ f(x_{k+1}) \u0026amp;\\le f(x_k) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} f(x_{k+1}) - f(x^\\star) \u0026amp;\\le f(x_k) - f(x^\\star) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ \\end{aligned} \\]\nLet \\(\\eta_{k+1} = f(x_{k+1}) - f(x^\\star) \\ge 0\\), the above becomes \\(\\eta_{k+1} \\le \\eta_{k} - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2\\). From equation \\(\\eqref{convexity}\\), \\[ \\begin{gather} \\eta_k = f(x_k) - f(x^\\star) \\le (x_k - x^\\star)^T \\nabla f(x_k) \\le ||x_k - x^\\star||_2||\\nabla f(x_k)||_2 \\\\ 0 \\le \\frac{\\eta_k}{||x_k - x^\\star||_2} \\le ||\\nabla f(x_k)||_2 \\\\ -||\\nabla f(x_k)||^2_2 \\le -\\frac{\\eta^2_k}{||x_k - x^\\star||^2_2} \\\\ \\end{gather} \\] Substituting it back to give \\[ \\eta_{k+1} \\le \\eta_k - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k) \\eta^2_k}{||x_k - x^\\star||^2_2} \\] From equation \\(\\eqref{closer}\\) we already have \\[ ||x_{k+1} - x^\\star||^2_2 \\le ||x_k - x^\\star||^2_2 \\le \\dots \\le ||x_0 - x^\\star||^2_2 \\] Thus, \\[ \\begin{aligned} \\eta_{k+1} \u0026amp;\\le \\eta_k - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k) \\eta^2_k}{||x_0 - x^\\star||^2_2} \\\\ \\frac{1}{\\eta_k} \u0026amp;\\le \\frac{1}{\\eta_{k+1}} - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k)}{||x_0 - x^\\star||^2_2} \\cdot \\frac{\\eta_k}{\\eta_{k+1}} \\\\ \\frac{1}{\\eta_{k+1}} - \\frac{1}{\\eta_k} \u0026amp;\\ge \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k)}{||x_0 - x^\\star||^2_2} \\cdot …","date":1641932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641932800,"objectID":"376c072d7898120d37b9443aa2595ee3","permalink":"https://chunxy.github.io/notes/articles/optimization/gradient-descent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/gradient-descent/","section":"notes","summary":"If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent.\n\\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner.","tags":null,"title":"Gradient Descent","type":"book"},{"authors":null,"categories":null,"content":" 事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\(B\\)，如果\\(A\\)发生时B也发生，则称\\(B\\)包含\\(A\\)，记作\\(A \\subseteq B\\)。如果\\(A,B\\)互相包含，则二者相等。\n事件的互斥和对立 若两事件\\(A\\)和\\(B\\)不能再同一试验中同时发生，则称它们是互斥的。互斥事件的一个特例是对立事件，对于事件\\(A\\)，其对立事件\\(\\bar A\\)为\\(\\{ \\text{$A$不发生} \\}\\)。\n事件的和（并）与加法定理 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$发生，或$B$发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的和，记作\\(C = A + B\\)。\n概率论中的加法定理描述的则是：若干互斥事件之和的概率，等于各事件的概率之和。\n全概率公式 设\\(B_1, B_2, \\cdots\\)为有限或无限个事件，它们两两互斥且在每次实验中至少发生一个（mutually exclusive and collectively exhaustive），即 \\[ \\begin{gather} B_i B_j = \\emptyset (i \\ne j) \\\\ B_1 + B_2 + \\cdots = \\Omega \\\\ P(B_1 + B_2 + \\cdots) = P(\\Omega) = 1 \\end{gather} \\] 根据事件运算的性质，\\(A = A \\Omega = A B_1 + A B_2 + \\cdots\\)，由于\\(B_1, B_2, \\cdots\\)两两互斥，则显然\\(A B_1, A B_2, \\cdots\\)也两两互斥，故依据加法定理，有 \\[ P(A) = \\sum_i P(A B_i) = P(A B_1) + P(A B_2) + \\cdots \\] 再根据条件概率公式，有 \\[ P(A) = \\sum_i P(B_i) P(A | B_i) = P(B_1) P(A|B_1) + P(B_2) P(A|B_2) + \\cdots \\]\n这便是全概率公式。\n事件的积（交） 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$，$B$都发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的积，记作\\(C = A B\\)。\n事件的差 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$发生，$B$不发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的差，记作\\(C = A - B\\)。\n我们对事件引入了和、积、差、对立运算。显然，这些运算符在数字运算中成立的运算规律，不一定对事件运算成立，比如说\\(A + A = A\\)而非\\(2 A\\)（无意义），\\(A A = A\\)而非\\(A^2\\)（无意义），\\((A - B) + B = A + B\\)而不是\\(A\\)。\n基于这些基本运算，我们也可以表示出更多的事件，比如：\n  表达式 含义    \\(ABC\\) 三者同时发生  \\(A B \\bar C + A B \\bar C + \\bar A B C\\) 三者有且仅有两件发生  \\(A \\bar B \\bar C + \\bar A B \\bar C + \\bar A \\bar B C\\) 三者有且仅有一件发生  \\(A + B + C\\) 三者至少其中之一发生    条件概率与独立性 设两个事件\\(A\\)和\\(B\\)，且\\(P(B) \\ne 0\\)，记\\(P(A|B)\\)为“在给定\\(B\\)发生的条件下\\(A\\)的条件概率”，则 \\[ P(A|B) = \\frac{P(AB)}{P(B)} \\] 在计数测度之下，这个式子很好证明。记\\(M_A\\)为使得事件\\(A\\)发生的基本事件个数，记\\(M_B\\)为使得事件\\(B\\)发生的基本事件个数，记\\(M_{AB}\\)为使得事件\\(A\\)和事件\\(B\\)同时发生的基本事件个数，记\\(M\\)为所有基本事件个数，则 \\[ P(A|B) = \\frac{M_{AB}}{M_B} = \\frac{M_{AB}/M}{M_B/M} = \\frac{P(AB)}{P(B)} \\]\n两个事件的独立 设两个事件\\(A\\)和\\(B\\)，\\(P(A)\\)和\\(P(A|B)\\)可能是有差异的，这个差异便反映了二者之间的关联。而如果\\(P(A) = P(A|B)\\)，则称这两事件独立。根据条件概率的定义，两事件\\(A\\)、\\(B\\)独立时，有 \\[ P(AB) = P(A) P(B) \\] 我们往往是根据事件属性，而非根据以上公式，来确定事件的独立性。比如说在连续抛两次硬币的试验中，这两次试验的结果之间确实不应该有什么关联，我们自然而然地认为它们之间是相互独立的（相互独立同分布）。\n有时我们会觉得所有的独立事件可能都来自于这样的多次相互独立同分布试验，但其实独立事件也可以来自一次试验，比如说从52张扑克牌中随机抽取一张，记事件\\(A\\)为抽中红桃花色、事件\\(B\\)为抽中数字6，则可以验证，这两个事件也是相互独立的。\n相互独立与乘法定理 设\\(A_1, A_2, \\cdots\\)为有限或无限个事件，如果从其中任意取出有限个\\(A_{i_1}, A_{i_2}, \\cdots, A_{i_m}\\)，都有 \\[ P(A_{i_1} A_{i_2} \\cdots A_{i_m}) = P(A_{i_1}) P(A_{i_2}) \\cdots P(A_{i_m}) \\] 则称事件\\(A_1, A_2, \\cdots\\)相互独立。概率论中的乘法定理描述的是：多个事件相互独立时，它们同时发生的概率，等于各自概率的乘积。\n需要注意的是，多个事件之间两两独立并不意味着它们相互独立，比如在掷两次硬币的实验中，定义以下事件： \\[ \\begin{gathered} A：第一次为正， P(A) = 1/2 \\\\ B：第二次为反， P(B) = 1/2 \\\\ C：两次结果相同， P(C) = 1/2 \\end{gathered} \\] 则可以轻易验证三者两两独立，但是\\(P(ABC) = 0 \\ne 1/8 = P(A) P(B) P(C)\\)。这三者的关系有如下图中的三个环：两两之间本可以相互分开（独立），但三者同在，便互相捆绑住了。\n","date":1670607435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670607435,"objectID":"7a2c5bc1dcd6160b647f0fd472d51543","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/","section":"notes","summary":"事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\","tags":null,"title":"事件与概率","type":"book"},{"authors":null,"categories":null,"content":" Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable. It is defined as \\[ I(X;Y) = \\sum_{(x,y) \\in X \\times Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] By its definition, \\[ \\begin{gather} I(X;Y) = I(Y;X) \\\\ I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \\otimes p_Y) \\\\ I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\end{gather} \\]\nGaussian Case To better illustrate the formula of mutual information between two Gaussian-distributed random variables \\(X\\) and \\(Y\\). We can concatenate them to form, say an \\(n\\)-dimensional random variable \\(Z\\), which is also Gaussian-distributed. Then the mutual information between \\(X\\) and \\(Y\\) can be computed as: \\[ I(X;Y) = \\frac 1 2 \\log \\frac{\\det \\Sigma_X \\det \\Sigma_Y}{\\det \\Sigma_Z} \\] The key to the derivation is that mutual information is the KL-divergence between the joint distribution and the product of the marginal distributions.\nThe joint can be described as \\[ p_{X:Y} = N(\\underbrace{\\mu_X:\\mu_Y}_\\mu, \\underbrace{ \\begin{bmatrix} \\ \\Sigma_{X} \u0026amp; \\Cov_{XY} \\\\ \\Cov_{YX} \u0026amp; \\Sigma_{Y} \\\\ \\end{bmatrix} }_\\Sigma ) \\]\nThe product of marginals can be described as \\[ p_{X} \\times p_{Y} = N(\\mu_x:\\mu_y, \\begin{bmatrix} \\ \\Sigma_{xx} \u0026amp; 0 \\\\ 0 \u0026amp; \\Sigma_{yy} \\\\ \\end{bmatrix} ) \\] The probability density function of an \\(n\u0026#39;\\)-dimensional Gaussian distribution is \\(p(x\u0026#39;) = \\frac{1}{\\sqrt{|2\\pi \\Sigma\u0026#39;|}}e^{-\\frac{1}{2}(x\u0026#39;-\\mu\u0026#39;)^T\\Sigma\u0026#39;^{-1}(x\u0026#39;-\\mu\u0026#39;)}\\). The entropy of this Gaussian distribution is \\(\\frac 1 2 n\u0026#39; + \\frac 1 2 \\log |2\\pi\\Sigma\u0026#39;|\\). In view of above, \\[ \\begin{aligned} I\u0026amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \\times p_Y) = \\int p_{X:Y}(\\underbrace{x:y}_z) \\log \\frac{p_{X:Y}(x:y)} {p_X(x) p_{Y}(y)} \\d z \\\\ \u0026amp;= \\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_{X:Y}(x:y) \\d z - \\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_X(x) \\d z \\\\ \u0026amp;\\quad\\quad\\quad -\\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_Y(y) \\d z \\\\ \u0026amp;= \\int p_{Z}(z) \\log p_{Z}(z) \\d z - \\int p_{X}(x) \\log p_X(x) \\d x - \\int p_{Y}(y) \\log p_Y(y) \\d y \\\\ \u0026amp;= -(\\log \\sqrt{\\det(2\\pi \\Sigma)} + \\frac n 2) + (\\log \\sqrt{\\det(2\\pi \\Sigma_{X}}) + \\frac {n_X} 2) \\\\ \u0026amp;\\quad\\quad\\quad +(\\log \\sqrt{\\det(2\\pi \\Sigma_{Y}}) + \\frac {n_Y} 2) \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\det \\Sigma_X \\det \\Sigma_Y}{\\det \\Sigma_Z} \\end{aligned} \\]\nKronecker Gaussian Consider the multivariate Gaussian distribution random vector \\(X_k\\) and \\(Y_k\\) of the same length \\(k\\). Suppose they are both independent internally and they have the component-wise correlation \\(corr(X_i, Y_j) = \\delta_{ij} \\rho\\), where \\(\\rho \\in (-1, 1)\\) (open to ensure the covariance matrix is invertible), \\(1 \\le i, j \\le k\\), \\(\\delta_{ij}\\) is the Kronecker’s delta: \\[ \\delta_{ij} = \\begin{cases} 0, \u0026amp; i \\ne j \\\\ 1, \u0026amp; i = j \\end{cases} \\] Let \\(Z_k\\) be the vector concatenated by \\(X_k\\) and \\(Y_k\\). It is easy to draw its covariance matrix \\(\\Sigma_{Z_k}\\) like \\[ \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{pmatrix} \\] The mutual information between the \\(X_k\\) and \\(Y_k\\) can be calculated as \\[ I(X;Y) = \\frac 1 2 \\log \\frac{\\det \\Sigma_{X_k} \\det \\Sigma_{Y_k}}{\\det \\Sigma_{Z_k}} = -\\frac 1 2 \\log \\det \\Sigma_{Z_k} \\] The problem remains as how to compute \\(\\det \\Sigma_{Z_{k}}\\). After applying the Laplacian expansion along the first column, it remains to deal with the determinants of following two matrices (dashed lines rule out the row/column to be deleted): \\[ \\begin{gather} A_k = \\underbrace{ \\left( \\begin{array}{c:ccccccc} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\hdashline 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{array} \\right) }_\\text{$2k$ columns}, B_k = \\underbrace{ \\left( \\begin{array}{c:ccccccc} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\hdashline \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\hdashline 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{array} \\right) }_\\text{$2k$ columns}, \\\\ \\det Z_k = \\left. …","date":1652962804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652962804,"objectID":"fa797733f5af1922ab9b2c6011dfcf35","permalink":"https://chunxy.github.io/notes/articles/information-theory/mutual-information/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/mutual-information/","section":"notes","summary":"Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable.","tags":null,"title":"Mutual Information","type":"book"},{"authors":null,"categories":null,"content":" 定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),g(x)\\)，如果它们各阶导数相等的越多，它们就越相似，换言之 \\[ \\text{各阶导数都相同} \\Rightarrow f(x) = g(x) \\] 可以说，函数的各阶导数即是它们的特征。\n对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即 \\[ \\text{各阶矩都相同} \\Rightarrow \\text{随机变量对应的分布相同} \\] 对于随机变量\\(X\\)，其特征函数定义为 \\[ \\varphi(t) = \\E[e^{itX}] \\] \\(e^{itX}\\)的泰勒级数为 \\[ e^{itX} = 1 + \\frac{itX}{1!} - \\frac{t^2X^2}{2!} + \\dots + \\frac{(itX)^n}{n!} \\] 代入特征函数可得 \\[ \\begin{aligned} \\varphi(t) \u0026amp;= \\E[1 + \\frac{itX}{1!} - \\frac{t^2X^2}{2!} + \\dots + \\frac{(itX)^n}{n!}] \\\\ \u0026amp;= \\E[1] + \\E[\\frac{itX}{1!}] - \\E[\\frac{t^2X^2}{2!}] + \\dots + \\E[\\frac{(itX)^n}{n!}] \\\\ \u0026amp;= 1 + \\frac{it \\overbrace{\\E[X]}^\\text{一阶矩} }{1!} - \\frac{t^2 \\overbrace{\\E[X^2]}^\\text{二阶矩} }{2!} + \\dots + \\frac{(it)^n \\overbrace{\\E[X^n]}^\\text{n阶矩} }{n!} \\\\ \\end{aligned} \\] 可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。\n理性认知 \\[ \\varphi(t) = \\E[e^{itX}] = \\int_{-\\infty}^{+\\infty} e^{itx} p(x)\\; dx \\]\n而对\\(p(x)\\)进行逆傅里叶变换可得 \\[ F(t) = \\int_{-\\infty}^{+\\infty} p(x) e^{-itx} dx \\] 可见二者互为共轭关系： \\[ \\varphi(t) = \\overline{F(t)} \\]\n应用 通过求\\(t = 0\\)时的各阶导数，可以快速求得各阶矩： \\[ \\varphi^{(k)}(0) = i^k \\E[X^k] \\]\n参考 特征函数的理解\n","date":1652097068,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652097068,"objectID":"abdc103820e24c5967145fdb79e8db20","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/","section":"notes","summary":"定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),","tags":null,"title":"特征函数","type":"book"},{"authors":null,"categories":null,"content":" Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary. These points are called support vectors.\nHard Margin SVM Suppose the data \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}), i=1, \\dots, M\\}\\), and is linearly separable. The separation hyperplane will be in the form of \\(w^Tx + b = 0\\). Let \\(y^{(i)} = 1\\) when \\(x^{(i)}\\) is above the hyperplane (\\(w^Tx^{(i)} + b \u0026gt; 0\\)), \\(y^{(i)} = -1\\) when \\(x^{(i)}\\) is below the hyperplane (\\(w^Tx^{(i)} + b \u0026lt; 0\\)).\nThe distance (margin) from a data point to the separation hyperplane will be \\[ d^{(i)} = \\frac{|w^Tx^{(i)} + b|}{||w||_2} = \\frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||_2}\\\\ \\] \\(d^{(i)}\\) is called the geometric distance, while \\(|w^Tx^{(i)} + b|\\) is called the functional distance.\nThe margin of the hyperplane \\(w^Tx + b = 0\\) will be \\[ d = \\min\\limits_{i \\in \\{1,\\dots,M\\}} d^{(i)} = \\min\\limits_{i \\in \\{1,\\dots,M\\}} \\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2} \\] The maximum margin solution is found by solving \\[ \\max\\limits_{w,b} \\min\\limits_{i \\in \\{1,\\dots,M\\}} \\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2} \\] Suppose \\((w^\\prime, b^\\prime)\\) is one solution to the above. Then \\((\\lambda w^\\prime, \\lambda b^\\prime)\\) is also a solution, because \\[ \\frac{y^{(i)}(\\lambda (w^{\\prime})^Tx^{(i)} + \\lambda b^\\prime)}{||\\lambda w^\\prime||_2} = \\frac{\\lambda y^{(i)}((w^{\\prime})^Tx^{(i)} + b^\\prime)}{\\lambda ||w^\\prime||_2} = \\frac{y^{(i)}((w^{\\prime})^Tx^{(i)} + b^\\prime)}{||w^\\prime||_2} \\] There is an extra degree of freedom in this problem. Therefore, we may impose \\[ \\min\\limits_{i \\in \\{1,\\dots,M\\}} y^{(i)}(w^Tx^{(i)}+b) = 1 \\] to consume this freedom. Consequently, the problem becomes \\[ \\begin{gather} \\max \\frac{1}{||w||_2} \\iff \\min \\frac{1}{2}||w||^2_2 \\\\ s.t.\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1, i = 1,\\dots,M \\end{gather} \\]\nSoft Margin SVM It may not happen that the real data are linearly-separable, or there may exist noisy samples that disrupt this linear separability. In such case, we may allow some samples to violate the margin. We define some slack variables \\(\\xi_i \\ge 0\\). \\(\\xi_i \u0026gt; 0\\) means that the sample is inside the margin (or even this sample will be misclassified), \\(\\xi_i = 0\\) means that the sample is outside the margin.\nOf course, these slack variables should be as small as possible. Then the problem becomes \\[ \\begin{gather} \\min \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M\\xi_i \\\\ s.t.\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1 - \\xi_i, \\xi_i \\ge 0, i=1,\\dots,M \\end{gather} \\] \\(C\\) is a penalty factor on violating the margin. To some extent, it avoid overfitting.\nTo solve this, first convert the problem into the standard form: \\[ \\begin{aligned} \\min \\frac{1}{2}||w||^2_2 \u0026amp;+ C\\sum_{i=1}^M\\xi_i \\\\ s.t.\\quad 1 - \\xi_i - y^{(i)}(w^Tx^{(i)} + b) \u0026amp;\\le 0, i=1,\\dots,M \\\\ -\\xi_i \u0026amp;\\le 0, i=1,\\dots,M \\end{aligned} \\] This problem gives a strong duality. The solution will satisfy the KKT conditions. We first write down the Lagrangian: \\[ L(w,b,\\xi,\\lambda,\\mu) = \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M\\xi_i + \\sum_{i=1}^M\\lambda_i(1 - \\xi_i - y^{(i)}(w^Tx^{(i)} + b)) - \\sum_{i=1}^M\\mu_i\\xi_i \\] Then we form the dual problem: \\[ \\max_{\\lambda,\\mu;\\lambda_i,\\mu_i \\ge 0} \\min_{w,b,\\xi}L(w,b,\\xi,\\lambda,\\mu) \\] Take derivative w.r.t. \\(w\\), \\(b\\), and \\(\\xi\\) to give \\[ \\begin{gather} \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^M\\lambda_iy^{(i)}x^{(i)}, \\frac{\\partial^2 L}{\\partial w\\partial w} = I \\succeq 0 \\\\ \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^M\\lambda_iy^{(i)}, \\frac{\\partial^2 L}{\\partial b\\partial b} = 0 \\succeq 0 \\\\ \\frac{\\partial L}{\\partial \\xi} = C - \\lambda - \\mu, \\frac{\\partial^2 L}{\\partial \\xi\\partial \\xi} = 0 \\succeq 0 \\end{gather} \\] The Hessian matrices of \\(L\\) w.r.t. \\(w\\), \\(b\\), and \\(\\xi\\) are positive semi-definite. Therefore, \\(\\min\\limits_{w,b,\\xi}L(w,\\xi,\\lambda,\\mu)\\) is obtained at its local minimum, i.e. where its first-order derivative meets \\(0\\): \\[ w = \\sum_{i=1}^M\\lambda_iy^{(i)}x^{(i)} \\\\ \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ \\lambda + \\mu = C \\] Substitute above back to \\(L\\) to transform the dual problem into \\[ \\begin{gather} \\max_{\\lambda,\\mu} \\frac{1}{2}\\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + C\\sum_{i=1}^M\\xi_i - \\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + \\sum_{i=1}^M\\lambda_i(1 - \\xi_i - y^{(i)}b) + \\sum_{i=1}^M(\\lambda_i - C)\\xi_i \\\\ s.t.\\quad \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ \\lambda_i,\\mu_i \\ge 0, i=1,\\dots,M \\\\ \\Downarrow \\\\ \\max_{\\lambda,\\mu}D(\\lambda,\\mu) = -\\frac{1}{2}\\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + \\sum_{i=1}^M\\lambda_i \\\\ s.t.\\quad \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ 0 \u0026lt; \\lambda_i \u0026lt; C, i=1,\\dots,M \\\\ \\end{gather} \\] Since we know the optimal solution exists, instead of taking derivative of \\(D\\) w.r.t. …","date":1641559935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559935,"objectID":"8cc4f6f8501a1451db0abf04dd53210c","permalink":"https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/support-vector-machine/","section":"notes","summary":"Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary.","tags":null,"title":"Support Vector Machine","type":"book"},{"authors":null,"categories":null,"content":" Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima? That is, does \\[ \\forall d, i,f(x + d \\cdot e_i) \\ge f(x) \\Rightarrow f(x) = \\min_zf(z), \\text{ where $e_i$ is the $i$-th standard basis ?} \\] The answer is yes. This is because \\[ \\nabla f(x) = \\left[ \\begin{array} \\\\ \\frac{\\partial f}{\\partial x_1}, \\cdots, \\frac{\\partial f}{\\partial x_n} \\end{array} \\right] = 0 \\]\nConvex and Non-differentiable Function, but Non-smooth Separable If \\(f\\) is only convex but not differentiable, the above will not necessarily hold. However, if \\[ f(x) = g(x) + \\sum_{i=1}^nh_i(x_i) \\] where \\(g(x)\\) is convex and differentiable, and each \\(h_i(x_i)\\) is convex, the global minima still holds. For any \\(y\\), \\[ \\begin{aligned} f(y) - f(x) \u0026amp;= g(y) - g(x) + \\sum_{i=1}^nh_i(y_i) - \\sum_{i=1}^nh_i(x_i) \\\\ \u0026amp;\\ge \\nabla g(x)^T(y - x) + \\sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\\\ \u0026amp;= \\sum_{i=1}^n[\\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \\end{aligned} \\] Because \\(f(x)\\) obtains minimum along each axes, for any \\(d\\) and \\(k\\), \\[ \\begin{aligned} f(x + d \\cdot e_k) \u0026amp;\\ge f(x) \\\\ g(x + d \\cdot e_k) + \\sum_{i=1,i\\ne k}^nh_i(x_i) + h_k(x_k + d) \u0026amp;\\ge g(x) + \\sum_{i=1}^nh_i(x_i) \\\\ g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) \u0026amp;\\ge 0 \\\\ \\end{aligned} \\] By the linearity of subgradient, \\(0 \\in \\partial(g_k + h_k) = \\nabla_k g + \\partial h_k \\Rightarrow -\\nabla_k g \\in \\partial h_k\\). That is \\[ h_k(y_k) - h_k(x_k) \\ge -\\nabla_k g(x)(y_k - x_k) \\] Then, \\[ f(y) - f(x) = \\sum_{i=1}^n[\\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \\ge 0 \\]\nCoordinate Descent.pdf\nCoordinate Descent in One Line, or Three if Accelerated | A Butterfly Valley (wordpress.com)\n","date":1641221431,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641221431,"objectID":"90f41cce8011359b9ca36fe147586b08","permalink":"https://chunxy.github.io/notes/articles/optimization/coordinate-descent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/coordinate-descent/","section":"notes","summary":"Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima?","tags":null,"title":"Coordinate Descent","type":"book"},{"authors":null,"categories":null,"content":" FlatNCE provides a way to compute the gradient of InfoNCE without introducing the rounding error when subtracting between two similar numbers.\nSpecifically, let \\(g^\\ominus_{ij}\\) is the affinity score between reference sample \\(x_i\\) and negative (noise) sample \\(y\u0026#39;_j\\); \\(g^\\oplus_{ii}\\) is the affinity score between positive sample and itself/its transformation \\(y_j\\); \\(i\\) is the batch index. Denote by \\(\\hat l_\\text{InfoNCE}\\) the batch estimate of the loss from InfoNCE: \\[ \\newcommand{detach}{\\mathop{\\text{detach}}} \\newcommand{logsumexp}{\\mathop{\\text{logsumexp}}} \\begin{gather} \\hat l_\\text{InfoNCE} = \\logsumexp_j g^\\ominus_{ij} - g^\\oplus_{ii} = \\log (\\sum_{j \\ne i} \\exp g^\\ominus_{ij} ) -g^\\oplus_{ii} \\\\ \\end{gather} \\]\nUsually, the above is calculated as \\[ \\hat l_\\text{InfoNCE} = \\log[\\sum_{j \\in {\\oplus, \\ominus} } \\exp(g^\\ominus_{ij} - \\max_k g^\\ominus_{ik}) ] + \\max_k g^\\ominus_{ik} - g^\\oplus_{ii} \\] When the learning saturates, \\(\\hat l_\\text{InfoNCE}\\) goes to \\(0\\), which means \\(\\max_k g^\\ominus_{ik}\\) becomes \\(g^\\oplus_{ii}\\) and thus \\[ \\hat l_\\text{InfoNCE} = \\log[\\sum_{j \\in {\\oplus, \\ominus} } \\exp(g^\\ominus_{ij} - g^\\oplus_{ii}) ] + \\underbrace{g^\\oplus_{ii} - g^\\oplus_{ii} }_{\\text{error-prone}} \\] A rounding error will very likely happen when subtracting two near numbers. Such error will accumulate and fail the InfoNCE. As said, FlatNCE provides a way to circumvent this rounding error.\nGradient Perspective Denote by \\(\\hat l_\\text{FlatNCE}\\) the batch estimate of the negative loss from FlatNCE: \\[ \\begin{aligned} \\hat l_\\text{FlatNCE} \u0026amp;= \\exp [ \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) - \\detach \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) ] \\\\ \u0026amp;= \\frac{\\exp \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) } {\\detach [ \\exp \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) ] } \\\\ \u0026amp;= \\frac{\\exp \\log \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ]} {\\detach \\{\\exp \\log \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\\\ \u0026amp;= \\frac{\\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ]} {\\detach \\{ \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\end{aligned} \\]\nBy putting the positive sample into the contrasting samples, \\[ \\hat l_\\text{FlatNCE}^\\oplus = \\frac{1 + \\sum_j \\exp \\big( g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) \\big)} {1 + \\text{detach}[\\sum_j \\exp \\big( g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) \\big)]} \\] where the \\(1\\) comes from adding the positive sample \\(y_i\\) to the set of negative samples (let’s denote this “negative” sample by \\(y\u0026#39;_0\\)). It can be easily found that\n\\[ \\nabla_\\theta \\hat l_\\text{FlatNCE}^\\oplus (g_\\theta) = \\nabla_\\theta \\hat l_\\text{InfoNCE} (g_\\theta) \\]\nWe may further find that the gradient of FlatNCE is an importance-weighted estimator of the form \\[ \\begin{aligned} \\nabla_\\theta \\hat l^\\oplus_\\text{FlatNCE} \u0026amp;= \\frac{\\sum_{j \\ne i} \\{ \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] [\\nabla_\\theta g_\\theta(x_i, y\u0026#39;_j) - \\nabla_\\theta g_\\theta(x_i, y_i)] \\} } {\\detach \\{ \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\\\ \u0026amp;= \\frac{\\sum_{j \\ne i} \\{ \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] [\\nabla_\\theta g_\\theta(x_i, y\u0026#39;_j) - \\nabla_\\theta g_\\theta(x_i, y_i)] \\} } { \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] } \\\\ \u0026amp;= \\sum_{k \\ne i} \\left\\{ \\underbrace {\\frac{ \\exp [ g_\\theta(x_i, y\u0026#39;_k) ] } { \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] } }_{w_k} \\nabla_\\theta g_\\theta(x_i, y\u0026#39;_k) \\right\\} - \\nabla_\\theta g_\\theta(x_i, y_i) \\\\ \\end{aligned} \\] As the learning progresses, \\(w_k\\)’s other than \\(w_0\\) will go to \\(0\\); \\(w_0\\) will go to \\(1\\), which will cause the gradient to vanish.\nLower-bound Perspective \\(-\\hat l_\\text{InfoNCE}\\) and \\(-\\hat l_\\text{FlatNCE}\\) are part of the lower bounds to the mutual information in two methods. Given \\(y_0\\) the positive sample and \\(y_{j\u0026gt;0}\\) are the negative samples,\n\\[ \\label{lemma3.3} \\begin{aligned} -\\hat l^{K, \\theta}_\\text{InfoNCE} \u0026amp;= -\\log \\{ \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] \\} \\\\ \u0026amp;= \\sup_v (v \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] - (-1 - \\log (-v)) \\\\ \u0026amp;\\Downarrow_{v = -e^{-u}} \\\\ \u0026amp;\\ge -e^{-u} \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] - (-1 - \\log (e^{-u}) \\\\ \u0026amp;= 1 - u - \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0) - u] \\end{aligned} \\]\nConsider \\(g_\\theta\\) as the primal critic and \\(u\\) as the dual critic. Since arbitrary choice of \\(u\\) and \\(g_\\theta\\) lower-bounds the mutual information, we can either jointly optimize \\(u\\) and \\(g_\\theta\\) or more preferably, train in an iterative fashion. Given \\(\\theta\\), set \\(u\\) to\n\\[ \\hat u(g_\\theta) = \\log ({\\frac 1 K \\sum_j \\exp[g_\\theta(x,y_j) - g_\\theta(x, y)]}) \\] Then we fix \\(u\\) and only update \\(\\theta\\). Because \\(u\\) is fixed, the only gradient comes from \\(g_\\theta\\). Plugin \\(\\hat u\\) to the right-hand side of …","date":1661636533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661636533,"objectID":"9eb397cbcf75e4bf725926644c9ccca3","permalink":"https://chunxy.github.io/notes/papers/flatnce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/flatnce/","section":"notes","summary":"FlatNCE provides a way to compute the gradient of InfoNCE without introducing the rounding error when subtracting between two similar numbers. Specifically, let \\(g^\\ominus_{ij}\\) is the affinity score between reference","tags":null,"title":"FlatNCE","type":"book"},{"authors":null,"categories":null,"content":" Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix.\nTo do so, we shall first apply the diagonalization to \\(\\Sigma_X\\): \\[ \\Sigma_X \\Phi = \\Phi \\Lambda \\] where \\(\\Phi\\) contains the normalized eigenvectors, \\(\\Lambda\\) is diagonal and contains the eigenvalues. Now let \\(Y = \\Phi X\\), we can verify that \\[ \\begin{aligned} \\Sigma_Y \u0026amp;= \\E \\{ \\Phi^T (\\x - \\mu_X) [\\Phi^T (\\x - \\mu_X)]^T \\} \\\\ \u0026amp;= \\E [\\Phi^T (\\x - \\mu_X) (\\x - \\mu_X)^T \\Phi] \\\\ \u0026amp;= \\Phi^T \\E [(\\x - \\mu_X) (\\x - \\mu_X)^T] \\Phi \\\\ \u0026amp;= \\Phi^T \\Sigma_X \\Phi = \\Phi^T \\Phi \\Lambda = \\Lambda \\end{aligned} \\] which is diagonal. To further make an identity matrix, we apply \\(Z = \\Lambda^{-1/2} Y = \\Lambda^{-1/2} \\Phi^T X\\) to give \\[ \\begin{aligned} \\Sigma_Z \u0026amp;= \\E \\{\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X) [\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X)]^T \\} \\\\ \u0026amp;= \\E [\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X) (\\x - \\mu_X)^T \\Phi \\Lambda^{-1/2}] \\\\ \u0026amp;= \\Lambda^{-1/2} \\Phi^T \\Sigma_X \\Phi \\Lambda^{-1/2} = I \\end{aligned} \\] Data whitening in Gaussian case is discussed here.\n","date":1660240322,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660240322,"objectID":"decbace4652c6f21d2d203edd7df5350","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/whitening/","section":"notes","summary":"Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix.","tags":null,"title":"Whitening","type":"book"},{"authors":null,"categories":null,"content":" KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\).\nIn information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).\nIn discrete form, \\[ D_\\text{KL}(p \\| q) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log \\frac{q(x)}{p(x)} = \\sum_{x \\in \\mathcal{X}} p(x)\\log \\frac{p(x)}{q(x)} \\] In continuous form, \\[ D_\\text{KL}(p \\| q) = -\\int_{x \\in \\mathcal{X}} p(x) \\log \\frac{q(x)}{p(x)}dx = \\int_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)}dx \\]\nVariational Lower-bound One property of KL-divergence is \\[ D_\\text{KL}(p || q) = \\sup_{T: \\Omega \\to \\R} \\E_{p} [T] - \\log (\\E_q[e^T]) \\]\nThe proof is as follows. Given a distribution \\(q\\) and a function \\(T\\), construct the Gibbs distribution \\(g\\) such that \\(g(x) = \\frac{q(x)e^{T(x)}}{Z}\\) where \\(Z = \\E_{q(x)} e^{T(x)}\\). Then \\[ \\begin{aligned} \\E\u0026amp;_{p(x)} T(x) - \\log Z = \\E_{p(x)} [T(x) - \\log Z] \\\\ \u0026amp;= \\E_{p(x)} [\\log e^{T(x)} - \\log \\E_{q(x)} e^{T(x)}] \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{e^{T(x)}} {\\E_{q(x)} e^{T(x)}} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{q(x) e^{T(x)}} {q(x) \\E_{q(x)} e^{T(x)}} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{g(x)} {q(x)} \\\\ \\end{aligned} \\] Finally KL-divergence minus above gives \\[ \\begin{aligned} \u0026amp;D_\\text{KL}(p || q) - (\\E_{p(x)} T(x) - \\log Z) \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{p(x)} {q(x)} - \\E_{p(x)} \\log \\frac{g(x)} {q(x)} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{p(x)} {g(x)} \\triangleq D_\\text{KL}(p || g) \\ge 0 \\end{aligned} \\]\nGaussian Case Suppose \\(X\\) and \\(Y\\) are random variables, both of some \\(n\\)-dimensional Gaussian distribution. Then the KL-divergence between them can be formulated as: \\[ \\begin{aligned} D_\\text{KL}(p_X || p_Y) \u0026amp;= \\int p_X(x) \\log \\frac{p_X(x)} {p_Y(x)} \\d x = \\int p_X(x) \\log [ \\sqrt \\frac{|\\Sigma_X|}{|\\Sigma_Y|} \\frac { e^{-\\frac 1 2 (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X)} } { e^{-\\frac 1 2 (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y)} } ] \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) [ (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X) + (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y) ] \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X) \\d x \\\\ \u0026amp;\\quad -\\frac 1 2 \\int p_X(x) (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y) \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) x^T \\Sigma_X^{-1} x \\d x + \\frac 1 2 \\mu_X^T \\Sigma_X^{-1} \\mu_X \\\\ \u0026amp;\\quad - \\frac 1 2 \\int p_X(x) x^T \\Sigma_Y^{-1} x \\d x + \\frac 1 2 \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\tr(\\Sigma_X^{-1} \\Sigma_X) - \\frac 1 2 \\mu_X \\Sigma_X^{-1} \\mu_X + \\frac 1 2 \\mu_X^T \\Sigma_X^{-1} \\mu_X \\\\ \u0026amp;\\quad - \\frac 1 2 \\tr(\\Sigma_Y^{-1} \\Sigma_X) - \\frac 1 2 \\mu_X \\Sigma_Y^{-1} \\mu_X + \\frac 1 2 \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y) + \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y - \\mu_X \\Sigma_Y^{-1} \\mu_X] \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y)] \\\\ \u0026amp;\\quad + \\frac 1 2 [\\mu_Y^T \\Sigma_Y^{-1} \\mu_Y - \\mu^T_X \\Sigma_Y^{-1} \\mu_X + \\mu_X^T \\Sigma_Y^{-1} \\mu_Y - \\mu_Y^T \\Sigma_Y^{-1} \\mu_X] \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y) + (\\mu_Y^T - \\mu_X^T) \\Sigma_Y^{-1} (\\mu_Y - \\mu_X)] \\\\ \\end{aligned} \\]\n","date":1650904755,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650904755,"objectID":"196422bcded609c81ff72b4d45b60acf","permalink":"https://chunxy.github.io/notes/articles/information-theory/kl-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/kl-divergence/","section":"notes","summary":"KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\).\nIn information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).","tags":null,"title":"KL-divergence","type":"book"},{"authors":null,"categories":null,"content":" If a probabilistic model contains only observable variables, Maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables). Latent variables are those that you cannot observe but you know its existence and its corresponding random event in random trial. In such case, Lagrange multipliers may be hard to apply because of the existence of the “log of sum” term.\nGiven observed samples \\(\\mathrm{X} = \\{\\x^{(1)}, \\x^{(2)}, ..., \\x^{(m)}\\}\\) (with unobservable latent variable samples \\(\\mathrm{Z}\\)), MLE tries to the find best parameters \\(\\hat \\theta\\) such that \\[ \\hat \\theta = \\arg\\max_{\\theta}\\log(p(\\mathrm{X};\\theta)) \\]\nThe log-likelihood function is \\[ \\begin{aligned} l(\\theta) \u0026amp;= \\log p(\\mathrm{X};\\theta) = \\sum_{\\x \\in \\mathrm{X}} \\log p(\\x; \\theta) \\\\ \u0026amp;= \\sum_{\\x \\in \\mathrm{X}} \\log \\E_{\\z \\sim q(\\z)} \\frac{p(\\x, \\z; \\theta)}{q(\\z)} \\\\ \u0026amp;\\Downarrow_\\text{by Jensen\u0026#39;s Inequality} \\\\ \u0026amp;\\ge \\sum_{\\x \\in \\mathrm{X}} \\E_{\\z \\sim q(\\z)} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)} \\\\ \u0026amp;\\text{where $q(\\z)$ is a reference distribution} \\end{aligned} \\]\nIf we maximize the \\(\\E_{\\z \\sim q(\\z)} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)}\\), the lower bound of \\(l(\\theta)\\) can be maximized, which gives us a good guarantee that \\(l(\\theta)\\) will increase monotonically. \\(\\E_{\\z \\sim q(\\z)} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)}\\) is a function of \\((\\phi, \\theta)\\), firstly we maximize it w.r.t. \\(\\phi\\), and then w.r.t. \\(\\theta\\), and then back and forth.\nThe first step is to fix \\(\\theta\\) and have \\(\\E_{\\z \\sim q(\\z)} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)}\\) reach its upper bound when the equality in Jensen’s inequality holds, where \\(\\frac{p(\\x, \\z; \\theta)}{q(\\z)}\\) is a constant \\(c\\) for every \\(\\z \\in \\mathcal{Z}\\): \\[ \\begin{aligned} p(\\x, \\z) \u0026amp;= cq(\\z) \\\\ \u0026amp;\\Downarrow \\\\ \\sum_{\\z \\in \\mathcal{Z}} p(\\x, \\z) \u0026amp;= c\\sum_{\\z \\in \\mathcal{Z}} q(\\z) \\\\ p(\\x;\\theta) \u0026amp;= c \\\\ \\end{aligned} \\] Thus we know how to choose \\(q\\): \\[ q(\\z) = \\frac{p(\\x, \\z)}{c} = \\frac{p(\\x, \\z)}{p(\\x;\\theta)} = p(\\z|\\x;\\theta) \\] The above step is called the expectation step because we are deriving a closed-form expression for \\(\\E_{\\z \\sim q(\\z)} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)}\\) w.r.t. \\(\\theta\\). The next step is the maximization step where we fix \\(q\\) and optimize w.r.t. \\(\\theta\\). We do these two steps back and forth, comprising the whole expectation maximization algorithm.\nExternal Materials A good comparison of latent variables\n","date":1641560317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560317,"objectID":"3914b1b09eb9d4d18bfa307ad2b4b736","permalink":"https://chunxy.github.io/notes/articles/optimization/expectation-maximization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/expectation-maximization/","section":"notes","summary":"If a probabilistic model contains only observable variables, Maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables).","tags":null,"title":"Expectation Maximization","type":"book"},{"authors":null,"categories":null,"content":" \\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function. The reason for these two constraints is that we hope \\[ \\begin{gather} D_f (p||q) = 0 \\text{ when $p=q$} \\\\ \\forall p, q, D_f (p||q) \\ge 0 \\end{gather} \\] When \\(f(x) = x\\log x\\), \\(f\\)-divergence becomes KL-divergence.\nVariational \\(f\\)-divergence When \\(p\\) and \\(q\\) have no closed-form expression, it is difficult to compute the \\(f\\)-divergence. Therefore in practice \\(f\\)-divergence is computed with a variational expression: \\[ D_f (p||q) = \\sup_{T:\\mathcal X \\to \\R} \\{ \\E_p[f(x)] + \\E_q[f^* \\circ T(x)] \\} \\] where \\(f^*\\) is the convex conjugate of \\(f\\). The derivation is as follows: \\[ \\begin{aligned} \u0026amp;D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\\\ \u0026amp;= \\int f^{**}(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\\\ \u0026amp;= \\int \\sup_t [\\frac{p(x)}{q(x)} t - f^*(t)] q(x)\\ \\d x \\\\ \u0026amp;= \\int \\sup_t[p(x) t - f^*(t) q(x)]\\ \\d x \\\\ \u0026amp;\\Downarrow_{T(x) = \\arg \\sup_t[p(x) t - f^*(t) q(x)]} \\\\ \u0026amp;= \\sup_{T:\\mathcal X \\to \\R} \\int [p(x) T(x) - f^*(T(x)) q(x)]\\ \\d x \\\\ \u0026amp;= \\sup_{T:\\mathcal X \\to \\R} \\{ \\E_p[f(x)] + \\E_q[f^* \\circ T(x)] \\} \\end{aligned} \\]\n","date":1651590621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651590621,"objectID":"10edecf4e0a759b431732c1841eebef8","permalink":"https://chunxy.github.io/notes/articles/information-theory/f-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/f-divergence/","section":"notes","summary":"\\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function.","tags":null,"title":"$f$-divergence","type":"book"},{"authors":null,"categories":null,"content":" Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx + b: \\R^N \\mapsto \\R\\), where \\(w \\in \\R^N, b \\in \\R\\), that best predicts the \\(y\\) for any feature vector \\(x\\). The “best” is usually measured by the Mean Square Error: \\[ MSE(f,\\mathcal D) = \\frac{1}{M}\\sum_{i=1}^M(y^{(i)} - f(x^{(i)}))^2 \\]\nOrdinary Least Squares OLS selects the linear regression parameters \\(w, b\\) the minimize the MSE: \\[ w^\\star, b^\\star = \\arg \\min_{w,b}\\frac{1}{M}\\sum_{i=1}^M(y^{(i)} - w^Tx^{(i)} - b)^2 \\] This is equivalent to the below Least Squares problem. Let \\[ \\begin{gather} X = \\left [ \\begin{array}{c|c} (x^{(1)})^T \u0026amp; 1 \\\\ (x^{(2)})^T \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \\\\ (x^{(M)})^T \u0026amp; 1 \\end{array} \\right ], W = \\begin{bmatrix} w \\\\ b \\\\ \\end{bmatrix}, Y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(M)} \\end{bmatrix} \\\\ \\\\ XW = Y \\end{gather} \\] \\(Y\\) may not lie in the column space of \\(X\\). Therefore we have to approximate \\(W\\) by \\[ \\begin{aligned} X\\hat W = proj_{Col(X)}Y \\Rightarrow Y \u0026amp;- X\\hat W \\in Nul(X^T) \\Rightarrow\\\\ X^T(Y - X\\hat W) \u0026amp;= 0 \\\\ X^TX\\hat W \u0026amp;= X^TY \\end{aligned} \\] Columns of \\(X\\) are independent \\(\\iff\\) \\(X^TX\\) is invertible. When \\(X^TX\\) is not invertible, there are infinite many solutions to \\(\\hat W\\). We can get one specific \\(\\hat W\\) by enforcing regularization on \\(\\hat W\\) or adding more samples when \\(M \u0026lt; N + 1\\). When \\(X^TX\\) is invertible, there is a unique solution that \\(\\hat W = (X^TX)^{-1}X^TY\\).\nThis gives the same result with minimizing the MSE: \\[ W^\\star = \\arg \\min_{W}MSE(W) = \\frac{1}{M}(Y - XW)^T(Y - XW) \\]\n\\[ \\begin{aligned} \\frac{\\partial MSE}{\\partial W} \u0026amp;= \\frac{\\partial(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)}{\\partial W} \\\\ \u0026amp;= \\frac{\\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\\partial W} \\\\ \u0026amp;= -2X^TY + 2X^TXW \\\\ \\end{aligned} \\]\n\\[ 0 = -2X^TY + 2X^TXW^\\star \\\\ X^TXW^\\star = X^TY \\]\nThere are chances that \\(N\\) is too large, making equation (7) too computationally expensive. In this case, we can use gradient descent. The update rule will be \\[ \\begin{aligned} W^{(t+1)} \u0026amp;= W^{(t)} - \\frac{\\eta}{2}\\nabla MSE(W^{(t)}) \\\\ \u0026amp;= W^{(t)} - \\eta(X^TXW^{(t)} -X^TY) \\end{aligned} \\]\nA Probabilistic View Maximum Likelihood Estimation In classification task, \\(x\\) is the feature, \\(y\\) is the label; in regression task, \\(x\\) is the ‘label’, \\(y\\) is the ‘feature’. From a probabilistic point of view, we would like estimate \\(p(feature|label)\\). In this case, we treat \\(y\\) as the ‘feature’ composed of a deterministic function with a noise sampled from an identical and independent Gaussian distribution, i.e., for random variable \\(\\mathcal X, \\mathcal Y\\) (to distinguish from matrix \\(X\\) and \\(Y\\)), \\[ \\mathcal Y = \\mathcal XW + \\epsilon, \\text{ where }p(\\epsilon;\\sigma^2) = \\mathcal N(\\epsilon;0, \\sigma^2) \\] Thus, \\[ p(Y|X;W,\\sigma^2) = p(\\epsilon=Y - XW;\\sigma^2) = \\mathcal N(Y - XW;0,\\sigma^2I) \\] Then OLS can be attacked by Maximum Likelihood Estimation. The log-likelihood function will be\n\\[ \\begin{aligned} l(W,\\sigma^2) \u0026amp;= \\log(L(W, \\sigma^2)) = \\log(p(Y|X;W,\\sigma^2)) = \\log \\mathcal N(Y - XW;0,\\sigma^2I) \\\\ \u0026amp;= \\log(\\frac{1}{\\sqrt{(2\\pi)^M|\\sigma^2I|}}e^{-\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW)}) \\\\ \u0026amp;= -\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) - \\frac{M}{2}\\log \\sigma^2 - \\frac{M}{2}\\log 2\\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\arg \\max_{W,\\sigma^2}l(W, \\sigma^2) \u0026amp;= \\arg \\max_{W,\\sigma^2}-\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) - \\frac{M}{2}\\log \\sigma^2 - \\frac{M}{2}\\log 2\\pi \\\\ \u0026amp;= \\arg \\min_{W,\\sigma^2}\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) + \\frac{M}{2}\\log \\sigma^2 \\\\ \\end{aligned} \\]\nTake derivative w.r.t. \\(W\\) and make it \\(0\\) to give \\(W^\\star\\): \\[ \\begin{gather} \\frac{1}{\\sigma^2}(X^TXW^\\star - X^TY) = 0 \\\\ X^TXW^\\star = X^TY \\end{gather} \\] Substitute the solution \\(W^\\star\\) back, take derivative w.r.t. \\(\\sigma^2\\) and make it \\(0\\) to give \\(\\sigma^{\\star2}\\): \\[ \\begin{gather} \\frac{M}{2\\sigma^{\\star2}} - \\frac{(Y - XW^\\star)^T(Y - XW^\\star)}{2(\\sigma^{\\star2})^2} = 0 \\\\ \\sigma^{\\star2} = \\frac{1}{M}(Y - XW^\\star)^T(Y - XW^\\star) \\end{gather} \\]\nMaximizing a Posteriori If we add a priori to \\(W\\) by \\(W \\sim \\mathcal N(0,\\frac{C}{2}I)\\), then \\(p(W) \\propto \\exp(-\\frac{1}{C}W^TW)\\). \\[ \\begin{aligned} p(W|X, Y) \u0026amp;= \\frac{p(W, X, Y)}{p(X, Y)} \\\\ \u0026amp;= \\frac{p(Y|X, W)P(X, W)}{P(X, Y)} \\\\ \u0026amp;= \\frac{p(Y|X, W)P(W)P(X)}{P(X, Y)} \\\\ \u0026amp;= \\frac{p(Y|X, W)P(W)}{P(Y|X)} \\\\ \u0026amp;\\propto p(Y|X,W)p(W) \\end{aligned} \\]\n\\[ W^\\star = \\arg \\max_W p(W|X,Y) \\]\nRidge Regression By adding a regularization term to OLS objective we obtain the Ridge Regression: \\[ \\min_{W} \\frac{1}{2}||Y - XW||^2_2 + \\frac{\\alpha}{2}||W||^2_2 \\] By regularization, we are “shrink” the amount of \\(||W||_2\\), whose magnitude is controlled by the \\(\\alpha\\). We prefer smaller weights because:\n smaller weights are more robust to the perturbations of input there are better chances to zero out …","date":1642195157,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642195157,"objectID":"f00bd3e187a404025f460c7e1969ba18","permalink":"https://chunxy.github.io/notes/articles/machine-learning/linear-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/linear-regression/","section":"notes","summary":"Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx","tags":null,"title":"Linear Regression","type":"book"},{"authors":null,"categories":null,"content":" Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\} \\] Or put it another way, the subgradient defines a hyper-plane passing through \\((x,f(x))\\): $$ ^T ( - ) = 0,\ny ^n, t \\ \\[ And \\] ^T ( - ) , (y, t) epi(f) \\[ This is because by definition, \\] \\[\\begin{aligned} g^T(y - x) - (f(y) - f(x)) \u0026amp;\\le 0, \\forall y \u0026amp;\\Rightarrow \\\\ g^T(y - x) - (t - f(x)) \u0026amp;\\le 0, \\forall y,\\forall t \\ge f(y) \u0026amp;\\Rightarrow \\\\ \\left[ \\begin{array} \\\\ g \\\\ -1 \\\\ \\end{array} \\right]^T \\Big( \\left[ \\begin{array} \\\\ y \\\\ t \\\\ \\end{array} \\right] - \\left[ \\begin{array} \\\\ x \\\\ f(x) \\\\ \\end{array} \\right] \\Big) \u0026amp;\\le 0, \\forall (y, t) \\in epi(f) \\end{aligned}\\] $$\nwhere \\(\\left[\\begin{array}\\\\ g \\\\ -1 \\\\\\end{array}\\right]\\) is the normal of the tangent plane of \\(f\\) at \\(x\\).\nProperties The subgradient \\(\\partial f(x)\\) is a set, instead of a single number like the gradient.\n Subgradient is a monotonic operator: \\[ (u - v)^T(y - x) \\ge 0, \\forall u \\in \\partial f(y), \\forall v \\in \\partial f(x) \\] This can be shown by: \\[ \\begin{aligned} \\left. \\begin{array} \\\\ f(y) \\ge f(x) + v^T(y - x) \\\\ f(x) \\ge f(y) + u^T(x - y) \\\\ \\end{array} \\right\\} \u0026amp;\\Rightarrow f(y) + f(x) \\ge f(x) + f(y) - (u - v)^T(y - x) \\\\ \u0026amp;\\Rightarrow (u - v)^T(y - x) \\ge 0 \\end{aligned} \\]\n \\(x^\\star = \\arg \\min_x f(x) \\iff 0 \\in \\partial f(x^\\star)\\).\n If \\(f\\) is differentiable at \\(x\\), then \\(\\partial f(x) = \\{\\nabla f(x)\\}\\).\nSuppose \\(p \\ne \\nabla f(x) \\and p \\in \\partial f(x)\\), then for \\(y = x + r(p - \\nabla f(x))\\), we have \\[ \\begin{aligned} f(y) - f(x) \u0026amp;\\ge p^T(y - x) \\\\ f(x + r(p - \\nabla f(x))) - f(x) \u0026amp;\\ge rp^T(p - \\nabla f(x)) \\\\ \\frac{f(x + r(p - \\nabla f(x))) - f(x)}{r} \u0026amp;\\ge p^T(p - \\nabla f(x)) \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} (p - \\nabla f(x))^T(p - \\nabla f(x)) \u0026amp;\\le \\frac{f(x + r(p - \\nabla f(x))) - f(x)}{r} - \\nabla f(x)^T(p - \\nabla f(x)) \\\\ ||p - \\nabla f(x)||^2_2 \u0026amp;\\le \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\\\ \\end{aligned} \\]\nTake limit on \\(r\\) on both sides to give \\[ \\lim_{r \\to 0}||p - \\nabla f(x)||^2_2 \\le \\lim_{r \\to 0}\\Big( \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\Big) \\\\ \\] By the definition of gradient, \\[ \\begin{aligned} f\u0026amp;(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x)) \\\\ \u0026amp;= \\mathcal{O}(||r(p - \\nabla f(x))||^2_2) \\\\ \u0026amp;= ||p - \\nabla f(x)||^2\\mathcal{O}(r^2) \\\\ \u0026amp;= \\mathcal{O}(r^2) \\end{aligned} \\]\n\\[ \\lim_{r \\to 0}\\Big( \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\Big) = \\lim_{r \\to 0}\\frac{\\mathcal{O}(r^2)}{r} = 0 \\]\nTherefore, \\[ \\begin{gather} ||p - \\nabla f(x)||^2_2 \\le 0 \\\\ 0 \\le ||p - \\nabla f(x)||^2_2 \\le 0 \\\\ ||p - \\nabla f(x)||^2_2 = 0 \\end{gather} \\] contradicting the assumption that \\(p \\ne \\nabla f(x)\\). Thus, \\(p = \\nabla f(x)\\).\n If \\(f(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x)\\), then \\(\\partial f(x) = \\alpha_1 \\partial f_1(x) + \\alpha_2 f_2(x)\\).\n  凸优化笔记16：次梯度 - 知乎 (zhihu.com)\n","date":1641917363,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641917363,"objectID":"d857b71bbd69f8637fc9c93ee18ab77e","permalink":"https://chunxy.github.io/notes/articles/optimization/subgradient/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/subgradient/","section":"notes","summary":"Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\}","tags":null,"title":"Subgradient","type":"book"},{"authors":null,"categories":null,"content":" The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space.\nPolynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\). In higher-dimensional input feature space, there are many more entries for feature mapping in each degree. Take a 2-D input for example.\n Degree 1: \\([x_1, x_2]^T\\) Degree 2: \\([x_1^2, x_1x_2, x_2^2]^T\\) Degree 3: \\([x_1^3, x_1^2x_2, x_1x_2^2, x_2^3]^T\\) …  Kernel Trick in Non-linear Regression Many Linear Regression algorithms learning algorithms depend on the calculation of inner products between feature vectors, either during training or in prediction, without directly depending on the feature vector. We can transform the feature vector by applying a feature mapping \\(\\phi\\) to do the Non-linear Regression task.\nHowever, it is not necessary to explicitly define the feature mapping \\(\\phi\\) when only the inner products are needed. Instead, we can define a function \\(\\mathcal K: \\R^N \\times \\R^N \\mapsto \\R\\) that directly calculate the inner products of pseudo-transformed features: \\[ \\mathcal K(x,x^\\prime) = \\phi_{pseudo}(x)^T\\phi_{pseudo}(x^\\prime) \\] This will be more flexible and computationally-efficient.\nKernel Ridge Regression Recall Ridge Regression: \\[ W^\\star = \\min_{W}\\frac{1}{2}||Y - X^TW||^2_2 + \\frac{\\alpha}{2}||W||^2_2 \\iff (XX^T + \\alpha I_{N+1})W^\\star = XY \\] If \\((XX^T + \\alpha I_{N+1})\\) is invertible, \\(W^\\star = (XX^T + \\alpha I_{N+1})^{-1}XY\\). By matrix identity \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] we can obtain that \\(W^\\star = X(X^TX + \\alpha I_M)^{-1}Y\\). \\(X \\in \\R^{(N+1)\\times M}, (X^TX + \\alpha I_M)^{-1}Y \\in \\R^{M}\\). \\(W^\\star\\) can be rewritten as a linear combination of columns of \\(X\\), i.e. \\(W^\\star\\) lies in the span of input vectors: \\[ W^\\star = \\sum_{i=1}^M\\lambda_ix^{(i)} \\] For a new data \\(x^*\\), we make prediction by \\[ y^* = (x^*)^TW = (x^*)^TX(X^TX + \\alpha I_M)^{-1}Y \\] which totally depends on the inner products.\nSupport Vector Regression Support Vector Regression learns a hyper-plane that incorporates within its margin as many points as possible. As a comparison, Support Vector Machine learns a hyper-plane that excludes outside its margin as many points as possible. Its objective is \\[ \\begin{aligned} \\min_{w,\\xi,\\xi^*} \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ s.t.\\quad y^{(i)} - w^Tx^{(i)} - b \\le \\epsilon + \\xi_i, i=1,\\dots,M \\\\ y^{(i)} - w^Tx^{(i)} - b \\ge \\epsilon + \\xi^*_i, i=1,\\dots,M \\\\ \\xi_i, \\xi^*_i \\ge 0, i=1,\\dots,M \\end{aligned} \\] \\(\\epsilon\\) is a hyper-parameter to be determined. Transform the problem into the standard optimization form: \\[ \\begin{aligned} \\min_{w,\\xi,\\xi^*} \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ s.t.\\quad y^{(i)} - w^Tx^{(i)} - b - \\epsilon - \\xi_i \\le 0, i=1,\\dots,M \\\\ w^Tx^{(i)} + b - y^{(i)} + \\epsilon + \\xi^*_i \\le 0, i=1,\\dots,M \\\\ -\\xi_i, -\\xi^*_i \\ge 0, i=1,\\dots,M \\end{aligned} \\] The Lagrangian function will be \\[ \\begin{aligned} L(w,\\xi,\\xi^*,\\lambda,\\mu,\\nu,\\nu^*) = \u0026amp;\\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ \u0026amp;+ \\sum_{i=1}^M\\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \\epsilon - \\xi_i) \\\\ \u0026amp;+ \\sum_{i=1}^M\\mu_i(w^Tx^{(i)} + b - y^{(i)} + \\epsilon + \\xi^*_i) \\\\ \u0026amp;- \\sum_{i=1}^M\\nu_i\\xi_i -\\sum_{i=1}^M\\nu^*_i\\xi^*_i \\end{aligned} \\]\nExternal Materials Support Vector Regression.pdf\n","date":1641559580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559580,"objectID":"a5f00276ecb2f14744a20fff20ed1b78","permalink":"https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/non-linear-regression/","section":"notes","summary":"The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space.\nPolynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\).","tags":null,"title":"Non-linear Regression","type":"book"},{"authors":null,"categories":null,"content":" 离散型 二项分布（binomial distribution） 如果离散型随机变量\\(X\\)服从二项分布，一般记作\\(X \\sim B(n, p)\\)。 \\[ \\begin{gathered} B(x;n,p) = {n \\choose x} p^x (1-p)^{n-x}, x = 0,1,\\dots \\\\ \\E[X] = np \\\\ \\Var[X] = np(1-p) \\end{gathered} \\]\n二项分布可以帮助纠正一个生活中很常见的谬误，比如说身高高于两米的人占人类总体的\\(1\\%\\)，那么是否说明随机选取的100个人中一定至少有1个人高于两米呢？记\\(X\\)为100个人中身高高于两米的人数，显然\\(X \\sim B(100, 0.01)\\)，经计算可得\\(P(X=0) \\approx 0.366\\)。其实也就意味着，100个人中，能至少看到1个身高高于两米的人的概率其实大约是\\(1-0.366 = 63.4\\%\\)。\n泊松分布（Poisson distribution） 泊松分布产生于\\(X\\)用来表示在一定时间或空间内出现的事件个数的场景中。泊松分布有一些基本假设，设观察的这一单位时间或空间为\\([0, 1)\\)，取一个很大的自然数\\(n\\)，将\\([0,1)\\)平分为\\(n\\)段窗口：\\(l_1 = [0, \\frac{1}{n}), l_2 = [\\frac{1}{n}, \\frac{2}{n}), \\dots, l_n = [\\frac{n-1}{n}, 1)\\)，则：\n在每段\\(l_i\\)内，恰发生一个事件的概率正比于这段的长度\\(\\frac{1}{n}\\)，即可取为\\(\\frac{\\lambda}{n}\\)；又假定\\(n\\)很大故\\(\\frac{1}{n}\\)很小时，不可能发生两次以上事件； \\(l_1, l_2, \\dots, l_n\\)中是否发生时间是相互独立的；  这样的基本假设下，单位窗口内发生事件的总数记为随机变量\\(X\\)。此时\\(X\\)应当服从二项分布，而当\\(n \\to \\infty\\)时，\\(X\\)则服从泊松分布，故泊松分布也可以看作是某种形式的二项分布取极限而得到： \\[ P(X = i; \\lambda) = \\lim_{n \\to \\infty} {n \\choose i} (\\frac{\\lambda}{n})^i (1 - \\frac{\\lambda}{n})^{n-i} \\] 将\\(\\lim_{n \\to \\infty} {n \\choose i} / n^i = 1 / i!\\)、\\(\\lim_{n \\to \\infty} (1 - \\frac{\\lambda}{n})^{n-i} = e^{-\\lambda}\\)代入即可得到泊松分布的分布律。\n一般如果\\(X \\sim B(n,p)\\)且\\(n\\)较大、\\(p\\)较小、\\(np = \\lambda\\)不太大时，\\(X\\)的分布接近于泊松分布\\(P(\\lambda)\\)。 \\[ \\begin{gathered} P(x;\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!}, x = 0,1,\\dots \\\\ \\E[X] = \\lambda \\\\ \\Var[X] = \\lambda \\end{gathered} \\]\n伯努利分布（Bernoulli distribution） 伯努利分布\\(B(1, p)\\)实际上是二项分布中\\(n = 1\\)的一个特例： \\[ \\begin{gathered} B(1;1,p) = p, B(0;1,p) = 1 - p \\\\ \\E[X] = p \\\\ \\Var[X] = p(1 - p) \\end{gathered} \\]\n多项分布（multinomial distribution） 多项分布其实就是二项分布的推广，不像二项分布，多项分布的取值的是多值的而不是二值的（binary）。假设有\\(k\\)种结果，且这\\(k\\)种结果互相对立、完备穷举（mutually exclusive and collectively exhaustive），此时它们的概率之和为\\(1\\)，即\\(p_1 + \\dots + p_k = 1\\)，多项分布计算的则是这\\(k\\)种结果分别发生\\(n_1, \\dots, n_k\\)次时的概率。令\\(N = n_1 + \\dots + n_k, \\vec p = [p_1, \\dots, p_k], \\vec n = [n_1, \\dots, n_k]\\)，则： \\[ P(\\vec n; \\vec p, N) = \\frac{N!}{n_1! \\dots n_k!} p_1^{n_1} \\dots p_k^{n_k} \\]\n分类分布（categorical distribution） 类似伯努利分布是二项分布\\(n=1\\)时的特例，分类分布则是多项分布\\(N=1\\)时的特例： \\[ \\begin{gathered} P(\\vec n; \\vec p, 1) = \\prod_{i=1}^k p_i ^{n_i} \\\\ \\E[X] = \\vec p \\\\ \\Var[X] = \\vec p (1 - \\vec p) \\end{gathered} \\]\n连续型 指数分布（exponential distribution） 指数分布最常见的一个场景是寿命估计。设想一中大批生产的电器元件，其寿命\\(X\\)是随机变量，在“无老化”的假定下——即“若元件在时刻\\(x\\)尚正常工作，则其失效率总为某个与\\(x\\)无关的常数\\(\\lambda \u0026gt; 0\\)”，\\(X\\)服从参数为\\(\\lambda\\)的指数分布。\n上述假设用概率语言描述则是 \\[ \\lim_{h \\to 0} P(x \\le X \\le x+h | X \u0026gt; x) / h = \\lambda \\] 注意到 \\[ P(x \\le X \\le x+h | X \u0026gt; x) = \\frac{P(\\{ x \\le X \\le x+h \\} \\cap \\{ X \u0026gt; x \\})}{P(X \u0026gt; x)} = \\frac{P(x \u0026lt; X \\le x+h)}{P(X \u0026gt; x)} \\] 所以 \\[ \\begin{aligned} \\lim_{h \\to 0} \\frac{P(x \u0026lt; X \\le x+h)}{h P(x \u0026lt; X))} \u0026amp;= \\lambda \\\\ \\lim_{h \\to 0} \\frac{F(x + h) - F(x)}{h(1 - F(x))} \u0026amp;= \\lambda \\\\ \\frac{F\u0026#39;(x)}{(1 - F(x))} \u0026amp;= \\lambda \\end{aligned} \\] 上述微分方程的通解为\\(F(x) = 1 - Ce^{-\\lambda x}\\)，而\\(F(0) = 0\\)，故\\(C = 1\\)。 \\[ \\begin{gathered} p(x;\\lambda) = \\begin{cases} \\lambda e^{-\\lambda x}, \u0026amp; x \u0026gt; 0 \\\\ 0, \u0026amp; x \\le 0 \\end{cases} \\\\ \\E[X] = \\lambda^{-1} \\\\ \\Var[X] = \\lambda^{-2} \\end{gathered} \\]\n正态分布（normal distribution） 正态分布也叫作高斯分布（Gaussian distribution），一维情况下： \\[ p(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] 二维情况下： \\[ p \\Big( (x,y); \\mu_X, \\mu_Y, \\sigma_X, \\sigma_Y, \\rho_{XY} \\Big) = \\frac{1}{2\\pi \\sqrt{(\\sigma_X^2 \\sigma_Y^2 - \\rho_{XY}^2)}} e^{-\\frac 1 {2(1 - \\rho_{XY}^2)} \\left(\\frac{(x-\\mu_X)^2} {\\sigma_X^2} - \\frac{2\\rho_{XY}(x - \\mu_X)(y - \\mu_Y)} {\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2} {\\sigma_Y^2} \\right)} \\] \\(n\\)维情况下： \\[ p(\\x; \\mu, \\Sigma) = \\frac{1}{\\sqrt{|(2\\pi) \\Sigma|}} e^{-\\frac{1}{2} (\\x-\\mu)^T \\Sigma^{-1} (\\x-\\mu)} \\]\n","date":1660237921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660237921,"objectID":"b2463e4efacb753026851bc581923c69","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/","section":"notes","summary":"离散型 二项分布（binomial distribution） 如","tags":null,"title":"常见分布","type":"book"},{"authors":null,"categories":null,"content":" Given dataset \\(\\mathcal D = \\{x^{(i)}, i=1,\\dots,M\\}\\), a clustering \\(\\mathcal C\\) of the \\(M\\) points into \\(K (K \\le M)\\) clusters is a partition of \\(\\mathcal D\\) into \\(K\\) disjoint groups \\(\\{C_1,\\dots,C_K\\}\\). Suppose we have a function \\(f\\) that evaluates the clustering \\(\\mathcal C\\) and returns lower score with better clustering. The best clustering will be \\[ \\arg \\min_{\\mathcal C} f(\\mathcal C) \\] The number of all possibilities of clustering with \\(M\\) elements is called the Bell number, denoted as \\(B_M\\). The calculation of the Bell number is based on dynamic programming. The number of ways to cluster \\(M+1\\) elements is the sum of number of ways to:\n select \\(1\\) element and cluster it, with the rest belong to one cluster select \\(2\\) elements and cluster them, with the rest belong to one cluster … select \\(M\\) elements and cluster them, with the rest belong to one cluster  Therefore, \\[ \\begin{gather} B_{M+1} = \\sum_{i=1}^M\\binom{M}{0}B_i \\\\ B_0 = 1 \\end{gather} \\] The exhaustive method will be computationally intractable. We need either an approximation algorithm or a scoring function with special properties.\nK-means K-means assumes there are \\(K\\) clusters. This greatly eliminates many possibilities described above. Its objective is \\[ \\min_{c_1,\\dots,c_K}\\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \\text{ where }z^{(i)} = \\arg \\min_{j \\in \\{1,\\dots,K\\}}||x^{(i)}-c_j||^2_2 \\] \\(z^{(i)}\\) is the cluster index \\(x^{(i)}\\) is assigned to. K-means’ objective is to assign each point to its closest cluster center and minimize the total within-cluster square errors. For cluster \\(j\\), let \\(C_j = \\{x^{(i)}|z^{(i)} = j\\}\\) be the set of points assigned to it, then the cluster center of cluster \\(j\\) is \\[ c_j = \\frac{1}{|C_j|}\\sum_{x^{(i)} \\in C_j}x^{(i)} \\] However, both the cluster center \\(\\newcommand{\\c}{\\mathrm{c}} \\c\\) and the assignment \\(\\newcommand{\\z}{\\mathrm{z}} \\z\\) is initially unknown. K-means solves this by randomly pick up initial cluster centers and enter the assign-data-to-clusters/update-cluster-centers loop, until the cluster centers converge or become satisfactory. Rewrite the objective of K-means as:\n\\[ \\min_{\\z,\\c}(l(\\z,\\c) = \\sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2) \\] K-means is in essence a coordinate descent of the loss \\(l(\\z,\\c)\\). The main loop of K-means is to:\n assign data points to its nearest cluster center, i.e. minimizing over the assignment \\(\\z\\). update cluster centers according to the points assigned to, i.e. minimizing over the centroids \\(\\c\\).  \\(l\\) is monotonically decreasing after each step in the above loop. Also, \\(l\\) is lower-bounded by \\(0\\). Therefore, \\(l\\) and thus K-means will converge finally.\nEach cluster in K-means has a circular shape because of the Euclidean distance it uses.\nGaussian Mixture Model A cluster can also be modelled by a multi-variate Gaussian with elliptical shape: the elliptical shape is controlled by the covariance matrix; the location is controlled by the mean. Gaussian Mixture Model is a weighted sum of Gaussians: \\[ p(x) = \\sum_{j=1}^K\\pi_j\\mathcal N(x;\\mu_j, \\Sigma_j) \\] \\(\\pi_j\\) is the prior that a sample is generated from the \\(j\\)-th Gaussian. \\(\\mathcal N(x;\\mu_j, \\Sigma_j)\\) is the probability to generate the sample \\(x\\) from the \\(j\\)-th Gaussian. Put it together, \\(p(x)\\) is the total probability of \\(x\\) over its latent \\(z\\), with \\(z = j\\) representing the prior condition that \\(x\\) is sampled from \\(j\\)-th Gaussian. \\[ p(x) = \\sum_{j=1}^Kp(x, z = j) = \\sum_{j=1}^Kp(x|z = j)p(z = j) \\] GMM is learned by \\[ \\begin{gather} \\max_{\\pi,\\mu,\\Sigma}\\Big(L(\\pi,\\mu,\\Sigma) = \\prod_{i=1}^Mp(x^{(i)})\\Big) \\\\ s.t.\\quad \\sum_{j=1}^K\\pi_j = 1 \\end{gather} \\] The log-likelihood function form will be \\[ \\begin{gather} \\max_{\\pi,\\mu,\\Sigma}\\Big(l(\\pi,\\mu,\\Sigma) = \\log L(\\pi,\\mu,\\Sigma)\\Big) \\\\ s.t.\\quad \\sum_{j=1}^K\\pi_j = 1 \\end{gather} \\]\nAttempt with Lagrange Multiplier \\[ \\begin{gather} L(\\pi,\\mu,\\Sigma) = \\prod_{i=1}^Mp(x^{(i)}) = \\prod_{i=1}^M\\sum_{j=1}^Kp(z_j)p(x|z = z_j) = \\prod_{i=1}^M\\sum_{j=1}^K\\pi_j\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j) \\\\ l(\\pi,\\mu,\\Sigma) = \\log\\big(\\prod_{i=1}^M\\sum_{j=1}^K\\pi_j\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j)\\big) = \\sum_{i=1}^M\\log\\sum_{j=1}^K\\pi_j\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j) \\end{gather} \\] The Lagrangian function will be \\[ J(\\pi,\\mu,\\Sigma,\\lambda) = -\\sum_{i=1}^M\\log\\sum_{j=1}^K\\pi_j\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j) + \\lambda(\\sum_{j=1}^K\\pi_j - 1) \\] This expression is just too hard to zero the derivatives. For example, writing \\(\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j)\\) as \\(z^{(i)}_j\\), then take derivative w.r.t. \\(\\pi\\) and make it zero to give \\[ \\begin{gather} \\frac{\\partial J}{\\partial \\pi_j} = -\\sum_{i=1}^M\\frac{\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j)}{\\sum_{k=1}^K\\pi_k\\mathcal N(x^{(i)};\\mu_j, \\Sigma_j)} + \\lambda \\\\ \\lambda = \\sum_{i=1}^M\\frac{\\mathcal N(x^{(i)};\\mu_1, \\Sigma_1)}{\\sum_{k=1}^K\\pi_k\\mathcal N(x^{(i)};\\mu_1, \\Sigma_1)} = \\sum_{i=1}^M\\frac{\\mathcal …","date":1649514723,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649514723,"objectID":"13bbf8789e9800de592b8a49a83f52bb","permalink":"https://chunxy.github.io/notes/articles/machine-learning/clustering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/clustering/","section":"notes","summary":"Given dataset \\(\\mathcal D = \\{x^{(i)}, i=1,\\dots,M\\}\\), a clustering \\(\\mathcal C\\) of the \\(M\\) points into \\(K (K \\le M)\\) clusters is a partition of \\(\\mathcal D\\) into \\(K\\) disjoint groups \\(\\{C_1,\\dots,C_K\\}\\).","tags":null,"title":"Clustering","type":"book"},{"authors":null,"categories":null,"content":" Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\}\\) initially.\n Select among \\(C\\) a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Then for each \\(\\mathrm x^{(i)}\\) from left to right, do the following:\n  \\[ \\hat Y = \\frac{\\mathrm x^{(k)} \\cdot Y}{||\\mathrm x ^{(k)}||_2} \\mathrm x ^{(k)} \\\\ Y_{res} = Y_{res} - \\hat Y \\]\nRemove \\(\\mathrm x^{(k)}\\) from \\(C\\). Go to step 2 until \\(Y_{res}\\) reaches \\(0\\) or we have used all columns in \\(C\\).  Apparently this algorithm is efficient. However, it does not guarantee an exact solution, even when \\(Y\\) lies in the column space of \\(X\\).\nForward Stagewise Like Forward Selection, Forward Stagewise selects a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Unlike Forward Selection, Forward Selection does not subtract the whole projection from \\(Y_{res}\\). Instead, it proceeds along \\(\\mathrm x^{(k)}\\) with a small step.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\},\\eta\\text{ is a small constant}\\) initially.\n Select among \\(C\\) a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Then for each \\(\\mathrm x^{(i)}\\) from left to right, do the following:\n  \\[ \\hat Y = \\eta \\frac{\\mathrm x^{(k)} \\cdot Y}{||\\mathrm x ^{(k)}||_2} \\mathrm x ^{(k)} \\\\ Y_{res} = Y_{res} - \\hat Y \\]\nRemove \\(\\mathrm x^{(k)}\\) from \\(C\\). Go to step 2 until \\(Y_{res}\\) is sufficiently small.  Apparently Forward Stagewise gives an exact solution when \\(\\eta\\) is small enough. But it is more time-consuming.\nLeast Angle Regression LARS a is compromise of Forward Selection and Forward Stagewise. Likewise, LARS selects a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). LARS proceeds stagewise like Forward Stagewise, however with its own methodology to determine the step size.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\}, \\mathrm d = \\arg\\max_{\\mathrm x \\in C}\\frac{Y \\cdot \\mathrm x }{||\\mathrm x||_2}\\) initially.\n Then do the following: \\[ \\hat Y = \\eta \\frac{\\mathrm d \\cdot Y}{||\\mathrm d||_2} \\mathrm d \\\\ Y_{res} = Y_{res} - \\hat Y \\] \\(\\eta\\) is determined such that there is some \\(\\mathrm x^{(k)} \\in C\\) onto which the projection of \\((Y - \\hat Y)\\) is the same as that onto \\(\\mathrm d\\). Or in other words, \\((Y - \\hat Y)\\) lies on the bisector hyper-plane between \\(\\mathrm x^{(k)}\\) and \\(\\mathrm d\\). Let \\(\\mathrm d\\) be along this bisector.\n Go to step 2 until \\(Y_{res}\\) is sufficiently small.\n  ","date":1639926598,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639926598,"objectID":"2fa379b4f344d23f6edb9cb8b878ce7d","permalink":"https://chunxy.github.io/notes/articles/optimization/least-angle-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/least-angle-regression/","section":"notes","summary":"Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.","tags":null,"title":"Least Angle Regression","type":"book"},{"authors":null,"categories":null,"content":" Unsupervised Dimension Reduction Dimensionality reduction\n reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity.  Compared to feature selection,\n the goal of feature selection is to remove features that are not informative with respect to the class label. This obviously reduces the dimensionality of the feature space; dimension reduction can be used to find a meaningful lower-dim feature space even when there is information in each feature dimension so that none can be discarded; dimension reduction is unsupervised while feature selection is supervised.  Compared to data compression,\n dimension reduction can be seen as a simplistic form of data compression. But they are not equivalent, as the goal of data compression is to reduce the entropy of the representation, which is not limited to the dimension reduction.  Linear Dimensionality Reduction Linear Dimensionality Reduction projects data onto lower-dimensional space by representing the data with a new basis consisting of some major components. Mathematically, \\[ \\begin{gather} x^{(i)} = \\sum_{k=1}^Kz^{(i)}_kb^{(k)}, \\text{ where $z^{(i)} \\in \\R^K$ is the weight, $b^{(k)} \\in \\R^N$ is the basis vector.} \\\\ X = BZ, \\text{ where $X = [x^{(i)}, \\dots, x^{(M)}], B = [b^{(1)}, \\dots, b^{(K)}], Z = [z^{(1)}, \\dots, z^{(M)}]$} \\end{gather} \\] The objective can be set to minimize the error when recovering from \\(B, Z\\) to \\(X\\), i.e. \\[ \\min_{B,Z} = ||X - BZ||^2_F = \\sum_{ij}(X - BZ)^2_{ij} \\]\nAlternating Least Squares By leveraging the idea of Coordinate Descent and OLS solution to Linear Regression, we can optimize \\(B\\) and \\(Z\\) alternatively, until convergence.\nFix \\(Z\\), take derivative w.r.t. \\(B\\) and make it zero to give \\[ \\begin{gather} 2(X - BZ)(-Z^T) = 0 \\\\ BZZ^T = XZ^T \\\\ B = XZ^T(ZZ^T)^{-1} \\end{gather} \\] Fix \\(B\\), take derivative w.r.t. \\(Z^T\\) and make it zero to give \\[ \\begin{gather} \\frac{\\partial||X - BZ||^2_F}{\\partial Z^T} = \\frac{\\partial||X^T - Z^TB^T||^2_F}{\\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\\\ 2(X^T - Z^TB^T)(-B) = 0 \\\\ Z^TB^TB = X^TB \\\\ Z^T = X^TB(B^TB)^{-1} \\\\ Z = (B^TB)^{-1}B^TX \\end{gather} \\] Assume we have run the ALS to convergence and obtain the global optimal parameters \\[ B^\\star, Z^\\star = \\arg \\min_{B,Z} = ||X - BZ||^2_F = \\sum_{ij}(X - BZ)^2_{ij} \\] Let any invertible matrix \\(R \\in \\R^{K \\times K}\\). We can construct a pair of \\(\\tilde B, \\tilde Z\\) such that \\(\\tilde B = B^\\star R, \\tilde Z = R^{-1}Z^\\star\\). Then, \\[ ||X - \\tilde B \\tilde Z||^2_F = ||X - B^\\star RR^{-1} Z^\\star||^2_F = ||X - B^\\star Z^\\star||^2_F \\] Thus the global optima is not unique. We may force regularization on \\(B, Z\\) and obtain the unique optima.\nSingular Value Decomposition Suppose the Singular Value Decomposition for \\(X\\) is \\(X = U\\Sigma V^T\\), where \\[ \\begin{gather} \\text{$U$ is $N \\times N$, $\\Sigma$ is diagonal, $V$ is $M \\times M$} \\\\ UU^T = I \\\\ VV^T = I \\\\ \\Sigma = diag_{N \\times M}(\\sigma_1, \\sigma_2, ..., \\sigma_p) \\\\ \\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_p \\ge 0 \\\\ p = \\min\\{N,M\\} \\end{gather} \\] By only preserving the first \\(K \\le \\rank(\\Sigma)\\) most prominent singular values, \\(X \\approx U_K\\Sigma_KV^T_K\\), where \\[ \\begin{gather} \\text{$U_K \\in \\R^{N \\times K}$ is first $K$ columns of $U$} \\\\ \\text{$V_K \\in \\R^{M \\times K}$ is first $K$ columns of $V$} \\\\ \\Sigma_K \\in \\R^{K \\times K} = diag(\\sigma_1, \\sigma_2, ..., \\sigma_K) \\\\ \\end{gather} \\] Eckart-Young-Mirsky Theorem will tell that \\(U_K\\Sigma_KV^T_K\\) is the best approximation of \\(X\\) among \\(N \\times M\\) of rank \\(K\\) in terms of Frobenius norm.\nWe can form the \\(B^\\star, Z^\\star\\) by \\[ B^\\star = U_K \\Sigma_K^{\\frac{1}{2}}, Z^\\star = \\Sigma_K^{\\frac{1}{2}} V^T_K \\]\nPrincipal Component Analysis As shown in ALS, the optimal solution of \\(B\\) and \\(Z\\) is not unique. ALS and SVD give their solutions, however without much interpretability. The solution given by Principal Component Analysis explicitly chooses the directions of the basis it uses.\nThe goal of PCA is to identify the directions along which the data exhibits the maximum variance. And then PCA projects the data onto the space formed by these directions.\nThe solution of PCA is \\(X^TX\\)’s \\(K\\) eigenvectors permuted according to their corresponding to eigenvalues, which is exactly the \\(U_K\\) in SVD. Thus, the solution of PCA can be constructed from SVD. Let \\(X = U\\Sigma V^T\\). If we choose \\(K\\) directions with largest directional variance as basis. Then \\(B^\\star = U_K\\).\nRandom Projection The above methods all minimize the Frobenius norm during reconstruction. This objective may become time-consuming when input dimension becomes large. We may use some randomly-generated vectors as basis to do the projection. This greatly saves time, at the expense of losing accuracy. We can measure such projection by checking whether the structure of the data can be preserved, e.g. the distance …","date":1641559196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559196,"objectID":"28bb7a94a07b3b6cdf6c93acf718c2e7","permalink":"https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/dimensionality-reduction/","section":"notes","summary":"Unsupervised Dimension Reduction Dimensionality reduction\n reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity.  Compared to feature selection,","tags":null,"title":"Dimension Reduction","type":"book"},{"authors":null,"categories":null,"content":" Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the data (compared with directional derivative, this is directional variance).\nLinear PCA Let \\(v \\in \\mathbb R^{N \\times 1}\\) and \\(||v||^2 = 1\\), the variance in the direction \\(v\\) is given by \\[ \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - \\mu)^2, \\text{where }\\mu = \\frac{1}{M}\\sum_{i=1}^Mv^Tx^{(i)} = v^T\\bar x \\] Under the assumption that data is pre-centered so that \\(\\bar x = \\frac{1}{M}\\sum_{i=1}^Mx^{(i)} = 0\\): \\[ \\begin{aligned} \u0026amp;\\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - \\mu)^2 = \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - v^T\\bar x)^2 \\\\ \u0026amp;= \\frac{1}{M}\\sum_{i=1}^Mv^T(x^{(i)} - \\bar x)(x^{(i)} - \\bar x)^Tv \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M(x^{(i)} - \\bar x)(x^{(i)} - \\bar x)^T)v \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\\\ \u0026amp;\\Downarrow_{\\text{by column-row expansion}} \\\\ \u0026amp;= \\frac{1}{M}v^TXX^Tv \\\\ \\end{aligned} \\] Suppose we want to identify the direction \\(v\\) of the maximum variance, we can formulate the problem as \\[ \\begin{gather} \\max_v \\quad \\frac{1}{M}v^TXX^Tv \\\\ s.t.\\quad v^Tv = 1 \\end{gather} \\] Let \\(\\Sigma = \\frac{1}{M}XX^T\\), which is real symmetric, we can form the Lagrangian function: \\[ L(v, \\lambda) = v^T\\Sigma v + \\lambda (1 - v^Tv) \\] We represent the constraint as \\(1 - v^Tv = 0\\) instead of \\(v^Tv - 1 = 0\\). The reason is if we take derivative w.r.t \\(v\\), we have \\[ \\frac{\\partial L}{\\partial v} = 2\\Sigma v - 2\\lambda v \\] which is more intuitive than \\(\\frac{\\partial L}{\\partial v} = 2\\Sigma v + 2\\lambda v\\). Let the derivative be \\(0\\) to have \\(\\Sigma v = \\lambda v\\). Since \\(v^tv = 1\\), \\(v \\ne 0\\), this means the optimal solution \\((\\lambda^\\star, v^\\star)\\) must be a pair of eigenvalue of eigenvector of \\(\\Sigma\\):\n\\[ v^\\star = v_i, \\lambda^\\star = \\lambda_i \\] where \\(v_i\\) is rescaled such that \\(v_i^Tv_i = 1\\). Substitute the result back to the objective to give\n\\[ \\begin{aligned} \\frac{1}{M}v^TXX^Tv \u0026amp;= (v_i)^T\\Sigma v_i \\\\ \u0026amp;= (v_i)^T\\lambda_iv_i \\\\ \u0026amp;= \\lambda_i(v_i)^Tv_i \\\\ \u0026amp;= \\lambda_i \\end{aligned} \\] So the \\(N\\) largest directions of variance will be the \\(\\Sigma\\)’s eigenvectors \\(v_1, v_2, ..., v_N\\), corresponding to the eigenvalues \\(\\lambda_1 \u0026gt; \\lambda_2 \u0026gt; ... \u0026gt; \\lambda_N\\).\nAnother view on PCA Finding \\(K\\) different \\(v\\) to \\[ \\max_{v} \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)})^2 \\\\ \\] is equivalent to finding \\(K\\) different \\(v\\) to \\[ \\min_{v} \\frac{1}{M}\\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2 \\] both subject to \\(||v||^2 = 1\\) and inter-orthogonality. This second notation is essentially the projection of \\(x^{(i)}\\) on \\(v\\) because \\(||v|| = 1\\) and thus the objective indicates minimizing the overall approximation error.\nKernel PCA We can map the data to a higher-dimension space: \\(x^{(i)} \\to \\phi(x^{(i)})\\). Let \\(\\mathcal{X} = [\\phi(x^{(1)}), \\phi(x^{(2)}), ..., \\phi(x^{(M)})]\\). Kernel tricks allow us not to explicitly define \\(\\phi\\), but to only focus on the inner product of mapped data: \\(\\mathcal K(x^{(i)}, x^{(j)}) = \\phi(x^{i})^T\\phi(x^{j})\\).\nSimilarly, the variance along direction \\(v\\), where \\(v^Tv=1\\), is given by \\[ \\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - \\mu)^2, \\text{where }\\mu = \\frac{1}{M}\\sum_{i=1}^Mv^T\\phi (x^{(i)}) = v^T\\bar\\phi(x) \\] Under the assumption that data is pre-centered so that \\(\\bar\\phi(x) = \\frac{1}{M}\\sum_{i=1}^M\\phi(x^{(i)}) = 0\\): \\[ \\begin{aligned} \u0026amp;\\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - \\mu)^2 = \\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - v^T\\bar\\phi(x))^2 \\\\ \u0026amp;= \\frac{1}{M}\\sum_{i=1}^Mv^T(\\phi(x^{(i)}) - \\bar{\\phi(x)})(\\phi (x^{(i)}) - \\bar\\phi(x))^Tv \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M(\\phi(x^{(i)}) - \\bar\\phi(x))(\\phi (x^{(i)}) - \\bar\\phi(x))^T)v \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M\\phi(x^{(i)})(\\phi(x^{(i)})^T)v \\\\ \u0026amp;\\Downarrow_{\\text{by column-row expansion}} \\\\ \u0026amp;= \\frac{1}{M}v^T\\mathcal{X}\\mathcal{X}^Tv \\\\ \\end{aligned} \\] Then comes the standard form of linear PCA. Let \\(\\Sigma = \\frac{1}{M}\\mathcal{X}\\mathcal{X}^T\\), we would like to solve \\[ \\max_v\\quad \\frac{1}{M}v^T\\mathcal{X}\\mathcal{X}^Tv \\\\ s.t.\\quad v^Tv=1 \\] This is equivalent to solving \\(\\Sigma\\)’s eigenvectors. Unfortunately, this cannot be directly solved like in linear PCA since we don’t know \\(\\phi\\). However note that\n\\[ \\begin{gather} \\begin{aligned} \\Sigma v \u0026amp;= \\lambda v \\\\ v \u0026amp;= \\frac{1}{\\lambda}\\Sigma v \\end{aligned} \\\\ \\begin{aligned} v \u0026amp;= \\frac{1}{M\\lambda}\\sum_{i=1}^M\\phi(x^{(i)})[(\\phi(x^{(i)})^Tv] \\\\ \u0026amp;= \\frac{1}{M\\lambda}\\sum_{i=1}^M[(\\phi(x^{(i)})^Tv]\\phi(x^{(i)}) \\end{aligned} \\end{gather} \\] \\(v\\) is some linear combination of \\(\\phi(x^{(i)})\\) and can be written as \\[ v = \\mathcal{X}\\alpha, \\alpha \\in \\R^N \\] Substitute this back to \\(\\Sigma v = \\lambda v\\) to give \\[ \\begin{aligned} \\frac{1}{M}\\mathcal{X}\\mathcal{X}^T\\mathcal{X}\\alpha \u0026amp;= \\lambda\\mathcal{X}\\alpha \\\\ \\frac{1}{M}\\mathcal{X}^T\\mathcal{X}\\mathcal{X}^T\\mathcal{X}\\alpha \u0026amp;= \\lambda\\mathcal{X}^T\\mathcal{X}\\alpha \\\\ …","date":1641917451,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641917451,"objectID":"ac0ca64bf6bb10436e2a600fb709eb7a","permalink":"https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/principal-component-analysis/","section":"notes","summary":"Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the","tags":null,"title":"Principal Component Analysis","type":"book"},{"authors":null,"categories":null,"content":" Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.\nThe authentic proof entails Gamma function and Laplace’s method. However in integer case, Stirling’s approximation can be approached with Poisson distribution. Start from a Poisson distribution with mean \\(\\lambda\\): \\[ P(r ; \\lambda) = e^{-\\lambda} \\frac{\\lambda^r}{r!} \\] When \\(X \\sim P(\\lambda_1)\\), \\(Y \\sim P(\\lambda_2)\\), and suppose \\(X\\) and \\(Y\\) are independent, \\(X + Y \\sim P(\\lambda_1 + \\lambda_2)\\).\n Proof \\[ \\begin{aligned} \u0026amp; \\Pr(X + Y = r) = \\sum_{k=0}^r \\Pr(X = k) \\Pr(Y = r-k) \\\\ \u0026amp;= \\sum_{k=0}^r e^{-\\lambda_1} \\frac{\\lambda_1^k}{k!} e^{-\\lambda_2} \\frac{\\lambda_2^{r-k}}{(r-k)!} \\\\ \u0026amp;= \\frac{e^{-(\\lambda_1 + \\lambda_2)}}{r!} \\sum_{k=0}^r \\frac{r!}{k! (r-k)!} \\lambda_1^k \\lambda_2^{r-k} \\\\ \u0026amp;= e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^r}{r!} \\\\ \u0026amp;= P(r; \\lambda_1 + \\lambda_2) \\end{aligned} \\]  Therefore, a random variable \\(X \\sim P(\\lambda)\\) (with integer \\(\\lambda\\)) can be treated as the addition of \\(\\lambda\\) independent \\(Y_i \\sim P(1)\\). By the central limit theorem, for a large \\(\\lambda\\), \\[ \\mathbb \\Pr(\\frac{\\underbrace{\\sum_i Y_i}_X - \\lambda}{\\sqrt{\\lambda}} \\le x) \\simeq \\Phi(x) \\] Or put it another way, the mass of the Poisson distribution \\(X\\) follows is well approximated by the density of the Gaussian distribution with mean \\(\\lambda\\) and variance \\(\\lambda\\): \\[ \\begin{aligned} P(x;\\lambda) \u0026amp;\\simeq N(x; \\lambda, \\lambda) \\\\ e^{-r} \\frac{\\lambda^r}{r!} \u0026amp;\\approx \\frac{1}{\\sqrt{2\\pi \\lambda}} e^{-\\frac{(r - \\lambda)^2}{2\\lambda}} \\end{aligned} \\] Plug \\(r = \\lambda\\) into this formula and rearrange it to have \\[ \\begin{aligned} e^{-\\lambda} \\frac{\\lambda^\\lambda}{\\lambda!} \u0026amp;\\approx \\frac{1}{\\sqrt{2\\pi \\lambda}} \\\\ \\lambda! \u0026amp;\\approx \\sqrt{2\\pi \\lambda} \\left( \\frac{\\lambda}{e} \\right)^\\lambda \\end{aligned} \\]\n","date":1652125189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652125189,"objectID":"d578addef391da2c6991619f0bd4741a","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/stirlings-approximation/","section":"notes","summary":"Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.","tags":null,"title":"Stirling's Approximation","type":"book"},{"authors":null,"categories":null,"content":" Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric. Another disadvantage of KL-divergence is that it is not bounded from above. Jenson-Shannon divergence, on the other hand, overcomes these two problems of KL-divergence.\nGiven two distributions \\(p\\) and \\(q\\) defined on the same sample space, Jenson-Shannon divergence is defined as \\[ \\newcommand{\\JSD}{\\mathop{\\text{JSD}}} \\JSD(p;q) \\coloneq \\frac{1}{2} (D_\\text{KL}(p || m) + D_\\text{KL}(q || m)), \\text{where $m = \\frac{p+q}{2}$} \\] Since the JSD is the addition of two KL-divergences, it is non-negative by the non-negativity of KL-divergence and it reaches \\(0\\) when \\(p=q\\). On the other hand, \\[ \\begin{aligned} \\JSD(p;q) \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{p(x)}{(p(x)+q(x))/2} + \\E_{x \\sim q} \\log \\frac{q(x)}{(p(x)+q(x))/2} \\big) \\\\ \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{2}{( 1 + e^{\\ln \\frac{q(x)}{p(x)}} )} + \\E_{x \\sim q} \\log \\frac{2}{( 1 + e^{\\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \\end{aligned} \\] Due to the concavity of \\(f(x) = \\log \\frac{2}{1 + e^x}\\), \\[ \\begin{aligned} \\JSD(p;q) \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{2}{( 1 + e^{\\ln \\frac{q(x)}{p(x)}} )} + \\E_{x \\sim q} \\log \\frac{2}{( 1 + e^{\\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \u0026amp;\\le \\frac{1}{2} \\big( \\log \\frac{2}{( 1 + e^{\\E_{x \\sim p} \\ln \\frac{q(x)}{p(x)}} )} + \\log \\frac{2}{( 1 + e^{\\E_{x \\sim q} \\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \u0026amp;\\le \\frac{1}{2} \\big( 2 \\log \\frac{2}{( 1 + e^{( \\E_{x \\sim p} \\ln \\frac{q(x)}{p(x)} + \\E_{x \\sim q} \\ln \\frac{p(x)}{q(x)} ) / 2} )} \\big) \\\\ \u0026amp;= \\log \\frac{2}{( 1 + e^{-\\frac{1}{2} ( D_\\text{KL}(p||q) + D_\\text{KL}(q||p) )} )} \\end{aligned} \\] This upper bound is attributed to Crooks. Since the KL-divergence can go to positive infinity, we can conclude that \\(\\JSD(p;q)\\) is upper-bounded by \\(\\log 2\\). The \\(\\newcommand{\\J}{\\mathop{\\text{J}}} \\J(p;q) \\coloneq \\frac{1}{2} ( D_\\text{KL}(p||q) + D_\\text{KL}(q||p) )\\) term is also known as Jeffreys divergence (the coefficient \\(\\frac{1}{2}\\) may be ignored in some other place). Even more accurately, the upper bound can be rewritten as (in this note)\n\\[ \\JSD(p;q) \\le \\min (\\frac{1}{4} \\J(p;q), \\log \\frac{2}{1 + e^{-\\J(p;q)}}) \\] A lower bound in terms of Jeffreys divergence can be derived as (in this note)\n\\[ \\JSD(p;q) \\ge \\frac{1}{4} \\ln(1 + 2\\J(p;q)) \\]\nRemarkably, the square root of JSD between two distributions satisfies the metric axioms.\nRelation with Entropy By definition, \\[ \\begin{aligned} \u0026amp;\\JSD(p;q) = \\frac{1}{2} (D_\\text{KL}(p || \\frac{p+q}{2}) + D_\\text{KL}(q || \\frac{p+q}{2})) \\\\ \u0026amp;= \\frac{1}{2} [H(p||\\frac{p+q}{2}) - H(p) + H(q||\\frac{p+q}{2}) - H(q)] \\\\ \u0026amp;= \\begin{aligned}[t] \u0026amp;-\\frac{1}{2} [\\E_{x \\sim p} \\log \\frac{p+q}{2}(x) + \\E_{x \\sim q} \\log \\frac{p+q}{2}(x)] \\\\ \u0026amp;- \\frac{1}{2} [H(p) + H(q)] \\\\ \\end{aligned} \\\\ \u0026amp;= -\\E_{x \\sim \\frac{p+q}{2}} \\frac{p+q}{2}(x) - \\frac{1}{2} [H(p) + H(q)] \\\\ \u0026amp;= H(\\frac{p+q}{2}) - \\frac{1}{2} [H(p) + H(q)] \\end{aligned} \\]\n","date":1651590621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651590621,"objectID":"c1c10226a5a23918fda252703ca375bb","permalink":"https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/jenson-shannon-divergence/","section":"notes","summary":"Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric.","tags":null,"title":"Jenson-Shannon Divergence","type":"book"},{"authors":null,"categories":null,"content":" 以下以二维随机变量为例，展示协方差以及相关系数的概念。\n协方差 设\\([X, Y]\\)为一组二维随机变量，如果\\(\\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\}\\)存在，则称 \\[ \\notag \\mathrm {Cov}(X, Y) \\triangleq \\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\} \\] 为随机变量\\(X\\)和\\(Y\\)的协方差。在实际中计算协方差时，更多的是使用以下公式：\n\\[ \\begin{aligned} \u0026amp;\\mathrm {Cov}(X, Y) = \\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\} \\\\ \u0026amp;= \\mathrm E[XY - X\\mathrm E(Y) - \\mathrm E(X)Y + \\mathrm E(X) \\mathrm E(Y)] \\\\ \u0026amp;= \\mathrm E(XY) - \\mathrm E(X)\\mathrm E(Y) - \\mathrm E(X)\\mathrm E(Y) + \\mathrm E(X) \\mathrm E(Y) \\\\ \u0026amp;= \\mathrm E(XY) - \\mathrm E(X) \\mathrm E(Y) \\end{aligned} \\] 而二维随机变量[X, Y]对应的协方差矩阵即为\n\\[ \\Sigma = \\begin{bmatrix} \\Cov(X,X) \u0026amp; \\Cov(X,Y) \\\\ \\Cov(Y,X) \u0026amp; \\Cov(Y,Y) \\\\ \\end{bmatrix} \\]\n相关系数 协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们可以首先将随机变量标准化： \\[ X^\\star = \\frac{X - \\E(X)}{\\sqrt{\\Var(X)}}，Y^\\star = \\frac{Y - \\E(Y)}{\\sqrt{\\Var(Y)}} \\] 再求协方差\\(\\Cov(X^\\star, Y^\\star)\\)，这便是随机变量\\(X\\)和\\(Y\\)的相关系数： \\[ \\rho(X, Y) = \\mathrm{Cov}(X^\\star, Y^\\star) = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}} \\]\n","date":1651401675,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651401675,"objectID":"64051c078bf20447141ce8e7a654b55f","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/","section":"notes","summary":"以下以二维随机变量为例，展示协方差以及相关系数的概念。 协方差","tags":null,"title":"协方差与相关系数","type":"book"},{"authors":null,"categories":null,"content":" Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, ..., \\sigma_K)\\), when distance between \\(X\\) and \\(Y\\) is defined as Frobenius norm: \\[ ||X - Y||_F = \\sqrt{\\sum_{ij}(X - Y)_{ij}^2} \\] Firstly we prove that, if \\(U\\) is orthogonal, then \\(||UA||_F = ||AU||_F = ||A||_F\\): \\[ \\begin{aligned} ||UA||_F^2 \u0026amp;= tr((UA)^TUA) \\\\ \u0026amp;= tr(A^TUUA) \\\\ \u0026amp;= tr(A^TA) \\\\ \u0026amp;= ||A||_F^2 \\end{aligned} \\] The same can be derived for \\(||AU||_F\\).\nFor any \\(m \\times n\\) matrix \\(Y\\) of rank \\(K \\le R\\),\n\\[ \\begin{aligned} ||X - Y||_F^2 \u0026amp;= ||U\\Sigma V^T - Y||_F^2 \\\\ \u0026amp;= ||U^TU\\Sigma V^TV - U^TYV||_F^2 \\\\ \u0026amp;= ||\\Sigma - U^TYV||_F^2 \\\\ \\end{aligned} \\] Let \\(Z = U^TYV\\), \\(Z\\) is also of rank \\(K\\). \\[ \\begin{aligned} ||X - Y||_F^2 \u0026amp;= ||\\Sigma - Z||_F^2 \\\\ \u0026amp;= \\sum_{ij}(\\Sigma_{ij} - Z_{ij})^2 \\\\ \u0026amp;= \\sum_{i=1}^R(\\sigma_{i} - Z_{ii})^2 + \\sum_{i\u0026gt;R}^{\\min\\{M,N\\}}Z_{ii}^2 + \\sum_{i \\ne j}Z_{ij}^2 \\\\ \\end{aligned} \\] The minimum is achieved if \\[ \\begin{gather} Z_{ii} = \\sigma_{i}, i = 1, 2, \\dots, K \\\\ Z_{ii} = \\sigma_{i}, i = K, K + 1, \\dots, R - 1 \\\\ Z_{ii} = 0, i = R, R + 1, \\dots, \\min\\{M,N\\} \\\\ Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \\dots, N, i \\ne j \\end{gather} \\] Such \\(Z\\) exists when \\(Y = U\\Sigma_KV^T\\).\nEckart-Young-Mirsky Theorem also holds for spectral norm .\n","date":1641559250,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559250,"objectID":"b8c76cf40262b9a1aceb4e7c878e7cb7","permalink":"https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/eckart-young-mirsky-theorem/","section":"notes","summary":"Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, .","tags":null,"title":"Eckart-Young-Mirsky Theorem","type":"book"},{"authors":null,"categories":null,"content":" Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form:\n\\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]\nTo solve it, we try to find the eigenvalues and eigenvectors of \\(A\\): \\[ A - \\lambda I = \\left[ \\begin{array} \\\\ a_{t-1} - \\lambda \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; -\\lambda \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; -\\lambda \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; -\\lambda \\end{array} \\right] \\] Apply Laplacian expansion along the first column to get its determinant as \\[ \\det (A - \\lambda I) = (a_{t-1} -\\lambda) (-\\lambda)^{t-1} - \\det B_{t-1} \\] where \\[ B_{t-1} = \\underbrace{ \\begin{bmatrix} a_{t-2} \u0026amp; a_{t-3} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; -\\lambda \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; -\\lambda \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; -\\lambda \\end{bmatrix} }_\\text{$t-1$ columns} \\] Apply Laplacian expansion along the first column of \\(B_{n-1}\\) to give the following recurrence relation: \\[ \\left. \\begin{array} \\\\ \\det B_{t-1} = a_{t-2} (-\\lambda)^{t-2} - \\det B_{t-2} \\\\ \\det B_1 = a_0 \\end{array} \\right\\} \\Rightarrow \\det B_{t-1} = (-1)^t (a_{t-2} \\lambda^{t-2} + a_{t-3} \\lambda^{t-3} + \\dots + a_0) \\] In all, \\[ \\begin{aligned} \\det (A - \\lambda I) \u0026amp;= (a_{t-1} - \\lambda)(-\\lambda)^{t-1} - (-1)^t (a_{t-2} \\lambda^{t-2} + a_{t-3} \\lambda^{t-3} + \\dots + a_0) \\\\ \u0026amp;= (-1)^t (\\lambda^t - a_{t-1} \\lambda^{t-1} - a_{t-2} \\lambda^{t-2} - \\dots - a_0) \\end{aligned} \\]\nAfter solving the eigenvalues \\(\\lambda_1 \\ge \\dots \\ge \\lambda_t\\) and corresponding eigenvectors \\(\\newcommand{\\v}{\\mathrm{v}} \\v_1, \\dots, \\v_t\\) from above equation, we can rewrite the vector formed by the initial \\(t\\) terms as the linear combination of \\(\\v_1, \\dots, \\v_t\\), say \\[ \\x_0 = \\left[ x_{t-1}, \\cdots, x_0 \\right]^T = c_1 \\v_1 + \\dots c_t \\v_t \\] Then for every \\(n \\ge t\\), \\[ x_n = \\left[ A^{n-t+1} \\x_0 \\right]_0 = \\left[ \\lambda_1^{n-t+1} c_1 \\v_1 + \\dots + \\lambda_t^{n-t+1} c_t \\v_t\\right]_0 \\] \\(x_n\\) will be asymptotic to \\(\\lambda_1^{n}\\) as \\(n \\to \\infty\\) if \\(\\lambda_1\\) is strictly larger than other eigenvalues.\n","date":1640462913,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640462913,"objectID":"92ec8bc2f4c577d51cece5567f970741","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/difference-equation/","section":"notes","summary":"Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form:\n\\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026 a_{t-2} \u0026 \\cdots \u0026 a_1 \u0026 a_0 \\\\ 1 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 \\cdots \u0026 0 \u0026 \\vdots \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\cdots \u0026 1 \u0026 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]","tags":null,"title":"Difference Equation","type":"book"},{"authors":null,"categories":null,"content":" Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in R^{N \\times 1}\\), find the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\). Columns of \\(A\\) are the propagation weights from a signal source to observers. Therefore they are considered independent (and are thus called independent components).\n\\(A\\) is called mixing matrix and \\(W = A^{-1}\\) is called unmixing matrix because \\(S = WX\\).\nSuppose \\(A = U\\Sigma V^T\\) by SVD. Assume observations are pre-centered:\n\\[ \\sum_{i=1}^Mx^{(i)} = 0 \\] Assume that signals are independent and the variance of each signal is \\(1\\) (taking the advantage of scale/sign ambiguity): \\[ \\frac{1}{M}\\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I \\] Then the covariance of \\(X\\) can be calculated as \\[ \\begin{aligned} C \u0026amp;= \\frac{1}{M}x^{(i)}(x^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}As^{(i)}(As^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}U\\Sigma V^Ts^{(i)}(U\\Sigma V^Ts^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}U\\Sigma V^Ts^{(i)}(s^{(i)})^TV\\Sigma^T U^T \\\\ \u0026amp;= U\\Sigma V^TIV\\Sigma^T U^T \\\\ \u0026amp;= U\\Sigma^2U^T \\text{ (by property of SVD, note that $\\Sigma$ is $n \\times n$ diagonal)} \\\\ \\end{aligned} \\]\nIf our assumptions are correct, then the \\(U\\) is the concatenated eigenvectors of the matrix \\(CC^T\\). \\(\\Sigma^2\\) is the diagonal matrix consisting of corresponding eigenvalues of the matrix \\(C^TC\\). Since \\(C\\) is symmetric, \\(CC^T = C^TC = C^2\\).\nIn other words, \\(U\\) and \\(\\Sigma\\) can be solved by eigen decomposition. Define \\(\\hat x^{(i)} = \\Sigma^{-1}U^Tx^{(i)}\\): \\[ \\begin{aligned} \\hat C \u0026amp;= \\frac{1}{M}\\hat x^{(i)}(\\hat x^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}\\Sigma^{-1}U^Tx^{(i)}(\\Sigma^{-1}U^Tx^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}\\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\\Sigma^{-1} \\\\ \u0026amp;= \\Sigma^{-1}U^T(\\frac{1}{M}x^{(i)}(x^{(i)})^T)U\\Sigma^{-1} \\\\ \u0026amp;= \\Sigma^{-1}U^TU\\Sigma^2U^TU\\Sigma^{-1} \\\\ \u0026amp;= I \\\\ \\end{aligned} \\]\n\\[ S = WX = A^{-1}X = V\\Sigma^{-1}U^TX = V\\hat X \\]\nThe remaining job is to find the \\(V\\). We resort to multi-information, a generalization of mutual-information from Information Theory to judge how close a distribution is to statistical independence for multiple variables. \\[ I(s) = \\sum_{s \\in \\mathcal{S}}p(s)log\\frac{p(s)}{\\prod_jp(s_j)} \\] It is a non-negative quantity that reaches the minimum if and only if all variables are statistically independent.\nThe multi-information can be written as a function of entropy, which is defined as \\[ H(s) = -\\sum_{s \\in \\mathcal{S}}p(s)logp(s) \\] The multi-information can be written as the difference between the sum of entropies of marginal distribution and the entropy of joint distribution \\[ \\begin{aligned} I(s) \u0026amp;= \\sum_jH(s_j) - H(s) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - H(V\\hat x) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - (H(\\hat x) + log|V|) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - (H(\\hat x) + log1) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - H(\\hat x) \\\\ \\end{aligned} \\] Because we are assuming signal are independent from each other, \\[ \\begin{aligned} V^\\star \u0026amp;= \\arg \\min_V\\sum_jH((V\\hat x)_j) - H(\\hat x) \\\\ \u0026amp;= \\arg \\min_V\\sum_jH((V\\hat x)_j) \\end{aligned} \\] Calculating entropy and then taking derivative is no easy task and therefore ICA algorithms focus on approximations or equivalences to the above equation. For example, it can be approximated by finding the rotation that maximizes the expected log-likelihood of the observed data by assuming that the source distributions are known.\nNon-Gaussian and PCA The key assumption of ICA is that signals are non-Gaussian. We are trying to recover the \\(S\\) (by finding \\(W\\)) to be as non-Gaussian as possible. \\(\\hat X\\) consists of independent components because its covariance matrix \\(\\hat C\\) is shown to be an identity matrix (and thus diagonal). \\(S\\) is reconstructed by the linear combination of independent components. Thus it will be the most non-Gaussian when each variable is formed by exactly one component of \\(\\hat X\\) by Central Limit Theorem, i.e. \\(S\\)’s components are independent.\nConsider the case when \\(S\\) consists of \\(N\\) independent Gaussian, which invalidate the above analysis. If we forcibly calculate \\(V^\\star\\), due to the property of multi-variate Gaussian that its iso-density maps are spherical, then any \\(N \\times N\\) rotation matrix will be a solution to Equation (9), including the left singular matrix of \\(X\\), i.e. the concatenated eigenvectors of \\(XX^T\\), which is exactly the projection basis obtained in PCA. If any \\(N \\times N\\) rotation matrix can be a solution, there are infinite many solutions to \\(A\\), and thus ICA will just fail.\nThe conclusion drawn above is based on the ideal case, i.e. many enough observations. Even in the real case where signals are Gaussian, we may get a unique solution to \\(V^\\star\\).\nExternal Materials ICA\n","date":1639909003,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639909003,"objectID":"1afb1b9362a37f571fdcad44487772cb","permalink":"https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/independent-component-analysis/","section":"notes","summary":"Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in R^{N \\times 1}\\), find the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\).","tags":null,"title":"Independent Component Analysis","type":"book"},{"authors":null,"categories":null,"content":" Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to robustly fit the model in the presence of outliers.\nRANSAC does the following:\n Randomly sample a subset of data of an fairly enough amount for training. Fit a model to the this subset. Determine data points in the whole data set as inliers or outliers by comparing the residuals (prediction errors) to a threshold. The set of inliers is called a consensus set. Repeat above for some iterations and retrain the final model with the largest consensus set (since inliers should be the majority).  Parameters of RANSAC include:\n \\(s\\) - number of points to fit the model\n \\(t\\) - threshold of the residual\n \\(e\\) - proportion the outliers\n \\(\\delta\\) - probability of success (at least one iteration is finished with no outlier)\n \\(T\\) - number of iterations to be determined\n  Then,\n \\(p\\text{(training subset has no outliers)} = (1 - e)^s\\) \\(p\\text{(training subset has at least one outlier)} = 1 - (1 - e)^s\\) \\(p\\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T\\)  We want \\[ \\begin{gather} p\\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T \u0026lt; 1 - \\delta \\\\ T \u0026gt; \\log\\frac{1 - \\delta}{1 - (1 - e)^s} \\end{gather} \\] The threshold \\(t\\) is usually set as the median absolute deviation of \\(y\\).\n随机抽样一致算法（Random sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)\n","date":1650472711,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650472711,"objectID":"87967618cd135ba7baa60243cf5eea00","permalink":"https://chunxy.github.io/notes/articles/machine-learning/ransac/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/ransac/","section":"notes","summary":"Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to","tags":null,"title":"RANSAC","type":"book"},{"authors":null,"categories":null,"content":" Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)). We seek to obtain a scalar \\(z\\) by projecting \\(x\\) onto a unit vector \\(v\\): \\(z^{(i)} = v^Tx^{(i)}\\). Of all the possibilities we would like to select the one that maximizes the separability of the scalars between classes.\nThe mean of each class in the \\(x\\)-space and the \\(z\\)-space is \\[ \\mu_c = \\frac{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]x^{(i)}}{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]} \\\\ \\tilde \\mu_c = \\frac{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]v^T x^{(i)}}{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]} = v^T\\mu_c\\\\ \\] The scatter of each class in the \\(x\\)-space and the \\(z\\)-space is defined as \\[ \\begin{aligned} S_c \u0026amp;= \\sum_{i=1}^M \\mathbb I[y^{(i)} = c](x^{(i)} - \\mu_c)(x^{(i)} - \\mu_c)^T \\\\ \\tilde s^2_c \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c](z^{(i)} - \\tilde u_c)^2 \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)} - u_c) \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} - u_c)^Tv \\\\ \u0026amp;= v^TS_cv \\end{aligned} \\] FLD suggests in \\(z\\)-space maximizing the distance among the means of classes (inter-class scatter) and minimizing the variance over each class (within-class scatter), i.e. \\[ \\begin{aligned} \\max_v J(v) \u0026amp;\\coloneq \\frac{(\\tilde \\mu_1 - \\tilde \\mu_2)^2}{\\tilde s^2_1 + \\tilde s^2_2} \\\\ \u0026amp;= \\frac{(v^T\\mu_1 - v^T\\mu_2)^2}{v^TS_1v + v^TS_2v} \\\\ \u0026amp;= \\frac{v^T(\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^Tv}{v^T(S_1 + S_2)v} \\\\ \\end{aligned} \\] \\(S_W = S_1 + S_2\\) is called within-class scatter, \\(S_B = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^T\\) is called inter-class scatter. Then, \\[ J(v) = \\frac{v^TS_Bv}{v^TS_Wv} \\\\ \\]\nTake derivative w.r.t. \\(v\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial J}{\\partial v} \u0026amp;= v^TS_Wv \\frac{\\partial v^TS_Bv}{\\partial v} - v^TS_Bv \\frac{\\partial v^TS_Wv}{\\partial v}\u0026amp; \\\\ \u0026amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv \u0026amp;= 0 \\\\ S_Bv - \\frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv \u0026amp;= 0 \\\\ S_Bv \u0026amp;= J(v) S_Wv \\end{aligned} \\]\nIf \\(S_W\\) is invertible, \\(v\\) is some eigenvector of \\(S^{-1}_WS_B\\): \\[ S^{-1}_WS_Bv = J(v) v \\]\n\\(S_Bv = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^Tv = \\alpha(\\mu_1 - \\mu_2)\\) always points to the same direction as \\((\\mu_1 - \\mu_2)\\). Thus we can immediately give a \\(v\\) and verify: \\[ v = S^{-1}_W(\\mu_1 - \\mu_2) \\] \\[ S^{-1}_WS_B\\underbrace{S^{-1}_W(\\mu_1 - \\mu_2)}_v = S^{-1}_W\\alpha(\\mu_1 - \\mu_2) = \\underbrace{\\alpha}_{J(v)} \\underbrace{S^{-1}_W(\\mu_1 - \\mu_2)}_v \\\\ \\]\nMulti-class ","date":1641563438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641563438,"objectID":"da6eeb056fd655413d43b9cc90763227","permalink":"https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/fishers-linear-discriminant/","section":"notes","summary":"Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)).","tags":null,"title":"Fisher's Linear Discriminant","type":"book"},{"authors":null,"categories":null,"content":" The notation used is as follows:\n  Symbol Notation    \\(\\mathcal D\\) the dataset  \\(x\\) the sample  \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise  \\(y\\) the real value of \\(x\\)  \\(\\bar y\\) the mean of the real values  \\(f\\) the model learned with \\(\\mathcal D\\)  \\(f(x)\\) the prediction of \\(f\\) with \\(x\\)  \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\)  \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error    By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026amp;[l(f(x), y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026amp;\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y)^2] + E[(y - y_\\mathcal D)^2] + 2E[(\\bar f(x) - y)(y - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026amp;\\quad + 2E[\\bar f(x) - y] \\underbrace{E[y - y_\\mathcal D]}_0 \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] + 2E[(\\bar f(x) - \\bar y)(\\bar y - y)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] \\\\ \u0026amp;\\quad + 2E[\\bar f(x) - \\bar y]\\underbrace{E[\\bar y - y]}_0 \\\\ \u0026amp;= \\underbrace{E[(f(x) - \\bar f(x))^2]}_{variance} + \\underbrace{E[(\\bar f(x) - \\bar y)^2]}_{bias^2} + \\underbrace{E[(y - y_\\mathcal D)^2]}_{noise} + \\underbrace{E[(\\bar y - y)^2]}_{scatter} \\\\ \\end{aligned} \\] 5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium\n","date":1641562759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641562759,"objectID":"3dc8edadde35832274ff1a6f19b70d23","permalink":"https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/bias-variance-decomposition/","section":"notes","summary":"The notation used is as follows:\n  Symbol Notation    \\(\\mathcal D\\) the dataset  \\(x\\) the sample  \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise  \\(y\\) the real value of \\(x\\)  \\(\\bar y\\) the mean of the real values  \\(f\\) the model learned with \\(\\mathcal D\\)  \\(f(x)\\) the prediction of \\(f\\) with \\(x\\)  \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\)  \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error    By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026[l(f(x), y_\\mathcal D)] \\\\ \u0026= E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y)^2] + E[(y - y_\\mathcal D)^2] + 2E[(\\bar f(x) - y)(y - y_\\mathcal D)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026\\quad + 2E[\\bar f(x) - y] \\underbrace{E[y - y_\\mathcal D]}_0 \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] + 2E[(\\bar f(x) - \\bar y)(\\bar y - y)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] \\\\ \u0026\\quad + 2E[\\bar f(x) - \\bar y]\\underbrace{E[\\bar y - y]}_0 \\\\ \u0026= \\underbrace{E[(f(x) - \\bar f(x))^2]}_{variance} + \\underbrace{E[(\\bar f(x) - \\bar y)^2]}_{bias^2} + \\underbrace{E[(y - y_\\mathcal D)^2]}_{noise} + \\underbrace{E[(\\bar y - y)^2]}_{scatter} \\\\ \\end{aligned} \\] 5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium","tags":null,"title":"Bias-variance Decomposition","type":"book"},{"authors":null,"categories":null,"content":" 设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，那么\\(f(X)\\)的期望以及分布情况会是什么样的呢？\n我们这里只讨论\\(f\\)是单调函数的情况，令\\(Y = f(X)\\)，那么 \\[ P_Y(y) = P_Y(Y \\le y) = P_X(f(X) \\le y) \\]\n 若\\(f\\)单调递增， \\[ \\begin{gather} P_Y(f(X) \\le y) = P_X(X \\le f^{-1}(y)) = P_X(f^{-1}(y)) \\\\ \\nonumber \\\\ \\begin{split} p_Y(y) \u0026amp;= \\frac{\\partial P_Y(Y \\le y)}{\\partial y} \\\\ \u0026amp;= \\frac{\\partial P_X(f^{-1}(y))}{\\partial y} \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot (f^{-1})^\\prime (y) \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)| \\\\ \\end{split} \\end{gather} \\]\n 若\\(f\\)单调递减， \\[ \\begin{gather} P_Y(f(X) \\le y) = P_X(X \\ge f^{-1}(y)) = 1 - P_X(X \\le f^{-1}(y)) = 1 - P_X(f^{-1}(y)) \\\\ \\nonumber \\\\ \\begin{split} p_Y(y) \u0026amp;= \\frac{\\partial P_Y(Y \\le y)}{\\partial y} \\\\ \u0026amp;= \\frac{\\partial [1 - P_X(f^{-1}(y))]}{\\partial y} \\\\ \u0026amp;= -p_X(f^{-1}(y)) \\cdot (f^{-1})^\\prime (y) \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)| \\\\ \\end{split} \\end{gather} \\]\n  总而言之，\\(p_Y(y) = p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)|\\)。\n","date":1648721304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648721304,"objectID":"c1f19050491a0688b653fc4bc963d7c1","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/","section":"notes","summary":"设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，","tags":null,"title":"随机变量的函数","type":"book"},{"authors":null,"categories":null,"content":" 前置知识 Chebyshev不等式  定理\n设随机变量\\(X\\)的期望\\(\\E(X)\\)及方差\\(\\Var(X)\\)存在，则对于任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ P(|X-\\E(X)| \\ge \\epsilon) \\le \\frac{\\Var(X)}{\\epsilon^2} \\]\n 证明\n\\(X\\)为连续型随机变量 \\[ \\begin{aligned} \u0026amp;P(|X-\\E(X)| \\ge \\epsilon) = \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} p(x)dx \\\\ \u0026amp;\\le \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} \\bigg( \\frac{X - \\E(x)}{\\epsilon} \\bigg)^2 p(x)dx \\\\ \u0026amp;= \\frac{1}{\\epsilon^2} \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} \\big( X - \\E(x) \\big)^2 p(x)dx \\\\ \u0026amp;\\le \\frac{1}{\\epsilon^2} \\mathop \\int_{x \\in X} \\big( X - \\E(x) \\big)^2 p(x)dx \\\\ \u0026amp;= \\frac{\\Var(X)}{\\epsilon^2} \\\\ \\end{aligned} \\]\n \\(X\\)为离散型随机变量 \\[ \\begin{aligned} \u0026amp;P(|X-\\E(X)| \\ge \\epsilon) = \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} P(x) \\\\ \u0026amp;\\le \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} \\bigg( \\frac{x - \\E(x)}{\\epsilon} \\bigg)^2 P(x) \\\\ \u0026amp;= \\frac{1}{\\epsilon^2} \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} \\big( x - \\E(x) \\big)^2 P(x) \\\\ \u0026amp;\\le \\frac{1}{\\epsilon^2} \\mathop \\sum_{x \\in X} \\big( x - \\E(x) \\big)^2 P(x) \\\\ \u0026amp;= \\frac{\\Var(X)}{\\epsilon^2} \\\\ \\end{aligned} \\]\n   Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是作为证明其它大数定理的基础工具。\n依概率收敛 随机变量序列即是由随机变量构成。对于一个普通数列\\(\\{x_n\\}\\)来说，若其收敛于\\(c\\)，则当\\(n\\)充分大时，\\(x_n\\)和\\(c\\)的距离可以达到任意小。而随机变量序列\\(X_1, X_2, \\dots\\)的极限却不能按照这样定义，因为\\(X_n\\)取值不确定，不可能和某个数字\\(c\\)的距离任意小。\n随机变量是事件的映射，当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。\n 定义\n设\\(X_1, X_2, \\dots,\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \u0026lt; \\epsilon) = 1\\)，则称该随机变量序列依概率收敛于\\(c\\)，记作\\(X_n \\stackrel{P}{\\to} c\\)。或者，对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \\ge \\epsilon) = 0\\)。 ### Markov不等式\n  Chebyshev不等式其实是Markov不等式的一个特例。令\\(X\\)为一非负随机变量、\\(\\alpha\\)为一非负实数，Markov不等式描述的是以下关系： \\[ P(X \\ge \\alpha) \\le \\frac{\\E(X)}{\\alpha} \\] 以连续型随机变量为例，证明如下： \\[ P(X \\ge \\alpha) = \\int_{x \\ge \\alpha} p(x) \\d x \\le \\int_{x \\ge \\alpha} \\frac{x}{\\alpha} p(x) \\d x \\le \\int_x \\frac{x}{\\alpha} p(x) \\d x = \\frac{\\E(X)}{\\alpha} \\] 由于\\(|X - \\E(x)| \\ge \\epsilon \\iff (X - \\E(X))^2 \\ge \\epsilon^2\\)，将Markov不等式中的的\\(X\\)替换为\\((X - \\E(X))^2\\)、\\(\\alpha\\)替换为\\(\\epsilon^2\\)，即可得到Chebyshev不等式。不过由于Markov不等式有随机变量非负的要求，适用范围就小了一些；而且同样，Markov不等式的这种估计也是很粗糙的。\n弱大数定律（Weak Law of large numbers） Chebyshev大数定律  定理\n设随机变量序列\\(X_1,X_2,\\dots\\)两两不相关，若存在常数\\(c\\)，使得\\(\\Var(X_i) \\le c \\ne +\\infty, i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \u0026lt; \\epsilon) = 1 \\] 亦即\\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{P}{\\to} \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)\\)。\n 证明\n由于该随机序列两两不相关，故根据期望及方差的性质， \\[ \\E(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n} \\sum_{i=1}^n \\E(X_i),\\quad \\Var(\\frac{1}{n} \\sum_{i=1}^N X_i) = \\frac{1}{n^2} \\sum_{i=1}^n \\Var(X_i) \\le \\frac{c}{n} \\] 根据切比雪夫不等式， \\[ \\begin{gather} 0 \\le P(|\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) \u0026lt; \\frac{\\Var(\\frac{1}{n} \\sum_{i=1}^N X_i)}{\\epsilon^2} \\le \\frac{c}{n \\epsilon} \\\\ \\underbrace{\\lim_{n \\to \\infty} 0}_0 \\le \\lim_{n \\to \\infty} P(\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) \\le \\underbrace{\\lim_{n \\to \\infty} \\frac{c}{n \\epsilon}}_0 \\Rightarrow\\\\ \\lim_{n \\to \\infty} P(\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) = 0 \\end{gather} \\]\n  Khinchin大数定律 相互独立同分布大数定律  设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu, \\Var(X_i) = \\sigma^2 \\ne \\infty, i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P (|\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu| \u0026lt; \\epsilon) = 1 \\]  相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家Khinchin证明该定律依然成立，即：\n 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P (|\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu| \u0026lt; \\epsilon) = 1 \\]  Bernoulli大数定律  随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(X_i \\sim B(1,p), i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\frac{1}{n} \\sum_{i=1}^n X_i - p| \u0026lt; \\epsilon) = 1 \\]  显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。\n中心极限定理 注意，在本节中，我们用\\(\\Phi\\)表示标准正态分布的分布函数。\nLindburg-Levy中心极限定理 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu, \\Var(X_i) = \\sigma^2 \\ne \\infty, i=1,2,\\dots\\)，则对任意实数\\(x\\)，有 \\[ \\lim_{n \\to \\infty} P(\\frac{\\sum_{i=1}^n X_i - n \\mu}{\\sqrt n \\sigma} \\le x) = \\Phi(x) \\]\nde Moivre-Laplace中心极限定理 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，且\\(X_i \\sim B(1,p), i=1,2,\\dots\\)，则对任意实数\\(x\\)，有 \\[ \\lim_{n \\to \\infty} P(\\frac{\\sum_{i=1}^n X_i - np}{\\sqrt{np(1-p)}} \\le x) = \\Phi(x) \\] 显然de Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。\n前面的Bernoulli大数定律告诉我们可以用\\(\\frac{1}{n} \\sum_{i=1}^n X_i\\)（频率）近似\\(p\\)（概率），而至于近似程度如何，却不得而知。de Moivre-Laplace中心极限定理则告诉我们当\\(n\\)足够大时，近似程度如何： \\[ P(|\\frac{1}{n}\\sum_{i=1}^n X_i - p| \\le \\epsilon) = P(|\\frac{\\sum_{i=1}^n X_i - np}{\\sqrt{np(1-p)}}| \\le \\frac{\\sqrt n \\epsilon}{\\sqrt{p(1-p)}}) \\simeq 2\\Phi(\\frac{\\sqrt n \\epsilon}{\\sqrt{p(1-p)}}) - 1 \\]\n上式实际是在用正态分布近似二项分布（多个伯努利分布随机变量加和为伯努利分布），比如在Galton Board游戏中，我们就可以应用de Moivre-Laplace中心极限定理来近似实际概率。\n","date":1653038853,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653038853,"objectID":"2452e7b6b1863a32e0e9c71664055c9b","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/","section":"notes","summary":"前置知识 Chebyshev不等式 定理 设随机变量\\(X\\)的期","tags":null,"title":"大数定律和中心极限定理","type":"book"},{"authors":null,"categories":null,"content":" \\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由正态分布衍生而来，常见统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。\n前置知识 Gamma函数 \\[ \\Gamma (x) = \\int_0^{+\\infty} e^{-t} t^{x-1} dt \\quad (x \u0026gt; 0) \\]\n\\(\\Gamma\\)函数具有\\(\\Gamma(x + 1) = x \\Gamma(x)\\)的性质：\n\\[ \\Gamma (x+1) = \\int_0^{+\\infty} e^{-t} t^{x} dt = [-e^{-t} t^x] \\bigg|^{+\\infty}_{t=0} - \\int_0^{+\\infty} -e^{-t} xt^{x-1} dt \\] 根据洛必达法则，\\(\\lim_{t \\to +\\infty} = \\frac{-t^x}{e^t} = \\lim_{t \\to +\\infty} \\frac{x!}{e^t} = 0\\)，故 \\[ \\Gamma (x+1) = 0 + x\\int_0^{+\\infty} e^{-t} t^{x-1} dt = x \\Gamma(x) \\]\n \\(\\Gamma(1) = \\int_0^{+\\infty} e^{-t} dt = 1\\)，故\\(x\\)为正整数时，\\(\\Gamma(x) = x!\\)；\n \\(\\Gamma(1/2) = \\sqrt \\pi\\)，故\\(x = 2k + 1\\)为正奇数时，\\(\\Gamma(\\frac{x}{2}) = \\sqrt \\pi \\prod_{i=0}^{k-1} \\frac{2 * i + 1}{2}\\)：\n\\[ \\begin{gather} \\Gamma (\\frac{1}{2}) = \\int_0^{+\\infty} e^{-t} t^{-\\frac{1}{2}} \\d t \\stackrel{u = t^\\frac{1}{2}}{\\Longrightarrow} \\int_0^{+\\infty} e^{-u^2} u^{-1} \\;2u \\d u = 2 \\int_0^{+\\infty} e^{-u^2} \\d u = \\int_{-\\infty}^{+\\infty} e^{-u^2} \\d u \\\\ \\notag \\\\ \\begin{aligned} \u0026amp;\\Gamma^2(\\frac{1}{2}) = (\\int_{-\\infty}^{+\\infty} e^{-u^2} \\d u)^2 \\\\ \u0026amp;= (\\int_{-\\infty}^{+\\infty} e^{-u^2} du)(\\int_{-\\infty}^{+\\infty} e^{-v^2} \\d v) \\\\ \u0026amp;= \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-(u^2+v^2)} \\d u \\d v \\\\ \u0026amp;\\downarrow_{u = r\\sin\\theta, v = r\\cos\\theta} \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} e^{-r^2}\\; r \\d r \\d \\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} [-\\frac{1}{2}e^{-r^2}] \\bigg|_{r=0}^{+\\infty} \\d \\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\frac{1}{2} \\d \\theta \\\\ \u0026amp;= \\pi \\\\ \u0026amp;\\Gamma (\\frac{1}{2}) = \\sqrt \\pi \\end{aligned} \\end{gather} \\]\n  参考 The Gamma Function || The derivation of \\(\\Gamma(1/2)\\)\n\\(\\chi^2\\)分布 设\\(X_1,\\dots,X_n\\)为相互独立的标准正态分布随机变量，即\\(X_i \\sim N(0,1)\\)则称\\(Y = X_1^2 + \\dots + X_n^2\\)服从自由度为\\(n\\)的\\(\\chi^2\\)分布，记作\\(Y \\sim \\chi^2(n)\\)，其中\\(\\E[Y] = n, \\Var[Y] = 2n\\)。\n \\(n=1\\)时，容易得到\\(\\forall y \\le 0, P_Y(y) = 0, p_Y(y)= 0\\)， $$ \\[\\begin{align} \\begin{split} \\forall y \u0026gt; 0, P_Y(y) \u0026amp;= 2P_X(\\sqrt y) - 1 \\\\ \\end{split} \\\\ \\begin{split} \u0026amp;p_Y(y) = 2P_X\u0026#39;(\\sqrt y) \\frac 1 {2 \\sqrt y} \\\\ \u0026amp;= \\frac 1 {\\sqrt {2\\pi y}} e^{-\\frac 1 2 y} \\\\ \u0026amp;= \\frac 1 {2^\\frac{1}{2} \\Gamma(\\frac 1 2)} y^{\\frac 1 2 - 1} e^{-\\frac y 2} \\end{split} \\end{align}\\] \\[ $\\chi^2(1)$分布的密度函数为： \\] p_Y(y) =\n\\[\\begin{cases} \\frac 1 {2^\\frac{1}{2} \\Gamma(\\frac 1 2)} y^{\\frac 1 2 - 1} e^{-\\frac y 2}, \u0026amp;y \u0026gt; 0 \\\\ 0, \u0026amp; y \\le 0 \\end{cases}\\] $$\n \\(n=k\\)时，令\\(X_1,\\dots,X_k\\)表示一个\\(k\\)维空间中的点， \\[ \\begin{aligned} p_Y(y) = P_Y(Y \\le y) \u0026amp;= \\int_\\mathcal V \\prod_{i=1}^k N(0,1,x_i)\\; \\d x_1 \\dots \\d x_k \\\\ \u0026amp;= \\int_\\mathcal V \\frac{e^{-\\frac 1 2 (x_1^2 + \\dots + x_k^2)}} {(2\\pi)^{k / 2}}\\ \\d x_1 \\dots \\d x_k \\\\ \\end{aligned} \\] 其中\\(\\mathcal V\\)表示\\(\\sum_{i=1}^k x_i^2 \\le y\\)的积分区域。可以看出，\\(\\mathcal V\\)对应一个\\(k\\)维球体，且其半径\\(R = \\sqrt y\\)。对此，作高维球坐标变换： \\[ \\begin{aligned} \u0026amp;P_Y(Y \\le y) = \\int_\\mathcal V \\prod_{i=1}^k N(0,1,x_i)\\; \\d x_1 \\dots \\d x_k = \\int_\\mathcal V \\frac{e^{-\\frac 1 2 (x_1^2 + \\dots + x_k^2)}} {(2\\pi)^{k / 2}}\\ \\d x_1 \\dots \\d x_k \\\\ \u0026amp;= \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\int_0^\\sqrt{y} \\\\ \u0026amp;\\quad\\quad\\quad\\frac{e^{-\\frac 1 2 (r^2\\cos^2 \\varphi_1 + r^2\\sin^2 \\varphi_1 \\cos^2 \\varphi_2 + \\dots + r^2\\sin^2 \\varphi_1 \\dots \\sin^2 \\varphi_{k-2} \\cos^2 \\varphi_{k-1} + r^2\\sin^2 \\varphi_1 \\dots \\sin^2 \\varphi_{k-2} \\sin^2 \\varphi_{k-1})} } {(2\\pi)^{k / 2}}\\\\ \u0026amp;\\quad\\quad\\quad r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\ \\d r\\ \\d \\varphi_1 \\dots \\d \\varphi_k \\\\ \u0026amp;= \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\int_0^\\sqrt{y} \\frac{e^{-\\frac 1 2 r^2}} {(2\\pi)^{k / 2}} r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\ \\d r\\ \\d \\varphi_1 \\dots \\d \\varphi_k \\\\ \u0026amp;= \\int_0^\\sqrt{y} \\underbrace{ \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\frac{1} {(2\\pi)^{k / 2}} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2})\\ \\d \\varphi_1 \\dots \\d \\varphi_k}_{c_k} e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \u0026amp;= c_k \\int_0^\\sqrt{y} e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \\end{aligned} \\]\n其中\\(c_k\\)是和\\(k\\)相关的常数项，并且由于\\(P_Y(Y \\le \\infty) = 1\\)，有 \\[ \\begin{aligned} 1 \u0026amp;= c_k \\int_0^\\infty e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \u0026amp;\\Downarrow_{r = \\sqrt{2t}} \\\\ 1 \u0026amp;= c_k \\int_0^\\infty e^{-t} \\sqrt{2t}^{k-1} \\frac{1}{\\sqrt{2t}} \\d t \\\\ 1 \u0026amp;= 2^{(k-2)/2} c_k \\int_0^\\infty e^{-t} t^{(k-2)/2} \\d t \\\\ 1 \u0026amp;= 2^{(k-2)/2} c_k \\Gamma(\\frac{k}{2}) \\\\ c_k \u0026amp;= \\frac{1} {2^{(k-2)/2} \\Gamma(\\frac{k}{2})} \\end{aligned} \\]\n故可得密度函数： \\[ \\begin{aligned} \u0026amp;p_Y(y) = \\frac{\\d P_Y(Y \\le y)}{\\d y} \\\\ \u0026amp;= \\frac{\\d [c_k \\int_0^\\sqrt{y} e^{-\\frac 1 2 r^2} r^{k-1} \\d r]}{\\d y} \\\\ \u0026amp;= \\frac{1} {2^{(k-2)/2} \\Gamma(\\frac{k}{2})} e^{-\\frac y 2} y^\\frac{k-1}{2} \\frac{1}{2\\sqrt y} \\\\ \u0026amp;= \\frac{1} {2^{\\frac k 2} \\Gamma(\\frac{k}{2})} e^{-\\frac y 2} y^{\\frac{k}{2} - 1} \\end{aligned} \\]\n最终可得密度函数如下： \\[ p_Y(y) = \\begin{cases} \\frac 1 {2^\\frac{n}{2} \\Gamma(\\frac n 2)} e^{-\\frac y 2} y^{\\frac n 2 - 1}, \u0026amp;y \u0026gt; 0 \\\\ 0, \u0026amp; y \\le 0 \\end{cases} \\] 另外，该密度函数也可以通过数学归纳法验证。\n  参考 Chi Squared Distribution || Generating Function of Chi Squared …","date":1657405282,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657405282,"objectID":"fce05e30e309e1e125615c419259b0b9","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/","section":"notes","summary":"\\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由","tags":null,"title":"三大分布与正态总体的抽样分布","type":"book"},{"authors":null,"categories":null,"content":" Distribution or Random Variable? Both Cross Entropy and KL-divergence describe the relationship between two distributions.\nBoth Conditional Entropy and Mutual Information describe the relationship between two random variables.\nSince the notion of entropy is the basis of all other concepts in this section and the entropy is more well-defined on discrete random variable, they are more meaningful to be applied in discrete case, though they are usually easily extended to continuous case.\nKL-divergence, Entropy and Cross Entropy By definition, \\[ \\begin{align} D_{KL}(p||q) \u0026amp;= -\\mathrm{E}_{x \\sim p} \\log \\frac{q(x)}{p(x)} \\\\ \u0026amp;= -\\mathrm{E}_{x \\sim p} \\log q(x) - (-\\mathrm{E}_{x \\sim p} \\log {p(x)}) \\\\ \u0026amp;= H(p||q) - H(p) \\end{align} \\] By the convexity of \\(-\\log\\), \\[ \\begin{align} D_{KL}(p||q) \u0026amp;= \\mathrm{E}_{x \\sim p} [-\\log \\frac{q(x)}{p(x)}] \\\\ \u0026amp;\\ge -\\log(\\mathrm{E}_{x \\sim p} \\frac{q(x)}{p(x)}) \\\\ \u0026amp;= 0 \\end{align} \\] That is \\[ H(p||q) \\ge H(p) \\] The equality holds if and only if \\(p = q\\).\nKL-divergence and Mutual Information Given that \\(X\\) and \\(Y\\) have the same dimension, \\[ \\begin{aligned} \u0026amp;I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\\\ \u0026amp;= \\sum_{x,y} p(x) p(y|x) \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= \\sum_{x} \\sum_{y} p(x) p(y|x) \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= \\sum_{x} p(x) D_{KL}[p(y|x), p(y)] \\\\ \u0026amp;= \\E_{x} D_{KL}[p(y|x), p(y)] \\\\ \u0026amp;= \\E_{y} D_{KL}[p(x|y), p(x)] \\\\ \\end{aligned} \\]\n","date":1650662010,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650662010,"objectID":"c117287a25f16f15da190d218940051d","permalink":"https://chunxy.github.io/notes/articles/information-theory/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/overview/","section":"notes","summary":"Distribution or Random Variable? Both Cross Entropy and KL-divergence describe the relationship between two distributions.\nBoth Conditional Entropy and Mutual Information describe the relationship between two random variables.\nSince the notion of entropy is the basis of all other concepts in this section and the entropy is more well-defined on discrete random variable, they are more meaningful to be applied in discrete case, though they are usually easily extended to continuous case.","tags":null,"title":"Overview","type":"book"},{"authors":null,"categories":null,"content":"  定义\n设\\((X_1,\\dots,X_n)\\)为取自总体的一组样本，若函数\\(g(X_1,\\dots,X_n)\\)不直接包含总体分布中的任何参数，则称\\(g(X_1,\\dots,X_n)\\)为统计量。\n  样本均值和样本方差 \\[ \\begin{gather} \\text{样本均值：}\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\\\ \\text{样本方差：}S^2 = \\frac{1}{n-1} \\sum_{i=1}^N (X_i - \\bar X)^2 = \\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2) \\end{gather} \\]\n令\\(A_k = \\frac{1}{n} \\sum_{i=1}^n X_i^k\\)为样本的\\(k\\)阶原点矩，\\(M_k = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^k\\)为样本的\\(k\\)阶中心矩，当\\(k=2\\)时，\\(S_n^2 \\triangleq M_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2\\)。\n由于统计量是随机变量的函数，故统计量也是随机变量。设总体\\(X\\)的期望\\(\\E(X) = \\mu\\)，方差\\(\\Var(X) = \\sigma^2\\)，关于统计量有如下定理： \\[ \\begin{gather} \u0026amp; \\E(\\bar X) = \\mu, \\Var(\\bar X) = \\frac{\\sigma^2}{n} \\\\ \\notag \\\\ \u0026amp; \\E(S^2) = \\sigma^2, \\E(S_n^2) = \\frac{n-1}{n} \\sigma^2 \\\\ \\notag \\\\ \u0026amp; \\bar X \\stackrel{P}{\\to} \\mu, S^2 \\stackrel{P}{\\to} \\sigma^2, S_n^2 \\stackrel{P}{\\to} \\sigma^2 \\end{gather} \\] 有关证明如下： \\[ \\begin{gather} \\E(\\bar X) = \\E (\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n} \\sum_{i=1}^n \\E(X_i) = \\mu \\\\ \\Var(\\bar X) = \\Var(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n^2} \\sum_{i=1}^n D(X_i) = \\frac{\\sigma^2}{n} \\end{gather} \\]\n\\[ \\begin{gather} \\begin{aligned}[t] \\E(S^2) \u0026amp;= \\E [\\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{1}{n-1} \\big( \\sum_{i=1}^n \\E (X_i^2 ) - n \\E(\\bar X^2) \\big) \\\\ \u0026amp;\\Downarrow_ {\\E(X_i^2) = \\Var(X_i) + \\E^2(X_i) = \\sigma^2 + \\mu^2, \\E(\\bar X^2) = \\Var(\\bar X) + \\E^2(\\bar X) = \\frac{\\sigma^2}{n} + \\mu^2} \\\\ \u0026amp;= \\frac{1}{n-1} \\big( \\sum_{i=1}^n (\\sigma^2 + \\mu^2) - n (\\frac{\\sigma^2}{n} + \\mu^2) \\big) \\\\ \u0026amp;= \\sigma^2 \\end{aligned} \\begin{aligned}[t] \\E(S_n^2) \u0026amp;= \\E [\\frac{n-1}{n} \\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{n-1}{n} \\E [\\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{n-1}{n} \\sigma^2 \\end{aligned} \\end{gather} \\]\n归根结底，样本方差使用\\(\\frac{1}{n-1}\\)而不是\\(\\frac{1}{n}\\)的原因是，其使用的“均值”为\\(\\bar X\\)而不是\\(\\mu\\)，这导致了一个自由度的缺失。而假设\\(\\mu\\)已知，我们定义一个新的统计量\\(S\u0026#39;^2 = \\frac{1}{n} \\sum_{i=1}^N (X_i - \\mu)^2 = \\frac{1}{n} (n \\mu^2 - 2n\\mu \\bar X + \\sum_{i=1}^n X_i^2)\\)，我们会发现\\(\\E(S\u0026#39;^2) = \\sigma^2\\)： \\[ \\begin{aligned} \u0026amp;\\E(S\u0026#39;^2) = \\frac{1}{n} \\E(n \\mu^2 - 2n\\mu \\bar X + \\sum_{i=1}^n X_i^2) \\\\ \u0026amp;= \\frac{1}{n} (n \\mu^2 - 2n\\mu \\E(\\bar X) + \\sum_{i=1}^n \\E (X_i^2)) \\\\ \u0026amp;= \\frac{1}{n} (n \\mu^2 - 2n\\mu^2 + \\sum_{i=1}^n (\\sigma^2 + \\mu^2)) \\\\ \u0026amp;= \\sigma^2 \\end{aligned} \\] 至于三个统计量的依概率收敛证明，根据相互独立同分布大数定律，有 \\[ \\begin{gather} \\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{P}{\\to} \\mu \\\\ \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\stackrel{P}{\\to} \\frac{1}{n} \\sum_{i=1}^n \\E (X_i^2) = \\sigma^2 + \\mu^2 \\end{gather} \\]\n对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)，存在\\(N_1, N_2 \u0026gt; 0\\)，使得当\\(n \u0026gt; \\max(N_1, N_2)\\)时，始终有 \\[ \\begin{gather} 0 \u0026lt; P(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| \\ge \\epsilon / 2) \u0026lt; \\delta / 2 \\\\ 0 \u0026lt; P(|\\mu^2 - \\bar X^2| \\ge \\epsilon / 2) \u0026lt; \\delta / 2 \\\\ \\end{gather} \\] 记事件\\(A\\)为\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| \\ge \\epsilon / 2\\)、事件\\(B\\)为\\(|\\bar X^2 - \\mu^2| \\ge \\epsilon / 2\\)、事件\\(C\\)为\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 - \\sigma^2| \\ge \\epsilon / 2\\)。由于\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| + |\\mu^2 - \\bar X^2| \\ge |\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 - \\sigma^2|\\)，则事件\\(C\\)发生时，事件\\(A\\)、\\(B\\)至少发生其中之一，即事件\\(C\\)是事件\\(A\\)与事件\\(B\\)并集的子集。故\n\\[ 0 \u0026lt; P(\\text{事件$C$}) \\le P(\\text{事件$A$ 或 事件$B$}) \\le P(\\text{事件$A$}) + P(\\text{事件$B$}) \u0026lt; \\delta \\] 即\\(0 \u0026lt; P(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 \\ - \\sigma^2| \\ge \\epsilon) \u0026lt; \\delta\\)。又由于对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)该结论都成立，故 \\[ \\begin{gathered} \\lim_{n \\to \\infty} P(|\\underbrace{\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2}_{S_n^2} - \\sigma^2| \\ge \\epsilon) = 0 \\iff \\\\ S_n^2 \\stackrel{P}{\\to} \\sigma^2 \\end{gathered} \\] 运用类似的\\(\\epsilon, \\delta\\)语言，我们可以证明\\(S^2 = \\frac{n}{n-1} S_n^2 \\stackrel{P}{\\to} \\sigma^2\\)。\n次序统计量 令\\((X_{(1)}, \\dots, X_{(n)})\\)为样本\\((X_1, \\dots, X_n)\\)排序后的结果，则\\(X_{(1)} = \\min (X_1, \\dots, X_n), X_{(n)} = \\max (X_1, \\dots, X_n)\\)亦是统计量。\n记\\(X_{(1)}, X_{(n)}\\)的概率密度函数分别为\\(p_{X_{(1)}}, p_{X_{(n)}}\\)，则 \\[ \\begin{gather} p_{X_{(1)}}(u) = n \\big( 1 - P_X(u) \\big)^{n-1} p_X(u) \\\\ p_{X_{(n)}}(u) = n \\big( P_X(u) \\big)^{n-1} p_X(u) \\end{gather} \\] 记\\(X_{(k)}\\)的概率密度函数为\\(p_{X_{(k)}}\\)，则…\n","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"f7bebf8b361cf293130b4fb34d56e9ce","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/","section":"notes","summary":"定义 设\\((X_1,\\dots,X_n)\\)为取自总体的一组","tags":null,"title":"统计量","type":"book"},{"authors":null,"categories":null,"content":" 点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知，但其参数\\(\\theta\\)未知。设\\(X_1, \\dots, X_n\\)为总体的一个样本，若用一个统计量\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)来估计\\(\\theta\\)，则称\\(\\hat \\theta\\)为参数\\(\\theta\\)的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。\n矩估计 矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的\\(k\\)阶原点矩\\(\\mu_k = \\E(X^k)\\)，样本的\\(k\\)阶原点矩为\\(A_k = \\frac 1 n \\sum_{i=1}^n X_i^k\\)，如果未知参数\\(\\theta = \\varphi(\\mu_1, \\dots, \\mu_m)\\)，则其估计量\\(\\hat \\theta = \\varphi(A_1, \\dots, A_m)\\)，这种估计总体未知参数的方法叫作矩估计法。\n矩估计往往不唯一，如设\\(X \\sim P(\\lambda)\\)，则由于\\(\\E(X) = \\lambda\\)，\\(\\hat \\lambda\\)可写作\\(\\bar X\\)；又\\(\\Var(X) = \\lambda\\)，\\(\\hat \\lambda\\)可写作\\(\\frac 1 n \\sum_{i=1}^n X_i^2 - \\bar X^2\\)。此时往往采用较低阶的矩来估计未知参数。\n最大似然估计 设总体有分布律\\(X \\sim P(X=x;\\theta)\\)或密度函数\\(X \\sim p(x;\\theta)\\)，\\(x_1, \\dots, x_n\\)为取自总体的一组样本观测值，将样本的联合分布律或联合密度函数看作\\(\\theta\\)的函数： \\[ L(\\theta) = \\prod_{i=1}^n P(X=x_i;\\theta)\\ \\text或 \\ L(\\theta) = \\prod_{i=1}^n p(x_i;\\theta) \\] \\(L(\\theta)\\)又称作\\(\\theta\\)的似然函数，似然函数满足关系式\\(L(\\hat \\theta) = \\max_{\\theta} L(\\theta)\\)的解\\(\\hat \\theta\\)为\\(\\theta\\)的最大似然估计量。\n优良性评判 无偏性（Unbiased） 设\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)是\\(\\theta\\)的一个估计量，\\(\\theta\\)的取值空间为\\(\\Theta\\)，若对任意的\\(\\theta \\in \\Theta\\)，有 \\[ \\E [\\hat \\theta(X_1, \\dots, X_n)] = \\theta \\] 则称\\(\\hat \\theta\\)是\\(\\theta\\)的一个无偏估计（量），否则则称作有偏估计（量）。如果有 \\[ \\lim_{n \\to \\infty} \\E [\\hat \\theta(X_1, \\dots, X_n)] = \\theta, \\text{亦即} \\hat \\theta \\stackrel{L_1}{\\to} \\theta \\] 则称\\(\\hat \\theta\\)是\\(\\theta\\)的一个渐进无偏估计（量）。\n估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值也许偏大，取另一些样本时估计量也许偏小，但多次取样本进行估计，平均来讲偏差为\\(0\\)。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即系统误差。\n最小方差（Minimum-variance） 设\\(\\hat \\theta_1 = \\hat \\theta_2\\)是\\(\\theta\\)的两个估计量，\\(\\theta\\)的取值空间为\\(\\Theta\\)，若对任意的\\(\\theta \\in \\Theta\\)，有\\(\\Var(\\hat \\theta_1) \\le \\Var(\\hat \\theta_2)\\)，且至少有一个\\(\\theta \\in \\Theta\\)使得该不等式严格成立，则称\\(\\hat \\theta_1\\)比\\(\\hat \\theta_2\\)有效。\n一致性（Consistent） 设\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)是\\(\\theta\\)的一个估计量，若对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\hat \\theta - \\theta| \\ge \\epsilon) = 0, \\text{亦即} \\hat \\theta \\stackrel{P}{\\to} \\theta \\] 则称估计量\\(\\hat \\theta\\)具有一致性。一致性是一个很基本的要求：随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。\nCramer-Rao不等式 实际上，点估计量不仅仅可以估计未知参数\\(\\theta\\)本身（假设为一元情况），更可以估计未知参数的某个函数\\(g(\\theta)\\)，即给定总体的一个样本\\(X_1, \\dots, X_n\\)，用统计量\\(\\hat g = \\hat g(X_1, \\dots, X_n)\\)估计\\(g(\\theta)\\)。估计量最好的效果便是达到最小方差无偏（minimum-variance unbiased \u0026lt;MVU\u0026gt;）估计，Cramer-Rao不等式给出了点估计量\\(\\hat g\\)方差的一个下界。 \\[ \\label{cr} \\Var(\\hat g) \\ge (g\u0026#39;(\\theta))^2 / (nI(\\theta)) \\] 其中，\\(I(\\theta) = \\int [(\\frac{\\partial p(x;\\theta)}{\\partial \\theta})^2 / p(x;\\theta)] \\d x\\)为Fisher Information。当\\(g(\\theta) = \\theta\\)，即只估计未知参数本身时，有\\(\\Var(\\hat g) \\ge 1 / (nI(\\theta))\\)。\n\\(\\eqref{cr}\\)成立有一定的条件，其本身就暗含了\\(\\frac{\\partial p(x;\\theta)}{\\partial \\theta}\\)存在及\\(g\u0026#39;(\\theta)\\)存在的条件。记 \\[ S = S(X_1, \\dots, X_n, \\theta) = \\sum_{i=1}^n \\frac{\\partial \\ln p(X_i;\\theta)} {\\partial \\theta} = \\sum_{i=1}^n [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\] \\(\\int p(x;\\theta)\\ \\d x = 1\\)，此式两边同时对\\(\\theta\\)求导，并假定此处求导可以移至积分号内部，可得到\\(\\int \\frac{\\partial p(x;\\theta)}{\\partial \\theta} \\d x = 0\\)。根据LOTUS， \\[ \\E [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] = \\int [\\frac{\\partial p(x;\\theta)} {\\partial \\theta} / p(x;\\theta)] p(x;\\theta)\\ \\d x = \\int \\frac{\\partial p(x;\\theta)} {\\partial \\theta}\\d x = 0 \\]\n由于\\(X_1, \\dots, X_n\\)的独立性， \\[ \\begin{aligned} \\Var(S) \u0026amp;= \\sum_{i=1}^n \\Var [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\\\ \u0026amp;= \\sum_{i=1}^n \\{ \\E [\\big (\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta) \\big)^2] - \\E^2 [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\} \\\\ \u0026amp;= \\sum_{i=1}^n \\E [\\big (\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta) \\big)^2] \\\\ \u0026amp;= n \\int \\big (\\frac{\\partial p(x;\\theta)} {\\partial \\theta} / p(x;\\theta) \\big)^2 p(x;\\theta)\\ \\d x \\\\ \u0026amp;= n I(\\theta) \\end{aligned} \\]\n根据协方差的性质， \\[ \\label{cov_prop} [\\Cov(\\hat g, S)]^2 \\le \\Var(\\hat g) \\Var(S) = \\Var(\\hat g) n I(\\theta) \\]\n又\\(\\E(S) = 0\\)， \\[ \\begin{aligned} \\Cov(\\hat g, S) = \\E (\\hat g S) \u0026amp;= \\int \\dots \\int \\hat g(x_1, \\dots, x_n) \\sum_{i=1}^n [\\frac{\\partial p(x_i;\\theta)} {\\partial \\theta} / p(x_i;\\theta)] \\prod_{i=1}^n p(x_1;\\theta)\\ \\d x_1 \\dots \\d x_n \\\\ \u0026amp;= \\int \\dots \\int \\hat g(x_1, \\dots, x_n) \\frac{\\partial p(x_1;\\theta) \\dots p(x_n;\\theta)} {\\partial \\theta}\\ \\d x_1 \\dots \\d x_n \\end{aligned} \\] 假定此处对\\(\\theta\\)求导可以移至积分号外部， \\[ \\begin{aligned} \\Cov(\\hat g, S) \u0026amp;= \\frac \\partial{\\partial \\theta} \\int \\dots \\int \\hat g(x_1, \\dots, x_n) p(x_1;\\theta) \\dots p(x_n;\\theta)\\ \\d x_1 \\dots \\d x_n \\\\ \u0026amp;= \\frac \\partial{\\partial \\theta} g(\\theta) = g\u0026#39;(\\theta) \\end{aligned} \\] 将上式重新带入\\(\\eqref{cov_prop}\\)，从而得到\\(\\eqref{cr}\\)。\n参考 对Cramer-Rao不等式的理解 || Wiki (see the multi-variate case)\n区间估计 点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作置信区间（confidence interval）。\n设总体\\(X \\sim f(x;\\theta)\\)的分布形式已知，但其参数\\(\\theta\\)未知。设\\(X_1, \\dots, X_n\\)为总体的一个样本，给定一个很小的数\\(0 \u0026lt; \\alpha \u0026lt; 1\\)，若有统计量\\(\\theta_l = \\theta_l (X_1, \\dots, X_n) …","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"ce2db8f3089e450f05ac119bf4d9c6b6","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","section":"notes","summary":"点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知","tags":null,"title":"参数估计","type":"book"},{"authors":null,"categories":null,"content":" 假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间估计中，我们还能得到一个这个参数置信区间和相应的置信水平。而在假设检验中，我们要做的则是根据某个假设（hypothesis）以及给定的样本，决定是否接受这个假设。注意我们特地将假设和估计（estimation）区分开，因为这个假设并不是由样本得到的一个估计量，而是一条已有的断言（assertion）；我们要做的，则是给出在什么样的情况下（对应置信区间），我们能够以多高的信心否定这个断言（对应置信水平）。\n至于假设检验和区间估计的关系，其实我们会发现，如果本身就能对未知参数做有效的区间估计，那其实对它的假设检验设计，自然迎刃而解。\n一般检验方法 一般检验方法指Neyman-Pearson方法。\n建立假设 对要检验的问题，一般有一个原假设\\(H_0\\)（也叫零假设，null hypothesis）以及一个备择假设\\(H_1\\)（alternative hypothesis）。原假设一般是总体的某个未知参数\\(\\theta\\)等于某个具体值\\(\\theta_0\\)，即 \\[ H_0: \\theta = \\theta_0 \\] 这种只包含一个假设值（即\\(\\theta_0\\)）的原假设又叫作简单原假设（simple hypothesis null）。而备择假设一般和原假设互斥，它通常有以下三种形式：\n\\(H_1: \\theta \\ne \\theta_0\\)，此时\\(H_0\\)与\\(H_1\\)为对立关系，我们要检验\\(\\theta\\)落在\\(\\theta_0\\)两侧的可能，这样的检测问题也称为双边检验（two-sided test）； \\(H_1: \\theta \u0026gt; \\theta_0\\)，此时我们要检验\\(\\theta\\)落在\\(\\theta_0\\)右侧的可能，这样的检测问题也称为右侧的单边检验； \\(H_1: \\theta \u0026lt; \\theta_0\\)，此时我们要检验\\(\\theta\\)落在\\(\\theta_0\\)左侧的可能，这样的检测问题也称为左侧的单边检验；  选择否定域形式 根据已有的样本，我们能够给出未知参数的点估计量\\(\\hat \\theta\\)（在假设检验中又称作检验统计量\u0026lt;test statistic\u0026gt;），如果\\(\\hat \\theta\\)和\\(\\theta_0\\)的距离小于某个临界值\\(c \u0026gt; 0\\)（critical value），我们就可以接受原假设（即便\\(\\hat \\theta\\)与\\(\\theta_0\\)不完全相等），否则则否定原假设。使得原假设被接受的样本所在的区域就被称作接受域（acceptance region）；使得原假设被否定的样本所在的区域就被称作否定域（也叫拒绝域，rejection region）；一般我们习惯先构造否定域\\(W\\)，则剩余区域就为接受域\\(\\overline W\\)： \\[ \\begin{gather} W = \\{ (x_1, \\dots, x_n) \\big | \\|\\hat \\theta(x_1, \\dots, x_n) - \\theta_0\\| \u0026gt; c \\} \\\\ \\overline W = W^c \\end{gather} \\] 对于某些参数，可能本身越小越好（比如故障率），所以我们仅需要进行右侧的单边检测，此时对应的否定域为 \\[ W = \\{ (x_1, \\dots, x_n) \\big | \\hat \\theta(x_1, \\dots, x_n) - \\theta_0 \u0026gt; c \\} \\\\ \\] 此时可以等价认为\\(H_0: \\theta \\le \\theta_0\\)，这种情况下，\\(H_0\\)是一个复合原假设（composite hypothesis null），因为其包含的假设值不止一个。\n对于另外一些参数，可能本身越大越好（比如身高均值），所以我们仅需要左侧的单边检测，此时对应的否定域为 \\[ W = \\{ (x_1, \\dots, x_n) \\big | \\hat \\theta(x_1, \\dots, x_n) - \\theta_0 \u0026lt; c \\} \\\\ \\] 同理，此时可以等价认为\\(H_0: \\theta \\ge \\theta_0\\)，这种情况下，\\(H_0\\)也是一个复合原假设。\n设定显著性水平 给定假设，我们已经可以根据样本属于接受域还是否定域，做出接受或是否定假设的决策了。但和点估计中的问题一样，我们依然是基于样本提供的不完全信息做出的判断，所以我们的判断不总是正确的。这种判断会有四种结果：\n   判断：接受\\(H_0\\) 判断：否定\\(H_0\\)    实际：\\(H_0\\)成立 判断正确 第一类错误  实际：\\(H_1\\)成立 第二类错误 判断正确    通常较低的第一类错误风险\\(P(\\text{否定$H_0$};\\text{$H_0$成立})\\)和较低的第二类错误风险\\(P(\\text{接受$H_0$};\\text{$H_1$成立})\\)不可兼得，因为在检验统计量确定后，这两个概率主要是由临界值\\(c\\)导出的否定域大小来控制的。而我们更希望降低第一类错误发生的风险。也就是说我们一旦否定，\\(H_0\\)很大概率确实是不成立的；尽管这意味着我们在接受\\(H_0\\)时，\\(H_0\\)有可能不成立——不过虚惊一场总好过后知后觉。所以实际应用中，\\(H_0\\)往往对应了比较严重的结果，我们不希望在\\(H_0\\)成立时，我们却没有发现（即否定\\(H_0\\)）；或者\\(H_0\\)本身就对应了我们比较想否定的结果，这样我们否定时，它确实不成立的概率也更高。\n我们会将第一类错误发生的概率限制在\\(\\alpha\\)之内，这个\\(\\alpha\\)便是显著性水平（significance level）。显著性水平其实代表了我们对小概率事件的接受程度，即我们认为概率小于\\(\\alpha\\)的事件应该是小概率事件，并且是不应该被正好碰上的；而此时在\\(H_0\\)成立的假设下，“给定的一组样本属于否定域”正是这样的一个小概率事件，如果碰上了这样的小概率事件，则有理由怀疑\\(H_0\\)不成立。\n确定临界值 在确定显著性水平后，我们便可以进一步确定临界值，从而给出完整的否定域。此时我们调整临界值\\(c\\)，从而使得 \\[ P(\\text{否定$H_0$};\\text{$H_0$成立}) = P( (X_1, \\dots, X_n) \\in W;\\theta = \\theta_0) \\le \\alpha \\] 问题就变成了一个简单的分布问题。此处需要指出的是，\\(P(\\text{否定$H_0$};\\text{$H_0$成立})\\)不应写作\\(P( (X_1, \\dots, X_n) \\in W | \\theta = \\theta_0)\\)，因为假设检验是频率学派中的概念，频率学派中的未知参数\\(\\theta\\)并没有先验分布。\n\\(P( (X_1, \\dots, X_n) \\in W;\\theta = \\theta_0)\\)又叫作功效函数（power function），记作\\(\\beta_W(\\cdot)\\)，它表示在未知参数取特定值时，一组随机样本属于否定域的概率。 前面我们之所以说单边检验等价于原假设对应某个形式的复合假设（比如\\(H_0: \\theta \\le \\theta_0\\)或\\(H_0: \\theta \\ge \\theta_0\\)），是因为这种情况下，有 \\[ \\max_{h \\in H_0} \\beta_W(h) = \\beta_W(\\theta_0) \\le \\alpha \\]\n\\(p\\)值和\\(p\\)值检验法 \\(p\\)值检验法指的是数学家Fisher提出的检验方法。\n假设检验的\\(p\\)值是在原假设\\(H_0\\)成立的情况下，检验统计量\\(\\hat \\theta(X_1, \\dots, X_n)\\)出现其具体观测值\\(z = \\hat \\theta(x_1,\\dots,x_n)\\)或者比之更极端的值的概率，即\\(p = P(\\hat \\theta = z; \\theta = \\theta_0)\\)（类似likelihood）。\\(p\\)值检验中，我们检验\\(p\\)值是否足够小，如果\\(p\\)值小到一定程度，我们还是会否定\\(H_0\\)，即\n 如果\\(p \\le \\alpha\\)，则我们在显著性水平\\(\\alpha\\)下否定原假设\\(H_0\\)； 如果\\(p \u0026gt; \\alpha\\)，则我们在显著性水平\\(\\alpha\\)下接受原假设\\(H_0\\)。  参考 hypothesis testing - Are type I error \u0026amp; FWER both conditional probabilities? - Cross Validated (stackexchange.com)\n第 3 章 假设检验 | 数理统计讲义 (bookdown.org)\n","date":1670353333,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670353333,"objectID":"6069d02896d73e792ecc9429414214b4","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/","section":"notes","summary":"假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间","tags":null,"title":"假设检验","type":"book"},{"authors":null,"categories":null,"content":" 贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j) P(A|B_j)} \\] 这便是贝叶斯公式。贝叶斯公式中的项目也有它们在贝叶斯学派中相应的称呼： \\[ \\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{evidence}} \\]\n贝叶斯推断 在参数估计问题中，记\\(D = \\{ X_1, \\dots, X_n \\}\\)为样本、\\(\\theta\\)为参数，并用将\\(D\\)代入后验概率中的事件\\(A\\)、\\(\\theta\\)代入后验概率中的\\(B_i\\)，我们得到：\n\\[ P(\\theta | D) = \\frac{P(\\theta) P(D|\\theta)} {\\sum_j P(\\theta) P(D|\\theta)} \\\\ \\] 取决于\\(\\theta\\)和单个样本的取值是连续型或是离散型，上式中的\\(P\\)可代表密度函数或分布律，而分母中的求和运算应当在\\(\\theta\\)取值连续的时候被替换在\\(\\theta\\)所有可行范围内的积分运算。比较值得注意的一点是，贝叶斯推断为参数\\(\\theta\\)引入了先验分布，而在频率学派中，参数\\(\\theta\\)是不存在什么先验分布的。\n最大后验估计 最大后验估计（maximum a posteriori estimation）得到的点估计是以下： \\[ \\hat \\theta = \\arg \\max_{\\theta} \\frac{P(\\theta) P(D | \\theta)}{\\int P(\\theta\u0026#39;)(D|\\theta\u0026#39;) \\d \\theta\u0026#39;} = \\arg \\max_{\\theta} {P(\\theta) P(D | \\theta)} \\]\n最小均方差估计 最小均方差估计（minimum mean squared error estimation）得到的点估计是以下： \\[ \\theta^\\star = \\arg \\min_{\\hat \\theta} \\E_{\\theta \\sim \\text{posterior}} [(\\hat \\theta - \\theta)^2] \\] 换言之，此时的点估计\\(\\theta^\\star\\)即为\\(\\E_{\\theta \\sim \\text{posterior}} \\theta\\)。\n","date":1670606533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670606533,"objectID":"9a02d38a8ecdf812d564db25204f3ad1","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/","section":"notes","summary":"贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j) P(A|B_j)} \\] 这","tags":null,"title":"贝叶斯推断","type":"book"},{"authors":null,"categories":null,"content":"Laplace Expansion Also known as Cofactor Expansion, Laplace Expansion is an expression of an $n \\times n$ matrix as the weighted sum of determinants of some $(n-1) \\times (n-1)$ sub-matrices.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac2d2e997e3a8637f4ccbafd9422f595","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/laplace-expansion/","section":"notes","summary":"Laplace Expansion Also known as Cofactor Expansion, Laplace Expansion is an expression of an $n \\times n$ matrix as the weighted sum of determinants of some $(n-1) \\times (n-1)$ sub-matrices.","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":"Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair $(f,f^)$ has the following property: $$ f(x) + f^(t) \\ge x^T \\cdot t $$ As a conjugate, $f^{} = f$: $$ \\begin{aligned} f^{}(t) \u0026amp;= \\sup_x [x^T \\cdot t - f^*(x)] \\ \u0026amp;= \\sup_x [x^T \\cdot t - \\sup_y [y^T \\cdot x - f(y)]] \\ \u0026amp;= \\sup_x [x^T \\cdot t + \\inf_y [f(y) - y^T \\cdot x]] \\ \u0026amp;= \\sup_x \\inf_y [x^T (t-y) + f(y)]\t\\ \u0026amp;\\Downarrow_\\text{Mini-max Theorem} \\ \u0026amp;= \\inf_y \\sup_x [x^T (t-y) + f(y)] \\end{aligned} $$ The above reaches the infimum only if $y=t$. Otherwise, $\\sup_x [x^T (t-y) + f(y)]$ can make it to infinity. Therefore, $$ f^{**}(t) = \\inf_y \\sup_x [f(t)] = f(t) $$ Wiki || 凸优化-凸共轭\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"57d01174ce8319ba1023ce0b56087404","permalink":"https://chunxy.github.io/notes/articles/optimization/convex-conjugate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/convex-conjugate/","section":"notes","summary":"Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair","tags":null,"title":"","type":"notes"}]