[{"authors":null,"categories":null,"content":"Articles contains the notes where I combined my own elaboration with other blogs, forum discussions, lecture notes, Wiki, etc. According to the subject, they are further grouped into:\n Information Theory Machine Learning Mathematics Optimization  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"32f1ae15161db38a68d36a5b05971087","permalink":"https://chunxy.github.io/notes/articles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/","section":"notes","summary":"Articles contains the notes where I combined my own elaboration with other blogs, forum discussions, lecture notes, Wiki, etc. According to the subject, they are further grouped into:\n Information Theory Machine Learning Mathematics Optimization  ","tags":null,"title":"Articles","type":"book"},{"authors":null,"categories":null,"content":"Books contains the notes mostly extracted from many classical textbooks. These notes are just grouped according to the book title:\n Information Theory, Inference and Learning Algorithms Linear Algebra and Its Applications 概率论与数理统计  Nowadays ebooks are more convenient and available than traditional paper-based books. However, you cannot flip through an ebook as easily as in a paper-based book to locate the content you want to recap. So if you are also an ebook-fan like me, I believe it is a beneficial habit to turn your reading scratches to systematic notes like above.\n","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"33c9e341aa36b46c894c7dbd62dc3b5f","permalink":"https://chunxy.github.io/notes/books/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/","section":"notes","summary":"Books contains the notes mostly extracted from many classical textbooks. These notes are just grouped according to the book title: Information Theory, Inference and Learning Algorithms Linear Algebra and Its","tags":null,"title":"Books","type":"book"},{"authors":null,"categories":null,"content":"  Jacobian Matrix Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .\n  Spherical Coordinates The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be\n  Lipschitz Continuity For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in \\dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"e7ed34de58dff49e385796884a33c783","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/","section":"notes","summary":"Jacobian Matrix Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .","tags":null,"title":"Calculus","type":"book"},{"authors":null,"categories":null,"content":"  Entropy The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy reaches its maximum when the underlying distribution \\(p\\) is a uniform distribution.\n  Conditional Entropy The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known.\n  Cross Entropy The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).\n  Mutual Information Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable.\n  KL-divergence KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\). In information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).\n  f-divergence \\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function.\n  Jenson-Shannon Divergence Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric.\n  Overview Both Cross Entropy and KL-divergence describe the relationship between two distributions. Both conditional entropy and mutual information, as well as joint entropy, describe the relationship between two random variables.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"21c069359db95b8f5e32ca83dd5fa9b8","permalink":"https://chunxy.github.io/notes/articles/information-theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/","section":"notes","summary":"Entropy The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy reaches its maximum when the underlying distribution \\(p\\) is a uniform distribution.","tags":null,"title":"Information Theory","type":"book"},{"authors":null,"categories":null,"content":"  Introduction The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point (Claude Shannon, 1948). Similar to that stated above, the fundamental subject of machine learning is about with best effort recovering the original data using the so-called model or algorithm.\n  Source Coding Theorem Notations and Concepts An ensemble \\(X\\) (we specifically use the random variable symbol to denote the ensemble) is a triplet \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(X\\) denotes\n  Symbol Code Other than block code where symbols are encoded in chunks, symbol code will assign each symbol a unique codeword. Among the codeword schemes, we prefer those where no codeword is a prefix of any other codeword.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"648da0d35c916615b2e059c81f3e0f9d","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/","section":"notes","summary":"Introduction The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point (Claude Shannon, 1948). Similar to that stated above, the fundamental subject of machine learning is about with best effort recovering the original data using the so-called model or algorithm.","tags":null,"title":"Information Theory, Inference and Learning Algorithms","type":"book"},{"authors":null,"categories":null,"content":"  Determinant Derivation of Determinant Determinant may be the most infamous concept in linear algebra, in terms of its odd definition and computation. Sometimes, we may wonder why there has to be a determinant.\n  Eigenvectors and Eigenvalues Eigenvectors and Eigenvalues An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\).\n  Singular Value Decomposition Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gathered} \\text{$U$ is $m \\times m$, $V$ is $n \\times n$} \\\\ U U^T =\n  Real Symmetric Matrix Real Symmetric Matrix Let \\(A\\) be an \\(n \\times n\\) real-valued symmetric matrix. We have its properties as follows. Real-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued.\n  Difference Equation Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form: \\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]\n  Matrix Identity A useful matrix identity: \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] It can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026amp;= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form: \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] It can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026amp;= A(I_M+BA)^{-1} \\\\ \\iff A \u0026amp;= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A \\end{aligned} \\]\n  Quadratic Form Quadratic Form Quadratic form involves many concepts like real symmetric matrix, positive definiteness and singular value decomposition. It can be quite helpful to glue these things together. A quadratic function \\(f\\) of \\(n\\) variables, or say a vector \\(\\x\\) of length \\(n\\), is the sum of second-order terms: \\[ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j \\]\n  Metrics Spectral Normalization Spectral normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), which is exactly \\(A\\)’s largest singular value \\(\\sigma_{\\max}(A)\\).\n   Positive semi-definite matrix involves many concepts like quadratic form, real symmetric matrix and singular value decomposition. It can be quite helpful to glue these things together here. Quadratic Form A quadratic function $f$ of $n$ variables, or say a vector $\\x$ of length $n$, is the sum of second-order terms: $$ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j $$\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"6e77d541ab651be2804d292bd4482e95","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/","section":"notes","summary":"Determinant Derivation of Determinant Determinant may be the most infamous concept in linear algebra, in terms of its odd definition and computation. Sometimes, we may wonder why there has to be a determinant.","tags":null,"title":"Linear Algebra","type":"book"},{"authors":null,"categories":null,"content":"  Coordinate System and Change of Basis Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, .\n  Orthogonality and Projection Orthogonality and Independence If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other. Orthonormality An \\(m \\times n\\) matrix \\(U\\) has orthonormal columns if and only if \\(U^TU = I\\).\n  Gram-Schmidt Orthogonalization Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\). Given a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define\n  Least Squares Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\).\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"2c296658266b2ff674bc5d326158b641","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/","section":"notes","summary":"Coordinate System and Change of Basis Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, .","tags":null,"title":"Linear Algebra and Its Applications","type":"book"},{"authors":null,"categories":null,"content":"  Bullet Points Bullet Points This post lists out various topics under the machine learning subject. Data data modalities numbers texts images videos audios data cleaning imbalanced data data normalization (one pitfall) standardization\n  Linear Discriminant Analysis Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is.\n  Logistic Regression Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\).\n  Support Vector Machine Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary.\n  Linear Regression Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx\n  Non-linear Regression The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space. Polynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\).\n  Clustering Big Picture The clustering algorithms can be broadly split into two categories depending on whether the number of clusters is given or to be determined by user. Partitional ones pre-set the number of clusters; while hierarchical ones output a dendrogram that illustrates how clusters are built level by level.\n  Dimension Reduction Unsupervised Dimension Reduction Dimensionality reduction reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity. Compared to feature selection,\n  Principal Component Analysis Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the\n  Eckart-Young-Mirsky Theorem Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, .\n  Independent Component Analysis Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in \\R^{N \\times 1}\\), independent component analysis finds the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\).\n  RANSAC Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to\n  Fisher\u0026#39;s Linear Discriminant Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)).\n  Bias-variance Decomposition The notation used is as follows: Symbol Notation \\(\\mathcal D\\) the dataset \\(x\\) the sample \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise \\(y\\) the real value of \\(x\\) \\(\\bar y\\) the mean of the real values \\(f\\) the model learned with \\(\\mathcal D\\) \\(f(x)\\) the prediction of \\(f\\) with \\(x\\) \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\) \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026amp;[l(f(x), y_\\mathcal D)] = E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026amp;\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) …","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"4a28399944c3ca08382a34ee9f39e2d2","permalink":"https://chunxy.github.io/notes/articles/machine-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/","section":"notes","summary":"Bullet Points Bullet Points This post lists out various topics under the machine learning subject. Data data modalities numbers texts images videos audios data cleaning imbalanced data data normalization (one pitfall) standardization","tags":null,"title":"Machine Learning","type":"book"},{"authors":null,"categories":null,"content":"Mathematics is itself a huge topic. So I further group the Mathematics subsection into sub-subsections:\n Calculus Linear Algebra Numerical Analysis Probability and Statistics  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"caabf42bcf0fbef2c6df7dcee4f1ce10","permalink":"https://chunxy.github.io/notes/articles/mathematics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/","section":"notes","summary":"Mathematics is itself a huge topic. So I further group the Mathematics subsection into sub-subsections:\n Calculus Linear Algebra Numerical Analysis Probability and Statistics  ","tags":null,"title":"Mathematics","type":"book"},{"authors":null,"categories":null,"content":"The “Notes” section contains the notes I took from various sources, e.g. blogs, forum discussions, books, papers, etc, which I gave credit informally by simply putting the source address in my posts.\nI generally group my notes into Articles, Books and Papers, according to the source format, under which more topics and subtopics are further defined for better categorization. These notes are thus more organized and structured than Blogs.\nHowever, they are more of a tedious manual than an inviting roadmap. I try to put rigorous (within my command) definitions and proofs in them, just for fast recap, as well as being my second brain. But these notes may lack of the justification for why the definitions are necessary and why the proofs should be done in this way, which I believe instead is more vital in learning.\n","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"ab7a472e5d2a0dba3f39440bfb9334ba","permalink":"https://chunxy.github.io/notes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/","section":"notes","summary":"The “Notes” section contains the notes I took from various sources, e.g. blogs, forum discussions, books, papers, etc, which I gave credit informally by simply putting the source address in my posts.","tags":null,"title":"Chunxy' Notes","type":"book"},{"authors":null,"categories":null,"content":"  Fourier Transform Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two\n  Stirling\u0026#39;s Approximation Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"0bb8d49c4270a2e9cbca37fbd9592b1d","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/","section":"notes","summary":"Fourier Transform Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two","tags":null,"title":"Numerical Analysis","type":"book"},{"authors":null,"categories":null,"content":"  Lagrange Multiplier Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026amp;\\inf f_0(x) \\\\ s.t.\\quad \u0026amp; f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp; h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).\n  Convex Optimization Definitions Convex Set A set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).\n  Gradient Descent If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent. \\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner.\n  Coordinate Descent Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima?\n  Expectation Maximization If a probabilistic model contains only observable variables, maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables).\n  Subgradient Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\}\n  Least Angle Regression Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.\n   Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"9c9979febb0e862d1e8d3228769675eb","permalink":"https://chunxy.github.io/notes/articles/optimization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/","section":"notes","summary":"Lagrange Multiplier Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026\\inf f_0(x) \\\\ s.t.\\quad \u0026 f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026 h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).","tags":null,"title":"Optimization","type":"book"},{"authors":null,"categories":null,"content":"Papers, as the name suggests, contains the notes I took from the academic papers I read:\n Bounding Mutual Information Contrastive Predictive Coding Noise Contrastive Estimation  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"76727f9a2d5c315359826ee4597017e2","permalink":"https://chunxy.github.io/notes/papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/","section":"notes","summary":"Papers, as the name suggests, contains the notes I took from the academic papers I read:\n Bounding Mutual Information Contrastive Predictive Coding Noise Contrastive Estimation  ","tags":null,"title":"Papers","type":"book"},{"authors":null,"categories":null,"content":"  Law of Total Variance Conditional Expectation Let \\(X\\) and \\(Y\\) be two discrete random variables. The conditional probability function of \\(X\\) given \\(Y=y\\) is \\[ \\Pr(X=x|Y=y) = \\frac{\\Pr(X=x, Y=y)}{P(Y=y)} \\] Thus the conditional expectation of \\(X\\) given that \\(Y=y\\) is \\[ \\E(X|Y=y) \\coloneq \\sum_x x \\Pr(X=x|Y=y) \\] Clearly the conditional expectation \\(\\E(X|Y)\\) is a function of \\(Y\\), or put it another way, a random variable depending on \\(Y\\), instead of \\(X\\).\n  Gaussian Distribution Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\] To verify that it integrates to \\(1\\),\n  Unconscious Statistics Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).\n  Whitening Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix.\n  随机变量的收敛 依概率收敛（convergence in probability）\n  特征函数 定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"c69caf43e206cc97cb8f8ad1a813d6ef","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/","section":"notes","summary":"Law of Total Variance Conditional Expectation Let \\(X\\) and \\(Y\\) be two discrete random variables. The conditional probability function of \\(X\\) given \\(Y=y\\) is \\[ \\Pr(X=x|Y=y) = \\frac{\\Pr(X=x, Y=y)}{P(Y=y)} \\] Thus the conditional expectation of \\(X\\) given that \\(Y=y\\) is \\[ \\E(X|Y=y) \\coloneq \\sum_x x \\Pr(X=x|Y=y) \\] Clearly the conditional expectation \\(\\E(X|Y)\\) is a function of \\(Y\\), or put it another way, a random variable depending on \\(Y\\), instead of \\(X\\).","tags":null,"title":"Probability and Statistics","type":"book"},{"authors":null,"categories":null,"content":"  总览 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）\n  事件与概率 事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\\n  常见分布 离散型 二项分布（binomial distribution） 如\n  协方差与相关系数 以下以二维随机变量为例，展示协方差以及相关系数的概念。虽然协\n  随机变量的函数 设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，\n  大数定律和中心极限定理 前置知识 Chebyshev不等式 定理 设随机变量\\(X\\)的期\n  三大分布与正态总体的抽样分布 \\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由\n  统计量 定义：设\\((X_1,\\dots,X_n)\\)为取自总体的一\n  参数估计 点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知\n  贝叶斯推断 贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j)\n  假设检验 假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"9dbea9556e81ba7c3226a6643554efbc","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","section":"notes","summary":"总览 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子","tags":null,"title":"概率论与数理统计","type":"book"},{"authors":null,"categories":null,"content":"  Spectral Normalization Spectral normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), which is exactly \\(A\\)’s largest singular value \\(\\sigma_{\\max}(A)\\).\n  Frobenius Normalization Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F \\triangleq \\sqrt{\\sum_{i,j}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 = \\sum_{ij}A_{ij}^2 \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026amp;= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026amp;= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026amp;= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026amp;= \\tr(AA^T) = \\tr(A^TA) \\end{aligned} \\]\n  Chebyshev Distance Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ d_p(x, x^\\prime) = ||x - x^\\prime||_p \\coloneq (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\] where \\(p \\to \\infty\\). Chebyshev distance is\n  ","date":1657457929,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657457929,"objectID":"c8fe1129e63cb1356c7400a85d34b5af","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/","section":"notes","summary":"Spectral Normalization Spectral normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), which is exactly \\(A\\)’s largest singular value \\(\\sigma_{\\max}(A)\\).","tags":null,"title":"Metrics","type":"book"},{"authors":null,"categories":null,"content":" Derivation of Determinant Determinant may be the most infamous concept in linear algebra, in terms of its odd definition and computation. Sometimes, we may wonder why there has to be a determinant.\nSo instead of giving its definition directly, we first lay down some properties we expect the determinant to have. Then we try to construct the determinant from the ground up and prove its existence and uniqueness.\nDeterminant in essence captures the volume of the parallelepiped formed by vectors of an \\(n \\times n\\) matrix. When \\(n=1\\), it is the length; when \\(n=2\\), it is the area of parallelogram; when \\(n=3\\), it is the volume of parallelepiped. What about when \\(n\\) goes larger?\nAs said, there are some basic properties that we expect the determinant (the volume) to have, and that indeed hold for cases \\(n = 1,2,3\\). In the following, let \\(A\\) be an \\(n \\times n\\) matrix and denote \\(\\det(\\v_1, \\dots, \\v_n)\\) as the determinant of the matrix formed by a system of vectors \\(\\v_1, \\dots, \\v_n\\).\nBasic properties Linearity in each argument: \\[ \\det(\\v_1, \\dots, \\underbrace{\\alpha \\v_k + \\beta \\u_k}_k, \\dots, \\v_n) = \\alpha \\det(\\v_1, \\dots, \\underset{k}{\\v_k}, \\dots, \\v_n) + \\beta \\det(\\v_1, \\dots, \\underset{k}{\\u_k}, \\dots, \\v_n) \\]\n Anti-symmetry: \\[ \\det(\\v_1, \\dots, \\underset{j}{\\v_j}, \\dots, \\underset{k}{\\v_k}, \\dots, \\v_n) = -\\det(\\v_1, \\dots, \\underset{j}{\\v_k}, \\dots, \\underset{k}{\\v_j}, \\dots, \\v_n) \\]\n Normalization: \\[ \\det(I) = 1 \\]\n  With these basic properties, we can derive some advanced properties for determinant.\nDerived properties Preservation under column replacement: \\[ \\det(\\v_1, \\dots, \\underbrace{\\v_j + \\alpha \\v_k}_{j}, \\dots, \\underset{k}{\\v_k}, \\dots, \\v_n) = \\det(\\v_1, \\dots, \\underset{j}{\\v_j}, \\dots, \\underset{k}{\\v_k}, \\dots, \\v_n) \\]  Zero determinants If \\(A\\) has a zero column, then \\(\\det A = 0\\).\n If \\(A\\) has two equal columns, then \\(\\det A = 0\\).\n If one column of \\(A\\) is the multiple of another, then \\(\\det A = 0\\).\n If columns of \\(A\\) are linearly dependent, then \\(\\det A = 0\\).\n  Diagonal matrices and triangular matrices Determinant of a diagonal matrix equal the product of the diagonal entries. Determinant of a triangular matrix equal the product of the diagonal entries.  Transpose and product \\(\\det A^T = \\det A\\).\nThis property implies that, all the statements above about columns, can be applied to rows. Thus, to compute the determinant of a matrix, we can apply row operations to transform it to reduced row echelon form first, and then obtain the result by computing the product of the diagonal entries.\n \\(\\det (AB) = (\\det A)(\\det B)\\).\n  Construction Now with these properties on hand, how can we find the definition of the determinant and how can we know that the definition is unique over these properties?\nConsider an \\(n \\times n\\) matrix \\(A = \\{ a_{jk} \\}_{j,k=1}^n\\). Let \\(\\v_1, \\dots, \\v_n\\) be its columns and \\(\\newcommand{\\e}{\\mathrm{e}} \\e_1, \\dots, \\e_n\\) be the unit vectors. We have \\[ \\v_k = a_{1, k} \\e_1 + a_{2, k} \\e_2 + \\dots + a_{n, k} \\e_{n} = \\sum_{j=1}^n a_{j, k} \\e_j \\] By the linearity in each argument, expand the first column to give \\[ \\det(\\v_1, \\dots, \\v_n) = \\det(\\sum_{j=1}^n a_{j, 1} \\e_i, \\v_2, \\dots, \\v_n) = \\sum_{j=1}^n a_{j, 1} \\det(\\e_j, \\v_2, \\dots, \\v_n) \\] Further expand the second column to give \\[ \\begin{aligned} \u0026amp;\\det(\\v_1, \\dots, \\v_n) = \\sum_{j_1=1}^n a_{j_1, 1} \\det(\\e_{j_1}, \\v_2, \\dots, \\v_n) \\\\ \u0026amp;= \\sum_{j_1=1}^n a_{j_1, 1} \\det(\\e_{j_1}, \\sum_{j_2=1}^n a_{j_2, 2} e_{j_2}, \\v_3, \\dots, \\v_n) \\\\ \u0026amp;= \\sum_{j_1=1}^n a_{j_1, 1} \\sum_{j_2=1}^n a_{j_2, 2} \\det(\\e_{j_1}, e_{j_2}, \\v_3, \\dots, \\v_n) \\\\ \u0026amp;= \\sum_{j_1=1}^n \\sum_{j_2=1}^n a_{j_1, 1} a_{j_2, 2} \\det(\\e_{j_1}, e_{j_2}, \\v_3, \\dots, \\v_n) \\end{aligned} \\] Expand the remaining columns to give \\[ \\det(\\v_1, \\dots, \\v_n) = \\sum_{j_1=1}^n \\sum_{j_2=1}^n \\dots \\sum_{j_n=1}^n a_{j_1, 1} a_{j_2, 2} \\dots a_{j_n, n} \\det(\\e_{j_1}, e_{j_2}, \\dots, \\e_{j_n}) \\] This yields \\(n^n\\) terms! But luckily, many terms are zero, as long as any two of \\(j_1, \\dots, j_n\\) coincides. To eliminate zero terms, consider the permutation of \\(\\{ 1, \\dots, n \\}\\). If \\(j_1, \\dots, j_n\\) are chosen to be a permutation, \\(\\det(\\e_{j_1}, e_{j_2}, \\dots, \\e_{j_n})\\) is nonzero. We may use a function \\(\\sigma: \\{ 1, \\dots, n \\} \\to \\{ 1, \\dots, n \\}\\) to denote a permutation. Let the set of all permutations of set \\(\\{ 1, \\dots n \\}\\) be \\(\\mathrm{Perm}(n)\\). Then, \\[ \\det(\\v_1, \\dots, \\v_n) = \\sum_{\\sigma \\in \\mathrm{Perm}(n)} a_{\\sigma(1), 1} a_{\\sigma(2), 2} \\dots a_{\\sigma(n), n} \\det(e_{\\sigma(1)}, e_{\\sigma(2)}, \\dots, e_{\\sigma(n)}) \\] The matrix with columns \\(e_{\\sigma(1)}, e_{\\sigma(2)}, \\dots, e_{\\sigma(n)}\\) can be obtained from identity matrix by finitely many column exchanges. So \\(\\det(e_{\\sigma(1)}, e_{\\sigma(2)}, \\dots, e_{\\sigma(n)})\\) is \\(1\\) or \\(-1\\) depending on the number of column exchanges. We informally define the sign of function \\(\\sigma\\) to be \\(1\\) if an even number of …","date":1684237332,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684237332,"objectID":"32be3b80f2bc18a4c2d70559ba9ac19a","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/determinant/","section":"notes","summary":"Derivation of Determinant Determinant may be the most infamous concept in linear algebra, in terms of its odd definition and computation. Sometimes, we may wonder why there has to be a determinant.","tags":null,"title":"Determinant","type":"book"},{"authors":null,"categories":null,"content":" Conditional Expectation Let \\(X\\) and \\(Y\\) be two discrete random variables. The conditional probability function of \\(X\\) given \\(Y=y\\) is \\[ \\Pr(X=x|Y=y) = \\frac{\\Pr(X=x, Y=y)}{P(Y=y)} \\] Thus the conditional expectation of \\(X\\) given that \\(Y=y\\) is \\[ \\E(X|Y=y) \\coloneq \\sum_x x \\Pr(X=x|Y=y) \\] Clearly the conditional expectation \\(\\E(X|Y)\\) is a function of \\(Y\\), or put it another way, a random variable depending on \\(Y\\), instead of \\(X\\).\nConditional Variance Conditional variance can be similarly defined. \\(\\Var(X|Y=y)\\) is the conditional variance of \\(X\\) given \\(Y=y\\) and \\(\\Var(X|Y)\\) is a random variable depending on \\(Y\\): \\[ \\Var(X|Y) \\coloneq \\E[(X - \\mu_{X|Y})^2 | Y] = \\E(X^2|Y) - \\E(X|Y)^2 \\]\nLaws of Total Expectation and Variance If all the expectations below exist, then for any random variable \\(X\\) and \\(Y\\), we have \\[ \\E(X) = \\E_{y \\sim p_Y} [\\E(X|Y=y)] \\quad \\textbf{Law of Total Expectation} \\] and \\[ \\Var(X) = \\E_{y \\sim p_Y} [\\Var(X|Y=y)] + \\Var_{y \\sim p_Y} [\\E(X|Y=y)] \\quad \\textbf{Law of Total Variance} \\]\n","date":1680685602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680685602,"objectID":"74319081d501400d57ae8cc73d0c6a53","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/","section":"notes","summary":"Conditional Expectation Let \\(X\\) and \\(Y\\) be two discrete random variables. The conditional probability function of \\(X\\) given \\(Y=y\\) is \\[ \\Pr(X=x|Y=y) = \\frac{\\Pr(X=x, Y=y)}{P(Y=y)} \\] Thus the conditional expectation of \\(X\\) given that \\(Y=y\\) is \\[ \\E(X|Y=y) \\coloneq \\sum_x x \\Pr(X=x|Y=y) \\] Clearly the conditional expectation \\(\\E(X|Y)\\) is a function of \\(Y\\), or put it another way, a random variable depending on \\(Y\\), instead of \\(X\\).","tags":null,"title":"Law of Total Variance","type":"book"},{"authors":null,"categories":null,"content":" 概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）到数字的映射，是一个具体事件数字化的过程。概率论和统计的一个共同的主要话题就是随机变量，可以说它们像是随机变量的一体两面。\n概率论更加注重随机变量取值集合相对应事件的概率。为此，概率论需要讨论事件所有可能的试验结果、事件的运算、事件的独立性、随机变量的取值范围、随机变量的概率分布等话题。\n统计中的随机变量来自于对总体的随机抽样，这个过程中，我们会得到一组样本，每个样本在被观测之前，都是服从总体分布的随机变量；观测（observation）之后，它们便有了一个具体的观测值（realization）。我们可以将观测行为类比概率论中的试验，而这种观测行为将会导致一个随机变量坍缩成为一个具体的观测值。\n可以这样理解概率论与统计：概率论是根据事件总体的自身属性，正向推导所有事件对应概率分布的分布函数；统计是根据某个概率分布（即总体分布）采样得到的结果，反向推导该分布的属性。\n  概率论 统计    概率空间（事件总体） 样本空间（总体）  试验 样本  随机变量的数字特征 样本的数字特征（统计量）  渐进理论 统计推段    统计推断是统计的终极话题。从任务角度，统计推断主要包括参数估计和假设检验；从方法角度，统计推断分为频率学派和贝叶斯学派。由于笔者习惯采用频率学派视角，故将贝叶斯推断单独抽出，另放它处。\n","date":1667927521,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667927521,"objectID":"d457d492963dc7122080eed24322b500","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/","section":"notes","summary":"概率论与统计：总览 随机变量其实是一个从事件（概率空间的子集）","tags":null,"title":"总览","type":"book"},{"authors":null,"categories":null,"content":" Covariance matrix of a random vector can usually be deduced from the distribution’s property, or estimated from samples. But how to generate an arbitrary covariance matrix? How should we populate the entries in a square matrix so that it makes a legitimate covariance matrix?\nFirst Method In general, we construct the target covariance matrix \\(\\Sigma\\) by giving its eigenvalues and its orthonormal eigenvectors (any real symmetric matrix, including the covariance matrix of course, can be constructed in this way). A diagonal matrix of eigenvalues (denoted as \\(D\\)) are easy to synthesize. It remains that how to synthesize a square matrix that has orthonormal column vectors.\nLet the dimension of the target covariance matrix be \\(n\\). Given an arbitrary square matrix \\(M\\) of dimension \\(n\\), we can decompose \\(M M^T\\), which is a real symmetric matrix, into \\(U \\Lambda U^T\\), where \\(U\\) is the orthonormal matrix consisting of \\(M M^T\\)’s eigenvectors and \\(\\Lambda\\) is the diagonal matrix populated with \\(M M^T\\)’s eigenvalues, due to the property of real symmetric matrix.\nThen we define \\[ \\begin{align} (M M^T)^{1/2} \u0026amp;\\coloneqq U \\Lambda^{1/2} U^T \\\\ (M M^T)^{-1/2} \u0026amp;\\coloneqq U \\Lambda^{-1/2} U^T \\\\ \\end{align} \\] We take \\(E = (M M^T)^{-1/2} M\\). Now \\(E\\) will contain the orthonormal column vectors as expected. To verify, \\[ \\begin{aligned} \u0026amp;E E^T = \\left[ (M M^T)^{-1/2} M \\right] \\left[ M^T ((M M^T)^{-1/2})^T \\right] \\\\ \u0026amp;= \\left[ (M M^T)^{-1/2} \\right] \\left[ M M^T \\right] \\left[ ((M M^T)^{-1/2})^T \\right] \\\\ \u0026amp;= \\left[ U \\Lambda^{-1/2} U^T \\right] \\left[ U \\Lambda U^T \\right] \\left[ U \\Lambda^{-1/2} U^T \\right] \\\\ \u0026amp;= U \\Lambda^{-1/2} \\underbrace{\\left[ U^T U \\right]}_{I} \\Lambda \\underbrace{\\left[ U^T U \\right]}_{I} \\Lambda^{-1/2} U^T \\\\ \u0026amp;= U \\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} U^T = U U^T = I \\end{aligned} \\] Thus, the targeting covariance matrix can be constructed as \\(\\Sigma = E D E^T\\).\nSecond Method The easiest way to generate a legitimate covariance matrix would be to arbitrarily synthesize a square matrix \\(A\\) of dimension \\(n\\), and take \\(\\Sigma = A A^T\\) (see here).\nBy doing so, we can obtain a covariance matrix very fast. But you lose the control over it. Since \\(A\\) is totally arbitrary, you can tell little about \\(\\Sigma\\)’ s eigenvalues, eigenvectors, etc.\n","date":1660389801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660389801,"objectID":"5bf21f5bc7eafa0b686f7b485a307a68","permalink":"https://chunxy.github.io/blogs/generating-covariance-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/generating-covariance-matrix/","section":"blogs","summary":"Covariance matrix of a random vector can usually be deduced from the distribution’s property, or estimated from samples. But how to generate an arbitrary covariance matrix? How should we populate the entries in a square matrix so that it makes a legitimate covariance matrix?","tags":null,"title":"Generating Covariance Matrix","type":"blogs"},{"authors":null,"categories":null,"content":"  The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point (Claude Shannon, 1948).\n Similar to that stated above, the fundamental subject of machine learning is about with best effort recovering the original data using the so-called model or algorithm.\nThe recovery can be done by memoizing. In communication system, this is exactly what we want: we need to try our best to convey the message with the achievable highest fidelity. But it may be overwhelmingly costly when the volume of data becomes larger, such as in the case faced by machine learning. Thus, a better way would be to summarizing, i.e. using a function to reveal the correlation (here I don’t mean the mathematical term correlation) among data.\nThis book reviews the classic concepts and methods of the recovery process in communication system field. But it would also be interesting for machine learning folks, as the two fields meet the same capacity limit of the recovery.\nA System View Let’s make the life easier by introducing a few concepts. In communication, we have source message to convey. After being passed to and encoded by the encoder, the message travels along the channel to be finally decoded by the decoder and received by the receiver.\nThe encoding process includes transforming the data into digital form and introducing redundant information for the actual transmission. The redundancy is necessary to keep the underlying data from the noisy channel, which may alter bits, or add/drop bits (these two cases are very rare though) to the bit flow transmitting on it.\nInformation theory is concerned with the theoretical limit and potential for such systems. The coding theory is concerned with designing such a coding theme as to reach the limit/potential.\nError-correcting Codes By the name of “error-correcting”, we mean that we want to be able to detect and correct errors; the retransmission is not an option.\nThe way we do this, as stated above, is to add redundancy. But that the redundancy needs to be cost-effective. The rate is the ratio between the amount of source data and that of the transmitted data; the higher the better.\nThere are some examples of coding scheme in this genre:\n repetition code block code  Hamming code   The Best Performance? There seems to be a tradeoff between the error probability (which we want to minimize) and the rate (which we want to maximize), which can be seen mathematically in the case of repetition code and Hamming code.\nIt seems that the error-rate curve, no matter for which kind of coding scheme, would pass the \\((0, 0)\\) point: in order to achieve a vanishingly small error probability, one would have to reduce the rate correspondingly to zero.\nHowever, Shannon proved that this curve may intersect with the rate axis at some nonzero point! And this maximum rate at which the communication is possible with arbitrarily small error probability is called the capacity of the channel. The capacity is only related to the noise level of this channel. For any noisy binary symmetric channel with noise level of \\(f\\) (i.e., the probability that a bit is flipped), its capacity is \\[ C(f) = 1 - H_2(f) = 1 - \\left[ f \\log \\frac{1}{f} + (1-f) \\log \\frac{1}{1-f} \\right] \\]\n","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"345b03e0c1e2cb520910c9849d85986d","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/introduction/","section":"notes","summary":"The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point (Claude Shannon, 1948).\n Similar to that stated above, the fundamental subject of machine learning is about with best effort recovering the original data using the so-called model or algorithm.","tags":null,"title":"Introduction","type":"book"},{"authors":null,"categories":null,"content":"本文为Elliptic Curve Cryptography系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。\nTable of Contents  加密算法分支 椭圆曲线与群  椭圆曲线 群（Group） 椭圆曲线上的群 标量积（Scalar Multiplication） 对数运算（Logarithm）   椭圆曲线与有限域  有限域（Finite Field） 有限域上的椭圆曲线 再看群 标量积与子群 离散对数运算（Discrete Logarithm）   椭圆曲线加密算法  Elliptic Curve Diffie-Hellman Elliptic Curve Digital Signature Algorithm 再看离散对数运算 ECC与RSA      加密算法分支  基于椭圆曲线\n基于椭圆曲线的加密算法包括ECC（Elliptic Curve Cryptography）、ECDH和ECDSA。ECDH与ECDSA是基于ECC发展而来。\n 基于模余运算\n基于模余运算的加密算法包括RSA、DSA、DH以及其他衍生算法。\n  椭圆曲线与群 椭圆曲线 一条椭圆曲线就是一组满足\\(y^2 = x^3 + ax + b\\)且\\(4a^3 + 27b^2 \\ne 0\\)的二维平面点集。\\(4a^3 + 27b^2 \\ne 0\\)的条件是为了保证曲线不存在奇点（singularity）；\\(y^2 = x^3 + ax + b\\)又被称作椭圆曲线的Weierstrass normal form。\n除了这条曲线上的点，我们还需要一个无穷远处的点，我们用\\(0\\)这个特殊的符号来表示这个点，所以椭圆曲线更准确的表达式为 \\[ \\{(x,y) \\in \\R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \\ne 0\\} \\cup \\{0\\} \\] 椭圆曲线的一条显而易见的性质是，它是关于\\(x\\)轴对称的。\n群（Group） 一个集合\\(G\\)加上一个二元运算\\(\\oplus\\)，若满足以下条件，就构成了数学上的一个群：\n 封闭性（closure）：\\(a \\in G, b \\in G \\to a \\oplus b \\in G\\)； 结合律（associativity）：\\((a + b) + c = a + (b + c)\\)； 存在一个单位元（identity element）\\(0\\)，使得\\(a + 0 = 0 + a = a\\)，即单位元与任何元素进行运算，不改变该元素的值； 每个数都存在一个逆元（inverse）。  若该群进一步满足\n 交换律（commutativity）：\\(a + b = b + a\\)。  则称该群为阿贝尔群（Abelian group）。\n椭圆曲线上的群 对于我们定义的椭圆曲线集合，我们\n 定义无穷远处的\\(0\\)为单位元；\n 定义逆元为该点关于\\(x\\)轴另一侧的对称点；\n 定义二元运算\\(\\oplus\\)如下：\n若一条直线与椭圆曲线的三个交点分别为\\(P,Q,R\\)，则\\(P \\oplus Q \\oplus R = 0\\)，我们称这三个点是对齐的（aligned）。在此处我们没有规定三个点之间的顺序，即三个点之间可以任意交换位置，也就是说我们的定义的二元运算是满足交换律的，我们定义的群是一个阿贝尔群。\n给定两个非零、非对称的点\\(P = (x_P, y_p), Q = (x_q, y_Q)\\)，我们可以很轻松地找到\\(R = P \\oplus Q\\): \\[ \\begin{align} x_R \u0026amp;= m^2 - x_P - x_Q \\\\ y_R \u0026amp;= y_P + m(x_R - x_P) \\\\ \u0026amp;= y_Q + m(x_R - x_Q) \\end{align} \\] 其中： \\[ m = \\begin{cases} \\frac{y_P - y_Q}{x_P - xQ}, \u0026amp; P \\ne Q \\\\ \\frac{3x_P^2 + a}{2y_P}, \u0026amp; P = Q \\end{cases} \\]\n  标量积（Scalar Multiplication） 给定之前的二元加法运算，我们可以定义出相应的群中元素与标量之间的乘法运算： \\[ n P = \\underbrace{P + \\dots + P}_{n \\text{ times}} \\] 这样的乘法运算可以在\\(\\Omicron(\\log n)\\)时间内完成。\n对数运算（Logarithm） 给定\\(n\\)和\\(P\\)，我们可以很高效地完成标量积运算\\(Q = nP\\)；但如果给定\\(Q\\)和\\(P\\)，我们如何计算出对数运算（虽然这里是除法，但是为了和密码学中的标记保持一致，这里使用了对数）\\(n = Q \\div P\\)呢？\n椭圆曲线与有限域 有限域（Finite Field） 有限域首先是一系列元素的集合，比如说由整数模余某个质数\\(p\\)得到的集合（通常表示为\\(\\Z/p\\)或\\(\\newcommand{F}{\\mathbb F} \\F_p\\)）；有限域还定义了两种二元运算：加法和乘法，且这两种运算应该满足如下条件：\n 在有限域上都是封闭的、满足结合律以及交换律的； 存在单位元； 每个元素都存在相应的逆元。  除此之外，乘法运算还应该满足分配律（distributive）：\\(x \\cdot (y + z) = x \\cdot y + x \\cdot z\\)。\n\\(\\F_p\\)包含了从\\(0\\)到\\(p-1\\)的所有整数，而加法、乘法操作之后要追加模余（除数为\\(p\\)）操作。\n 若\\(a + b = 0 \\pmod p\\)，则\\(a\\)，\\(b\\)互为加法逆元（additive inverse），\\(a=-b, b=-a\\)； 若\\(ab = 1 \\pmod o\\)，则\\(a\\)，\\(b\\)互为乘法逆元（multiplicative inverse），\\(a=b^{-1},b=a^{-1}\\)；\\(xy^{-1}\\)有时也表示为\\(x/y\\)；\\(n\\)的乘法逆元可以通过Extended Euclidean Algorithm，其时间复杂度为\\(\\Omicron(\\log n)\\)。  可以证明，\\(\\F_p\\)也是一个阿贝尔群。\n有限域上的椭圆曲线 椭圆曲线本身的定义为： \\[ \\{(x,y) \\in \\R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \\ne 0\\} \\cup \\{0\\} \\] 加上有限域的限制之后，变为 \\[ \\{(x,y) \\in \\F^2 | y^2 = x^3 + ax + b \\pmod p, 4a^3 + 27b^2 \\ne 0 \\pmod p, a, b \\in \\F_p \\} \\cup \\{0\\} \\] 由于有限域的限制，此时所有的点全部出现第一象限。该图像关于\\(y = p / 2\\)对称，因为若\\(y_1 + y_2 = p\\)， \\[ \\begin{aligned} y_1^2 \u0026amp;= (p - y_2)^2 \\\\ \u0026amp;= p^2 - 2py_2 + y_2^2 \\\\ \u0026amp;= y_2^2 \\pmod p \\end{aligned} \\]\n再看群  对于一个点\\(Q = (x_Q, y_Q)\\)，其逆元\\(-Q\\)定义为\\(-Q = (x_Q, -y_Q \\mod p)\\)；\n 我们这样定义有限域上椭圆曲线上的点之间的二元运算\\(\\oplus\\)，同之前一样，三个对齐的点（aligned points）\\(P,Q,R\\)满足 \\[ P \\oplus Q \\oplus R = 0 \\] 只不过这里“对齐”的含义与之前有所不同，之前的对齐指的是几何上的共线，即三个点满足\\(ax + by + c = 0\\)；而这里的对齐指的是： \\[ ax + by + c = 0 \\pmod p \\] 有趣的是，计算加法的公式和之前没有发生太大变化（证明）。给定两个非零、非对称的点\\(P = (x_P, y_p), Q = (x_q, y_Q)\\)，我们可以很轻松地找到\\(R = P \\oplus Q\\)： \\[ \\begin{align} x_R \u0026amp;= (m^2 - x_P - x_Q) \\mod p \\\\ y_R \u0026amp;= (y_P + m(x_R - x_P)) \\mod p \\\\ \u0026amp;= (y_Q + m(x_R - x_Q)) \\mod p \\end{align} \\]\n其中： \\[ m = \\begin{cases} (y_P - y_R)(x_P - x_R)^{-1} \\mod p, \u0026amp; P \\ne Q \\\\ (3x_P^2 + a)(2y_P)^{-1} \\mod p, \u0026amp; P = Q \\end{cases} \\]\n  群中元素的个数叫做群的秩（order），可以通过Schoof’s algorithm计算求得。\n标量积与子群 标量积依旧遵循之前的定义，给定正整数\\(n\\)和群中的点\\(P\\)， \\[ nP = \\underbrace{P + \\dots + P}_{n \\text{ times}} \\] 标量积其实就是对某个点\\(P\\)不断做加法，其中一个有趣的性质是，\\(0P, 1P, 2P, \\dots\\)的结果会以某个最小正周期周期\\(k\\)循环（证明）。这也就意味着，群中对加法\\(P\\)的倍数是关于加法封闭的（closed under addition），它们又构成了一个循环子群（cyclic subgroup），\\(P\\)又称作这个循环子群的基点（base point/generator），\\(k\\)是这个循环子群的秩（subgroup order）。\n根据Lagrange’s theorem，子群的秩是其父群的秩的约数。\n寻找基点 在ECC算法中，我们一般会先计算父群的秩\\(N\\)，找出它一个比较大的约数\\(n\\)，让\\(n\\)作为子群的秩，\\(h = N / n\\)称作这个子群的余因子（cofactor），再根据这个子群的秩去找这个子群的基点。一般来说，\\(n\\)会从\\(N\\)的质因子中选取，基本算法如下：\n 计算父群的秩\\(N\\)；\n 计算\\(N\\)的质因子\\(n\\)，从大到小排列进行试验：\n计算余因子\\(h = N / n\\)；\n 随机选择椭圆曲线上的一点\\(P\\)；\n 计算\\(G = hP\\)；\n 如果\\(G\\)为\\(0\\)，则重新选择\\(P\\)进行试验；否则这意味着\\(G\\)就是秩为\\(n\\)的子群的基点。\n   需要注意的是，ECC算法能够运行的前提是，\\(n\\)必须是\\(N\\)的质因子。\n离散对数运算（Discrete Logarithm） 现在我们解答之前提出的对数运算问题，给定\\(Q\\)和\\(P\\)，目前没有算法能够在多项式时间之内求解满足\\(Q = kP\\)的\\(k\\)。这个问题有点类似于给定整数\\(a\\)和\\(b\\)，如何求解满足\\(b = a^k \\pmod p\\)的\\(k\\)？这两个问题目前都没有算法能在多项式时间之内求解，这也是ECC算法安全的根本。\n椭圆曲线加密算法 寻找到之前秩为\\(n\\)、基点为\\(G\\)的子群后，我们就可以生成私钥和公钥了：\n 私钥是从\\(\\{1,\\dots,n-1\\}\\)中随机抽取的数字\\(d\\)； 公钥是点\\(H = dG\\)。  下面介绍两个基于ECC的公钥加密算法。\nElliptic Curve Diffie-Hellman ECDH是DH算法在椭圆曲线中的变体，它实际上是一种密钥交换算法，而不是加密算法。它的大致流程如下：\n Alice和Bob各自随机生成私钥和公钥：\\(H_A = d_A G, H_B = d_B G\\)，注意，Alice和Bob使用了相同的基点； Alice和Bob在非安全信道上交换各自的公钥，即使中间人拦截到 …","date":1655996601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655996601,"objectID":"921d4559d7269b7450dea97b0144cd64","permalink":"https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","section":"blogs","summary":"本文为Elliptic Curve Cryptography系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。\n","tags":null,"title":"椭圆曲线加密算法","type":"blogs"},{"authors":null,"categories":null,"content":" The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy reaches its maximum when the underlying distribution \\(p\\) is a uniform distribution. The maximum value is \\(\\log k\\) if the support is finite and has \\(k\\) many distinct values. This can be derived with the Jenson’s inequality and understood via the level of chao of a distribution. From an analysis point of view, the entropy is defined on a \\(k\\)-tuple whose domain is compact so that the maxima must exist; we can always find a larger entropy if any two of \\(p_1, \\dots,p_k\\) are not equal.\nThe entropy of continuous distribution \\(q\\) (probability density function) is usually similarly defined as \\[ H(q) = -\\E_{x\\sim q}\\log q(x) = -\\int q(x)\\log q(x)\\d x \\] This is actually the differential entropy introduced by Shannon. In fact, it is not a good continuous analog of discrete entropy and it was not rigorously derived. For example, this formula can be negative. Therefore, in the case of entropy, the random variable had better be discrete, despite the wide usage of differential entropy.\nGaussian Case The entropy of a \\(n\\)-dimensional Gaussian distribution \\(p(x) = \\frac{e^{-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1}(x-\\mu)}}{\\sqrt{|2\\pi\\Sigma|}}\\) can be derived as follows: \\[ \\begin{aligned} H(p) \u0026amp;\\triangleq -\\int p(x) \\log p(x) \\d x = -\\int p(x) [-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1} (x-\\mu) - \\frac 1 2 \\log |2\\pi\\Sigma|] \\d x \\\\ \u0026amp;= \\frac 1 2 \\int p(x) (x-\\mu)^T \\Sigma^{-1}(x-\\mu) \\d x + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= \\frac 1 2 \\int p(x) x^T \\Sigma^{-1} x \\d x + \\frac 1 2 \\int p(x) \\mu^T \\Sigma^{-1} \\mu \\d x \\\\ \u0026amp;\\quad - \\frac 1 2 \\int p(x) \\mu^T \\Sigma^{-1} x \\d x - \\frac 1 2 \\int p(x) x^T \\Sigma^{-1} \\mu \\d x \\\\ \u0026amp;\\quad + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= [\\frac 1 2 \\tr(\\Sigma^{-1} \\Sigma) + \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu] + \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu \\\\ \u0026amp;\\quad - \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu - \\frac 1 2 \\mu^T \\Sigma^{-1} \\mu \\\\ \u0026amp;\\quad + \\frac 1 2 \\log |2\\pi\\Sigma| \\\\ \u0026amp;= \\frac 1 2 n + \\frac 1 2 \\log |2\\pi\\Sigma| \\end{aligned} \\]\n","date":1651186036,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651186036,"objectID":"79348391dc77fdceb73b7108eee60233","permalink":"https://chunxy.github.io/notes/articles/information-theory/entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/entropy/","section":"notes","summary":"The entropy of discrete distribution \\(p\\) (probability mass function) is defined as \\[ H(p) = -\\mathrm{E}_{x \\sim p}\\log p(x) \\] The entropy reaches its maximum when the underlying distribution \\(p\\) is a uniform distribution.","tags":null,"title":"Entropy","type":"book"},{"authors":null,"categories":null,"content":" Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026amp;\\inf f_0(x) \\\\ s.t.\\quad \u0026amp; f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp; h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\). Then the Lagrangian function is \\(L:\\R^n \\times \\R^r \\times \\R^s \\mapsto \\R\\) (there is an implicit constraint that the variables must reside in the natural domain of the functions):\n\\[ L(x, \\lambda, \\mu) = f_0(x) + \\sum_{i=1}^r\\lambda_if_i(x) + \\sum_{i=1}^s\\mu_ih_i(x) \\] Define the function \\(P: \\mathcal X \\mapsto \\R\\) as \\[ P(x) = \\sup\\limits_{\\lambda,\\mu;\\lambda_i \\ge 0}L(x,\\lambda,\\mu) \\] The primal problem is \\[ \\begin{equation} \\inf_{x \\in \\mathcal X} P(x) \\label{primal} \\end{equation} \\] It is easy to have \\(P(x) =f_0(x)\\) on \\(\\mathcal X\\). Thus the primal problem is equivalent to the original problem. Denote primal problem \\(\\eqref{primal}\\)’s optimal value as \\(p^\\star\\).\nDefine the dual function \\(D: \\R^r \\times \\R^s \\mapsto \\R\\) as \\[ D(\\lambda, \\mu) = \\inf_{x \\in \\mathcal \\R^n}L(x, \\lambda, \\mu) \\] The Lagrangian dual problem is \\[ \\begin{equation} \\sup_{\\lambda,\\mu;\\lambda_i \\ge 0} D(\\lambda, \\mu) \\label{dual} \\end{equation} \\] Denote the dual problem \\(\\eqref{dual}\\)’s optimal value as \\(d^\\star\\). We always have \\(p^\\star \\ge d^\\star\\).\n Proof\nFor any feasible \\(x \\in \\mathcal X, (\\lambda, \\mu) \\in {\\R^r}^+ \\times \\R^s\\), \\[ P(x) = \\sup\\limits_{\\lambda\u0026#39;,\\mu\u0026#39;;\\lambda\u0026#39;_i \\ge 0}L(x,\\lambda\u0026#39;,\\mu\u0026#39;) \\ge L(x,\\lambda,\\mu) \\ge \\inf_{x\u0026#39; \\in \\R^n}L(x\u0026#39;, \\lambda, \\mu) =D (\\lambda, \\mu) \\] Therefore \\(p^\\star \\ge d^\\star\\).\n  Strong Duality \\(p^\\star \\ge d^\\star\\) is called weak duality because it always holds. \\(p^\\star = d^\\star\\) is called strong duality because it does not hold in general. Assume, though, a strong duality holds, let \\(x^\\star\\) be the primal optima, \\((\\lambda^\\star, \\mu^\\star)\\) be the dual optima, then\n\\[ f_0(x^\\star) = P(x^\\star) = D(\\lambda^\\star, \\mu^\\star) \\\\ \\]\n Proof \\[ \\begin{gathered} f_0(x^\\star) = p^\\star = d^\\star = D(\\lambda^\\star, \\mu^\\star) = \\inf_{x \\in \\R^n}L(x,\\lambda^\\star,\\mu^\\star) \\le L(x^\\star,\\lambda^\\star,\\mu^\\star) \\\\ f_0(x^\\star) = p^\\star = P(x^\\star) = \\sup\\limits_{\\lambda,\\mu;\\lambda_i \\ge 0}L(x^\\star,\\lambda,\\mu) \\ge L(x^\\star,\\lambda^\\star,\\mu^\\star) \\\\ L(x^\\star,\\lambda^\\star,\\mu^\\star) \\le f_0(x^\\star) \\le L(x^\\star,\\lambda^\\star,\\mu^\\star) \\end{gathered} \\]\nTherefore, \\[ f_0(x^\\star) = P(x^\\star) = D(\\lambda^\\star, \\mu^\\star) = L(x^\\star,\\lambda^\\star,\\mu^\\star) \\]\n  If the strong duality holds and \\((x^\\star,\\lambda^\\star,\\mu^\\star)\\) is the optima, they must satisfy the KKT conditions, and we can leverage the KKT conditions to solve the optima and optimal value:\nKarush-Kuhn-Tucker Conditions In standard optimization problem, KKT conditions refer to the below four:\n primal constraint: \\(f_1(x) \\dots,f_r(x) \\le 0, h_1(x),\\dots,h_s(x) = 0\\);\n dual constraint: \\(\\lambda_1,\\dots,\\lambda_r \\ge 0\\);\n complementary slackness: \\(\\lambda_1f_1(x),\\dots,\\lambda_rf_r(x) = 0\\);\n vanishing gradient of Lagrangian w.r.t. \\(x\\) : \\[ \\nabla f_0(x) + \\sum_{i=1}^r\\lambda_i\\nabla f_i(x) + \\sum_{i=1}^s\\mu_i\\nabla h_i(x) = 0 \\]\n  Note that solutions satisfying KKT conditions do not imply a strong duality or an optimal point. For a better discussion between strong duality and KKT conditions, please go to this discussion.\nWhen-to-apply Slater Condition Strong duality does not hold generally. But it does hold in standard convex optimization problem. In such case, KKT conditions are also sufficient for strong duality provided that \\(x^\\star\\) is an interior point of the feasible region.\nGeneral Case In cases where we cannot tell strong duality directly, we may still try to apply Lagrangian multiplier to convert the primal problem to the less-constrained dual problem (\\(\\lambda \\ge 0\\) is much looser than the constraints in the original problem). That is, we solve \\(d^\\star\\) first, and then check if \\(d^\\star = p^\\star\\). We do so with the following process:\n firstly take derivative of \\(L\\) w.r.t. the unconstrained \\(x\\) and make it zero (vanishing gradient) to obtain the closed-form expression of \\(D(\\lambda, \\mu)\\), find \\(\\lambda^\\star \\ge 0\\) (dual constraint) and \\(\\mu^\\star\\) that maximizes the \\(D(\\lambda, \\mu)\\) and solve \\(x^\\star\\) w.r.t. \\(\\lambda^\\star\\) and \\(\\mu^\\star\\), finally verify that \\(x^\\star\\) satisfies the constraints (primal constraint and the implied complementary slackness) in the original problem and \\(f_0(x^\\star) = d^\\star\\) (strong duality).  If we can successfully go through the above process, we can still solve the problem with Lagrangian multiplier.\n","date":1641560406,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560406,"objectID":"bb7039f27464630ccfa027fb1b9a7f87","permalink":"https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/lagrange-multiplier/","section":"notes","summary":"Standard Form The standard form of the Lagrange multiplier optimization problem is \\[ \\begin{aligned} \u0026\\inf f_0(x) \\\\ s.t.\\quad \u0026 f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026 h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Denote the feasible set of \\(x\\) that satisfies all the requirements in the above problem as \\(\\mathcal X \\subseteq \\R^n\\).","tags":null,"title":"Lagrange Multiplier","type":"book"},{"authors":null,"categories":null,"content":" Bullet Points This post lists out various topics under the machine learning subject.\nData  data modalities  numbers texts images videos audios  data cleaning imbalanced data data normalization (one pitfall)  standardization  data augmentation data splitting  cross validation RANSAC  feature extraction/engineering  domain expertise kernel method   Model  supervised vs. unsupervised  supervised learning  classification  logistic regression support vector machine Bayes classifier linear discriminant analysis  regression (linear and non-linear case)  unsupervised learning  discovers inherent properties (latent variables) in the data includes  clustering dimensionality reduction/manifold embedding anomaly detection latent-variable model  hidden Markov model independent component analysis     discriminative vs. generative classification vs. regression  from classification model to regression model from binary classification to multi-class classification  linear vs. non-linear parametric vs. non-parametric ensemble method  bootstrap aggregating  random forest  gradient boosting  least square boosting AdaBoost LogitBoost   regularization and overfitting  Evaluation  training criterion\n mean squared error cross entropy impurity  Gini index entropy   testing criterion\n confusion matrix\n  Real\\Pred Positive Negative    Positive TP FN  Negative FP TN     accuracy precision recall F-score  precision-recall curve\n receiver operation curve\n mean average precision\n   ","date":1639922389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639922389,"objectID":"cfea1ab9e517412946340cb8d3dd7922","permalink":"https://chunxy.github.io/notes/articles/machine-learning/bullet-points/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/bullet-points/","section":"notes","summary":"Bullet Points This post lists out various topics under the machine learning subject. Data data modalities numbers texts images videos audios data cleaning imbalanced data data normalization (one pitfall) standardization","tags":null,"title":"Bullet Points","type":"book"},{"authors":null,"categories":null,"content":" Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, ..., p_n\\) such that: \\[ x = q_1b_1 + q_2b_2 + ... + q_nb_n \\] These scalars are called the coordinates of \\(x\\) relative to the basis \\(\\mathcal{B}\\). \\[ [x]_\\mathcal{B} = \\begin{bmatrix} q_1 \\cdots q_n \\end{bmatrix}^T \\] is the coordinate vectors of \\(x\\) relative to \\(\\mathcal{B}\\). The mapping \\(x \\mapsto [x]_\\mathcal{B}\\) is called the coordinate mapping determined by \\(\\mathcal{B}\\).\nLet \\(P_\\mathcal{B} = [b_1, b_2, ..., b_n]\\), then \\(x = P_\\mathcal{B}[x]_\\mathcal{B}\\)\nLet \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) both be a basis for an n-dimensional vector space \\(\\R^n\\). Then there is a unique \\(n \\times n\\) matrix \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}\\) such that: \\[ [x]_\\mathcal{C} = \\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\] The columns of \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}\\) are the \\(\\mathcal{C}\\)-coordinate vectors of the vectors in the basis \\(\\mathcal{B}\\): \\[ \\mathop{P}\\limits_\\mathcal{C \\leftarrow B} = [[b_1]_\\mathcal{C}, [b_2]_\\mathcal{C}, ..., [b_n]_\\mathcal{C}] \\]\n\\[ \\begin{aligned} \u0026amp;P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} = P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B} \\begin{bmatrix} q_1 \\cdots q_n \\end{bmatrix}^T \\\\ \u0026amp;= P_\\mathcal{C}(q_1[b_1]_\\mathcal{C} + q_2[b_2]_\\mathcal{C} + ... + q_n[b_n]_\\mathcal{C}) \\\\ \u0026amp;= q_1P_\\mathcal{C}[b_1]_\\mathcal{C} + q_2P_\\mathcal{C}[b_2]_\\mathcal{C} + ... + q_nP_\\mathcal{C}[b_n]_\\mathcal{C} \\\\ \u0026amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\\\ \u0026amp;= x \\end{aligned} \\]\nSpecifically, when \\(\\mathcal{B} = \\mathcal{I}\\) is the standard basis, \\[ \\begin{aligned} \\mathop{P}\\limits_\\mathcal{C \\leftarrow I} \u0026amp;= [[e_1]_\\mathcal{C}, [e_2]_\\mathcal{C}, ..., [e_n]_\\mathcal{C}] \\\\ P_\\mathcal{C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow I} \u0026amp;= P_\\mathcal{C}[[e_1]_\\mathcal{C}, [e_2]_\\mathcal{C}, ..., [e_n]_\\mathcal{C}] \\\\ \u0026amp;= [e_1, e_2, ..., e_n] \\\\ \u0026amp;= I \\end{aligned} \\] Since \\(P_\\mathcal{C}\\) is invertible, \\(\\mathop{P}\\limits_\\mathcal{C \\leftarrow I} = P_\\mathcal{C}^{-1}\\). Therefore, \\([x]_\\mathcal{B} = P_\\mathcal{B}^{-1}x\\) in equation (2)\n\\[ \\begin{aligned} \\\\ [x]_\\mathcal{C} \u0026amp;= \\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\\\ (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}[x]_\\mathcal{C} \u0026amp;= (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B}[x]_\\mathcal{B} \\\\ [x]_\\mathcal{B} \u0026amp;= (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}[x]_\\mathcal{C} \\\\ \\end{aligned} \\] In other words, \\(\\mathop{P}\\limits_\\mathcal{B \\leftarrow C} = (\\mathop{P}\\limits_\\mathcal{C \\leftarrow B})^{-1}, \\mathop{P}\\limits_\\mathcal{B \\leftarrow C}\\mathop{P}\\limits_\\mathcal{C \\leftarrow B} = I\\)\n","date":1639860457,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639860457,"objectID":"75f9386bf8aae53c2959ba86f33e7c1e","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/","section":"notes","summary":"Let \\(\\mathcal{B} = \\{b_1, b_2, ..., b_n\\}\\) be a basis for a vector space \\(V\\). Then for \\(x = [x_1, x_2, ..., x_n]^T\\) in \\(V\\), there exists a unique set of scalars \\(q_1, q_2, .","tags":null,"title":"Coordinate System and Change of Basis","type":"book"},{"authors":null,"categories":null,"content":"其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。\n当然，减少严谨的符号运算，并不意味着完全不出现符号，因为算法本身就是对问题的抽象，剥掉这层抽象，就没办法进行架构在抽象之上的信息传递了。\nTable of Contents  问题描述及定义 解决思路 正文之前  负权边 负权环路 非负权环路 放缩操作   解决方法  Bellman-Ford算法 Dijkstra算法 有向无环图中的最短路径 对比      问题描述及定义 单源最短路径问题，旨在求解带权有向图（weighted directed graph）中1，从某个点（vertex）出发，到图中任意一点的最短距离，某些情况下，还需要找出这一条最短距离的路径，称之为最短路径，若无特殊指明且不致歧义，以下最短路径问题均指代单源最短路径问题。\n更严格一些，设\\(G(V, E)\\)表示带权有向图，\\(w : E \\to \\mathbb{R}\\)表示权重，路径\\(p = \\left \u0026lt;v_0, v_1, ... v_k\\right \u0026gt;\\)的距离定义为： \\[ W(p) = \\sum\\limits_{i = 1}^k w(v_{k-1}, v_k) \\] 其中， \\[ \\begin{gather*} \\forall i \\in [0, k], v_i \\in V \\\\ \\forall i \\in [1, k], (v_{i-1}, v_i) \\in E \\end{gather*} \\] 我们从某一点\\(s\\)（叫作起点，或者源点）出发，记其到图中任意一点\\(v\\)的最短路径距离为\\(\\delta(v)\\)，单源最短路径求解的就是任意一条从\\(s\\)到\\(v\\)的路径\\(p\\)，使得\\(W(p) = \\delta(v)\\)。\n为了方便，我们为每一个点\\(v \\in V\\)设立一个中间变量\\(d\\)，用\\(v.d\\)来表示求解过程中的最短距离的可行上界，也就是说始终有\\(\\delta(v) \\leq v.d\\)，算法初始化时，\\(v.d = +\\infty\\)，算法运行过程中，我们通过寻找路径使\\(v.d\\)这个上界不断减小，直到\\(v.d = \\delta(v)\\)。\n解决思路 最短路径问题（包括多源最短路径问题）都隐含着一个最优子结构（optimal substructure），即：\n 如果\\(p\\)是一条连接两个点的最短路径，那么\\(p\\)的任意一条子路径，一定也是连接其两个端点的最短路径。\n 这条性质可由反证法轻松得到，也将是后续寻找最短路径所需要理解的一个重要概念。\n正文之前 负权边 负权边指的是图中某些边的权重为负。虽然负权边不会对最短路径的最优子结构性质产生任何影响，但是后面我们会看到，负权边会导致Dijkstra算法失效。\n负权环路 负权环路指的是图中某些边构成一条环路（loop），并且这条环路上的所有边的权重相加结果为负。\n一旦从起点可以到达这个环路上的点，那么最短路径问题就变得没有意义了：我们可以不断重复地走这条环路，然后 “拐出” 环路，到达目标点，使得到达目标点的路径的权重变得任意小（arbitrarily short），所以也就不存在什么 “最短路径” 了。\n一个成熟的算法应当能够检测出图中是否有可以由\\(s\\)到达的负权环路，如果没有，则算法照常进行；如果有，应予以通报。\n非负权环路 非负权环路指的是图中某些边构成一条环路，并且这条环路上的所有边的权重相加的结果大于等于0。\n负权环路会使最短路径问题没有意义，那么非负权环路呢？或者说，最短路径是否包含非负权环路呢？\n答案是否，如果一条最短路径包含了非负权环路，我们大可将这段环路从路径中 “拿掉”，得到的路径和原路径可以达到同样的终点，并且新路径的权重不大于原路径的权重。\n放缩操作 放缩操作的对象是边，对于边\\((u,v)\\)，放缩操作将检测能否优化点\\(v\\)的上界：\nRELAX(u, v, w): if v.d \u0026gt; u.d + w(u, v): v.d = u.d + w(u, v) v.predecessor = u 即如果路径\\(s \\sim u \\to v\\)的长度小于当前\\(v\\)的上界，我们便可以借此优化\\(v\\)的上界，并同时通过将的\\(v\\)前继设为\\(u\\)来记录这一次优化。\n解决方法 Bellman-Ford算法 Bellman-Ford算法是最短路径问题中最为robust的一种了，能处理负权边、能检测负权环路、不要求当前图为有向无环图（directed acyclic graph）。Bellman-Ford算法基本框架如下：\n  原图中存在可由起点抵达的负权环路，返回 false，用以告知存在负权环路，最短路径问题无意义 原图中不存在可由起点抵达的负权环路，返回 true，用以告知最短路径问题已解决，并将结果蕴含在相应的数据结构中   // 算法主体 for i = 1 to |V| - 1 for each edge (u, v) in E RELAX(u, v, w) // 检测是否存在负权回路 for each edge (u, v) in E if v.d \u0026gt; u.d + w(u, v) return false return true  算法主体\n我们不妨先假设原图中不存在负权环路，先思考Bellman-Ford在解决最短路径问题时的正确性。\n根据以上的讨论，任意一点的最短路径中不存在环，故任意一点\\(t\\)的最短路径最多由\\(|V|-1\\)条边，\\(|V|\\)个点构成。设： \\[ p=\\left \u0026lt;v_0,v_1, ... v_k\\right \u0026gt;，其中v_0 = s，v_k = t \\] 在寻找\\(t\\)的最短路径时，任一\\(v \\in \\{u | (u,t) \\in E, u.d = \\delta(s, u)\\}\\)（即此时\\(v\\)的最短路径已找到，且点\\(v\\)有一条连向点\\(t\\)的边） ，都是\\(v_{k-1}\\)（也就是\\(t\\)在其最短路径中的前继）的一个候选，我们需要证明的是，\\(t\\)的实际前继能够在Bellman-Ford算法运行之下被发现，从而被真正地选为\\(t\\)的前继。\n根据前面的讨论，路径\\(p\\)的前缀\\(\\left \u0026lt;v_0, v_1\\right \u0026gt;, \\left \u0026lt; v_0, v_1, v_2\\right \u0026gt;...\\)分别是\\(v_1, v_2, ...\\)的最短路径，在外侧第一轮for-loop后，边\\((v_0, v_1)\\)一定会被放缩，而由于\\(\\left \u0026lt;v_0, v_1\\right\u0026gt;\\)实际是最短路径，故放缩之后，\\(v_1.d = \\delta(v_1)\\)且将不再变化（因为这已经是最小）；在外侧第二轮for-loop后，边\\((v_1, v_2)\\)一定会被放缩，而由于\\(\\left \u0026lt;v_0, v_1, v_2\\right\u0026gt;\\)实际是最短路径，故放缩之后，\\(v_2.d = \\delta(v_2)\\)且将不再变化（因为这已经是最小）\\(\\dots\\)如此放缩\\(k\\)轮后，我们便寻找到了\\(v_1, ... v_k\\)一众节点的最短路径以及其最短路径的前继。\n 负权回路检测\n至于负权回路检测部分的正确性，则不得不引入一些公式，但其实并不复杂。\n假设原图存在可由到达的负权回路\\(c = \\left \u0026lt;v_0, v_1, ... v_k\\right \u0026gt;, v_0 = v_k\\)，其中，\\(W(c) = \\sum_{i=0}^{k-1}w(v_i, v_{i+1}) \u0026lt; 0\\)。运用反证法，即假设最终不存在点\\(u,v\\)，使得\\(v.d \u0026gt; u.d + w(u, v)\\)，则有： \\[ \\begin{aligned} v_i.d \u0026amp;\\leq v_{i-1}.d + w(v_{i-1}, v_i), \\forall i \\in [1, k] \\Rightarrow \\\\ \\sum_{i=1}^k v_i.d \u0026amp;\\leq \\sum_{i=1}^k (v_{i-1}.d + w(v_{i-1},v_i)) \\\\ v_k.d + \\sum_{i=1}^{k-1} v_i.d \u0026amp;\\leq v_0.d + \\sum_{i=1}^{k-1} v_{i}.d + \\sum_{i=1}^k w(v_{i-1},v_i) \\\\ 0 \u0026amp;\\leq \\sum_{i=1}^k w(v_{i-1},v_i) = W(c) \\end{aligned} \\] 与\\(W(c) \u0026lt; 0\\)矛盾，故得证。\n  Dijkstra算法 Dijkstra算法相对于Bellman-Ford算法来说，可以在时间复杂度上有所优化，但是能够处理的情形也就少了一些：Dijkstra算法不能处理负权边（所以更不用提负权环路了）。Dijkstra算法基本框架如下：\n  维持一个点集\\(S\\)，点集\\(S\\)由最短路径已确定的点构成； 不断向中加入能够确定最短路径的点，直到所有中的点都被加入。   S = {} Q = G.V while Q is not empty u = EXTRACT-MIN(Q) add u to S for each v in G.adj[u] RELAX(u, v, w) 当然，难点在于如何根据\\(S\\)找出能够确定最短路径的点。寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集\\(S\\)，正是一个最短路径已经确定的点的集合。\n\\(\\forall u \\in S\\)且\\((u, t) \\in E\\)，对\\((u, t)\\)进行放缩后得到的值\\(t.d\\)，都是\\(\\delta(t)\\)的一个备选，\\(S\\)中每加入一个点\\(v\\)（非\\(t\\)的点），若边\\((v,t)\\)存在，对该边进行放缩后，\\(\\delta(t)\\)的备选（也就是放缩后的\\(t.d\\)）就会多一个，而\\(\\delta(t)\\)自然是这些备选中最小的那个。而当\\(t\\)的最短路径确定后，便可以将\\(t\\)加入到点集\\(S\\)中，\\(S\\)不断扩展，直至最终包含整个点集\\(V\\)，也就是所有点的最短路径都被找到。\n之前提到过，Dijkstra算法不能处理负权边的情况，但上述 Dijkstra算法的讨论中似乎也没有涉及到负权边，为什么它就不能处理了呢？并且，我们只知道放缩后\\(t.d\\)的是\\(\\delta(t)\\)的备选，那么对\\(t.d\\)的放缩要进行到什么时候，才能确认\\(t.d=\\delta(t)\\)呢？Dijkstra算法告诉我们，\\(\\forall u \\in V - S\\)，有\\(t.d \\leq u.d\\)时，\\(\\delta(t) = t.d\\)，也就是当\\(t\\)的上界小于所有待确认点\\((V-S)\\)的上界时，我们就能确定\\(t\\)的最短路径，也就能够将点\\(t\\)加入到\\(S\\)中。\n为什么？如果没有负权边，我们可以会发现，\\(\\forall u \\in V - S\\)，其上界\\(u.d\\)总是由放缩操作得到的，所以在算法运行过程中，它必然是单调递减的，而且它代表了一条具体的到达\\(u\\)的路径。但为什么\\(V - S\\)中的所有点的上界的最小值，却能够成为某个特定点\\(t\\)的最短路径呢？\n我们来看看\\(t\\)的上界成为最小上界的之前之后都发生了什么，换言之，在此之前，或者在此之后，\\(t.d\\)有没有可能更小？之前是不会更小了，因为我们说过，\\(t.d\\)是单调递减的；那么之后呢？如果在\\(t\\)的上界成为\\((V-S)\\)中的最小上界、从而被加入\\(S\\)之后，我们在新的某一轮中选取另外一个点\\(u \\in V - S\\)，作为此轮加 …","date":1624267992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624267992,"objectID":"b2158400d728ad52bd17a581d7d97410","permalink":"https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","section":"blogs","summary":"其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。\n","tags":null,"title":"单源最短路径问题","type":"blogs"},{"authors":null,"categories":null,"content":" Three Perspectives on NCE Non-parametric estimation The traditional log-likelihood function will be \\(\\ell = \\sum_x \\ln p_\\theta(x)\\). In NCE, we learn \\[ p_\\theta(1|x) = \\sigma(G(x;\\theta) - \\gamma) = \\frac{1}{1 + e^{-G(x;\\theta) + \\gamma}} \\] And corresponding loss function becomes \\[ \\begin{aligned} \\mathcal {L} \u0026amp;= -\\E_{x \\sim \\tilde p(x)} \\ln p_\\theta(1|x) - \\E_{x \\sim q(x)} \\ln p_\\theta(0|x) \\\\ \u0026amp;= -\\int \\tilde p(x) \\ln p_\\theta(1|x) dx - \\int q(x) \\ln p_\\theta(0|x) dx \\\\ \u0026amp;= - \\int [\\tilde p(x) + q(x)] [\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(1|x) + \\frac{q(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(0|x)]dx \\end{aligned} \\]\nLet \\(P(y|x) = \\begin{cases}\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)}, y=1 \\\\\\frac{q(x)}{\\tilde p(x) + q(x)},y=0\\end{cases}\\), \\[ \\label{loss} \\begin{aligned} \\arg \\min_{\\theta, \\gamma} \\mathcal{L} \u0026amp;= \\arg \\min_{\\theta, \\gamma} -\\int [\\tilde p(x) + q(x)][\\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(1|x) + \\int \\frac{q(x)}{\\tilde p(x) + q(x)} \\ln p_\\theta(0|x)]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} -\\int [\\tilde p(x) + q(x)][P(1|x) \\ln p_\\theta(1|x) + P(0|x)\\ln p_\\theta(0|x)]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} \\int [\\tilde p(x) + q(x)][P(1|x) \\ln \\frac{1}{p_\\theta(1|x)} + P(0|x)\\ln \\frac{1}{p_\\theta(0|x)}]dx \\\\ \u0026amp;= \\arg \\min_{\\theta, \\gamma} \\int [\\tilde p(x) + q(x)] H[P(y|x)||p_\\theta(y|x)]dx \\end{aligned} \\]\nSince cross entropy \\(H(p||q) \\ge H(p)\\) and the minimum is reached only when \\(p = q\\), the global minimum for equation \\(\\eqref{loss}\\) is reached when \\(p_\\theta(y|x) = P(y|x)\\). Therefore, \\[ \\begin{aligned} p_\\theta(1|x) = \\frac{1}{1 + e^{-G(x;\\theta) + \\gamma}} \u0026amp;= \\frac{\\tilde p(x)}{\\tilde p(x) + q(x)} = P(1|x) \\\\ \\frac{q(x)}{\\tilde p(x)} \u0026amp;= e^{-G(x;\\theta) + \\gamma} \\\\ \\tilde p(x) \u0026amp;= \\frac{q(x) e^{G(x;\\theta)}}{e^\\gamma} \\end{aligned} \\] \\(\\theta\\) and \\(\\gamma\\) are learnt so that \\(q(x) e^{G(x;\\theta) - \\gamma}\\) fit the real distribution. It becomes more intuitive when \\(q\\) is a uniform distribution and \\(q(x)\\) is a constant \\(U\\), where \\(e^\\gamma\\) will be the learnt normalizing factor.\nhttps://kexue.fm/archives/5617/comment-page-1\nMaximum likelihood estimation (the original paper’s view) The model is defined as \\(\\ln p_\\theta(x) = \\ln p^0_\\alpha(x) + c\\). The MLE will maximize the objective function \\[ \\begin{aligned} J_T(\\theta) \u0026amp;= \\frac{1}{T_d} [\\sum_{i=1}^{T_d \\\\} \\ln h(x_i;\\theta) + \\sum_{i=1}^{T_n} \\ln (1 - h(y_i;\\theta)) \\text{, ($x_i$\u0026#39;s are samples, $y_i$\u0026#39;s are noises, $T_n = \\nu T_d$)} \\\\ \u0026amp;\\stackrel{P}\\to J(\\theta) \\triangleq \\E_{x \\sim \\tilde p} \\ln h(x;\\theta) + \\nu \\E_{x \\sim q} \\ln (1 - h(x;\\theta)) \\\\ \\end{aligned} \\] where \\[ \\begin{gather} r_\\nu(u) = \\frac{1}{1 + \\nu e^{-u}} \\\\ G(x; \\theta) = \\ln p_\\theta(x) - \\ln q(x) \\\\ h(x;\\theta) = r_\\nu(G(x;\\theta)) = \\frac{1}{1 + \\nu e^{-G(x;\\theta)}} \\\\ \\end{gather} \\] Denote by \\(\\tilde J\\) the objective function \\(J\\) seen as a function of \\(f(.) = \\ln p_\\theta(.)\\), \\[ \\tilde J(f) = \\E_{x \\sim \\tilde p} \\ln r_\\nu(f(x) - \\ln q(x)) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) - q(x)]) \\] For \\(\\epsilon \u0026gt; 0\\) and \\(\\phi(x)\\) a perturbation of \\(f\\), \\[ \\tilde J(f + \\epsilon \\phi) = \\E_{x \\sim \\tilde p} \\ln r_\\nu(f(x) + \\epsilon \\phi(x) - \\ln q(x)) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) + \\epsilon \\phi - q(x)]) \\]\nBy Taylor’s expansion,\n\\[ \\begin{aligned} \\ln r_\\nu(u + \\epsilon u_1 + \\epsilon^2 u_2) \u0026amp;\\approx \\ln r_\\nu (u) + r_{\\frac{1}{\\nu}}(-u)(\\epsilon u_1 + \\epsilon^2 u_2) - \\frac{r_\\nu (u)r_\\frac{1}{\\nu}(-u)}{2}(\\epsilon u_1 + \\epsilon^2 u_2)^2 + \\Omicron \\big((\\epsilon u_1 + \\epsilon^2 u_2)^3 \\big) \\\\ \u0026amp;= \\ln r_\\nu (u) + \\epsilon u_1r_{\\frac{1}{\\nu}}(-u) + \\epsilon^2(u_2r_{\\frac{1}{\\nu}}(-u) - \\frac{1}{2}u_1^2 r_{\\frac{1}{\\nu}}(-u)r_\\nu (u)) + \\Omicron \\big(\\epsilon^3 \\big) \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} \\ln \\big(1 - r_v(u + \\epsilon u_1 + \\epsilon^2 u_2) \\big) \u0026amp;\\approx \\ln \\big(1 - r_v(u) \\big) - r_v(u)(\\epsilon u_1 + \\epsilon^2 u_2) - \\frac{r_{\\frac{1}{\\nu}}(-u) r_\\nu(u)}{2} (\\epsilon u_1 + \\epsilon^2 u_2)^2 + \\Omicron \\big((\\epsilon u_1 + \\epsilon^2 u_2)^3 \\big) \\\\ \u0026amp;= \\ln(1 - r_v(u)) - \\epsilon u_1 r_v(u) - \\epsilon^2 \\big( u_2 r_v(u) + \\frac{1}{2} u_1^2 r_{\\frac{1}{\\nu}}(-u) r_\\nu(u) \\big) + \\Omicron(\\epsilon^3) \\end{aligned} \\]\nSubstitute \\(u\\) with \\(f(x)\\), \\(u_1\\) with \\(\\phi(x)\\), \\(u_2\\) with \\(0\\) to give \\[ \\begin{aligned} \\tilde J(f + \\epsilon \\phi) \\approx \u0026amp;\\E_{x \\sim \\tilde p} \\ln r_\\nu \\big(f(x) + \\epsilon \\phi(x) - \\ln q(x) \\big) + \\nu \\E_{x \\sim q} \\ln (1 - r_\\nu[f(x) + \\epsilon \\phi - q(x)]) \\\\ = \u0026amp;\\E_{x \\sim \\tilde p} \\{\\ln r_\\nu \\big(f(x) - \\ln q(x) \\big) + \\epsilon \\phi(x) r_{\\frac{1}{\\nu}} \\big(\\ln q(x) - f(x) \\big) \\} \\\\ \u0026amp;+\\nu \\E_{x \\sim q} \\{ \\ln \\big(1 - r_\\nu[f(x) -\\ln q(x)] \\big) - \\epsilon \\phi(x) r_\\nu \\big( f(x) - \\ln q(x) \\big) \\} + \\Omicron(\\epsilon^2) \\\\ = \u0026amp;\\tilde J(f) + \\epsilon \\int \\phi(x) \\big(r_\\frac{1}{\\nu} [\\ln q(x) - f(x)] \\tilde p(x) - \\nu r_\\nu[f(x) - \\ln q(x)] q(x) \\big) + \\Omicron(\\epsilon^2) \\end{aligned} \\] The above equation attains the local …","date":1652354761,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652354761,"objectID":"c29c64575064d59ab58ff2d26987147a","permalink":"https://chunxy.github.io/notes/papers/noise-contrastive-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/noise-contrastive-estimation/","section":"notes","summary":"Three Perspectives on NCE Non-parametric estimation The traditional log-likelihood function will be \\(\\ell = \\sum_x \\ln p_\\theta(x)\\). In NCE, we learn \\[ p_\\theta(1|x) = \\sigma(G(x;\\theta) - \\gamma) = \\frac{1}{1 +","tags":null,"title":"Noise Contrastive Estimation","type":"book"},{"authors":null,"categories":null,"content":" Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, ..., x_n) \\\\ ... \\\\ y_m = f_m(x_1, x_2, ..., x_n) \\\\ \\end{cases} \\] Then Jacobian matrix is \\(m \\times n\\): \\[ \\begin{aligned} J \u0026amp;= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \u0026amp; \\frac{\\partial f}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial x_n} \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\frac{\\partial f_m}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_m}{\\partial x_n} \\\\ \\end{bmatrix} \\end{aligned} \\] When \\(f\\) is a linear transformation, i.e., \\(f\\) is a \\(m \\times n\\) matrix \\(T\\), \\(y = Tx\\), then, \\[ J = T \\]\nWhen \\(f\\) is a linear transformation and \\(n = m\\), i.e., \\(f\\) is a \\(n \\times n\\) square matrix \\(T\\), \\[ \\begin{bmatrix} dy_1 \\\\ dy_2 \\\\ \\vdots \\\\ dy_n \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} \u0026amp; \\frac{\\partial f_n}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_n}{\\partial x_n} \\\\ \\end{bmatrix} \\begin{bmatrix} dx_1 \\\\ dx_2 \\\\ \\vdots \\\\ dx_n \\\\ \\end{bmatrix} \\] That is, \\[ \\underbrace{ \\begin{bmatrix} dy_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; dy_2 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; dy_n \\end{bmatrix}}_A \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ \\end{bmatrix} = \\underbrace{ \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} \u0026amp; \\frac{\\partial f_n}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_n}{\\partial x_n} \\\\ \\end{bmatrix}}_J \\underbrace{ \\begin{bmatrix} dx_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; dx_2 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; dx_n \\end{bmatrix}}_B \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ \\end{bmatrix} \\] \\(A\\) and \\(JB\\) are both diagonal. From above equation we can find that \\(A = JB\\). Therefore, \\[ \\begin{aligned} |A| \u0026amp;= |JB| \\\\ dy_1 \\dots dy_n \u0026amp;= |J|dx_1 \\dots dx_n \\\\ \\end{aligned} \\]\n","date":1652134140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652134140,"objectID":"b4e8ede93b88935cf7e33406e7b28137","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/jacobian-matrix/","section":"notes","summary":"Suppose \\(f: \\R^n \\to \\R^m\\), with input \\(x \\in \\R^n\\) and output \\(y \\in \\R^m\\): \\[ f = \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, .","tags":null,"title":"Jacobian Matrix","type":"book"},{"authors":null,"categories":null,"content":" The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be described as below:\n Spherical to Cartesian \\[ \\begin{alignat}{2} r \u0026amp;= \\sqrt{x_1^2 + \\dots + x_k^2} \u0026amp;\u0026amp; \\\\ \\varphi_1 \u0026amp;= \\arccot \\frac{x_1} {\\sqrt{x_k^2 + \\dots + x_2^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_1} {\\sqrt{x_k^2 + \\dots + x_1^2}} \\\\ \\varphi_2 \u0026amp;= \\arccot \\frac{x_2} {\\sqrt{x_k^2 + \\dots + x_3^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_2} {\\sqrt{x_k^2 + \\dots + x_2^2}} \\\\ \u0026amp; \\vdots \u0026amp;\u0026amp;\\vdots\\\\ \\varphi_{k-2} \u0026amp;= \\arccot \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}} \u0026amp;\u0026amp;= \\arccos \\frac{x_{k-2}} {\\sqrt{x_k^2 + x_{k-1}^2 + x_{k-2}^2}} \\\\ \\varphi_{k-1} \u0026amp;= 2 \\arccot \\frac{x_{k-1} + \\sqrt{x_k^2 + x_{k-1}^2}}{x_k} \u0026amp;\u0026amp;= \\begin{cases} \\arccos \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}}, \u0026amp;x_n \\ge 0 \\\\ 2\\pi - \\arccos \\frac{x_{k-1}} {\\sqrt{x_k^2 + x_{k-1}^2}}, \u0026amp;x_n \u0026gt; 0\\\\ \\end{cases} \\end{alignat} \\]\n Cartesian to spherical \\[ \\begin{align} x_1 \u0026amp;= r \\cos(\\varphi_1) \\\\ x_2 \u0026amp;= r \\sin(\\varphi_1) \\cos(\\varphi_2) \\\\ \\notag \u0026amp;\\vdots \\\\ x_{k-1} \u0026amp;= r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\\\ x_k \u0026amp;= r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\\\ \\end{align} \\] The corresponding Jacobian Matrix is \\[ J^{(k)} = \\left[ \\begin{array}{ccccc|c} \\cos (\\varphi_1) \u0026amp; -r \\sin(\\varphi_1) \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\sin(\\varphi_1) \\cos(\\varphi_2) \u0026amp; r \\cos(\\varphi_1) \\cos(\\varphi_2) \u0026amp; -r \\sin(\\varphi_1) \\sin(\\varphi_2) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\hline \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\ \u0026amp; \\ \u0026amp; -r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\\\ \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\ \u0026amp; \\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\end{array} \\right] \\] \\(|J^{(2)}|\\) can be easily derived as \\(r\\); and \\(|J^{(k)}|\\) can be constructed from \\(|J^{(k-1)}|\\). Comparing \\(J^{(k)}\\) and \\(J^{(k-1)}\\),\n\\(J^{(k)}\\) has an extra column \\(k\\) and an extra row \\(k\\), On row \\(k-1\\), before column \\(k\\), \\(J^{(k)}\\) has an extra \\(\\cos(\\varphi_{k-1})\\) term in each element than the elements of \\(J^{(k-1)}\\) on row \\(k-1\\), On row \\(k\\), before column \\(k\\), \\(J^{(k)}\\) has an extra \\(\\sin(\\varphi_{k-1})\\) term in each element than the elements of \\(J^{(k-1)}\\) on row \\(k-1\\), \\(J^{(k)}\\) and \\(J^{(k-1)}\\) are totally the same on the region delimited by row \\(1\\), row \\(k-2\\), column \\(1\\), column \\(k-1\\).  Apply the Laplace expansion along column \\(k\\) and combine the property of determinant to give \\[ \\begin{aligned} |J^{(k)}| =\\ \u0026amp; \\underbrace{0 + \\dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\sin(\\varphi_{k-1}) \\sin(\\varphi_{k-1}) \\big( \\sin(\\varphi_{k-1}) |J^{(k-1)}| \\big)] + \\\\ \u0026amp; (-1)^{n+n} [r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\cos(\\varphi_{k-1}) \\big( \\cos(\\varphi_{k-1}) |J^{(k-1)}| \\big)] \\\\ =\\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) \\big( \\sin_{\\varphi_{k-1}}^2 + \\cos{\\varphi_{k-1}}^2 \\big) |J^{(k-1)}| \\\\ =\\ \u0026amp; r \\sin(\\varphi_1) \\dots \\sin(\\varphi_{k-2}) |J^{(k-1)}| \\\\ \\end{aligned} \\] By induction, \\[ |J^{(k)}| = r^{k-1} \\sin^{k-2}(\\varphi_1) \\sin^{k-3}(\\varphi_2) \\dots \\sin(\\varphi_{k-2}) \\] Therefore when changing basis from orthogonal coordinate system to polar coordinate system, \\[ \\d x_1 \\dots \\d x_k = r^{k-1} \\sin^{k-2}(\\varphi_1) \\sin^{k-3} (\\varphi_2) \\dots \\sin(\\varphi_{k-2}) \\d r \\d \\varphi_1 \\dots \\d \\varphi_{k-1} \\]\n  Wiki || 3d Case Blog 1 || 3d Case Blog 2\nVolume of Sphere With change of basis between spherical coordinate and Cartesian, we may compute the volume of sphere in any \\(n\\)-dimension. \\[ \\begin{aligned} V_n \u0026amp;= \\int_{B_n} 1 \\; \\d x_1 \\d x_2 \\dots \\d x_n \\\\ \u0026amp;= \\int_{0}^{R} \\int_{0}^{2 \\pi} \\underbrace{\\int_{0}^{\\pi} \\dots \\int_{0}^{\\pi}}_{n-2} \\\\ \u0026amp;\\quad r^{n-1} \\sin(\\varphi_1)^{n-2} \\sin(\\varphi_2)^{n-3} \\dots \\sin(\\varphi_{n-2}) \\d r \\d \\varphi_1 \\dots \\d \\varphi_{n-1} \\\\ \u0026amp;= \\int_{0}^{R} r^{n-1} \\d r \\int_{0}^{2 \\pi} \\d \\varphi_{n-1} \\\\ \u0026amp;\\quad \\int_{0}^{\\pi} \\sin(\\varphi_{n-2}) \\d \\varphi_{n-2} \\int_{0}^{\\pi} \\sin^2(\\varphi_{n-3}) \\d \\varphi_{n-3} \\dots \\int_{0}^{\\pi} \\sin^{n-2}(\\varphi_{1}) \\d \\varphi_{1} \\\\ \\end{aligned} \\] Notice that \\[ \\int_{0}^{\\pi} \\sin^{n}(x) \\d x = \\sqrt{\\pi} \\frac{\\Gamma (\\frac{n-1}{2})}{\\Gamma (\\frac{n}{2})} \\] Therefore, \\[ \\begin{aligned} V_n \u0026amp;= \\int_{0}^{R} r^{n-1} \\d r \\int_{0}^{2 \\pi} \\d \\varphi_{n-1} \\\\ \u0026amp;\\quad \\int_{0}^{\\pi} \\sin(\\varphi_{n-2}) \\d \\varphi_{n-2} \\int_{0}^{\\pi} \\sin^2(\\varphi_{n-3}) \\d \\varphi_{n-3} \\dots \\int_{0}^{\\pi} \\sin^{n-2}(\\varphi_{1}) \\d \\varphi_{1} \\\\ \u0026amp;= \\frac{R^n}{n} \\cdot 2\\pi \\cdot \\sqrt{\\pi} \\frac{\\Gamma(0)}{\\Gamma(1/2)} \\cdot \\sqrt{\\pi} \\frac{\\Gamma(1/2)}{\\Gamma(2/2)} \\cdots \\sqrt{\\pi} \\frac{\\Gamma((n-3)/2)}{\\Gamma((n-2)/2)} \\\\ \u0026amp;= R^n \\frac{1}{n/2} \\sqrt{\\pi^n} \\frac{1}{\\Gamma((n-2)/2)} \\\\ \u0026amp;= \\frac{R^n \\sqrt{\\pi^n}}{\\Gamma(n/2)} \\end{aligned} \\] 《三体》中的数学——为什么很高 …","date":1652127973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652127973,"objectID":"2a6309977dd0e92cdd0292fd696fa72d","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/spherical-coordinates/","section":"notes","summary":"The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say \\(k\\)-d. In \\(k\\)-d case, their conversion can be","tags":null,"title":"Spherical Coordinates","type":"book"},{"authors":null,"categories":null,"content":" Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two basis is zero. Inner product can also be defined on some common interval between two functions, and thus the orthogonality.\nFrequency Domain It is intuitive to model after the inner product between vectors. Function (signal) on its domain can be viewed as an “infinite-dimension” vector. We represent this infinity in the definition of function inner product by integration. In particular, given two functions \\(s\\) and \\(g\\), an interval \\(I\\), the inner product is \\[ \\int\\limits_{x \\in I}s(x)g(x)dx \\] \\(s\\) and \\(g\\) are orthogonal on interval \\(I\\) if their inner product \\(\\int_{x \\in I}s(x)g(x)dx = 0\\).\nA set of basis of \\(\\R^N\\) Euclidean space contains \\(N\\) independent orthogonal basis. For an “infinite-dimension” function space, there are an infinite number of basis, among which a group of sine and cosine functions satisfy. For integer \\(k\\) and positive integers \\(m, n\\), \\[ \\begin{aligned} \\left\\{ \\begin{array} \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx = \\pi, m = n, m, n \\ge 1 \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx = 0, m \\ne n, m, n \\ge 1 \\\\ \\end{array} \\right. \\\\ \\left\\{ \\begin{array} \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx = \\pi, m = n, m, n \\ge 1 \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx = 0, m \\ne n, m, n \\ge 1 \\\\ \\end{array} \\right. \\\\ \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx = 0, m, n \\ge 1 \\end{aligned} \\]\n Proof\n\\(m = n\\) \\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos^2(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{1 - \\cos(2nx)}{2}dx \\\\ \u0026amp;= \\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin^2(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{1 + \\cos(2nx)}{2}dx \\\\ \u0026amp;= \\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(nx) \\cos(nx)dx \\\\ \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\sin(2nx)}{2}dx \\\\ \u0026amp;= 0 \\end{aligned} \\]\n \\(m \\ne n\\) \\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\cos(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\cos((m+n)x) + \\cos((m-n)x)}{2}dx \\\\ \u0026amp;= \\frac{\\frac{\\sin((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} + \\frac{\\sin((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\sin(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\cos((m+n)x) - \\cos((m-n)x)}{2}dx \\\\ \u0026amp;= \\frac{\\frac{\\sin((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} - \\frac{\\sin((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n\\[ \\begin{aligned} \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\cos(mx) \\sin(nx)dx \u0026amp;= \\int_{-\\pi + 2k\\pi}^{\\pi + 2k\\pi} \\frac{\\sin((n+m)x) + \\sin((n-m)x)}{2}dx \\\\ \u0026amp;= -\\frac{\\frac{\\cos((m+n)x)}{m+n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi} + \\frac{\\cos((m-n)x)}{m-n}\\bigg|^{x=\\pi + 2k\\pi}_{x=-\\pi + 2k\\pi}}{2} \\\\ \u0026amp;= 0 \\end{aligned} \\]\n   In other words, we can use the linear combination of \\(1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x), \\dots\\) to fit any continuous function on interval \\([-\\pi, \\pi]\\). Or use \\(1, \\cos(2\\pi fx), \\sin(2\\pi fx), \\cos(2\\pi f2x), \\sin(2\\pi f2x), \\dots\\) to fit any function on interval \\([\\frac{-1}{2f} + \\frac{k}{f}, \\frac{1}{2f} + \\frac{k}{f}]\\), which can be any interval by choosing the value of \\(f\\) (frequency) and the integer \\(k\\).\nA continuous function \\(s\\) approximated with such series up to level \\(N\\) can be written as: \\[ \\begin{align} \u0026amp;\\begin{split} s_N(x) \u0026amp;= a_0 + \\sum_{i=1}^N \\big( \\underbrace{a_n}_{A_n\\sin(\\varphi_n)} \\cos(2\\pi fnx) + \\underbrace{b_n}_{A_n\\cos(\\varphi_n)} \\sin(2\\pi fnx) \\big) \\\\ \u0026amp;= a_0 + \\sum_{n=1}^N \\bigg( A_n\\sin(2\\pi fnx + \\varphi_n) \\bigg) \\text{, where} \\\\ \\end{split} \\\\ \\notag \u0026amp;A_n = \\sqrt{a_n^2 + b_n^2}, \\sin(\\varphi_n) = \\frac{a_n}{\\sqrt{a_n^2 + b_n^2}}, \\cos(\\varphi_n) = \\frac{b_n}{\\sqrt{a_n^2 + b_n^2}} \\end{align} \\] \\(A_n\\) can be interpreted as the amplitude, \\(\\varphi_n\\) as the phase, \\(nf\\) as the frequency.\nComplex Frequency Domain By Euler’s Formula we have \\[ e^{ix} = \\cos x + i\\sin x \\] Thus \\(s_N(x)\\) can be re-written as \\[ \\begin{alignat}{2} s_N(x) \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\bigg( \\underbrace{a_n}_{A_n\\cos \\phi_n} \\cos(2\\pi fnx) + \\underbrace{b_n}_{A_n\\sin \\phi_n} \\sin(2\\pi fnx) \\bigg) \\\\ \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\bigg( A_n\\cos(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp;= a_0 \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\frac{A_n}{2} \\bigg( \\cos(2\\pi fnx - \\phi_n) + i\\sin(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp; \u0026amp;\u0026amp;+ \\sum_{n=1}^N \\frac{A_n}{2} \\bigg( \\cos(2\\pi fnx - \\phi_n) - i\\sin(2\\pi fnx - \\phi_n) \\bigg) \\\\ \u0026amp; \u0026amp;\u0026amp;\\Downarrow_\\text{by multiplication rule between complex numbers in polar form} \\\\ \u0026amp;= \u0026amp;\u0026amp; …","date":1652125189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652125189,"objectID":"4b793860ef0dddc5cda7df9cda2ef02b","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/fourier-transform/","section":"notes","summary":"Continuous-time Fourier Transform Fourier Series In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two","tags":null,"title":"Fourier Transform","type":"book"},{"authors":null,"categories":null,"content":" Rationale CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the original data. The original data can either be the data before the encoding, or the future data after various steps.\nCPC learns the representation by minimizing the following loss function: \\[ \\newcommand{\\c}{\\mathrm{c}} \\label{loss} \\mathcal L_N = -\\E_{t \\sim \\Phi} \\log \\frac{f_\\theta(\\x_l,\\c)} {\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} = -\\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{f_\\theta(\\x_l,\\c)} {\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} \\] \\(t = (\\x_1, \\dots, \\x_{N+1}, \\c^*; \\ell)\\) is a tuple of random variables and \\(\\Phi\\) is the distribution from which \\(t\\) is drawn. \\((\\x:\\c)_{1:N+1}\\) are drawn from the joint distribution \\(\\tilde p(\\x,\\c)\\). All \\(\\c_i\\)’s but one randomly-chosen \\(\\c^*\\) are trimmed from the original samples. \\(\\c^*\\) is known but it is unknown which sample it is associated with. \\(\\ell\\) denotes the index of this unique sample we are trying to predict. In essence, \\(\\theta\\) is parameterizing the representation \\(\\c\\). The formal score function \\(f_\\theta\\) is simply chosen to be a deterministic cosine similarity between \\(\\x\\) and \\(\\c\\).\n\\(\\x_{1:N+1}\\) consists of one positive sample \\(\\x^*\\) that is matched with \\(\\c^*\\) and more other independent negative (noise) samples \\(\\x_i\\)’s that are not matched with \\(\\c\\). \\(N\\) is the fixed ratio of the number of negative samples to the number of positive samples. Let \\(P(\\ell=i|\\x_{1:N+1},\\c^*)\\) represent the probability that \\(\\x_i\\) is the positive sample given \\(\\x_1, \\dots x_{N+1}\\) and \\(\\c^*\\). \\[ \\begin{aligned} P(\\ell=i|\\x_{1:N+1},\\c^*) \u0026amp;= \\frac{P(\\ell=i, \\x_{1:N+1},\\c^*)}{\\sum_{j=1}^{N+1} P(\\ell=j, \\x_{1:N+1},\\c^*)} \\\\ \\end{aligned} \\]\nSubstitute \\(P(\\ell=i,\\x_{1:N+1},\\c^*) = P(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} P(\\x_j)\\) to give \\[ \\begin{aligned} P(\\ell=i|\\x_{1:N+1},\\c^*) \u0026amp;= \\frac{P(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} P(\\x_j)}{\\sum_{j=1}^{N+1} [P(\\x_j,\\c^*) \\prod_{k=1,k \\ne j}^{N+1} P(\\x_k)]} \\\\ \u0026amp;= \\frac{\\tilde p(\\x_i,\\c^*) \\prod_{j=1,j \\ne i}^{N+1} \\tilde p_X(\\x_j)} {\\sum_{j=1}^{N+1} [\\tilde p(\\x_j,\\c^*) \\prod_{k=1,k \\ne j}^{N+1} \\tilde p_X(\\x_j)]} \\\\ \u0026amp;= \\frac{\\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} } {\\sum_{j=1}^{N+1} \\frac{\\tilde p(\\x_j,\\c^*)}{\\tilde p_X(\\x_j)}} \\\\ \\end{aligned} \\] The loss function \\(\\eqref{loss}\\) is in fact the expectation (the outer \\(\\E\\)) of the categorical cross entropy (the inner \\(\\E\\)) of identifying the sample as positive or negative. The minimum of loss function is thus reached when the two categorical distributions are identical. That is, \\[ \\begin{gather} P_\\theta(l = i|x_{1:N+1},\\c^*) = \\frac{f_\\theta(\\x_i,\\c)}{\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)} = \\frac{\\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} } {\\sum_{j=1}^{N+1} \\frac{\\tilde p(\\x_j,\\c^*)}{\\tilde p_X(\\x_j)}} = P(\\ell=i|\\x_{1:N+1},\\c^*) \\\\ f_\\theta(\\x_i,\\c) = \\frac{\\sum_{\\x^\\prime \\in X}f_\\theta(\\x^\\prime, \\c)}{\\sum_{\\x^\\prime \\in X} \\frac{\\tilde p(\\x^\\prime|\\c)}{\\tilde p_X(\\x^\\prime)}} \\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} \\\\ f_\\theta(\\x_i,\\c) \\propto \\frac{\\tilde p(\\x_i,\\c^*)}{\\tilde p_X(\\x_i)} \\end{gather} \\]\nBounding the Mutual Information CPC helps estimate the lower bound of the mutual information between the encoded representation and the original data when optimizing the InfoNCE loss: \\[ \\begin{aligned} \u0026amp;\\mathcal L_N^{\\text{opt}} = -\\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)}} {\\sum_{\\x\u0026#39; \\in X} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)}} \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\frac{\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)} + \\sum_{\\x\u0026#39; \\in X, \\x\u0026#39; \\ne x_l} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)}} {\\frac{\\tilde p(\\x_l, \\c^*)}{\\tilde p_X(\\x_l)}} \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} \\sum_{\\x\u0026#39; \\in X, \\x\u0026#39; \\ne x_l} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)} \\big) \\\\ \u0026amp;\\approx \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\E_{\\tilde p_X(\\x\u0026#39;)} \\frac{\\tilde p(\\x\u0026#39;, \\c^*)}{\\tilde p_X(\\x\u0026#39;)} \\big) \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( 1 + \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\big) \\\\ \u0026amp;\\ge \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} \\log \\big( \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)} (N - 1) \\big) \\\\ \u0026amp;= \\E_{p(\\x_{1:N+1},\\c^*)} \\E_{p(l|\\x_{1:N+1}, \\c^*)} [\\log \\frac{\\tilde p_X(\\x_l)} {\\tilde p(\\x_l, \\c^*)}] + \\log (N - 1) \\\\ \u0026amp;= -I(\\x;\\c^*) + \\log (N - 1) \\end{aligned} \\] Therefore, \\(I(\\x;\\c^\\star) \\ge \\log(N-1) - \\mathcal L^{\\mathrm{opt}}_{N}\\).\nExternal Materials Paper Review || CPC Formulation || NCE and InfoNCE || Demo of Bounding Mutual Information\n","date":1651231922,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651231922,"objectID":"58805b758267fe4a8cc5f2347a8ec405","permalink":"https://chunxy.github.io/notes/papers/contrastive-predictive-coding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/contrastive-predictive-coding/","section":"notes","summary":"Rationale CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the original data. The original data can either be the data before the encoding, or the future data after various steps.","tags":null,"title":"Contrastive Predictive Coding","type":"book"},{"authors":null,"categories":null,"content":" The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known. The entropy of \\(Y\\) conditioned on \\(X\\) is defined as \\[ H(Y|X) = -\\sum_{(x,y) \\in \\mathcal{X} \\times \\mathcal{Y}} p_{(X,Y)}(x,y) \\log \\frac{p_{(X,Y)}(x,y)}{p_X(x)} \\]\n","date":1649862201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649862201,"objectID":"87fa413dc1946d1c48b838a54f76f550","permalink":"https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/conditional-entropy/","section":"notes","summary":"The conditional entropy measures the the amount of information needed to describe the outcome of a random variable \\(Y\\) given that the value of another random variable \\(X\\) is known.","tags":null,"title":"Conditional Entropy","type":"book"},{"authors":null,"categories":null,"content":" For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in \\dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.\nLipschitz Constant If \\(K\\) is the minimum number to make the above condition hold, then \\(K\\) is called the Lipschitz constant.\nThe Lipschitz constant for a general differentiable function \\(f\\) will be the maximum spectral norm of its gradient \\(\\nabla f\\) over its domain. \\[ ||f||_{Lip} = \\sup_x \\sigma[\\nabla f(x)] = \\sup_x \\sup_{||v||=1} \\nabla f(x) \\cdot v \\] where \\(\\sigma[\\nabla f(x)]\\) denotes the spectral norm of \\(f\\)’s gradient at \\(x\\).\n The Lipschitz constant for a matrix transformation will be the matrix’s spectral norm. The Lipschitz constant for a \\(\\R \\mapsto \\R\\) function will be its largest subgradient over its domain  Composition of Functions Suppose two functions \\(f\\) and \\(g\\) are Lipschitz continuous respectively. Then, \\[ \\nabla (f \\circ g)(x) = \\nabla f [g(x)] \\nabla g(x) \\] by the chain rule of derivatives. \\(f \\circ g\\)’s Lipschitz constant will be \\[ \\begin{aligned} \\sigma(\\nabla (f \\circ g)(x)) \u0026amp;= \\sup_{||v|| = 1} ||\\{\\nabla f[g(x)] \\nabla g(x)\\} v|| \\\\ \u0026amp;= \\sup_{||v|| = 1} \\{||\\nabla g(x) v||\\} \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;\\le \\sup_{||v|| = 1} \\sigma[\\nabla g(x)] \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;= \\sigma[\\nabla g(x)] \\sup_{||v|| = 1} \\{||\\nabla f[g(x)] \\frac{\\nabla g(x) v}{||\\nabla g(x) v||}||\\} \\\\ \u0026amp;= \\sigma[\\nabla g(x)] \\cdot \\sigma\\{\\nabla f[g(x)]\\} \\end{aligned} \\] In other words, \\(||f \\circ g||_{Lip} = ||f||_{Lip} \\cdot ||g||_{Lip}\\)\nLipschitz Continuity, convexity, subgradients – Marco Tulio Ribeiro – (washington.edu)\nLipschitz continuous gradient · Xingyu Zhou’s blog\n","date":1643587341,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643587341,"objectID":"a018239acbdc9ed4220798346aa0dcb0","permalink":"https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/calculus/lipschitz-continuity/","section":"notes","summary":"For a continuous mapping \\(f\\), it is \\(K\\)-Lipschitz continuous if there exists a number \\(K\\) such that \\(\\forall x,y \\in \\dom(f)\\) \\[ ||f(x) - f(y)|| \\le K||x - y|| \\] If the gradient of \\(f\\) is \\(K\\)-Lipschitz continuous, we further say \\(f\\) is \\(K\\)-smooth.","tags":null,"title":"Lipschitz Continuity","type":"book"},{"authors":null,"categories":null,"content":" Spectral normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), which is exactly \\(A\\)’s largest singular value \\(\\sigma_{\\max}(A)\\).\nTo prove it, firstly note that: \\[ \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} \\iff \\max_{\\mathrm z} \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} \\] We may force a constraint on \\(\\mathrm z\\) such that \\(||\\mathrm z||^2_2 = 1\\). This is because \\[ \\frac{||Ac\\mathrm z||^2_2}{||c\\mathrm z||^2_2} = \\frac{c^2||A\\mathrm z||^2_2}{c^2||\\mathrm z||^2_2} = \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} \\] The problem becomes \\[ \\begin{gather} \\max_{\\mathrm z} \\frac{||A\\mathrm z||^2_2}{||\\mathrm z||^2_2} = ||A\\mathrm z||^2_2 = \\mathrm z^TA^TA\\mathrm z \\\\ s.t. ||\\mathrm z||^2_2 = 1 \\end{gather} \\] This can be solved by Lagrange multiplier, where the Lagrangian function will be \\[ L(\\mathrm z, \\lambda) = \\mathrm z^TA^TA\\mathrm z + \\lambda(||\\mathrm z||^2_2 - 1) \\]\nThe extrapolation of the spectral normalization will be related to Rayleigh quotient.\nmatrices - Why is the maximum Rayleigh quotient equal to the maximum eigenvalue? - Mathematics Stack Exchange\n","date":1643491438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643491438,"objectID":"daf295077a13c1a7211087e16860920e","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/","section":"notes","summary":"Spectral normalization of an \\(M \\times N\\) matrix \\(A\\) is defined as \\[ ||A||_2 = \\max_{\\mathrm z} \\frac{||A\\mathrm z||_2}{||\\mathrm z||_2} = \\sqrt{\\lambda_{\\max}(A^TA)} = \\sigma_{\\max}(A) \\] where \\(\\rm z \\in \\R^N\\), \\(\\lambda_{\\max}(A^TA)\\) is the maximum eigenvalue of matrix \\(A^TA\\), which is exactly \\(A\\)’s largest singular value \\(\\sigma_{\\max}(A)\\).","tags":null,"title":"Spectral Normalization","type":"book"},{"authors":null,"categories":null,"content":" Definitions  Convex Set\nA set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).\n Convex Function\nA function \\(f: \\R^N \\mapsto \\R\\) is convex if \\(dom(f)\\) is a convex set and \\[ f(\\alpha x^{(1)} + (1 - \\alpha)x^{(2)}) \\le \\alpha f(x^{(1)}) + (1 - \\alpha)f(x^{(2)}), \\forall x^{(1)}, x^{(2)} \\in dom(f), \\alpha \\in [0,1] \\] \\(f\\) is concave if \\(-f\\) is convex.\n  Differentiable function \\(f\\) with a convex domain is convex if and only if \\[ f(y) \\ge f(x) + \\nabla^T f(x)(y - x), \\forall x, y \\in dom(f) \\] \\(f\\) is twice differentiable if \\(\\nabla^2f(x)_{ij} = \\frac{\\partial^2f(x)}{\\partial x_ix_j}\\) exists at each \\(x \\in dom(f)\\). \\(f\\) is convex if and only if\n\\[ \\nabla^2f(x) \\succeq 0 \\]\n Theorem: Any local minima of a convex function \\(f: \\R^N \\mapsto \\R\\) is also a global minima.\nProof:\nSuppose \\(y\\) is a local minima. Then there exists a number \\(r \u0026gt; 0\\) such that \\(\\forall x \\in dom(f) \\land ||x-y||_2 \\le r \\Rightarrow f(x) \\ge f(y)\\). Suppose instead there exists a global minima \\(z\\). Then it must hold that \\[ \\begin{gather} ||y-z||_2 \u0026gt; r \\\\ 0 \u0026lt; \\frac{r}{||y-z||_2} \u0026lt; 1 \\end{gather} \\] There exists some \\(0 \u0026lt; \\epsilon \u0026lt; r\\) such that \\(0 \u0026lt; \\theta = 1 - \\frac{r - \\epsilon}{||y-z||_2} \u0026lt; 1\\). Then the distance from \\(y\\) to point \\((\\theta y + (1 - \\theta)z)\\) is \\[ ||y - (\\theta y + (1 - \\theta)z)||_2 = (1 - \\theta)||y-z||_2 = \\frac{r - \\epsilon}{||y-z||_2}||y-z||_2 \u0026lt; r \\] That is to say \\[ f(\\theta y + (1 - \\theta)z) \\ge f(y) \\] However, since \\(f\\) is convex and \\(0 \u0026lt; \\theta \u0026lt; 1\\), \\[ f(\\theta y + (1 - \\theta)z) \\le \\theta f(y) + (1 - \\theta)f(z) \\] Since \\(z\\) is the global minima, \\[ f(\\theta y + (1 - \\theta)z) \\le \\theta f(y) + (1 - \\theta)f(z) \u0026lt; \\theta f(y) + (1 - \\theta)f(y) = f(y) \\] which contradicts that \\(f(\\theta y + (1 - \\theta)z) \\ge f(y)\\) derived. In other words, \\(y\\) is a global minima.\n Standard Form Standard form of the optimization problem is \\[ \\begin{aligned} \u0026amp;\\min f_0(x) \\\\ s.t.\\quad \u0026amp;f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp;h_i(x) = 0, i = 1,\\dots,s \\\\ \\end{aligned} \\] Standard form of the convex optimization problem is \\[ \\begin{aligned} \u0026amp;\\min f_0(x) \\\\ s.t.\\quad \u0026amp;f_i(x) \\le 0, i = 1,\\dots,r \\\\ \u0026amp;a_i^Tx = b_i (Ax = b), i = 1,\\dots,s \\\\ \u0026amp;\\text{$f_0,\\dots,f_r$ are convex, and $a_0,\\dots,a_s \\in \\R^N$} \\end{aligned} \\]\n","date":1641560217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560217,"objectID":"c358464778e629d4dbcdff3a6c72282a","permalink":"https://chunxy.github.io/notes/articles/optimization/convex-optimization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/convex-optimization/","section":"notes","summary":"Definitions  Convex Set\nA set \\(\\mathcal X\\) is called a convex set if \\(x^{(1)}, x^{(2)} \\in \\mathcal X\\), then \\(\\forall \\alpha \\in [0,1], \\alpha x^{(1)} + (1 - \\alpha)x^{(2)} \\in \\mathcal X\\).","tags":null,"title":"Convex Optimization","type":"book"},{"authors":null,"categories":null,"content":" Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is. Take the Gaussian distribution as an example: \\[ \\begin{aligned} p(x|y=C_j) \u0026amp;= \\mathcal{N}(x;\\mu_j, \\Sigma) \\\\ \u0026amp;= \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}e^{-\\frac{1}{2}(x-\\mu_j)^T\\Sigma^{-1}(x-\\mu_j)} \\end{aligned} \\] Then the classification function will be \\(f_{LDA} = \\arg \\max\\limits_{j}p(x|y=C_j)p(y=C_j)\\).\nLDA’s parameters \\(\\theta = (\\mu, \\Sigma, \\varphi)\\) is also learned with Maximum Likelihood Estimation: \\[ \\begin{aligned} L(\\theta) \u0026amp;= \\prod_{i=1}^mp(x^{(i)}, y^{(i)};\\theta) \\\\ \u0026amp;= \\prod_{i=1}^mp(x^{(i)}|y^{(i)};\\theta)p(y^{(i)}) \\end{aligned} \\] The log-likelihood function will be \\[ \\begin{aligned} l(\\theta) \u0026amp;= \\log L(\\theta) \\\\ \u0026amp;= \\sum_{i=1}^m\\log p(x^{(i)}|y^{(i)};\\mu,\\Sigma) + \\sum_{i=1}^m\\log p(y^{(i)}, \\varphi) \\\\ \\end{aligned} \\] We can maximize above two summations separately.\nLet \\[ \\begin{aligned} l_1(\\mu,\\Sigma) \u0026amp;= \\sum_{i=1}^m\\log p(x^{(i)}|y^{(i)};\\mu,\\Sigma) \\\\ \u0026amp;= -\\frac{1}{2}\\sum_{i=1}^m(x^{(i)}-\\mu_{j|y^{(i)}})^T\\Sigma^{-1}(x^{(i)}-\\mu_{j|y^{(i)}}) + \\log|\\Sigma| + n\\log2\\pi\\\\ \\end{aligned} \\] Take derivative w.r.t. \\(\\mu_j\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial l_1}{\\partial \\mu_j} = \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)\\Sigma^{-1}(x^{(i)} - \\mu_j) \\\\ \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)\\Sigma^{-1}(x^{(i)} - \\mu_j) \u0026amp;= 0 \\\\ \\Sigma^{-1}\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \\mu_j) \u0026amp;= 0 \\\\ \\end{aligned} \\] Since the covariance matrix \\(\\Sigma\\) is real-symmetric and invertible and thus its nullspace is \\(0\\), \\[ \\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \\mu_j) = 0 \\\\ \\mu_j = \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)} \\] Take derivative w.r.t. \\(\\Sigma\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial l_1}{\\partial \\Sigma} = -\\frac{1}{2}\\sum_{i=1}^m(\\Sigma^{-1} -\\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T\\Sigma^{-1}) \\\\ \\sum_{i=1}^m(\\Sigma^{-1} -\\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T\\Sigma^{-1}) = 0 \\\\ \\Sigma^{-1}\\sum_{i=1}^m(I - \\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T) = 0 \\\\ \\sum_{i=1}^m(I - \\Sigma^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T) = 0 \\\\ \\sum_{i=1}^mI = \\Sigma^{-1}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\sum_{i=1}^m\\Sigma = \\Sigma\\Sigma^{-1}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^T \\\\ \\end{aligned} \\] Let \\[ \\begin{aligned} l_2(\\varphi) \u0026amp;= \\sum_{i=1}^m\\log p(y^{(i)}; \\varphi) \\\\ \u0026amp;= \\sum_{i=1}^m\\log\\prod_{j=1}^c\\varphi_j^{\\mathbb{I}(y^{(i)}=C_j)} \\\\ \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\log \\varphi_j \\end{aligned} \\] Note that \\(p(y^{(i)};\\varphi) = \\prod_{j=1}^c\\varphi_j^{\\mathbb{I}(y^{(i)}=C_j)} = \\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\varphi_j\\). We chose the product notation because it could be easily “logged”.\nThere is also a constraint in maximization of \\(l_2\\): \\(\\sum_{j=1}^c\\varphi_j = 1\\). We can form the corresponding Lagrangian function: \\[ J(\\varphi, \\lambda) = \\sum_{i=1}^m\\sum_{j=1}^c\\mathbb{I}(y^{(i)}=C_j)\\log \\varphi_j + \\lambda(-1 + \\sum_{j=1}^c\\varphi_j) \\] Take derivative w.r.t. \\(\\varphi_j\\) and make it zero to give \\[ \\begin{gather} \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\varphi_j} + \\lambda = 0 \\\\ \\varphi_j = -\\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\lambda} \\end{gather} \\] Because \\(\\sum_{j=1}^c\\varphi_j = 1\\), we have \\[ \\begin{gather} -\\frac{\\sum_{j=1}^c\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{\\lambda} = 1 \\\\ \\lambda = -M \\end{gather} \\] Then, \\[ \\varphi_j = \\frac{\\sum_{i=1}^m\\mathbb{I}(y^{(i)}=C_j)}{M} \\]\n","date":1641559342,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559342,"objectID":"28e9e3528c82215d54ec265584617cc9","permalink":"https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/linear-discriminant-analysis/","section":"notes","summary":"Linear discriminant analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is.","tags":null,"title":"Linear Discriminant Analysis","type":"book"},{"authors":null,"categories":null,"content":" Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F \\triangleq \\sqrt{\\sum_{i,j}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 = \\sum_{ij}A_{ij}^2 \u0026amp;= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026amp;= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026amp;= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026amp;= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026amp;= \\tr(AA^T) = \\tr(A^TA) \\end{aligned} \\]\n","date":1640015034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640015034,"objectID":"7a68b4d1cd7bdb941faed2c7b109ee09","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/","section":"notes","summary":"Frobenius Normalization of an \\(m \\times n\\) matrix \\(A\\) is defined as \\[ ||A||_F \\triangleq \\sqrt{\\sum_{i,j}A_{ij}^2} \\] It can be found that \\[ \\begin{aligned} ||A||_F^2 = \\sum_{ij}A_{ij}^2 \u0026= \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}A_{ij} = \\sum_{i=1}^n\\sum_{j=1}^mA_{ji}A_{ji} \\\\ \u0026= \\sum_{i=1}^m(\\sum_{j=1}^nA_{ij}A_{ji}^T) = \\sum_{i=1}^n(\\sum_{j=1}^mA_{ij}^TA_{ji}) \\\\ \u0026= \\sum_{i=1}^m(A_{i:}A_{:i}^T) = \\sum_{i=1}^n(A_{i:}^TA_{:i})\\\\ \u0026= \\sum_{i=1}^m(AA^T)_{ii} = \\sum_{i=1}^n(A^TA)_{ii}\\\\ \u0026= \\tr(AA^T) = \\tr(A^TA) \\end{aligned} \\]","tags":null,"title":"Frobenius Normalization","type":"book"},{"authors":null,"categories":null,"content":" Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ d_p(x, x^\\prime) = ||x - x^\\prime||_p \\coloneq (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\] where \\(p \\to \\infty\\). Chebyshev distance is in effect \\(\\max\\limits_i (|x_i - x^\\prime_i|)\\).\nProof of Discrete Form Let \\(a_i = |x_i - x^\\prime_i|\\) and without loss of generality let \\(a_1 = \\max\\limits_ia_i = \\max\\limits_i|x_i - x^\\prime_i|\\). \\[ \\begin{aligned} d_p(x, x^\\prime) \u0026amp;= \\lim_{p \\to \\infty}(\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(\\sum_{i=1}^na_i^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(a_1^p\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \u0026amp;= \\lim_{p \\to \\infty}(a_1^p)^{1/p} \\cdot \\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \u0026amp;= a_1 \\cdot \\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\\\ \\end{aligned} \\] Because \\(\\forall i, a_1 \u0026gt; a_i\\), then \\(\\frac{a_i}{a_1} \\le 1\\), and \\[ \\begin{aligned} 1 \\le \u0026amp;\\sum_{i=1}^n(\\frac{a_i}{a_1})^p \\le n \\\\ 1^{1/p} \\le \u0026amp;(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le n^{1/p} \\text{, where $p \u0026gt; 1$} \\\\ \\lim_{p \\to \\infty}1^{1/p} \\le \u0026amp;\\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le \\lim_{p \\to \\infty}n^{1/p} \\\\ 1 \\le \u0026amp;\\lim_{p \\to \\infty}(\\sum_{i=1}^n(\\frac{a_i}{a_1})^p)^{1/p} \\le 1 \\\\ \\end{aligned} \\]\nTherefore, \\(d_p(x, x^\\prime) = a_1 \\cdot 1 = a_1 = \\max\\limits_i|x_i - x^\\prime_i|\\).\nProof of Continuous Form Let \\(f(x)\\) be continuous and bounded on interval \\((a, b)\\), then, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} = \\sup\\limits_{x \\in (a,b)}|f(x)| \\]\nLet \\(S = \\sup\\limits_{x \\in (a,b)}|f(x)|\\), then \\(\\forall\\varepsilon \u0026gt; 0, \\exists x_0 \\in (a,b), |f(x_0)| \u0026gt; S - \\varepsilon\\). By continuity, \\(\\lim\\limits_{x \\to x_0}|f(x)| \u0026gt; S - \\varepsilon\\), then \\(\\exists\\delta \u0026gt; 0, \\forall x \\in U(x_0, \\delta), ||f(x)| - \\lim\\limits_{x \\to x_0}|f(x)|| \u0026lt; \\varepsilon \\rightarrow |f(x)| \u0026gt; S - 2\\varepsilon\\). \\[ \\begin{aligned} (\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \u0026amp;\\ge (\\int_{x_0 - \\delta}^{x_0 + \\delta} (S - 2\\varepsilon)^pdx)^{1/p} \\\\ \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \u0026amp;\\ge \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} (S - 2\\varepsilon)^pdx)^{1/p} \\\\ \u0026amp;= \\lim\\limits_{p \\to +\\infty}(2\\delta(S - 2\\varepsilon)^p)^{1/p} \\\\ \u0026amp;= S - 2\\varepsilon \\end{aligned} \\] Since \\(U(x_0, \\delta)\\) in within the interval \\((a,b)\\) and \\(|f(x)|^p \\ge 0\\), then, \\[ \\begin{aligned} (\\int_a^b |f(x)|^pdx)^{1/p} \u0026amp;\\ge (\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \\\\ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \u0026amp;\\ge \\lim\\limits_{p \\to +\\infty}(\\int_{x_0 - \\delta}^{x_0 + \\delta} |f(x)|^pdx)^{1/p} \\\\ \u0026amp;\\ge S - 2\\varepsilon \\end{aligned} \\] Because \\(\\varepsilon\\) is arbitrarily and positively valued, \\(\\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \\ge S\\)\nOn the other hand, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} \\le \\lim\\limits_{p \\to +\\infty}(\\int_a^b S^pdx)^{1/p} = \\lim\\limits_{p \\to +\\infty}((b - a)S^p)^{1/p} = S \\] then, \\[ \\lim\\limits_{p \\to +\\infty}(\\int_a^b |f(x)|^pdx)^{1/p} = S = \\sup\\limits_{x \\in (a,b)}|f(x)| \\]\nExternal Material p范数的极限（无穷范数）为什么是极大值范数？ - 知乎 (zhihu.com)\n","date":1639859304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639859304,"objectID":"100621809c5d4a049b152158436e8417","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/","section":"notes","summary":"Chebyshev distance is a specific form of Minkowski norm (\\(l_p\\) norm): \\[ d_p(x, x^\\prime) = ||x - x^\\prime||_p \\coloneq (\\sum_{i=1}^n|x_i - x^\\prime_i|^p)^{1/p} \\] where \\(p \\to \\infty\\). Chebyshev distance is","tags":null,"title":"Chebyshev Distance","type":"book"},{"authors":null,"categories":null,"content":" Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\). It takes a probabilistic approach and maps \\(f(x)\\) to a probability value between \\(0\\) and \\(1\\) by applying a sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\). That is \\[ \\begin{aligned} p(y = +1|x) \u0026amp;= \\sigma(f(x)) \\\\ p(y = -1|x) \u0026amp;= 1 - \\sigma(f(x)) = \\sigma(-f(x)) \\\\ \\end{aligned} \\] Or equivalently, \\(p(y|x) = \\sigma(yf(x))\\). Then, \\(f_{LR}(x) = \\arg \\max\\limits_{y \\in \\{-1, 1\\}} \\sigma(y(f(x)))\\).\nLogistic regression is a discriminative classifier because we are directly modelling \\(p(y|x)\\), with no intermediate step on \\(p(x|y)\\).\nGiven dataset \\(\\mathcal{D} = \\{ (x^{(i)}, y^{(i)}): i=1, \\dots, M \\}\\), logistic regression is learning by maximizing the conditional log-likelihood of \\(\\mathcal{D}\\). \\[ l(w, b) = \\log L(w, b) = \\log \\prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) = \\sum_{i=1}^M\\log p(y^{(i)}|x^{(i)};w,b) \\]\n$$ \\[\\begin{aligned} \u0026amp;(w^\\star, b^\\star) = \\arg \\max\\limits_{w, b}l(w, b) \\\\ \u0026amp;= \\arg \\max\\limits_{w, b}\\sum_{i=1}^M\\log p(y^{(i)}|x^{(i)};w,b) \\\\ \u0026amp;= \\arg \\max\\limits_{w, b}\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\\\ \u0026amp;= \\arg \\min\\limits_{w, b}-\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\\\ \\end{aligned}\\] \\[ Let $J(w, b) = -\\sum_{i=1}^M\\log \\sigma(y^{(i)}(w^Tx^{(i)}+b))$ be the target function. There is no closed-form solution to this optimization problem. Rather, it is to be solved by some iterative algorithm, e.g. gradient descent. For each iteration, parameters are updated by \\] \\[\\begin{gather} w \\leftarrow w - \\eta\\frac{\\partial J}{\\partial w} \\\\ b \\leftarrow b - \\eta\\frac{\\partial J}{\\partial b} \\end{gather}\\] \\[ Specifically, \\] \\[\\begin{aligned} \\frac{\\partial J}{\\partial w} \u0026amp;= -\\sum_{i=1}^M\\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial w} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial \\sigma(y^{(i)}f(x^{(i)}))} \\frac{\\partial \\sigma(y^{(i)}f(x^{(i)}))}{\\partial (y^{(i)}f(x^{(i)}))} \\frac{\\partial (y^{(i)}f(x^{(i)}))}{\\partial w} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{1}{\\sigma(y^{(i)}f(x^{(i)}))} \\sigma(y^{(i)}f(x^{(i)}))(1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\\\ \u0026amp;= -\\sum_{i=1}^M (1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\\\ \\end{aligned}\\] $$\n\\[ \\begin{aligned} \\frac{\\partial J}{\\partial b} \u0026amp;= -\\sum_{i=1}^M\\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial b} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{\\partial\\log \\sigma(y^{(i)}f(x^{(i)}))}{\\partial \\sigma(y^{(i)}f(x^{(i)}))} \\frac{\\partial \\sigma(y^{(i)}f(x^{(i)}))}{\\partial (y^{(i)}f(x^{(i)}))} \\frac{\\partial (y^{(i)}f(x^{(i)}))}{\\partial b} \\\\ \u0026amp;= -\\sum_{i=1}^M \\frac{1}{\\sigma(y^{(i)}f(x^{(i)}))} \\sigma(y^{(i)}f(x^{(i)}))(1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\\\ \u0026amp;= -\\sum_{i=1}^M (1 - \\sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\\\ \\end{aligned} \\]\nNote that the posterior obtained by binary Linear Discriminant Analysis also has the form of \\[ p(y|x) = \\frac{1}{1 + e^{-(w^Tx + b)}} \\] This is not to say LDA and LR are the same in binary case. LDA is a generative model which learns \\(p(x|y)\\) and \\(p(y)\\), LR is a discriminative model which only learns \\(p(y|x)\\). One can discriminate with a generative model, but not reversely.\nSome History Historically, there has been efforts on adapting linear regression to the classification task where the output is a probability value between \\(0\\) and \\(1\\), instead of between \\(-\\infty\\) and \\(+\\infty\\). Many attempts have been given to mapping \\((0, 1)\\) to \\((-\\infty, +\\infty)\\) first and then apply linear regression on transformed values. An early work uses the quantile function of standard normal distribution. The model was named as “probabilistic unit” (probit). However, this model is too computationally-expensive at that time. Later on a work that uses the quantile function of logistic distribution followed on, naming its model as “logistic unit” (logit). Essentially, the logit function is the log of the odds: \\[ \\mathrm{logit} (p) = \\ln \\frac{p}{1-p} \\] In machine learning, un-normalized scores for different classes are usually called logits too. It makes some sense since these unbounded values are to be mapped to \\((0, 1)\\), which means they are logit-ted values.\nMulti-class Logistic Regression One way to train use Logistic Regression in multi-class classification in to for each class \\(i = {1, \\dots, C}\\), assign a weight vector \\(w^{(i)}\\), the probability is define by the softmax function: \\[ p(y = c|x) =\\frac{\\exp(w^{(c)}\\cdot x + b^{(c)})}{\\sum_{i=1}^C \\exp(w^{(i)} \\cdot x + b^{(i)})} \\] Then \\(f_\\text{Multiclass LR}(x) = \\arg \\max\\limits_{y \\in \\{1,\\dots,C\\}}p(y|x)\\)\nTo prevent overfitting, a prior distribution is usually added to \\(w\\). Suppose each dimension of \\(w\\) is independently sampled from a Gaussian distribution \\(\\mathcal N(0, \\frac{C}{2})\\), then, \\[ p(w) \\propto \\exp(-\\frac{1}{C}w^Tw) \\]\n","date":1655497979,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655497979,"objectID":"2daad7c28250e9d73e1b3180fce1e548","permalink":"https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/logistic-regression/","section":"notes","summary":"Two-class Logistic Regression Logistic Regression is a binary linear classifier. Suppose the feature space is \\(\\R^N\\), then it processes the feature vector \\(x\\) with a linear function \\(f(x) = w^Tx + b\\), where \\(w \\in \\R^N\\) and \\(b \\in \\R\\).","tags":null,"title":"Logistic Regression","type":"book"},{"authors":null,"categories":null,"content":" \\(I_\\text{BA}\\) The very basic bound on the Mutual Information is based on the non-negativity of KL-divergence. \\[ \\begin{aligned} I(X;Y) = \u0026amp;\\E_{p(x,y)} \\log \\frac{p(x|y)}{p(x)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} + \\\\ \u0026amp;\\underbrace{\\E_{p(x,y)} \\log \\frac{p(x|y)}{q(x|y)}}_{\\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\\\ \\ge \u0026amp;\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} \\\\ = \u0026amp;\\E_{p(x,y)} \\log q(x|y) + H(X) \\triangleq I_\\text{BA} \\end{aligned} \\] This bound is not usually tractable since \\(p(x)\\) and thus \\(H(X)\\) has no closed-form expression.\nThis bound is tight when \\(q(x|y) = p(x|y)\\).\n\\(I_\\text{UBA}\\) When \\(q(x|y)\\) is replaced with an unnormalized model, that is \\[ q(x|y) = \\frac{p(x)}{Z(y)} e^{f(x,y)}, \\text{where } Z(y) = \\E_{p(x)} e^{f(x,y)} \\] Substituting this back to the \\(I_\\text{BA}\\) will give \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} \\log \\frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\triangleq I_\\text{UBA} \\end{aligned} \\]\nNote that by scaling \\(q(x|y)\\) with \\(p(x)\\), the \\(p(x)\\) term is canceled.\nThis bound is tight when \\(q(x|y) = \\frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\\). That is \\[ \\begin{aligned} e^{f(x,y)} \u0026amp;= \\frac{p(x, y)}{p(x)p(y)} Z(y) \\\\ e^{f(x,y)} \u0026amp;= \\frac{Z(y)}{p(y)} p(y|x) \\\\ f(x,y) \u0026amp;= \\ln p(y|x) + \\underbrace{\\ln \\frac{Z(y)}{p(y)}}_{c(y)} \\end{aligned} \\]\n\\(I_\\text{DV}\\) By further applying Jensen’s inequality to the \\(\\E_{p(y)} Z(y)\\) term in \\(I_\\text{UBA}\\), we get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\log\\E_{p(y)} [Z(y)] \\triangleq I_\\text{DV} \\end{aligned} \\]\nThe original \\(I_\\text{DV}\\) bound is in effect derived from the variational lower bound of KL-divergence: \\[ \\begin{aligned} I\u0026amp;(X;Y) = D_\\text{KL}(p_{(x,y)} || p(x) \\otimes p(y)) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\log [\\E_{p(x) \\otimes p(y)} f(x,y)] \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\log [\\E_{p(y)} \\E_{p(x)} e^{f(x,y)}] \\\\ \u0026amp;= \\E_{p(x,y)} f(x,y) - \\log \\E_{p(y)} Z(y) \\\\ \\end{aligned} \\]\n\\(I_\\text{TUBA}\\) \\(\\log\\) function also has the following property: \\[ \\begin{aligned} \\forall x,a\u0026gt;0,\\log(x) \u0026amp;\\le \\frac{x}{a} + \\log(a) - 1 \u0026amp;\\iff \\\\ a + a\\log(x) \u0026amp;\\le x + a\\log(a) \u0026amp;\\iff \\\\ a\\log(x) - x \u0026amp;\\le a\\log(a) - a \\\\ \\end{aligned} \\] Therefore, we can insert the inequality \\(\\log Z(y) \\le \\frac{Z(y)}{a(y)} + \\log a(y) - 1\\) into the \\(I_\\text{UBA}\\) to get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} \\log Z(y) \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} [\\frac{Z(y)}{a(y)} + \\log a(y) - 1 \\big)] \\triangleq I_\\text{TUBA} \\end{aligned} \\]\nThis bound is tight when\nthe tight condition of \\(I_\\text{UBA}\\) holds: \\(f(x,y) = \\log p(y|x) + \\underbrace{\\log \\frac{Z(y)}{p(y)}}_{c(y)}\\), and the tight condition of \\(I_\\text{TUBA}\\) holds: \\(a(y) = Z(y)\\).  \\(I_\\text{NWJ}\\) By setting \\(a(y) = e\\) in \\(I_\\text{TUBA}\\), we get \\[ \\begin{aligned} I(X;Y) \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - \\E_{p(y)} [\\frac{Z(y)}{a(y)} + \\log a(y) - 1 \\big)] \\\\ \u0026amp;\\ge \\E_{p(x,y)} f(x,y) - e^{-1}\\E_{p(y)} Z(y)\\triangleq I_\\text{NWJ} \\end{aligned} \\] Since \\(I_\\text{NWJ}\\) is a special case of \\(I_\\text{TUBA}\\), its bound is tight when \\(Z(y)\\) self-normalizes to \\(e\\). In this case \\[ \\begin{aligned} f(x,y) \u0026amp;= \\log p(y|x) + \\log \\frac{e}{p(y)} \\\\ \u0026amp;= 1 + \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= 1 + \\log \\frac{p(x|y)}{p(x)} \\end{aligned} \\]\n\\(I_\\text{NCE}\\) Our goal is to estimate the \\(I(X_1;Y)\\) given one sample from \\(p(x_1)p(y|x_1)\\) and \\(K-1\\) additional independent samples \\(x_{2:K} \\sim p^{K-1}(x_{2:K})\\). For any random variable \\(Z\\) that is independent from \\(X_1\\) and \\(Y\\), \\[ \\begin{aligned} I(X_1,Z;Y) \u0026amp;= \\E_{p(x_1,z,y)} \\frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\\\ \u0026amp;= \\E_{p(x_1,z,y)} \\frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\\\ \u0026amp;= I(X_1;Y) \\end{aligned} \\] Therefore, \\(I(X_1;Y) = I(X_1,X_{2:K};Y)\\). Bounding \\(I(X_1;Y)\\) becomes bounding \\(I(X_1,X_{2:K};Y)\\), which can be estimated using any of the preceding methods.\nSet the critic to \\(f(x_{1:K},y) = 1 + \\overbrace{\\log \\frac{e^{g(x,y)}} {a(y;x_{1:K})}}^{h(x_{1:K},y)}\\), where \\(x\\) is the sample among \\(x_{1:K}\\) that is paired with \\(y\\). Usually \\((x,y)_{1:K}\\) are sampled from the same marginal distribution of \\(\\tilde p(x,y)\\). \\(y\\) is then uniformly drawn among \\(y_{1:K}\\). Substitute these to the \\(I_\\text{NWJ}\\), we have \\[ \\label{infonce} \\begin{aligned} I(X_{1:K};Y) \u0026amp;\\ge \\E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\\E_{p(y)} Z(y) \\\\ \u0026amp;= \\E_{p(x_{1:K},y)} [1 + \\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \\E_{p(y)} [\\E_{p(x_{1:K})} e^{1 + \\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\\\ \u0026amp;= 1 + \\E_{p(x_{1:K})p(y|x_{1:K})} [\\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \\E_{p(y)} [e\\E_{p(x_{1:K})} \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\\\ \u0026amp;= 1 + \\E_{p(x_{1:K})p(y|x_{1:K})} [\\log \\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \\E_{p(x_{1:K})p(y)} \\frac{e^{g(x,y)}}{a(y,x_{1:K})} \\\\ \\end{aligned} \\] Further set \\(a(y;x_{1:K}) = \\frac{1}{K} \\sum_{i=1}^K e^{g(x_i, y)}\\). Substitute this into the last term in equation …","date":1654178480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654178480,"objectID":"0d74aa602870085f61f54726c27c2b64","permalink":"https://chunxy.github.io/notes/papers/bounding-mutual-information/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/bounding-mutual-information/","section":"notes","summary":"\\(I_\\text{BA}\\) The very basic bound on the Mutual Information is based on the non-negativity of KL-divergence. \\[ \\begin{aligned} I(X;Y) = \u0026\\E_{p(x,y)} \\log \\frac{p(x|y)}{p(x)} \\\\ = \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\\\ = \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} + \\\\ \u0026\\underbrace{\\E_{p(x,y)} \\log \\frac{p(x|y)}{q(x|y)}}_{\\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\\\ \\ge \u0026\\E_{p(x,y)} \\log \\frac{q(x|y)}{p(x)} \\\\ = \u0026\\E_{p(x,y)} \\log q(x|y) + H(X) \\triangleq I_\\text{BA} \\end{aligned} \\] This bound is not usually tractable since \\(p(x)\\) and thus \\(H(X)\\) has no closed-form expression.","tags":null,"title":"Bounding Mutual Information","type":"book"},{"authors":null,"categories":null,"content":" The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).\nThe Cross Entropy of distribution \\(q\\) relative to a distribution \\(p\\) is defined as \\[ H(p||q) = -\\mathrm{E}_{x \\sim p} \\log q(x) \\]\n","date":1650904786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650904786,"objectID":"4beb35e5c9dceb8cc4a3971a051aded0","permalink":"https://chunxy.github.io/notes/articles/information-theory/cross-entropy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/cross-entropy/","section":"notes","summary":"The cross entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution \\(q\\), instead of the true distribution \\(p\\).","tags":null,"title":"Cross Entropy","type":"book"},{"authors":null,"categories":null,"content":" If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent.\n\\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner. Quantitatively, smoothness assumes that \\(f\\) has a Lipschitz gradient. That means there exists an \\(L \u0026gt; 0\\) such that \\[ ||\\nabla f(x) - \\nabla f(y)||_2 \\le L||x - y||_2, \\forall x,y \\in \\mathrm{dom}(f) \\] We will say that such function is \\(L\\)-smooth or strongly smooth. \\(L\\)-smoothness is equivalent to \\[ f(y) \\le f(x) + (y - x)^T \\nabla f(x) + \\frac{L}{2}||y - x||^2_2 \\]\nProof  Further, \\(L\\)-smoothness is equivalent to \\(\\frac{L}{2}||x||^2_2 - f(x)\\) being convex.\nWith the smoothness, or the Lipschitz gradient, we can bound \\(f(y)\\) from above.\nWe will assume \\(L\\)-smoothness in all cases below. Meanwhile, the local minima (in convex case, the global minima) is denoted as \\(x^\\star\\).\nConvergence with Convexity With convexity, we can bound \\(f(y)\\) from below with linear approximation: \\[ f(y) \\ge f(x) + (y - x)^T \\nabla f(x) \\]\nConvergence Rate Fixed Step Size Now consider a \\(L\\)-smooth function \\(f\\). We adopt a fixed step size \\(\\alpha = \\frac{1}{L}\\) so that the update in each iteration will be \\[ x_{k+1} = x_k - \\frac{1}{L}\\nabla f(x) \\] By the definition of \\(L\\)-smoothness, \\[ \\begin{equation} \\label{diff} \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x_k) + (-\\frac{1}{L}\\nabla f(x_k))^T \\nabla f(x_k) + \\frac{L}{2}||-\\frac{1}{L}\\nabla f(x_k)||^2_2 \\\\ \u0026amp;= f(x_k) - \\frac{1}{2L}||\\nabla f(x_k)||^2_2 \\end{aligned} \\end{equation} \\] \\(f(x_{k+1}) \\le f(x_k)\\) holds in each iteration.\nBy the convexity of \\(f\\), \\[ \\begin{gather} f(x^\\star) \\ge f(x_k) + (x^\\star - x_k)^T \\nabla f(x_k) \\\\ \\label{convexity} f(x_k) \\le f(x^\\star) + (x_k - x^\\star)^T \\nabla f(x_k) \\end{gather} \\] Substituting the result back to the right-hand side of equation \\(\\eqref{diff}\\), \\[ \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x^\\star) + (x_k - x^\\star)^T \\nabla f(x_k) - \\frac{1}{2M}||\\nabla f(x_k)||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \u0026amp;\u0026lt; (x_k - x^\\star)^T L(x_k - x_{k+1}) - \\frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \u0026amp;\u0026lt; \\frac{L}{2}(2(x_k - x^\\star)^T(x_k - x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\\\ \\end{aligned} \\] By using the fact that \\[ \\begin{aligned} ||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\\\ 2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2 \\end{aligned} \\] We have \\[ \\label{closer} f(x_{k+1}) - f(x^\\star) \\le \\frac{L}{2}(||x_k - x^\\star||^2_2 - ||x_{k+1} - x^\\star||^2_2) \\]\nThis indicates that \\(x_{k+1}\\) is closer to the global minima \\(x^\\star\\) than \\(x_k\\). In addition, \\[ \\begin{aligned} \\sum_{i=0}^k [f(x_{i+1}) - f(x^\\star)] \u0026amp;\\le \\sum_{i=0}^k \\frac{L}{2}(||x_i - x^\\star||^2_2 - ||x_{i+1} - x^\\star||^2_2) \\\\ \u0026amp;= \\frac{L}{2}(||x_0 - x^\\star||^2_2 - ||x_{k+1} - x^\\star||^2_2) \\\\ \u0026amp;\\le \\frac{L}{2}||x_0 - x^\\star||^2_2 \\end{aligned} \\]\nBecause \\(f(x_{i+1}) \\le f(x_i), \\forall i=0,\\dots,k\\), \\(f(x_{k+1}) \\le f(x_i), \\forall i=0,\\dots,k\\), then \\[ \\begin{aligned} k(f(x_{k+1}) - f(x^\\star)) \\le \\frac{L}{2}||x_0 - x^\\star||^2_2 \\\\ f(x_{k+1}) - f(x^\\star) \\le \\frac{L}{2k}||x_0 - x^\\star||^2_2 \\end{aligned} \\]\nVariant Step Size In real application scenario, it may be difficult to know \\(L\\) exactly. However, as long as we choose \\(\\alpha_k \\le \\frac{1}{L}\\) in each iteration, the convergence and the rate still holds. \\[ \\begin{aligned} f(x_{k+1}) \u0026amp;\\le f(x_k) + (-\\alpha_k \\nabla f(x_k))^T \\nabla f(x_k) + \\frac{L}{2}||-\\alpha_k \\nabla f(x_k)||^2_2 \\\\ \u0026amp;= f(x_k) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ f(x_{k+1}) \u0026amp;\\le f(x_k) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} f(x_{k+1}) - f(x^\\star) \u0026amp;\\le f(x_k) - f(x^\\star) - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2 \\\\ \\end{aligned} \\]\nLet \\(\\eta_{k+1} = f(x_{k+1}) - f(x^\\star) \\ge 0\\), the above becomes \\(\\eta_{k+1} \\le \\eta_{k} - (\\alpha_k - \\frac{L}{2} \\alpha_k^2)||\\nabla f(x_k)||^2_2\\). From equation \\(\\eqref{convexity}\\), \\[ \\begin{gather} \\eta_k = f(x_k) - f(x^\\star) \\le (x_k - x^\\star)^T \\nabla f(x_k) \\le ||x_k - x^\\star||_2||\\nabla f(x_k)||_2 \\\\ 0 \\le \\frac{\\eta_k}{||x_k - x^\\star||_2} \\le ||\\nabla f(x_k)||_2 \\\\ -||\\nabla f(x_k)||^2_2 \\le -\\frac{\\eta^2_k}{||x_k - x^\\star||^2_2} \\\\ \\end{gather} \\] Substituting it back to give \\[ \\eta_{k+1} \\le \\eta_k - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k) \\eta^2_k}{||x_k - x^\\star||^2_2} \\] From equation \\(\\eqref{closer}\\) we already have \\[ ||x_{k+1} - x^\\star||^2_2 \\le ||x_k - x^\\star||^2_2 \\le \\dots \\le ||x_0 - x^\\star||^2_2 \\] Thus, \\[ \\begin{aligned} \\eta_{k+1} \u0026amp;\\le \\eta_k - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k) \\eta^2_k}{||x_0 - x^\\star||^2_2} \\\\ \\frac{1}{\\eta_k} \u0026amp;\\le \\frac{1}{\\eta_{k+1}} - \\frac{(\\alpha_k - \\frac{L}{2}\\alpha^2_k)}{||x_0 - x^\\star||^2_2} \\cdot \\frac{\\eta_k}{\\eta_{k+1}} \\\\ \\frac{1}{\\eta_{k+1}} - \\frac{1}{\\eta_k} \u0026amp;\\ge \\frac{(\\alpha_k - …","date":1641932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641932800,"objectID":"376c072d7898120d37b9443aa2595ee3","permalink":"https://chunxy.github.io/notes/articles/optimization/gradient-descent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/gradient-descent/","section":"notes","summary":"If a convex function \\(f\\) is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that \\(f\\) will converge by gradient descent.\n\\(L\\)-smoothness Qualitatively, smoothness means that the gradient of \\(f\\) changes in a controlled, bounded manner.","tags":null,"title":"Gradient Descent","type":"book"},{"authors":null,"categories":null,"content":" 事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\(B\\)，如果\\(A\\)发生时B也发生，则称\\(B\\)包含\\(A\\)，记作\\(A \\subseteq B\\)。如果\\(A,B\\)互相包含，则二者相等。\n事件的互斥和对立 若两事件\\(A\\)和\\(B\\)不能再同一试验中同时发生，则称它们是互斥的。互斥事件的一个特例是对立事件，对于事件\\(A\\)，其对立事件\\(\\bar A\\)为\\(\\{ \\text{$A$不发生} \\}\\)。\n事件的和（并）与加法定理 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$发生，或$B$发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的和，记作\\(C = A + B\\)。\n概率论中的加法定理描述的则是：若干互斥事件之和的概率，等于各事件的概率之和。\n全概率公式 设\\(B_1, B_2, \\cdots\\)为有限或无限个事件，它们两两互斥且在每次实验中至少发生一个（mutually exclusive and collectively exhaustive），即 \\[ \\begin{gather} B_i B_j = \\emptyset (i \\ne j) \\\\ B_1 + B_2 + \\cdots = \\Omega \\\\ P(B_1 + B_2 + \\cdots) = P(\\Omega) = 1 \\end{gather} \\] 根据事件运算的性质，\\(A = A \\Omega = A B_1 + A B_2 + \\cdots\\)，由于\\(B_1, B_2, \\cdots\\)两两互斥，则显然\\(A B_1, A B_2, \\cdots\\)也两两互斥，故依据加法定理，有 \\[ P(A) = \\sum_i P(A B_i) = P(A B_1) + P(A B_2) + \\cdots \\] 再根据条件概率公式，有 \\[ P(A) = \\sum_i P(B_i) P(A | B_i) = P(B_1) P(A|B_1) + P(B_2) P(A|B_2) + \\cdots \\]\n这便是全概率公式。\n事件的积（交） 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$，$B$都发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的积，记作\\(C = A B\\)。\n事件的差 设两个事件\\(A\\)和\\(B\\)，定义新事件\\(C\\)为\\(C = \\{ \\text{$A$发生，$B$不发生} \\}\\)，则称事件\\(C\\)为事件\\(A\\)与事件\\(B\\)的差，记作\\(C = A - B\\)。\n我们对事件引入了和、积、差、对立运算。显然，这些运算符在数字运算中成立的运算规律，不一定对事件运算成立，比如说\\(A + A = A\\)而非\\(2 A\\)（无意义），\\(A A = A\\)而非\\(A^2\\)（无意义），\\((A - B) + B = A + B\\)而不是\\(A\\)。\n基于这些基本运算，我们也可以表示出更多的事件，比如：\n    表达式 含义    \\(ABC\\) 三者同时发生  \\(A B \\bar C + A B \\bar C + \\bar A B C\\) 三者有且仅有两件发生  \\(A \\bar B \\bar C + \\bar A B \\bar C + \\bar A \\bar B C\\) 三者有且仅有一件发生  \\(A + B + C\\) 三者至少其中之一发生    条件概率与独立性 设两个事件\\(A\\)和\\(B\\)，且\\(P(B) \\ne 0\\)，记\\(P(A|B)\\)为“在给定\\(B\\)发生的条件下\\(A\\)的条件概率”，则 \\[ P(A|B) = \\frac{P(AB)}{P(B)} \\] 在计数测度之下，这个式子很好证明。记\\(M_A\\)为使得事件\\(A\\)发生的基本事件个数，记\\(M_B\\)为使得事件\\(B\\)发生的基本事件个数，记\\(M_{AB}\\)为使得事件\\(A\\)和事件\\(B\\)同时发生的基本事件个数，记\\(M\\)为所有基本事件个数，则 \\[ P(A|B) = \\frac{M_{AB}}{M_B} = \\frac{M_{AB}/M}{M_B/M} = \\frac{P(AB)}{P(B)} \\]\n两个事件的独立 设两个事件\\(A\\)和\\(B\\)，\\(P(A)\\)和\\(P(A|B)\\)可能是有差异的，这个差异便反映了二者之间的关联。而如果\\(P(A) = P(A|B)\\)，则称这两事件独立。根据条件概率的定义，两事件\\(A\\)、\\(B\\)独立时，有 \\[ P(AB) = P(A) P(B) \\] 我们往往是根据事件属性，而非根据以上公式，来确定事件的独立性。比如说在连续抛两次硬币的试验中，这两次试验的结果之间确实不应该有什么关联，我们自然而然地认为它们之间是相互独立的（相互独立同分布）。\n有时我们会觉得所有的独立事件可能都来自于这样的多次相互独立同分布试验，但其实独立事件也可以来自一次试验，比如说从52张扑克牌中随机抽取一张，记事件\\(A\\)为抽中红桃花色、事件\\(B\\)为抽中数字6，则可以验证，这两个事件也是相互独立的。\n相互独立与乘法定理 设\\(A_1, A_2, \\cdots\\)为有限或无限个事件，如果从其中任意取出有限个\\(A_{i_1}, A_{i_2}, \\cdots, A_{i_m}\\)，都有 \\[ P(A_{i_1} A_{i_2} \\cdots A_{i_m}) = P(A_{i_1}) P(A_{i_2}) \\cdots P(A_{i_m}) \\] 则称事件\\(A_1, A_2, \\cdots\\)相互独立。概率论中的乘法定理描述的是：多个事件相互独立时，它们同时发生的概率，等于各自概率的乘积。\n需要注意的是，多个事件之间两两独立并不意味着它们相互独立，比如在掷两次硬币的实验中，定义以下事件： \\[ \\begin{gathered} A：第一次为正， P(A) = 1/2 \\\\ B：第二次为反， P(B) = 1/2 \\\\ C：两次结果相同， P(C) = 1/2 \\end{gathered} \\] 则可以轻易验证三者两两独立，但是\\(P(ABC) = 0 \\ne 1/8 = P(A) P(B) P(C)\\)。这三者的关系有如下图中的三个环：两两之间本可以相互分开（独立），但三者同在，便互相捆绑住了。\n","date":1670607435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670607435,"objectID":"7a2c5bc1dcd6160b647f0fd472d51543","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/","section":"notes","summary":"事件的运算 事件的包含和相等 同一试验下的两个事件\\(A\\)和\\","tags":null,"title":"事件与概率","type":"book"},{"authors":null,"categories":null,"content":" Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable. It is defined as \\[ I(X;Y) = \\sum_{(x,y) \\in X \\times Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] By its definition, \\[ \\begin{gather} I(X;Y) = I(Y;X) \\\\ I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \\otimes p_Y) \\\\ I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\end{gather} \\]\nGaussian Case To better illustrate the formula of mutual information between two Gaussian-distributed random variables \\(X\\) and \\(Y\\). We can concatenate them to form, say an \\(n\\)-dimensional random variable \\(Z\\), which is also Gaussian-distributed. Then the mutual information between \\(X\\) and \\(Y\\) can be computed as: \\[ I(X;Y) = \\frac 1 2 \\log \\frac{\\det \\Sigma_X \\det \\Sigma_Y}{\\det \\Sigma_Z} \\] The key to the derivation is that mutual information is the KL-divergence between the joint distribution and the product of the marginal distributions.\nThe joint can be described as \\[ p_{X:Y} = N(\\underbrace{\\mu_X:\\mu_Y}_\\mu, \\underbrace{ \\begin{bmatrix} \\ \\Sigma_{X} \u0026amp; \\Cov_{XY} \\\\ \\Cov_{YX} \u0026amp; \\Sigma_{Y} \\\\ \\end{bmatrix} }_\\Sigma ) \\]\nThe product of marginals can be described as \\[ p_{X} \\times p_{Y} = N(\\mu_x:\\mu_y, \\begin{bmatrix} \\ \\Sigma_{xx} \u0026amp; 0 \\\\ 0 \u0026amp; \\Sigma_{yy} \\\\ \\end{bmatrix} ) \\] The probability density function of an \\(n\u0026#39;\\)-dimensional Gaussian distribution is \\(p(x\u0026#39;) = \\frac{1}{\\sqrt{|2\\pi \\Sigma\u0026#39;|}}e^{-\\frac{1}{2}(x\u0026#39;-\\mu\u0026#39;)^T\\Sigma\u0026#39;^{-1}(x\u0026#39;-\\mu\u0026#39;)}\\). The entropy of this Gaussian distribution is \\(\\frac 1 2 n\u0026#39; + \\frac 1 2 \\log |2\\pi\\Sigma\u0026#39;|\\). In view of above, \\[ \\begin{aligned} I\u0026amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \\times p_Y) = \\int p_{X:Y}(\\underbrace{x:y}_z) \\log \\frac{p_{X:Y}(x:y)} {p_X(x) p_{Y}(y)} \\d z \\\\ \u0026amp;= \\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_{X:Y}(x:y) \\d z - \\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_X(x) \\d z \\\\ \u0026amp;\\quad\\quad\\quad -\\int p_{X:Y}(\\underbrace{x:y}_z) \\log p_Y(y) \\d z \\\\ \u0026amp;= \\int p_{Z}(z) \\log p_{Z}(z) \\d z - \\int p_{X}(x) \\log p_X(x) \\d x - \\int p_{Y}(y) \\log p_Y(y) \\d y \\\\ \u0026amp;= -(\\log \\sqrt{\\det(2\\pi \\Sigma)} + \\frac n 2) + (\\log \\sqrt{\\det(2\\pi \\Sigma_{X}}) + \\frac {n_X} 2) \\\\ \u0026amp;\\quad\\quad\\quad +(\\log \\sqrt{\\det(2\\pi \\Sigma_{Y}}) + \\frac {n_Y} 2) \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\det \\Sigma_X \\det \\Sigma_Y}{\\det \\Sigma_Z} \\end{aligned} \\]\nKronecker Gaussian Consider the multivariate Gaussian distribution random vector \\(X_k\\) and \\(Y_k\\) of the same length \\(k\\). Suppose they are both independent internally and they have the component-wise correlation \\(corr(X_i, Y_j) = \\delta_{ij} \\rho\\), where \\(\\rho \\in (-1, 1)\\) (open to ensure the covariance matrix is invertible), \\(1 \\le i, j \\le k\\), \\(\\delta_{ij}\\) is the Kronecker’s delta: \\[ \\delta_{ij} = \\begin{cases} 0, \u0026amp; i \\ne j \\\\ 1, \u0026amp; i = j \\end{cases} \\] Let \\(Z_k\\) be the vector concatenated by \\(X_k\\) and \\(Y_k\\). It is easy to draw its covariance matrix \\(\\Sigma_{Z_k}\\) like \\[ \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{pmatrix} \\] The mutual information between the \\(X_k\\) and \\(Y_k\\) can be calculated as \\[ I(X;Y) = \\frac 1 2 \\log \\frac{\\det \\Sigma_{X_k} \\det \\Sigma_{Y_k}}{\\det \\Sigma_{Z_k}} = -\\frac 1 2 \\log \\det \\Sigma_{Z_k} \\] The problem remains as how to compute \\(\\det \\Sigma_{Z_{k}}\\). After applying the Laplacian expansion along the first column, it remains to deal with the determinants of following two matrices (dashed lines rule out the row/column to be deleted): \\[ \\begin{gather} A_k = \\underbrace{ \\left( \\begin{array}{c:ccccccc} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\hdashline 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{array} \\right) }_\\text{$2k$ columns}, B_k = \\underbrace{ \\left( \\begin{array}{c:ccccccc} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\hdashline \\rho \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\hdashline 0 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\rho \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{array} \\right) }_\\text{$2k$ columns}, \\\\ \\det Z_k = \\left. …","date":1652962804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652962804,"objectID":"fa797733f5af1922ab9b2c6011dfcf35","permalink":"https://chunxy.github.io/notes/articles/information-theory/mutual-information/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/mutual-information/","section":"notes","summary":"Mutual information of two random variables \\(X\\) and \\(Y\\) is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable.","tags":null,"title":"Mutual Information","type":"book"},{"authors":null,"categories":null,"content":" Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary. These points are called support vectors.\nHard-margin SVM Suppose the data \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}), i=1, \\dots, M\\}\\), and is linearly separable. The separation hyperplane will be in the form of \\(w^Tx + b = 0\\). Let \\(y^{(i)} = 1\\) when \\(x^{(i)}\\) is above the hyperplane (\\(w^Tx^{(i)} + b \u0026gt; 0\\)), \\(y^{(i)} = -1\\) when \\(x^{(i)}\\) is below the hyperplane (\\(w^Tx^{(i)} + b \u0026lt; 0\\)).\nThe distance (margin) from a data point to the separation hyperplane will be \\[ d^{(i)} = \\frac{|w^Tx^{(i)} + b|}{||w||_2} = \\frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||_2}\\\\ \\] \\(d^{(i)}\\) is called the geometric distance, while \\(|w^Tx^{(i)} + b|\\) is called the functional distance.\nThe margin of the hyperplane \\(w^Tx + b = 0\\) will be \\[ d = \\min\\limits_{i \\in \\{1,\\dots,M\\}} d^{(i)} = \\min\\limits_{i \\in \\{1,\\dots,M\\}} \\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2} \\] The maximum margin solution is found by solving \\[ \\max\\limits_{w,b} \\min\\limits_{i \\in \\{1,\\dots,M\\}} \\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2} \\] Suppose \\((w^\\prime, b^\\prime)\\) is one solution to the above. Then \\((\\lambda w^\\prime, \\lambda b^\\prime)\\) is also a solution, because \\[ \\frac{y^{(i)}((\\lambda w^{\\prime})^Tx^{(i)} + \\lambda b^\\prime)}{||\\lambda w^\\prime||_2} = \\frac{\\lambda y^{(i)}((w^{\\prime})^Tx^{(i)} + b^\\prime)}{\\lambda ||w^\\prime||_2} = \\frac{y^{(i)}((w^{\\prime})^Tx^{(i)} + b^\\prime)}{||w^\\prime||_2} \\] There is an extra degree of freedom in this problem. Therefore, we may impose \\[ \\min\\limits_{i \\in \\{1,\\dots,M\\}} y^{(i)}(w^Tx^{(i)}+b) = 1 \\] to consume this freedom. Consequently, the problem becomes \\[ \\begin{gather} \\max \\frac{1}{||w||_2} \\iff \\min \\frac{1}{2}||w||^2_2 \\\\ s.t.\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1, i = 1,\\dots,M \\end{gather} \\]\nSoft-margin SVM It may not happen that the real data are linearly-separable, or there may exist noisy samples that disrupt this linear separability. In such case, we may allow some samples to violate the margin. We define some slack variables \\(\\xi_i \\ge 0\\). \\(\\xi_i \u0026gt; 0\\) means that the sample is inside the margin (or even this sample will be misclassified), \\(\\xi_i = 0\\) means that the sample is outside the margin.\nOf course, these slack variables should be as small as possible. Then the problem becomes \\[ \\begin{gather} \\min \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M\\xi_i \\\\ s.t.\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1 - \\xi_i, \\xi_i \\ge 0, i=1,\\dots,M \\end{gather} \\] \\(C\\) is a penalty factor on violating the margin. To some extent, it avoid overfitting.\nTo solve this, first convert the problem into the standard form: \\[ \\begin{aligned} \\min \\frac{1}{2}||w||^2_2 \u0026amp;+ C\\sum_{i=1}^M\\xi_i \\\\ s.t.\\quad 1 - \\xi_i - y^{(i)}(w^Tx^{(i)} + b) \u0026amp;\\le 0, i=1,\\dots,M \\\\ -\\xi_i \u0026amp;\\le 0, i=1,\\dots,M \\end{aligned} \\] This problem gives a strong duality. The solution will satisfy the KKT conditions. We first write down the Lagrangian: \\[ L(w,b,\\xi,\\lambda,\\mu) = \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M\\xi_i + \\sum_{i=1}^M\\lambda_i(1 - \\xi_i - y^{(i)}(w^Tx^{(i)} + b)) - \\sum_{i=1}^M\\mu_i\\xi_i \\] Then we form the dual problem: \\[ \\max_{\\lambda,\\mu;\\lambda_i,\\mu_i \\ge 0} \\min_{w,b,\\xi}L(w,b,\\xi,\\lambda,\\mu) \\] Take derivative w.r.t. \\(w\\), \\(b\\), and \\(\\xi\\) to give \\[ \\begin{gather} \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^M\\lambda_iy^{(i)}x^{(i)}, \\frac{\\partial^2 L}{\\partial w\\partial w} = I \\succeq 0 \\\\ \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^M\\lambda_iy^{(i)}, \\frac{\\partial^2 L}{\\partial b\\partial b} = 0 \\succeq 0 \\\\ \\frac{\\partial L}{\\partial \\xi} = C - \\lambda - \\mu, \\frac{\\partial^2 L}{\\partial \\xi\\partial \\xi} = 0 \\succeq 0 \\end{gather} \\] The Hessian matrices of \\(L\\) w.r.t. \\(w\\), \\(b\\), and \\(\\xi\\) are positive semi-definite. Therefore, \\(\\min\\limits_{w,b,\\xi}L(w,\\xi,\\lambda,\\mu)\\) is obtained at its local minimum, i.e. where its first-order derivative meets \\(0\\): \\[ w = \\sum_{i=1}^M\\lambda_iy^{(i)}x^{(i)} \\\\ \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ \\lambda + \\mu = C \\] Substitute above back to \\(L\\) to transform the dual problem into \\[ \\begin{gather} \\max_{\\lambda,\\mu} \\frac{1}{2}\\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + C\\sum_{i=1}^M\\xi_i - \\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + \\sum_{i=1}^M\\lambda_i(1 - \\xi_i - y^{(i)}b) + \\sum_{i=1}^M(\\lambda_i - C)\\xi_i \\\\ s.t.\\quad \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ \\lambda_i,\\mu_i \\ge 0, i=1,\\dots,M \\\\ \\Downarrow \\\\ \\max_{\\lambda,\\mu}D(\\lambda,\\mu) = -\\frac{1}{2}\\sum_{i=1}^M\\sum_{j=1}^M\\lambda_i\\lambda_jy^{(i)}y^{(j)}x^{(i)}\\cdot x^{(j)} + \\sum_{i=1}^M\\lambda_i \\\\ s.t.\\quad \\sum_{i=1}^M\\lambda_iy^{(i)} = 0 \\\\ 0 \u0026lt; \\lambda_i \u0026lt; C, i=1,\\dots,M \\\\ \\end{gather} \\] Since we know the optimal solution exists, instead of taking derivative of \\(D\\) w.r.t. …","date":1641559935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559935,"objectID":"8cc4f6f8501a1451db0abf04dd53210c","permalink":"https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/support-vector-machine/","section":"notes","summary":"Support vector machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary.","tags":null,"title":"Support Vector Machine","type":"book"},{"authors":null,"categories":null,"content":" Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima? That is, does \\[ \\forall d, i,f(x + d \\cdot e_i) \\ge f(x) \\Rightarrow f(x) = \\min_zf(z), \\text{ where $e_i$ is the $i$-th standard basis ?} \\] The answer is yes. This is because \\[ \\nabla f(x) = \\left[ \\begin{array} \\\\ \\frac{\\partial f}{\\partial x_1}, \\cdots, \\frac{\\partial f}{\\partial x_n} \\end{array} \\right] = 0 \\]\nConvex but Non-differentiable Function If \\(f\\) is only convex but not differentiable, the above will not necessarily hold. However, if \\(f\\) can be decomposed such that \\[ f(x) = g(x) + \\sum_{i=1}^nh_i(x_i) \\] where \\(g(x)\\) is convex and differentiable, and each \\(h_i(x_i)\\) is convex but possibly non-differentiable, the global minima still holds. For any \\(y\\), \\[ \\begin{aligned} f(y) - f(x) \u0026amp;= g(y) - g(x) + \\sum_{i=1}^nh_i(y_i) - \\sum_{i=1}^nh_i(x_i) \\\\ \u0026amp;\\ge \\nabla g(x)^T(y - x) + \\sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\\\ \u0026amp;= \\sum_{i=1}^n[\\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \\end{aligned} \\] Because \\(f(x)\\) obtains minimum along each axes, for any \\(d\\) and \\(k\\), \\[ \\begin{aligned} f(x + d \\cdot e_k) \u0026amp;\\ge f(x) \\\\ g(x + d \\cdot e_k) + \\sum_{i=1,i\\ne k}^nh_i(x_i) + h_k(x_k + d) \u0026amp;\\ge g(x) + \\sum_{i=1}^nh_i(x_i) \\\\ g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) \u0026amp;\\ge 0 \\\\ \\end{aligned} \\] By the linearity of subgradient, \\(0 \\in \\partial(g_k + h_k) = \\nabla_k g + \\partial h_k \\Rightarrow -\\nabla_k g \\in \\partial h_k\\). That is \\[ h_k(y_k) - h_k(x_k) \\ge -\\nabla_k g(x)(y_k - x_k) \\] Then, \\[ f(y) - f(x) = \\sum_{i=1}^n[\\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \\ge 0 \\]\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Coordinate Descent.pdf ======= ## External Materials \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; notes\nCoordinate Descent.pdf || Coordinate Descent in One Line, or Three if Accelerated | A Butterfly Valley (wordpress.com)\n","date":1641221431,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641221431,"objectID":"90f41cce8011359b9ca36fe147586b08","permalink":"https://chunxy.github.io/notes/articles/optimization/coordinate-descent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/coordinate-descent/","section":"notes","summary":"Convex and Differentiable Function Given a convex, differentiable \\(f: \\R^n \\mapsto \\R\\), if we are at a point \\(x\\) such that \\(f(x)\\) is minimized along each coordinate axis, have we found a global minima?","tags":null,"title":"Coordinate Descent","type":"book"},{"authors":null,"categories":null,"content":" FlatNCE provides a way to compute the gradient of InfoNCE without introducing the rounding error when subtracting between two similar numbers.\nSpecifically, let \\(g^\\ominus_{ij}\\) is the affinity score between reference sample \\(x_i\\) and negative (noise) sample \\(y\u0026#39;_j\\); \\(g^\\oplus_{ii}\\) is the affinity score between positive sample and itself/its transformation \\(y_j\\); \\(i\\) is the batch index. Denote by \\(\\hat l_\\text{InfoNCE}\\) the batch estimate of the loss from InfoNCE: \\[ \\newcommand{detach}{\\mathop{\\text{detach}}} \\newcommand{logsumexp}{\\mathop{\\text{logsumexp}}} \\begin{gather} \\hat l_\\text{InfoNCE} = \\logsumexp_j g^\\ominus_{ij} - g^\\oplus_{ii} = \\log (\\sum_{j \\ne i} \\exp g^\\ominus_{ij} ) -g^\\oplus_{ii} \\\\ \\end{gather} \\]\nUsually, the above is calculated as \\[ \\hat l_\\text{InfoNCE} = \\log[\\sum_{j \\in {\\oplus, \\ominus} } \\exp(g^\\ominus_{ij} - \\max_k g^\\ominus_{ik}) ] + \\max_k g^\\ominus_{ik} - g^\\oplus_{ii} \\] When the learning saturates, \\(\\hat l_\\text{InfoNCE}\\) goes to \\(0\\), which means \\(\\max_k g^\\ominus_{ik}\\) becomes \\(g^\\oplus_{ii}\\) and thus \\[ \\hat l_\\text{InfoNCE} = \\log[\\sum_{j \\in {\\oplus, \\ominus} } \\exp(g^\\ominus_{ij} - g^\\oplus_{ii}) ] + \\underbrace{g^\\oplus_{ii} - g^\\oplus_{ii} }_{\\text{error-prone}} \\] A rounding error will very likely happen when subtracting two near numbers. Such error will accumulate and fail the InfoNCE. As said, FlatNCE provides a way to circumvent this rounding error.\nGradient Perspective Denote by \\(\\hat l_\\text{FlatNCE}\\) the batch estimate of the negative loss from FlatNCE: \\[ \\begin{aligned} \\hat l_\\text{FlatNCE} \u0026amp;= \\exp [ \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) - \\detach \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) ] \\\\ \u0026amp;= \\frac{\\exp \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) } {\\detach [ \\exp \\logsumexp_{j \\ne i} ( g^\\ominus_{ij} - g^\\oplus_{ii} ) ] } \\\\ \u0026amp;= \\frac{\\exp \\log \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ]} {\\detach \\{\\exp \\log \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\\\ \u0026amp;= \\frac{\\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ]} {\\detach \\{ \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\end{aligned} \\]\nBy putting the positive sample into the contrasting samples, \\[ \\hat l_\\text{FlatNCE}^\\oplus = \\frac{1 + \\sum_j \\exp \\big( g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) \\big)} {1 + \\text{detach}[\\sum_j \\exp \\big( g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) \\big)]} \\] where the \\(1\\) comes from adding the positive sample \\(y_i\\) to the set of negative samples (let’s denote this “negative” sample by \\(y\u0026#39;_0\\)). It can be easily found that\n\\[ \\nabla_\\theta \\hat l_\\text{FlatNCE}^\\oplus (g_\\theta) = \\nabla_\\theta \\hat l_\\text{InfoNCE} (g_\\theta) \\]\nWe may further find that the gradient of FlatNCE is an importance-weighted estimator of the form \\[ \\begin{aligned} \\nabla_\\theta \\hat l^\\oplus_\\text{FlatNCE} \u0026amp;= \\frac{\\sum_{j \\ne i} \\{ \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] [\\nabla_\\theta g_\\theta(x_i, y\u0026#39;_j) - \\nabla_\\theta g_\\theta(x_i, y_i)] \\} } {\\detach \\{ \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) - g_\\theta (x_i, y_i) ] \\} } \\\\ \u0026amp;= \\frac{\\sum_{j \\ne i} \\{ \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] [\\nabla_\\theta g_\\theta(x_i, y\u0026#39;_j) - \\nabla_\\theta g_\\theta(x_i, y_i)] \\} } { \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] } \\\\ \u0026amp;= \\sum_{k \\ne i} \\left\\{ \\underbrace {\\frac{ \\exp [ g_\\theta(x_i, y\u0026#39;_k) ] } { \\sum_{j \\ne i} \\exp [ g_\\theta(x_i, y\u0026#39;_j) ] } }_{w_k} \\nabla_\\theta g_\\theta(x_i, y\u0026#39;_k) \\right\\} - \\nabla_\\theta g_\\theta(x_i, y_i) \\\\ \\end{aligned} \\] As the learning progresses, \\(w_k\\)’s other than \\(w_0\\) will go to \\(0\\); \\(w_0\\) will go to \\(1\\), which will cause the gradient to vanish.\nLower-bound Perspective \\(-\\hat l_\\text{InfoNCE}\\) and \\(-\\hat l_\\text{FlatNCE}\\) are part of the lower bounds to the mutual information in two methods. Given \\(y_0\\) the positive sample and \\(y_{j\u0026gt;0}\\) are the negative samples,\n\\[ \\label{lemma3.3} \\begin{aligned} -\\hat l^{K, \\theta}_\\text{InfoNCE} \u0026amp;= -\\log \\{ \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] \\} \\\\ \u0026amp;= \\sup_v (v \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] - (-1 - \\log (-v)) \\\\ \u0026amp;\\Downarrow_{v = -e^{-u}} \\\\ \u0026amp;\\ge -e^{-u} \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0)] - (-1 - \\log (e^{-u}) \\\\ \u0026amp;= 1 - u - \\frac 1 K \\sum_{j \u0026gt; 0} \\exp[g_\\theta(x_0,y_j) - g_\\theta(x_0,y_0) - u] \\end{aligned} \\]\nConsider \\(g_\\theta\\) as the primal critic and \\(u\\) as the dual critic. Since arbitrary choice of \\(u\\) and \\(g_\\theta\\) lower-bounds the mutual information, we can either jointly optimize \\(u\\) and \\(g_\\theta\\) or more preferably, train in an iterative fashion. Given \\(\\theta\\), set \\(u\\) to\n\\[ \\hat u(g_\\theta) = \\log ({\\frac 1 K \\sum_j \\exp[g_\\theta(x,y_j) - g_\\theta(x, y)]}) \\] Then we fix \\(u\\) and only update \\(\\theta\\). Because \\(u\\) is fixed, the only gradient comes from \\(g_\\theta\\). Plugin \\(\\hat u\\) to the right-hand side of …","date":1661636533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661636533,"objectID":"9eb397cbcf75e4bf725926644c9ccca3","permalink":"https://chunxy.github.io/notes/papers/flatnce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/papers/flatnce/","section":"notes","summary":"FlatNCE provides a way to compute the gradient of InfoNCE without introducing the rounding error when subtracting between two similar numbers. Specifically, let \\(g^\\ominus_{ij}\\) is the affinity score between reference","tags":null,"title":"FlatNCE","type":"book"},{"authors":null,"categories":null,"content":" Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\]\nTo verify that it integrates to \\(1\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} p(x) \\d x \u0026amp;= \\sqrt{(\\int_{-\\infty}^{+\\infty} p(x) \\d x) \\cdot (\\int_{-\\infty}^{+\\infty} p(y) \\d y)} \\\\ \u0026amp;= \\sqrt {\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y} \\\\ \\end{aligned} \\]\nLet \\(I^2 = \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty} p(x)p(y) \\d x \\d y\\), \\[ \\begin{gather} \\int_{-\\infty}^{+\\infty} p(x) \\d x = \\sqrt{I^2} \\\\ I^2 = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{y - \\mu}{\\sigma})^2} \\d x \\d y \\end{gather} \\] Let \\(u = \\frac{x-\\mu}{\\sigma}, v = \\frac{y-\\mu}{\\sigma}\\), \\[ \\begin{aligned} I^2 \u0026amp;= \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}u^2}\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}v^2} \\d u \\d v \\\\ \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(u^2 + v^2)} \\d u \\d v \\end{aligned} \\] Let \\(u = r \\sin \\theta, v = r \\cos \\theta\\), \\[ \\begin{aligned} \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(u^2+v^2)} \\d u \\d v \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} e^{-\\frac{1}{2} (r^2 \\sin^2\\theta + r^2 \\cos^2\\theta)} r \\d r \\d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} -e^{-\\frac{1}{2} r^2} \\d(-\\frac{1}{2} r^2) \\d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi}-e^{t}\\Big|_{t=0}^{t=-\\infty}d\\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi}d\\theta \\\\ \u0026amp;= 2\\pi \\end{aligned} \\] Therefore, \\(I^2 = \\frac{1}{2\\pi}2\\pi = 1\\), \\(\\int_{-\\infty}^{+\\infty}p(x)dx = \\sqrt{I^2} = 1\\)\nIndependent standard \\(n\\)-dimensional Let \\(Z = [Z_1, Z_2, ..., Z_n]^T\\), suppose \\(Z_i, Z_j (i,j=1,...,n \\and i \\ne j)\\) are independent and \\(Z_i (i=1,...,n)\\) observes standard Gaussian distribution, we can derive the joint distribution density function for random variable \\(Z\\) to be \\[ \\begin{aligned} \\newcommand{z}{\\mathrm{z}} p(\\z) \u0026amp;= p(z_1, z_2, ..., z_n) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z_i^2} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2}\\z^T\\z} \\\\ \\end{aligned} \\]\nFirst-order correlated \\(n\\)-dimensional We have given the joint distribution function of independent \\(n\\)-dimensional standard Gaussian distribution. What if \\(n\\) dimensions of \\(Z\\) are not standard, are not independent with each other, but are correlated only in first order?\nWe may begin with standard Gaussian random variables \\(X = [X_1, \\dots, X_n]\\). Then we can shift \\(X\\) by \\(\\mu\\) and linearly transform it with an invertible matrix \\(B^{-1}\\). \\(Z\\) mentioned above will just be such a matrix as \\(Z = B^{-1} (X - \\mu)\\) and \\[ \\begin{gather} X = B^{-1}(Z - \\mu) \\sim \\mathcal{N}(0, I) \\\\ p_X(\\x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} \\x^T\\x} \\end{gather} \\]\nSuppose \\(Z\\) is to take on values in \\(\\mathcal{Z}\\), which is a subset of \\(\\R^{n}\\), \\[ \\newcommand{z}{\\mathrm{z}} P_Z(Z \\in \\mathcal{Z}) = \\int_\\mathcal{Z} p_Z(\\z) \\d \\z \\] Since \\(Z = f(X) = BX + \\mu\\) and \\(B\\) is invertible, the mapping \\(X \\to Z\\) is one-to-one, therefore the multivariate Jacobian transformation is \\[ J(X \\to Z) = B^{-1} \\\\ \\] with its determinant \\(J = |J(X \\to Z)| = |B^{-1}| = |B|^{-1}\\). Note that \\[ \\begin{aligned} |J| \u0026amp;= \\sqrt{|B|^{-1}|B|^{-1}} \\\\ \u0026amp;= \\sqrt{|B|^{-1}|B^T|^{-1}} \\\\ \u0026amp;= \\sqrt{|BB^T|^{-1}} \\\\ \u0026amp;= |BB^T|^{-\\frac{1}{2}} \\end{aligned} \\] Therefore, \\[ \\begin{aligned} P_Z (Z \\in \\mathcal{Z}) \u0026amp;= P_X (X \\in f^{-1}(\\mathcal{Z})) \\\\ P_Z (Z \\in \\mathcal{Z}) \u0026amp;= \\int_{f^{-1}(\\mathcal{Z})} p_X(\\x) \\d \\x \\\\ \u0026amp;\\Downarrow_{\\x = f^{-1}(\\z)} \\\\ \\int_\\mathcal{Z} p_Z (\\z) \\d \\z \u0026amp;= \\int_\\mathcal{Z} p_X (f^{-1}(\\z)) |J| \\d \\z \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} \u0026amp;p_Z (\\z) = p_X (f^{-1}(\\z))|J| = p_X (B^{-1} (\\z - \\mu) |J| \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T(B^{-1})^T B^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T (B^T)^{-1} B^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2} (\\z-\\mu)^T (BB^T)^{-1} (\\z-\\mu)} |BB^T|^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{(2\\pi)^{\\frac{n}{2}}|BB^T|^\\frac{1}{2}} e^{-\\frac{1}{2} (\\z-\\mu)^T (BB^T)^{-1} (\\z-\\mu)} \\\\ \\end{aligned} \\]\nAlso note that \\[ \\begin{aligned} \\Sigma_Z \u0026amp;= E[(\\z-\\mu) (\\z-\\mu)^T] \\\\ \u0026amp;= E[B X X^T B^T] \\\\ \u0026amp;= B E[X X^T] B^T \\\\ \u0026amp;= B I B^T \\\\ \u0026amp;= B B^T \\end{aligned} \\]\nThus, \\[ p_Z(\\z) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma_Z|}} e^{-\\frac{1}{2} (\\z-\\mu)^T \\Sigma_Z^{-1} (\\z-\\mu)} \\]\nExternal Materials 为什么高斯分布概率密度函数的积分等于1 - 知乎 (zhihu.com) || 多元高斯分布完全解析 - 知乎 (zhihu.com)\n","date":1652036982,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652036982,"objectID":"6fb6c01d5200030f5c4ab1f8ad913498","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/","section":"notes","summary":"Gaussian Distribution One-dimensional Suppose \\(1\\)-d random variable \\(x \\sim N(\\mu, \\sigma^2)\\), then its density function is \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\] To verify that it integrates to \\(1\\),","tags":null,"title":"Gaussian Distribution","type":"book"},{"authors":null,"categories":null,"content":" KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\).\nIn information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).\nIn discrete form, \\[ D_\\text{KL}(p \\| q) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log \\frac{q(x)}{p(x)} = \\sum_{x \\in \\mathcal{X}} p(x)\\log \\frac{p(x)}{q(x)} \\] In continuous form, \\[ D_\\text{KL}(p \\| q) = -\\int_{x \\in \\mathcal{X}} p(x) \\log \\frac{q(x)}{p(x)}dx = \\int_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)}dx \\]\nVariational Lower-bound One property of KL-divergence is \\[ D_\\text{KL}(p || q) = \\sup_{T: \\Omega \\to \\R} \\E_{p} [T] - \\log (\\E_q[e^T]) \\]\nThe proof is as follows. Given a distribution \\(q\\) and a function \\(T\\), construct the Gibbs distribution \\(g\\) such that \\(g(x) = \\frac{q(x)e^{T(x)}}{Z}\\) where \\(Z = \\E_{q(x)} e^{T(x)}\\). Then \\[ \\begin{aligned} \\E\u0026amp;_{p(x)} T(x) - \\log Z = \\E_{p(x)} [T(x) - \\log Z] \\\\ \u0026amp;= \\E_{p(x)} [\\log e^{T(x)} - \\log \\E_{q(x)} e^{T(x)}] \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{e^{T(x)}} {\\E_{q(x)} e^{T(x)}} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{q(x) e^{T(x)}} {q(x) \\E_{q(x)} e^{T(x)}} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{g(x)} {q(x)} \\\\ \\end{aligned} \\] Finally KL-divergence minus above gives \\[ \\begin{aligned} \u0026amp;D_\\text{KL}(p || q) - (\\E_{p(x)} T(x) - \\log Z) \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{p(x)} {q(x)} - \\E_{p(x)} \\log \\frac{g(x)} {q(x)} \\\\ \u0026amp;= \\E_{p(x)} \\log \\frac{p(x)} {g(x)} \\triangleq D_\\text{KL}(p || g) \\ge 0 \\end{aligned} \\]\nGaussian Case Suppose \\(X\\) and \\(Y\\) are random variables, both of some \\(n\\)-dimensional Gaussian distribution. Then the KL-divergence between them can be formulated as: \\[ \\begin{aligned} D_\\text{KL}(p_X || p_Y) \u0026amp;= \\int p_X(x) \\log \\frac{p_X(x)} {p_Y(x)} \\d x = \\int p_X(x) \\log [ \\sqrt \\frac{|\\Sigma_X|}{|\\Sigma_Y|} \\frac { e^{-\\frac 1 2 (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X)} } { e^{-\\frac 1 2 (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y)} } ] \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) [ (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X) + (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y) ] \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) (x-\\mu_X)^T \\Sigma_X^{-1} (x-\\mu_X) \\d x \\\\ \u0026amp;\\quad -\\frac 1 2 \\int p_X(x) (x-\\mu_Y)^T \\Sigma_Y^{-1} (x-\\mu_Y) \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\int p_X(x) x^T \\Sigma_X^{-1} x \\d x + \\frac 1 2 \\mu_X^T \\Sigma_X^{-1} \\mu_X \\\\ \u0026amp;\\quad - \\frac 1 2 \\int p_X(x) x^T \\Sigma_Y^{-1} x \\d x + \\frac 1 2 \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y \\d x \\\\ \u0026amp;= \\frac 1 2 \\log \\frac{\\Sigma_X}{\\Sigma_Y} - \\frac 1 2 \\tr(\\Sigma_X^{-1} \\Sigma_X) - \\frac 1 2 \\mu_X \\Sigma_X^{-1} \\mu_X + \\frac 1 2 \\mu_X^T \\Sigma_X^{-1} \\mu_X \\\\ \u0026amp;\\quad - \\frac 1 2 \\tr(\\Sigma_Y^{-1} \\Sigma_X) - \\frac 1 2 \\mu_X \\Sigma_Y^{-1} \\mu_X + \\frac 1 2 \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y) + \\mu_Y^T \\Sigma_Y^{-1} \\mu_Y - \\mu_X \\Sigma_Y^{-1} \\mu_X] \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y)] \\\\ \u0026amp;\\quad + \\frac 1 2 [\\mu_Y^T \\Sigma_Y^{-1} \\mu_Y - \\mu^T_X \\Sigma_Y^{-1} \\mu_X + \\mu_X^T \\Sigma_Y^{-1} \\mu_Y - \\mu_Y^T \\Sigma_Y^{-1} \\mu_X] \\\\ \u0026amp;= \\frac 1 2 [\\log \\frac{\\Sigma_X}{\\Sigma_Y} - n - \\tr(\\Sigma_X^{-1} \\Sigma_Y) + (\\mu_Y^T - \\mu_X^T) \\Sigma_Y^{-1} (\\mu_Y - \\mu_X)] \\\\ \\end{aligned} \\]\n","date":1650904755,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650904755,"objectID":"196422bcded609c81ff72b4d45b60acf","permalink":"https://chunxy.github.io/notes/articles/information-theory/kl-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/kl-divergence/","section":"notes","summary":"KL-divergence KL-divergence, denoted as \\(D_{KL}(p\\|q)\\), is statistical distance, measuring how the probability distribution \\(q\\) is different from the reference probability distribution \\(p\\), both defined on \\(X \\in \\mathcal{X}\\).\nIn information theory, it measures the relative entropy from \\(q\\) to \\(p\\), which is the average number of extra bits required to represent a message with \\(q\\) instead of \\(p\\).","tags":null,"title":"KL-divergence","type":"book"},{"authors":null,"categories":null,"content":" If a probabilistic model contains only observable variables, maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables). Latent variables are those that you cannot observe but you know its existence and its influence in a random trial. In such case, Lagrange multipliers may be hard to apply because of the existence of the “log of sum” term.\nGiven observed samples \\(\\mathrm{X} = \\{\\x^{(1)}, \\x^{(2)}, ..., \\x^{(m)}\\}\\) (with unobservable latent variable samples \\(\\mathrm{Z}\\)), MLE tries to the find best parameters \\(\\hat \\theta\\) such that \\[ \\hat \\theta = \\arg\\max_{\\theta}\\log(p(\\mathrm{X};\\theta)) \\]\nThe log-likelihood function is \\[ \\begin{aligned} l(\\theta) \u0026amp;= \\log p(\\mathrm{X};\\theta) = \\sum_{\\x \\in \\mathrm{X}} \\log p(\\x; \\theta) \\\\ \u0026amp;= \\sum_{\\x \\in \\mathrm{X}} \\log \\E_{\\z \\sim q} \\frac{p(\\x, \\z; \\theta)}{q(\\z)} \\\\ \u0026amp;\\Downarrow_\\text{by Jensen\u0026#39;s Inequality} \\\\ \u0026amp;\\ge \\sum_{\\x \\in \\mathrm{X}} \\E_{\\z \\sim q} \\log \\frac{p(\\x, \\z; \\theta)}{q(\\z)} \\triangleq B(q, \\theta) \\\\ \u0026amp;\\text{where $q$ is an arbitrary reference probability measure} \\end{aligned} \\]\nIf we enlarge the \\(B(q, \\theta)\\), the lower bound of \\(l(\\theta)\\) can be lifted, which gives us a gentle guarantee that \\(l(\\theta)\\) may increase.\nExpectation maximization lifts this bound iteratively. To begin with, we choose a random initial estimation for \\(\\theta\\), say \\(\\theta^0\\). \\(B(q, \\theta)\\) is a function of \\((q, \\theta)\\). We can specifically set \\(q(\\z) = p(\\z | \\x; \\theta^{t})\\), where \\(\\theta^t\\) is the estimation in current iteration. Therefore, \\(B(q, \\theta)\\) becomes \\[ B(q, \\theta) = \\sum_{\\x \\in \\mathrm{X}} \\E_{\\z \\sim p(\\cdot | \\x; \\theta^{t})} \\log \\frac{p(\\x, \\z; \\theta)}{p(\\z | \\x; \\theta^{t})} \\] Because \\(\\log \\frac{1}{p(\\z | \\x; \\theta^{t})}\\) is irrelevant to the optimization of \\(\\theta\\), we can simplify the problem as maximizing \\[ Q(\\theta^t, \\theta) \\triangleq \\sum_{\\x \\in \\mathrm{X}} \\E_{\\z \\sim p(\\cdot | \\x; \\theta^{t})} \\log p(\\x, \\z; \\theta) \\label{q-function} \\] The above step is called the expectation step because we are choosing a probability measure for the expectation term in \\(B(q, \\theta)\\). The next step is the maximization step where we fix \\(\\theta^t\\) and maximize \\(Q\\)-function in \\(\\eqref{q-function}\\) w.r.t. \\(\\theta\\). This accounts for the estimation in the next iteration: \\[ \\theta^{t+1} = \\arg\\max_{\\theta} Q(\\theta^t, \\theta) \\] We do these two steps back and forth, comprising the whole expectation maximization algorithm.\nThe reason we choose \\(q(\\z) = p(\\z | \\x; \\theta^{t})\\), which is conditioned on \\(\\theta^t\\), is that we want to inject some dynamics into the algorithm. Say if we choose \\(q\\) to be some static measure not relative to \\(\\theta^t\\), the algorithm will end at the very first iteration. On the other hand, \\(p(\\z | \\x; \\theta^t)\\) is the most approachable probability measure w.r.t. \\(\\z\\) and relative to \\(\\theta^t\\). After all, in latent models, \\(p(\\z | \\x; \\theta^t)\\) is much easier to compute than \\(p(\\z; \\theta^t)\\).\n","date":1641560317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560317,"objectID":"3914b1b09eb9d4d18bfa307ad2b4b736","permalink":"https://chunxy.github.io/notes/articles/optimization/expectation-maximization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/expectation-maximization/","section":"notes","summary":"If a probabilistic model contains only observable variables, maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables).","tags":null,"title":"Expectation Maximization","type":"book"},{"authors":null,"categories":null,"content":" Eigenvectors and Eigenvalues An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\). This scalar \\(\\lambda\\) is the corresponding eigenvalue. Note that by definition, \\((\\lambda, \\vec 0)\\) is a pair of eigenvalue and eigenvector of any square matrix. However, \\(\\vec 0\\) is just too trivial an eigenvector that people exclude it from the eigen discussion.\nNote, though, \\(0\\) can be an eigenvalue. Also note that if \\((\\lambda, v)\\) is a pair of eigen of matrix \\(A\\), then \\((\\lambda, kv)\\) is also a pair of eigen.\nSimilarity If \\(A\\) and \\(B\\) are \\(n \\times n\\) matrices, then \\(A\\) is similar to \\(B\\) if there is an invertible matrix \\(P\\) such that \\(PAP^{-1} = B\\), or equivalently \\(P^{-1}BP = A\\).\nIf \\(A\\) and \\(B\\) are similar, then they have the same characteristic polynomial and hence then the same eigenvalues: \\[ B - \\lambda I = PAP^{-1} - \\lambda PP^{-1} = P(A - \\lambda I)P^{-1} \\\\ \\]\n\\[ \\begin{aligned} \u0026amp;\\det(B - \\lambda I) = \\det(P(A - \\lambda I)P^{-1}) \\\\ \u0026amp;= \\det(P) \\cdot \\det(A - \\lambda I) \\cdot \\det(P^{-1}) \\\\ \u0026amp;= \\det(A - \\lambda I) \\end{aligned} \\]\nIndependence between Eigenvectors  Theorem\nSuppose \\(v_1, v_2, ... , v_r\\) are the eigenvectors corresponding to the distinct eigenvalues \\(\\lambda_1, \\lambda_2, ..., \\lambda_r\\) of an \\(n \\times n\\) matrix \\(A\\) (\\(v_1, v_2, ... , v_r\\) are also called eigenvectors from different eigenspaces), then \\(v_1, v_2, ..., v_r\\) are linearly independent.\n Proof\nSuppose instead these vectors are dependent. Let \\(p\\) be the least index such that \\(v_{p+1}\\) is a linear combination of previous vectors. Then there exists scalars \\(c_1, c_2, ..., c_p\\) such that \\[ \\begin{equation} c_1v_1 + c_2v_2 + \\cdots + c_pv_p = v_{p+1} \\label{lincom} \\end{equation} \\] Multiply both sides with \\(A\\) to obtain \\[ \\begin{equation} c_1\\lambda_1v_1 + c_2\\lambda_2v_2 + \\cdots + c_p\\lambda_pv_p = \\lambda_{p+1}v_{p+1} \\label{eq1} \\end{equation} \\] Multiply both sides of \\(\\eqref{lincom}\\) with \\(\\lambda_{p+1}\\) to give \\[ \\begin{equation} c_1\\lambda_{p+1}v_1 + c_2\\lambda_{p+1}v_2 + \\cdots + c_p\\lambda_{p+1}v_p = \\lambda_{p+1}v_{p+1} \\label{eq2} \\end{equation} \\] Subtract \\(\\eqref{eq2}\\) from \\(\\eqref{eq1}\\) to give \\[ \\begin{equation} c_1(\\lambda_1 - \\lambda_{p+1})v_1 + c_2(\\lambda_2 - \\lambda_{p+1})v_2 + \\cdots + c_p(\\lambda_p - \\lambda_{p+1})v_p = 0 \\label{diff} \\end{equation} \\] Since \\(v_1, v_2, ..., v_p\\) are independent, the weights in \\(\\eqref{diff}\\) are all zeros. None of \\((\\lambda_i - \\lambda_{p+1})\\) is zero, so \\(c_i = 0\\) for all \\(i = 1,...p\\). Then \\(\\eqref{lincom}\\) says \\(v_{p+1}\\) is \\(0\\), which is impossible.\n Note\n\\(v_1\\), as the eigenvector, is nonzero so that the conclusion can hold even for \\(p=1\\).\n  Diagonalization  Theorem\nAn \\(n \\times n\\) matrix \\(A\\) is diagonalizable if and only if \\(A\\) has \\(n\\) linearly independent eigenvectors.\n Proof\n Necessity\nSuppose \\(A = PDP^{-1}\\), then \\(AP = PD\\). Let \\(\\newcommand{\\p}{\\mathrm{p}} P = [\\p_1, \\p_2, ..., \\p_n]\\). Since \\(D\\) is a diagonal matrix, \\[ AP = [A\\p_1, A\\p_2, ..., A\\p_n] = [D_{11} \\p_1, D_{22} \\p_2, ..., D_{nn} \\p_n] \\] Because \\(P\\) is invertible, \\(\\p_i\\)’s are linearly independent, which indicates that \\(\\mathrm p_i\\)’s are \\(A\\)’s \\(n\\) independent eigenvectors.\nFrom this, we could also see that if \\(A\\) is diagonalizable, then \\(P\\) must be the matrix concatenated with \\(A\\)’s eigenvectors and \\(D\\) must be the diagonal matrix filled with corresponding eigenvalues.\n Sufficiency\nEasy to show.\n   Not all matrices are diagonalizable. For example, the matrix \\(\\begin{bmatrix} 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \\end{bmatrix}\\) is not diagonalizable.\nThe diagonalization of a square matrix is also referred to as eigen decomposition.\nEigenvalues: Rank, Trace and Determinant Since the characteristic equation of an \\(n \\times n\\) matrix is a polynomial of degree \\(n\\), the equation always has exactly \\(n\\) roots, counting multiplicities, including complex roots. There are some relations between eigenvalues and matrix’s rank, trace and determinant: \\[ \\begin{gather} \\rank = \\text{the number of nonzero real eigenvalues, including multiplicities} \\\\ \\tr = \\text{sum of the eigenvalues} \\\\ \\det = \\text{product of the eigenvalues} \\end{gather} \\] ## Power Iteration\nWe may obtain the eigenvalues of an \\(n \\times n\\) diagonalizable matrix \\(A\\)’s by solving \\(\\det (A - \\lambda I) = 0\\). Then the corresponding eigenvectors can be solved. The order of complexity of this method is cubic.\nBut chances are that we don’t want all the eigenpairs, but instead only those with largest eigenvalues, like when we find the best rank-\\(r\\) approximation using SVD. It is an overkill to solve all eigenpairs. Luckily there is another lightweight iterative method that can help.\nBegin with an arbitrary vector \\(x_0 = x \\in \\R^n\\). The iteration rule is \\[ x_{t+1} = \\frac{A x_t}{||A x_t||} \\] Unroll \\(x_{t+1}\\) to get \\(x_t = \\frac{A^t x}{||A^t x||}\\). Because …","date":1640462913,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640462913,"objectID":"afba1d485102ac3a34eb2677daabbc0f","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/","section":"notes","summary":"Eigenvectors and Eigenvalues An eigenvector of a \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\rm x\\) such that \\(A\\rm x = \\lambda \\rm x\\) for some scalar \\(\\lambda\\).","tags":null,"title":"Eigenvectors and Eigenvalues","type":"book"},{"authors":null,"categories":null,"content":" Orthogonality and Independence If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other.\nOrthonormality An \\(m \\times n\\) matrix \\(U\\) has orthonormal columns if and only if \\(U^TU = I\\).\nAn orthogonal matrix is a square invertible matrix \\(U\\) such that \\(U^{-1} = U^T\\). By its definition, it has orthonormal columns and orthonormal rows.\nProjection Projection onto Orthogonal Basis Let \\(\\{u_1, u_2, ..., u_k\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\R^n\\). Then for each \\(y\\) in \\(W\\), the weights in the linear combination \\[ y = c_1u_1 + c_2u_2 + ... + c_ku_k \\] are given by \\[ c_i = \\frac{y \\cdot u_i}{u_i \\cdot u_i} \\label{coef} \\]\nProjection onto Vector Given a nonzero vector \\(u\\) in \\(\\R^n\\) and another vector \\(y\\) in \\(\\R^n\\), we wish to decompose \\(y\\) such that \\[ y = \\hat y + z \\] where \\(\\hat y = \\alpha u\\) for some scalar \\(\\alpha\\) and \\(z\\) is some vector orthogonal to \\(u\\). \\[ \\begin{aligned} z \u0026amp;= y - \\hat y \\\\ z \\cdot u \u0026amp;= (y - \\alpha u) \\cdot u \\\\ y \\cdot u - \\alpha u \\cdot u \u0026amp;= 0 \\\\ \\alpha \u0026amp;= \\frac{y \\cdot u}{u \\cdot u} \\end{aligned} \\] \\(\\hat y = \\frac{y \\cdot u}{u \\cdot u}u\\) is called the orthogonal projection of \\(y\\) onto \\(u\\), \\(z = y - \\hat y\\) is called the component of \\(y\\) orthogonal to \\(u\\).\nThe projection of \\(y\\) onto \\(cu\\) for any scalar \\(c\\) is the same as that onto \\(u\\). Therefore \\(\\hat y\\) is the projection onto the subspace \\(L\\) spanned by \\(u\\). In this sense, \\(\\hat y\\) is also denoted as \\(\\Pi_L(y)\\).\nProjection onto Subspace Let \\(W\\) be a linear subspace of \\(\\R^n\\), then each \\(y\\) in \\(\\R^n\\) can be uniquely written in the form: \\[ y = \\hat y + z \\] where \\(\\hat y\\) is in \\(W\\) and \\(z\\) is in \\(W^\\perp\\). \\(\\hat y\\) is called the orthogonal projection of \\(y\\) onto \\(W\\), denoted as \\(\\Pi_W(y)\\).\n\\(\\hat y\\) is also called the best approximation to \\(y\\) in \\(W\\), in the sense that: \\[ ||y - \\hat y||_2 \\le ||y - v||_2, \\forall v \\in W \\] It can be shown by \\[ y - v = (y - \\hat y) + (\\hat y - v) \\] which gives \\[ \\begin{aligned} \u0026amp;||y - v||_2^2 = ||(y - \\hat y) + (\\hat y - v)||_2^2 \\\\ \u0026amp;= ||y - \\hat y||_2^2 + ||\\hat y - v||_2^2 + 2(y - \\hat y)^T (\\hat y - v) \\\\ \u0026amp;\\Downarrow_{y - \\hat y \\in W^\\perp, \\hat y - v \\in W} \\\\ \u0026amp;= ||y - \\hat y||_2^2 + ||\\hat y - v||_2^2 \\\\ \u0026amp;\u0026gt; ||y - \\hat y||_2^2 \\end{aligned} \\]\nProjection onto Column Space When \\(A \\in \\R^{n \\times m}\\) is a matrix, for any \\(y \\in \\R^n\\) we may still want to find its projection onto the column space (also a linear subspace) spanned by \\(A\\). As mentioned above, \\(y\\) can be decomposed into \\(\\hat y\\) and \\(z\\) such that \\(\\hat y \\in \\Col A\\) and \\(z \\in (\\Col A)^\\perp = \\Nul A^T\\). Therefore, \\[ \\begin{gather} y = \\hat y + z \\label{decomp} \\\\ A^T z = 0 \\label{perp} \\\\ \\exists x \\in \\R^n, A x = \\hat y \\label{proj} \\end{gather} \\] Substitute \\(\\hat y\\) in \\(\\eqref{proj}\\) to \\(\\eqref{decomp}\\), and then substitute \\(z\\) in \\(\\eqref{decomp}\\) to \\(\\eqref{perp}\\) to give \\[ \\begin{aligned} A^T (y - A x) \u0026amp;= 0 \\\\ A^T A x \u0026amp;= A^T y \\\\ \\end{aligned} \\] Note that \\(\\rank {A^T A} = \\rank A\\). If either \\(A\\)’s columns are independent or \\(A\\) is of full rank, we can solve the above equation as \\[ x = (A^T A)^{-1} A^T y \\] Thus, \\[ \\hat y = A x = A (A^T A)^{-1} A^T y \\] For any \\(y \\in \\R^n\\), its projection onto \\(\\Col A\\) can be found by left-multiplying \\(A\\)’s projection matrix \\(P \\triangleq A (A^T A)^{-1} A^T\\). And we can verify that \\(z \\in \\Nul A^T\\): \\[ A^T z = A^T (y - \\hat y) = A^T (y - A (A^T A)^{-1} A^T y) = A^T y - A^T y = 0 \\] There are some interesting properties with this projection matrix \\(P\\):\n \\(P\\) is symmetric;\n \\(P\\) is idempotent in that \\(P^2 = P\\);\n \\(\\Col P = \\Col A\\).\n For every vector \\(x \\in \\Col A\\), by the definition of projection matrix, we have \\[ P x = x \\] which means \\(x \\in \\Col P\\) and thus \\(\\Col A \\subseteq \\Col P\\).\nFor every vector \\(y \\in \\Col P\\), there exists a vector \\(z\\) such that \\[ P z = y \\] By the definition of projection matrix, we know that \\((P z) \\in \\Col A\\) and thus \\(\\Col P \\subseteq \\Col A\\).\nTherefore, \\(\\Col P = \\Col A\\). Interestingly and as a direct result, we have \\[ A^T P = A^T \\]\n  General Projection Matrix The projection matrix derived above for a column space is actually an orthogonal projection matrix. The residual \\(z\\) of the original vector \\(y\\) after projection is perpendicular to \\(\\Col A\\) and thus to \\(\\hat y\\).\nIn a more general case, the residual is not necessarily perpendicular to \\(\\hat y\\). Note that \\(\\hat y\\) is also the closest point in \\(\\Col A\\) to \\(y\\). This instead is the definitive property of a projection, suitable for any set of vectors like linear subspace or non-convex set.\nWe can similarly develop the notion of a general projection matrix. If an \\(n \\times n\\) matrix \\(P\\) satisfies that \\(P^2 = P\\), then it is a projection matrix.\nNote that we develop the concept of orthogonal projection matrix from the orthogonal projection onto a linear subspace. But for …","date":1639995546,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639995546,"objectID":"97f37144a26b5ac4d5786f81f4626f9e","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/","section":"notes","summary":"Orthogonality and Independence If \\(\\{u_1, u_2, ..., u_k\\}\\) are orthogonal to each other, then they are independent with each other.\nOrthonormality An \\(m \\times n\\) matrix \\(U\\) has orthonormal columns if and only if \\(U^TU = I\\).","tags":null,"title":"Orthogonality and Projection","type":"book"},{"authors":null,"categories":null,"content":" \\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function. The reason for these two constraints is that we hope \\[ \\begin{gather} D_f (p||q) = 0 \\text{ when $p=q$} \\\\ \\forall p, q, D_f (p||q) \\ge 0 \\end{gather} \\] When \\(f(x) = x\\log x\\), \\(f\\)-divergence becomes KL-divergence.\nVariational \\(f\\)-divergence When \\(p\\) and \\(q\\) have no closed-form expression, it is difficult to compute the \\(f\\)-divergence. Therefore in practice \\(f\\)-divergence is computed with a variational expression: \\[ D_f (p||q) = \\sup_{T:\\mathcal X \\to \\R} \\{ \\E_p[f(x)] + \\E_q[f^* \\circ T(x)] \\} \\] where \\(f^*\\) is the convex conjugate of \\(f\\). The derivation is as follows: \\[ \\begin{aligned} \u0026amp;D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\\\ \u0026amp;= \\int f^{**}(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\\\ \u0026amp;= \\int \\sup_t [\\frac{p(x)}{q(x)} t - f^*(t)] q(x)\\ \\d x \\\\ \u0026amp;= \\int \\sup_t[p(x) t - f^*(t) q(x)]\\ \\d x \\\\ \u0026amp;\\Downarrow_{T(x) = \\arg \\sup_t[p(x) t - f^*(t) q(x)]} \\\\ \u0026amp;= \\sup_{T:\\mathcal X \\to \\R} \\int [p(x) T(x) - f^*(T(x)) q(x)]\\ \\d x \\\\ \u0026amp;= \\sup_{T:\\mathcal X \\to \\R} \\{ \\E_p[f(x)] + \\E_q[f^* \\circ T(x)] \\} \\end{aligned} \\]\n","date":1651590621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651590621,"objectID":"10edecf4e0a759b431732c1841eebef8","permalink":"https://chunxy.github.io/notes/articles/information-theory/f-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/f-divergence/","section":"notes","summary":"\\(f\\)-divergence can be treated as the generalization of the KL-divergence. It is defined as \\[ D_f (p||q) = \\int f(\\frac{p(x)}{q(x)}) q(x)\\ \\d x \\] where \\(f\\) has to satisfy that \\(f(1) = 0\\) and \\(f\\) is a convex function.","tags":null,"title":"$f$-divergence","type":"book"},{"authors":null,"categories":null,"content":" Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx + b: \\R^N \\mapsto \\R\\), where \\(w \\in \\R^N, b \\in \\R\\), that best predicts the \\(y\\) for any feature vector \\(x\\). The “best” is usually measured by the mean square error: \\[ \\mathrm{MSE}(f,\\mathcal D) = \\frac{1}{M}\\sum_{i=1}^M(y^{(i)} - f(x^{(i)}))^2 \\]\nAs a sidenote, it is very convenient to standardize the feature (so that the solution won’t depend to unit used in the measurement) and pre-center the label (so that the intercept term, or the bias term \\(b\\) can be omitted).\nOrdinary Least Squares OLS selects the linear regression parameters \\(w, b\\) that minimizes the mean squared error: \\[ w^\\star, b^\\star = \\arg \\min_{w,b}\\frac{1}{M}\\sum_{i=1}^M(y^{(i)} - w^Tx^{(i)} - b)^2 \\] This is equivalent to the below least squares problem. Let \\[ \\begin{gathered} X = \\left [ \\begin{array}{c|c} (x^{(1)})^T \u0026amp; 1 \\\\ (x^{(2)})^T \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \\\\ (x^{(M)})^T \u0026amp; 1 \\end{array} \\right ], W = \\begin{bmatrix} w \\\\ b \\\\ \\end{bmatrix}, Y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(M)} \\end{bmatrix} \\\\ \\\\ XW = Y \\end{gathered} \\] \\(Y\\) may not lie in the column space of \\(X\\). Therefore we have to approximate \\(W\\) by \\[ \\begin{aligned} X\\hat W = \\Pi_{\\Col(X)}Y \\Rightarrow Y \u0026amp;- X\\hat W \\in \\Nul(X^T) \\\\ \\Downarrow\\\\ X^T(Y - X\\hat W) \u0026amp;= 0 \\\\ X^TX\\hat W \u0026amp;= X^TY \\end{aligned} \\] Columns of \\(X\\) are independent \\(\\iff\\) \\(X^TX\\) is invertible. When \\(X^TX\\) is not invertible, there are infinite many solutions to \\(\\hat W\\). We can get one specific \\(\\hat W\\) by enforcing regularization on \\(\\hat W\\) or adding more samples when \\(M \u0026lt; N + 1\\). When \\(X^TX\\) is invertible, there is a unique solution that \\(\\hat W = (X^TX)^{-1}X^TY\\).\nThis gives the same result with minimizing the MSE: \\[ \\begin{gather} W^\\star = \\arg \\min_{W} \\mathrm{MSE}(W) = \\frac{1}{M}(Y - XW)^T(Y - XW) \\label{target} \\\\ \\Downarrow \\notag \\\\ \\notag \\\\ \\begin{aligned} \\frac{\\partial \\mathrm{MSE}}{\\partial W} \u0026amp;= \\frac{\\partial(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)}{\\partial W} \\\\ \u0026amp;= \\frac{\\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\\partial W} \\\\ \u0026amp;= -2X^TY + 2X^TXW \\\\ \\end{aligned} \\notag \\\\ \\Downarrow_{\\text{making it zero}} \\notag \\\\ \\notag \\\\ 0 = -2X^TY + 2X^TXW^\\star \\notag \\\\ X^TXW^\\star = X^TY \\notag \\end{gather} \\]\nThere are chances that \\(N\\) is too large, making equation \\(\\eqref{target}\\) much computationally expensive. In this case, we can use gradient descent. The update rule will be \\[ \\begin{aligned} W^{(t+1)} \u0026amp;= W^{(t)} - \\frac{\\eta}{2} \\nabla \\mathrm{MSE}(W^{(t)}) \\\\ \u0026amp;= W^{(t)} - \\eta(X^TXW^{(t)} -X^TY) \\end{aligned} \\]\nA Probabilistic View Maximum Likelihood Estimation In classification task, \\(x\\) is the feature, \\(y\\) is the label; in regression task, \\(x\\) is the “label”, \\(y\\) is the “feature”. From a probabilistic point of view, we would like estimate \\(p(\\text{feature}|\\text{label})\\). In this case, we treat \\(y\\) as the “feature” composed of a deterministic function and a noise sampled from an identical and independent Gaussian distribution, i.e., for random variable \\(\\mathcal X, \\mathcal Y\\) (to distinguish from matrix \\(X\\) and \\(Y\\)), \\[ \\mathcal Y = \\mathcal XW + \\epsilon, \\text{ where }\\epsilon \\sim \\mathcal N(0, \\sigma^2) \\] Thus, \\[ p(Y|X;W,\\sigma^2) = p(\\epsilon=Y - XW;\\sigma^2) = \\mathcal N(Y - XW;0,\\sigma^2I) \\] Then OLS can be attacked by maximum likelihood estimation. The log-likelihood function will be\n\\[ \\begin{aligned} l(W,\\sigma^2) \u0026amp;= \\log(L(W, \\sigma^2)) = \\log(p(Y|X;W,\\sigma^2)) = \\log \\mathcal N(Y - XW;0,\\sigma^2I) \\\\ \u0026amp;= \\log(\\frac{1}{\\sqrt{(2\\pi)^M|\\sigma^2I|}}e^{-\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW)}) \\\\ \u0026amp;= -\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) - \\frac{M}{2}\\log \\sigma^2 - \\frac{M}{2}\\log 2\\pi \\end{aligned} \\]\n\\[ \\begin{aligned} \\arg \\max_{W,\\sigma^2}l(W, \\sigma^2) \u0026amp;= \\arg \\max_{W,\\sigma^2}-\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) - \\frac{M}{2}\\log \\sigma^2 - \\frac{M}{2}\\log 2\\pi \\\\ \u0026amp;= \\arg \\min_{W,\\sigma^2}\\frac{1}{2\\sigma^2}(Y - XW)^T(Y - XW) + \\frac{M}{2}\\log \\sigma^2 \\\\ \\end{aligned} \\]\nTake derivative w.r.t. \\(W\\) and make it \\(0\\) to give \\(W^\\star\\): \\[ \\begin{gathered} \\frac{1}{\\sigma^2}(X^TXW^\\star - X^TY) = 0 \\\\ X^TXW^\\star = X^TY \\end{gathered} \\] Substitute \\(W^\\star\\) back, take derivative w.r.t. \\(\\sigma^2\\) and make it \\(0\\) to give \\(\\sigma^{\\star2}\\): \\[ \\begin{gathered} \\frac{M}{2\\sigma^{\\star2}} - \\frac{(Y - XW^\\star)^T(Y - XW^\\star)}{2(\\sigma^{\\star2})^2} = 0 \\\\ \\sigma^{\\star2} = \\frac{1}{M}(Y - XW^\\star)^T(Y - XW^\\star) \\end{gathered} \\]\nWe may further analyze the efficacy of \\(W^\\star\\) as a point estimator. Suppose \\(X^T X\\) is invertible. We have \\[ \\begin{aligned} \u0026amp;W^\\star = (X^T X)^{-1} X^T Y \\\\ \u0026amp;= (X^T X)^{-1} X^T (X W_\\text{real} + Z) \\\\ \u0026amp;= W_\\text{real} + (X^T X)^{-1} X^T Z \\end{aligned} \\] where \\(Z\\) contains the noise term for each sample. \\(W^\\star\\) is unbiased because \\[ \\begin{aligned} …","date":1642195157,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642195157,"objectID":"f00bd3e187a404025f460c7e1969ba18","permalink":"https://chunxy.github.io/notes/articles/machine-learning/linear-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/linear-regression/","section":"notes","summary":"Given a dataset \\(\\mathcal D = \\{(x^{(i)}, y^{(i)}),i=1,\\dots,M\\}\\) where \\(x^{(i)} \\in \\R^N\\) is the feature vector and \\(y^{(i)} \\in R\\) is the output, learn a linear function \\(f = w^Tx","tags":null,"title":"Linear Regression","type":"book"},{"authors":null,"categories":null,"content":" Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\} \\] Or put it another way, the subgradient defines a hyper-plane passing through \\((x,f(x))\\): $$ ^T ( - ) = 0,\ny ^n, t \\ \\[ And \\] ^T ( - ) , (y, t) epi(f) \\[ This is because by definition, \\] \\[\\begin{aligned} g^T(y - x) - (f(y) - f(x)) \u0026amp;\\le 0, \\forall y \u0026amp;\\Rightarrow \\\\ g^T(y - x) - (t - f(x)) \u0026amp;\\le 0, \\forall y,\\forall t \\ge f(y) \u0026amp;\\Rightarrow \\\\ \\left[ \\begin{array} \\\\ g \\\\ -1 \\\\ \\end{array} \\right]^T \\Big( \\left[ \\begin{array} \\\\ y \\\\ t \\\\ \\end{array} \\right] - \\left[ \\begin{array} \\\\ x \\\\ f(x) \\\\ \\end{array} \\right] \\Big) \u0026amp;\\le 0, \\forall (y, t) \\in epi(f) \\end{aligned}\\] $$\nwhere \\(\\left[\\begin{array}\\\\ g \\\\ -1 \\\\\\end{array}\\right]\\) is the normal of the tangent plane of \\(f\\) at \\(x\\).\nProperties The subgradient \\(\\partial f(x)\\) is a set, instead of a single number like the gradient.\n Subgradient is a monotonic operator: \\[ (u - v)^T(y - x) \\ge 0, \\forall u \\in \\partial f(y), \\forall v \\in \\partial f(x) \\] This can be shown by: \\[ \\begin{aligned} \\left. \\begin{array} \\\\ f(y) \\ge f(x) + v^T(y - x) \\\\ f(x) \\ge f(y) + u^T(x - y) \\\\ \\end{array} \\right\\} \u0026amp;\\Rightarrow f(y) + f(x) \\ge f(x) + f(y) - (u - v)^T(y - x) \\\\ \u0026amp;\\Rightarrow (u - v)^T(y - x) \\ge 0 \\end{aligned} \\]\n \\(x^\\star = \\arg \\min_x f(x) \\iff 0 \\in \\partial f(x^\\star)\\).\n If \\(f\\) is differentiable at \\(x\\), then \\(\\partial f(x) = \\{\\nabla f(x)\\}\\).\nSuppose \\(p \\ne \\nabla f(x) \\and p \\in \\partial f(x)\\), then for \\(y = x + r(p - \\nabla f(x))\\), we have \\[ \\begin{aligned} f(y) - f(x) \u0026amp;\\ge p^T(y - x) \\\\ f(x + r(p - \\nabla f(x))) - f(x) \u0026amp;\\ge rp^T(p - \\nabla f(x)) \\\\ \\frac{f(x + r(p - \\nabla f(x))) - f(x)}{r} \u0026amp;\\ge p^T(p - \\nabla f(x)) \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} (p - \\nabla f(x))^T(p - \\nabla f(x)) \u0026amp;\\le \\frac{f(x + r(p - \\nabla f(x))) - f(x)}{r} - \\nabla f(x)^T(p - \\nabla f(x)) \\\\ ||p - \\nabla f(x)||^2_2 \u0026amp;\\le \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\\\ \\end{aligned} \\]\nTake limit on \\(r\\) on both sides to give \\[ \\lim_{r \\to 0}||p - \\nabla f(x)||^2_2 \\le \\lim_{r \\to 0}\\Big( \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\Big) \\\\ \\] By the definition of gradient, \\[ \\begin{aligned} f\u0026amp;(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x)) \\\\ \u0026amp;= \\mathcal{O}(||r(p - \\nabla f(x))||^2_2) \\\\ \u0026amp;= ||p - \\nabla f(x)||^2\\mathcal{O}(r^2) \\\\ \u0026amp;= \\mathcal{O}(r^2) \\end{aligned} \\]\n\\[ \\lim_{r \\to 0}\\Big( \\frac{f(x + r(p - \\nabla f(x))) - f(x)- \\nabla f(x)^Tr(p - \\nabla f(x))}{r} \\Big) = \\lim_{r \\to 0}\\frac{\\mathcal{O}(r^2)}{r} = 0 \\]\nTherefore, \\[ \\begin{gather} ||p - \\nabla f(x)||^2_2 \\le 0 \\\\ 0 \\le ||p - \\nabla f(x)||^2_2 \\le 0 \\\\ ||p - \\nabla f(x)||^2_2 = 0 \\end{gather} \\] contradicting the assumption that \\(p \\ne \\nabla f(x)\\). Thus, \\(p = \\nabla f(x)\\).\n If \\(f(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x)\\), then \\(\\partial f(x) = \\alpha_1 \\partial f_1(x) + \\alpha_2 f_2(x)\\).\n  凸优化笔记16：次梯度 - 知乎 (zhihu.com)\n","date":1641917363,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641917363,"objectID":"d857b71bbd69f8637fc9c93ee18ab77e","permalink":"https://chunxy.github.io/notes/articles/optimization/subgradient/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/subgradient/","section":"notes","summary":"Definition Subgradient of a function \\(f: \\R^n \\mapsto \\R\\) at \\(x\\) is defined as \\[ \\partial f(x) = \\{g|f(y) \\ge f(x) + g^T(y - x), \\forall y \\in dom(f) \\}","tags":null,"title":"Subgradient","type":"book"},{"authors":null,"categories":null,"content":" The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space.\nPolynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\). In higher-dimensional input feature space, there are many more entries for feature mapping in each degree. Take a 2-D input for example.\n degree 1: \\([x_1, x_2]^T\\) degree 2: \\([x_1^2, x_1x_2, x_2^2]^T\\) degree 3: \\([x_1^3, x_1^2x_2, x_1x_2^2, x_2^3]^T\\) …  Kernel Trick in Non-linear Regression Many Linear Regression algorithms learning algorithms depend on the calculation of inner products between feature vectors, either during training or in prediction, without directly depending on the feature vector. We can transform the feature vector by applying a feature mapping \\(\\phi\\) to do the Non-linear Regression task.\nHowever, it is not necessary to explicitly define the feature mapping \\(\\phi\\) when only the inner products are needed. Instead, we can define a function \\(\\mathcal K: \\R^N \\times \\R^N \\mapsto \\R\\) that directly calculate the inner products of pseudo-transformed features: \\[ \\mathcal K(x,x^\\prime) = \\phi_{pseudo}(x)^T\\phi_{pseudo}(x^\\prime) \\] This will be more flexible and computationally-efficient.\nKernel Ridge Regression Recall Ridge Regression: \\[ W^\\star = \\min_{W}\\frac{1}{2}||Y - X^TW||^2_2 + \\frac{\\alpha}{2}||W||^2_2 \\iff (XX^T + \\alpha I_{N+1})W^\\star = XY \\] If \\((XX^T + \\alpha I_{N+1})\\) is invertible, \\(W^\\star = (XX^T + \\alpha I_{N+1})^{-1}XY\\). By matrix identity \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] we can obtain that \\(W^\\star = X(X^TX + \\alpha I_M)^{-1}Y\\). \\(X \\in \\R^{(N+1)\\times M}, (X^TX + \\alpha I_M)^{-1}Y \\in \\R^{M}\\). \\(W^\\star\\) can be rewritten as a linear combination of columns of \\(X\\), i.e. \\(W^\\star\\) lies in the span of input vectors: \\[ W^\\star = \\sum_{i=1}^M\\lambda_ix^{(i)} \\] For a new data \\(x^*\\), we make prediction by \\[ y^* = (x^*)^TW = (x^*)^TX(X^TX + \\alpha I_M)^{-1}Y \\] which totally depends on the inner products.\nSupport Vector Regression Support Vector Regression learns a hyper-plane that incorporates within its margin as many points as possible. As a comparison, Support Vector Machine learns a hyper-plane that excludes outside its margin as many points as possible. Its objective is \\[ \\begin{aligned} \\min_{w,\\xi,\\xi^*} \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ s.t.\\quad y^{(i)} - w^Tx^{(i)} - b \\le \\epsilon + \\xi_i, i=1,\\dots,M \\\\ y^{(i)} - w^Tx^{(i)} - b \\ge \\epsilon + \\xi^*_i, i=1,\\dots,M \\\\ \\xi_i, \\xi^*_i \\ge 0, i=1,\\dots,M \\end{aligned} \\] \\(\\epsilon\\) is a hyper-parameter to be determined. Transform the problem into the standard optimization form: \\[ \\begin{aligned} \\min_{w,\\xi,\\xi^*} \\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ s.t.\\quad y^{(i)} - w^Tx^{(i)} - b - \\epsilon - \\xi_i \\le 0, i=1,\\dots,M \\\\ w^Tx^{(i)} + b - y^{(i)} + \\epsilon + \\xi^*_i \\le 0, i=1,\\dots,M \\\\ -\\xi_i, -\\xi^*_i \\ge 0, i=1,\\dots,M \\end{aligned} \\] The Lagrangian function will be \\[ \\begin{aligned} L(w,\\xi,\\xi^*,\\lambda,\\mu,\\nu,\\nu^*) = \u0026amp;\\frac{1}{2}||w||^2_2 + C\\sum_{i=1}^M(\\xi_i + \\xi^*_i) \\\\ \u0026amp;+ \\sum_{i=1}^M\\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \\epsilon - \\xi_i) \\\\ \u0026amp;+ \\sum_{i=1}^M\\mu_i(w^Tx^{(i)} + b - y^{(i)} + \\epsilon + \\xi^*_i) \\\\ \u0026amp;- \\sum_{i=1}^M\\nu_i\\xi_i -\\sum_{i=1}^M\\nu^*_i\\xi^*_i \\end{aligned} \\]\nExternal Materials Support Vector Regression.pdf\n","date":1641559580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559580,"objectID":"a5f00276ecb2f14744a20fff20ed1b78","permalink":"https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/non-linear-regression/","section":"notes","summary":"The basic idea about non-linear regression is to perform the feature mapping \\(\\phi(x)\\), followed by linear regression in the new feature space.\nPolynomial Regression When \\(x\\) is a scalar, the feature mapping in \\(p\\)-th order Polynomial Regression is \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} \\] The regression function and the parameters are then like those in linear regression: \\[ f(x) = [w_0,w_1,w_2,\\dots,w^p]\\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^p \\end{bmatrix} = w^T\\phi(x) \\] \\(\\phi(x)\\) captures all features in each degree from \\(1\\) up to \\(p\\).","tags":null,"title":"Non-linear Regression","type":"book"},{"authors":null,"categories":null,"content":" 离散型 二项分布（binomial distribution） 如果离散型随机变量\\(X\\)服从二项分布，一般记作\\(X \\sim B(n, p)\\)。 \\[ \\begin{gathered} B(x;n,p) = {n \\choose x} p^x (1-p)^{n-x}, x = 0,1,\\dots \\\\ \\E[X] = np \\\\ \\Var[X] = np(1-p) \\end{gathered} \\]\n二项分布可以帮助纠正一个生活中很常见的谬误，比如说身高高于两米的人占人类总体的\\(1\\%\\)，那么是否说明随机选取的100个人中一定至少有1个人高于两米呢？记\\(X\\)为100个人中身高高于两米的人数，显然\\(X \\sim B(100, 0.01)\\)，经计算可得\\(P(X=0) \\approx 0.366\\)。其实也就意味着，100个人中，能至少看到1个身高高于两米的人的概率其实大约是\\(1-0.366 = 63.4\\%\\)。\n泊松分布（Poisson distribution） 泊松分布产生于\\(X\\)用来表示在一定时间或空间内出现的事件个数的场景中。泊松分布有一些基本假设，设观察的这一单位时间或空间为\\([0, 1)\\)，取一个很大的自然数\\(n\\)，将\\([0,1)\\)平分为\\(n\\)段窗口：\\(l_1 = [0, \\frac{1}{n}), l_2 = [\\frac{1}{n}, \\frac{2}{n}), \\dots, l_n = [\\frac{n-1}{n}, 1)\\)，则：\n在每段\\(l_i\\)内，恰发生一个事件的概率正比于这段的长度\\(\\frac{1}{n}\\)，即可取为\\(\\frac{\\lambda}{n}\\)；又假定\\(n\\)很大故\\(\\frac{1}{n}\\)很小时，不可能发生两次以上事件； \\(l_1, l_2, \\dots, l_n\\)中是否发生时间是相互独立的；  这样的基本假设下，单位窗口内发生事件的总数记为随机变量\\(X\\)。此时\\(X\\)应当服从二项分布，而当\\(n \\to \\infty\\)时，\\(X\\)则服从泊松分布，故泊松分布也可以看作是某种形式的二项分布取极限而得到： \\[ P(X = i; \\lambda) = \\lim_{n \\to \\infty} {n \\choose i} (\\frac{\\lambda}{n})^i (1 - \\frac{\\lambda}{n})^{n-i} \\] 将\\(\\lim_{n \\to \\infty} {n \\choose i} / n^i = 1 / i!\\)、\\(\\lim_{n \\to \\infty} (1 - \\frac{\\lambda}{n})^{n-i} = e^{-\\lambda}\\)代入即可得到泊松分布的分布律。\n一般如果\\(X \\sim B(n,p)\\)且\\(n\\)较大、\\(p\\)较小、\\(np = \\lambda\\)不太大时，\\(X\\)的分布接近于泊松分布\\(P(\\lambda)\\)。 \\[ \\begin{gathered} P(x;\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!}, x = 0,1,\\dots \\\\ \\E[X] = \\lambda \\\\ \\Var[X] = \\lambda \\end{gathered} \\]\n伯努利分布（Bernoulli distribution） 伯努利分布\\(B(1, p)\\)实际上是二项分布中\\(n = 1\\)的一个特例： \\[ \\begin{gathered} B(1;1,p) = p, B(0;1,p) = 1 - p \\\\ \\E[X] = p \\\\ \\Var[X] = p(1 - p) \\end{gathered} \\]\n多项分布（multinomial distribution） 多项分布其实就是二项分布的推广，不像二项分布，多项分布的取值的是多值的而不是二值的（binary）。假设有\\(k\\)种结果，且这\\(k\\)种结果互相对立、完备穷举（mutually exclusive and collectively exhaustive），此时它们的概率之和为\\(1\\)，即\\(p_1 + \\dots + p_k = 1\\)，多项分布计算的则是这\\(k\\)种结果分别发生\\(n_1, \\dots, n_k\\)次时的概率。令\\(N = n_1 + \\dots + n_k, \\vec p = [p_1, \\dots, p_k], \\vec n = [n_1, \\dots, n_k]\\)，则： \\[ P(\\vec n; \\vec p) = \\frac{N!}{n_1! \\dots n_k!} p_1^{n_1} \\dots p_k^{n_k} \\]\n多项分布可以拓展到连续情况，此时\\(n_1, \\dots, n_k \\in \\R_+\\)，而概率质量函数变为 \\[ p(\\vec n; \\vec p) = \\frac{\\Gamma(N + 1)}{\\Gamma(n_1 + 1) \\dots \\Gamma(n_k + 1)} p_1^{n_1} \\dots p_k^{n_k} \\] 连续情况下的多项分布也是sklearn中能将TFIDF特征应用到MultinomialNB的基本原理。\n分类分布（categorical distribution） 类似伯努利分布是二项分布\\(n=1\\)时的特例，分类分布则是多项分布\\(N=1\\)时的特例： \\[ \\begin{gathered} P(\\vec n; \\vec p, 1) = \\prod_{i=1}^k p_i ^{n_i} \\\\ \\E[X] = \\vec p \\\\ \\Var[X] = \\vec p (1 - \\vec p) \\end{gathered} \\]\n连续型 指数分布（exponential distribution） 指数分布最常见的一个场景是寿命估计。设想一种大批生产的电器元件，其元件寿命\\(X\\)是随机变量，在“无老化”的假定下——即“若元件在时刻\\(x\\)尚正常工作，则其失效率总为某个与\\(x\\)无关的常数\\(\\lambda \u0026gt; 0\\)”，那么\\(X\\)服从参数为\\(\\lambda\\)的指数分布。\n上述假设用概率语言描述则是 \\[ \\lim_{h \\to 0} P(x \\le X \\le x+h | X \u0026gt; x) / h = \\lambda \\] 注意到 \\[ P(x \\le X \\le x+h | X \u0026gt; x) = \\frac{P(\\{ x \\le X \\le x+h \\} \\cap \\{ X \u0026gt; x \\})}{P(X \u0026gt; x)} = \\frac{P(x \u0026lt; X \\le x+h)}{P(X \u0026gt; x)} \\] 所以 \\[ \\begin{aligned} \\lim_{h \\to 0} \\frac{P(x \u0026lt; X \\le x+h)}{h P(x \u0026lt; X))} \u0026amp;= \\lambda \\\\ \\lim_{h \\to 0} \\frac{F(x + h) - F(x)}{h(1 - F(x))} \u0026amp;= \\lambda \\\\ \\frac{F\u0026#39;(x)}{1 - F(x)} \u0026amp;= \\lambda \\end{aligned} \\] 上述微分方程的通解为\\(F(x) = 1 - Ce^{-\\lambda x}\\)，而\\(F(0) = 0\\)，故\\(C = 1\\)。 \\[ \\begin{gathered} p(x;\\lambda) = \\begin{cases} \\lambda e^{-\\lambda x}, \u0026amp; x \u0026gt; 0 \\\\ 0, \u0026amp; x \\le 0 \\end{cases} \\\\ \\E[X] = \\lambda^{-1} \\\\ \\Var[X] = \\lambda^{-2} \\end{gathered} \\]\n正态分布（normal distribution） 正态分布也叫作高斯分布（Gaussian distribution），一维情况下： \\[ p(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] 二维情况下： \\[ p \\Big( (x,y); \\mu_X, \\mu_Y, \\sigma_X, \\sigma_Y, \\sigma_{XY} \\Big) = \\frac{1}{2\\pi \\sqrt{(\\sigma_X^2 \\sigma_Y^2 - \\sigma_{XY}^2)}} e^{-\\frac 1 {2(1 - \\sigma_{XY}^2)} \\left(\\frac{(x-\\mu_X)^2} {\\sigma_X^2} - \\frac{2\\sigma_{XY}(x - \\mu_X)(y - \\mu_Y)} {\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2} {\\sigma_Y^2} \\right)} \\] \\(n\\)维情况下： \\[ p(\\x; \\mu, \\Sigma) = \\frac{1}{\\sqrt{|2\\pi \\Sigma|}} e^{-\\frac{1}{2} (\\x-\\mu)^T \\Sigma^{-1} (\\x-\\mu)} \\]\n","date":1660237921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660237921,"objectID":"b2463e4efacb753026851bc581923c69","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/","section":"notes","summary":"离散型 二项分布（binomial distribution） 如","tags":null,"title":"常见分布","type":"book"},{"authors":null,"categories":null,"content":" Big Picture The clustering algorithms can be broadly split into two categories depending on whether the number of clusters is given or to be determined by user. Partitional ones pre-set the number of clusters; while hierarchical ones output a dendrogram that illustrates how clusters are built level by level. Users are free to choose the level of clustering in this hierarchical clustering.\n Hierarchical algorithms\n Bottom-up agglomerative clustering\nThis approach starts with each object in a separate cluster, and repeatedly\n joins the most similar pair of clusters, update the similarity of the new cluster to others until there is only one cluster.  There are five further methods in this approach. The difference among them lies in the way to measure inter-cluster similarity.\n Single-linkage measures the similarity between two clusters as the distance between their closest members. Complete-linkage measures the similarity between two clusters as the distance between their furthest members. Average-linkage measures the similarity between two clusters as the average of distances of all the cross-cluster pairs. Centroid measures the similarity between two clusters as the distance between their centers. DBSCAN  Top-down divisive clustering\nThis approach starts with all the data in a single cluster, and repeatedly split each cluster into two using a partition algorithm until each object is in a separate cluster.\n  Partition algorithms\n K-means Gaussian mixture model   Algorithms Given dataset \\(\\mathcal D = \\{x^{(i)}, i=1,\\dots,M\\}\\), a clustering \\(\\mathcal C\\) of the \\(M\\) points into \\(K (K \\le M)\\) clusters is a partition of \\(\\mathcal D\\) into \\(K\\) disjoint groups \\(\\{C_1,\\dots,C_K\\}\\). Suppose we have a function \\(f\\) that evaluates the clustering \\(\\mathcal C\\) and returns lower score with better clustering. The best clustering will be \\[ \\arg \\min_{\\mathcal C} f(\\mathcal C) \\] The number of all possibilities of clustering with \\(M\\) elements is called the Bell number, denoted as \\(B_M\\). The calculation of the Bell number is based on dynamic programming. The number of ways to cluster \\(M+1\\) elements is the sum of number of ways to:\n select \\(1\\) element and cluster it, with the rest belong to one single cluster select \\(2\\) elements and cluster them, with the rest belong to one single cluster … select \\(M\\) elements and cluster them, with the rest belong to one single cluster  Therefore, \\[ \\begin{gather} B_{M+1} = \\sum_{i=1}^M\\binom{M}{0}B_i \\\\ B_0 = 1 \\end{gather} \\] The exhaustive method will be computationally intractable. We need either an approximation algorithm or a scoring function with special properties.\nK-means K-means assumes there are \\(K\\) clusters. This greatly eliminates many possibilities described above. Its objective is \\[ \\min_{c_1,\\dots,c_K}\\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \\text{ where }z^{(i)} = \\arg \\min_{j \\in \\{1,\\dots,K\\}}||x^{(i)}-c_j||^2_2 \\] \\(z^{(i)}\\) is the cluster index to which \\(x^{(i)}\\) is assigned. K-means’ objective is to assign each point to its closest cluster center and minimize the total within-cluster square errors. For cluster \\(j\\), let \\(C_j = \\{x^{(i)}|z^{(i)} = j\\}\\) be the set of points assigned to it, then the cluster center of cluster \\(j\\) is \\[ c_j = \\frac{1}{|C_j|}\\sum_{x^{(i)} \\in C_j}x^{(i)} \\] However, both the cluster center \\(\\newcommand{\\c}{\\mathrm{c}} \\c\\) and the assignment \\(\\newcommand{\\z}{\\mathrm{z}} \\z\\) is initially unknown. K-means solves this by randomly pick up initial cluster centers and enter the assign-data-to-clusters/update-cluster-centers loop, until the cluster centers converge or become satisfactory. Rewrite the objective of K-means as:\n\\[ \\min_{\\z,\\c}(l(\\z,\\c) \\coloneq \\sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2) \\] K-means is in essence a coordinate descent of the loss \\(l(\\z,\\c)\\). The main loop of K-means is to:\n assign data points to its nearest cluster center, i.e. minimizing over the assignment \\(\\z\\). update cluster centers according to the points assigned to, i.e. minimizing over the centroids \\(\\c\\).  \\(l\\) is monotonically decreasing after each step in the above loop. Also, \\(l\\) is lower-bounded by \\(0\\). Therefore, \\(l\\) and thus K-means will converge finally.\nEach cluster in K-means has a circular shape because of the Euclidean distance it uses.\nFor more discussion and an interesting image compression method with K-means, please refer here.\nGaussian Mixture Model A cluster can also be modelled by a multi-variate Gaussian with elliptical shape: the elliptical shape is controlled by the covariance matrix; the location is controlled by the mean. Gaussian mixture model is a weighted sum of, say \\(K\\), Gaussian distributions: \\[ p(x) = \\sum_{j=1}^K\\pi_j\\mathcal N(x;\\mu_j, \\Sigma_j) \\] \\(\\pi_j\\) is the prior that a sample is generated from the \\(j\\)-th Gaussian. \\(\\mathcal N(x;\\mu_j, \\Sigma_j)\\) is the probability to generate the sample \\(x\\) from the \\(j\\)-th Gaussian. Put it together, \\(p(x)\\) is the total …","date":1649514723,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649514723,"objectID":"13bbf8789e9800de592b8a49a83f52bb","permalink":"https://chunxy.github.io/notes/articles/machine-learning/clustering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/clustering/","section":"notes","summary":"Big Picture The clustering algorithms can be broadly split into two categories depending on whether the number of clusters is given or to be determined by user. Partitional ones pre-set the number of clusters; while hierarchical ones output a dendrogram that illustrates how clusters are built level by level.","tags":null,"title":"Clustering","type":"book"},{"authors":null,"categories":null,"content":" Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\}\\) initially.\n Select among \\(C\\) a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Then for each \\(\\mathrm x^{(i)}\\) from left to right, do the following:\n  \\[ \\begin{gathered} \\hat Y = \\frac{Y \\cdot \\mathrm x^{(k)}}{||\\mathrm x ^{(k)}||_2} \\mathrm x ^{(k)} \\\\ Y_{res} = Y_{res} - \\hat Y \\end{gathered} \\]\nRemove \\(\\mathrm x^{(k)}\\) from \\(C\\). Go to step 2 until \\(Y_{res}\\) reaches \\(0\\) or we have used all columns in \\(C\\).  Apparently this algorithm is efficient. However, it does not guarantee an exact solution, even when \\(Y\\) lies in the column space of \\(X\\).\nForward Stagewise Like forward selection, forward stagewise selects a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Unlike Forward Selection, Forward Selection does not subtract the whole projection from \\(Y_{res}\\). Instead, it proceeds along \\(\\mathrm x^{(k)}\\) with a small step.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\},\\eta\\text{ is a small constant}\\) initially.\n Select among \\(C\\) a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). Then for each \\(\\mathrm x^{(i)}\\) from left to right, do the following:\n  \\[ \\begin{gathered} \\hat Y = \\eta \\frac{Y \\cdot \\mathrm x^{(k)}}{||\\mathrm x ^{(k)}||_2} \\mathrm x ^{(k)} \\\\ Y_{res} = Y_{res} - \\hat Y \\end{gathered} \\]\nRemove \\(\\mathrm x^{(k)}\\) from \\(C\\). Go to step 2 until \\(Y_{res}\\) is sufficiently small.  Apparently forward stagewise gives an exact solution when \\(\\eta\\) is small enough. But it is more time-consuming.\nLeast Angle Regression LARS a is a compromise between forward selection and forward stagewise. Likewise, LARS selects a \\(\\mathrm x^{(k)}\\) such that \\(\\mathrm x^{(k)}\\) has the largest cosine distance to \\(Y_{res}\\). LARS proceeds stagewise like forward stagewise, however with its own methodology to determine the step size.\n\\(Y_{res} = Y, C = \\{\\mathrm x^{(1)}, \\mathrm x^{(2)}, \\dots, \\mathrm x^{(N)}\\}, d = \\arg\\max_{\\mathrm x \\in C}\\frac{Y \\cdot \\mathrm x }{||\\mathrm x||_2}\\) initially.\n Then do the following: \\[ \\begin{gathered} \\hat Y = \\eta \\frac{Y \\cdot d}{||d||_2} d \\\\ Y_{res} = Y_{res} - \\hat Y \\end{gathered} \\] \\(\\eta\\) is determined such that there is some \\(\\mathrm x^{(k)} \\in C\\) onto which the projection of \\((Y - \\hat Y)\\) is the same as that onto \\(d\\). Or in other words, \\((Y - \\hat Y)\\) lies on the bisector hyper-plane between \\(\\mathrm x^{(k)}\\) and \\(d\\). Let \\(d\\) be along this bisector.\n Go to step 2 until \\(Y_{res}\\) is sufficiently small.\n  ","date":1639926598,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639926598,"objectID":"2fa379b4f344d23f6edb9cb8b878ce7d","permalink":"https://chunxy.github.io/notes/articles/optimization/least-angle-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/least-angle-regression/","section":"notes","summary":"Forward Selection In cases where we are to solve \\(W\\) so that \\(Y = XW\\), where \\(Y \\in \\R^M, X \\in \\R^{M \\times N}, W \\in \\R^N\\), we can do it in an iterative and greedy way.","tags":null,"title":"Least Angle Regression","type":"book"},{"authors":null,"categories":null,"content":" Unsupervised Dimension Reduction Dimensionality reduction\n reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity.  Compared to feature selection,\n the goal of feature selection is to remove features that are not informative with respect to the class label. This obviously reduces the dimensionality of the feature space; dimensionality reduction can be used to find a meaningful lower-dim feature space even when there is information in each feature dimension so that none can be discarded; dimensionality reduction is unsupervised while feature selection is supervised.  Compared to data compression,\n dimensionality reduction can be seen as a simplistic form of data compression. But they are not equivalent, as the goal of data compression is to reduce the entropy of the representation, which is not limited to the dimensionality reduction.  Linear Dimensionality Reduction Linear dimensionality reduction projects data onto lower-dimensional space by representing the data with a new basis consisting of some major components. Mathematically, \\[ \\begin{gather} x^{(i)} = \\sum_{k=1}^Kz^{(i)}_kb^{(k)}, \\text{ where $z^{(i)} \\in \\R^K$ is the weight, $b^{(k)} \\in \\R^N$ is the basis vector.} \\\\ X = BZ, \\text{ where $X = [x^{(i)}, \\dots, x^{(M)}], B = [b^{(1)}, \\dots, b^{(K)}], Z = [z^{(1)}, \\dots, z^{(M)}]$} \\end{gather} \\] The objective can be set to minimize the error when recovering from \\(B, Z\\) to \\(X\\), i.e. \\[ \\min_{B,Z} = ||X - BZ||^2_F = \\sum_{ij}(X - BZ)^2_{ij} \\]\nAlternating Least Squares By leveraging the idea of coordinate descent and OLS solution to linear regression, we can optimize \\(B\\) and \\(Z\\) alternatively, until convergence.\nFix \\(Z\\), take derivative w.r.t. \\(B\\) and make it zero to give \\[ \\begin{gather} 2(X - BZ)(-Z^T) = 0 \\\\ BZZ^T = XZ^T \\\\ B = XZ^T(ZZ^T)^{-1} \\end{gather} \\] Fix \\(B\\), take derivative w.r.t. \\(Z^T\\) and make it zero to give \\[ \\begin{gather} \\frac{\\partial||X - BZ||^2_F}{\\partial Z^T} = \\frac{\\partial||X^T - Z^TB^T||^2_F}{\\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\\\ 2(X^T - Z^TB^T)(-B) = 0 \\\\ Z^TB^TB = X^TB \\\\ Z^T = X^TB(B^TB)^{-1} \\\\ Z = (B^TB)^{-1}B^TX \\end{gather} \\] Assume we have run the ALS to convergence and obtain the global optimal parameters \\[ B^\\star, Z^\\star = \\arg \\min_{B,Z} = ||X - BZ||^2_F = \\sum_{ij}(X - BZ)^2_{ij} \\] Let any invertible matrix \\(R \\in \\R^{K \\times K}\\). We can construct a pair of \\(\\tilde B, \\tilde Z\\) such that \\(\\tilde B = B^\\star R, \\tilde Z = R^{-1}Z^\\star\\). Then, \\[ ||X - \\tilde B \\tilde Z||^2_F = ||X - B^\\star RR^{-1} Z^\\star||^2_F = ||X - B^\\star Z^\\star||^2_F \\] Thus the global optima is not unique. We may force regularization on \\(B, Z\\) and obtain the unique optima.\nPrincipal Component Analysis Suppose the SVD for \\(X\\) is \\(X = U\\Sigma V^T\\), where \\[ \\begin{gather} \\text{$U$ is $N \\times N$, $\\Sigma$ is diagonal, $V$ is $M \\times M$} \\\\ UU^T = I \\\\ VV^T = I \\\\ \\Sigma = diag_{N \\times M}(\\sigma_1, \\sigma_2, ..., \\sigma_p) \\\\ \\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_p \\ge 0 \\\\ p = \\min\\{N,M\\} \\end{gather} \\] By only preserving the \\(K \\le \\rank(\\Sigma)\\) most prominent singular values, \\(X\\) can be approximated as \\(U_K\\Sigma_KV^T_K\\), where \\[ \\begin{gather} \\text{$U_K \\in \\R^{N \\times K}$ is first $K$ columns of $U$} \\\\ \\text{$V_K \\in \\R^{M \\times K}$ is first $K$ columns of $V$} \\\\ \\Sigma_K \\in \\R^{K \\times K} = diag(\\sigma_1, \\sigma_2, ..., \\sigma_K) \\\\ \\end{gather} \\] Eckart-Young-Mirsky theorem will tell that \\(U_K\\Sigma_KV^T_K\\) is the best approximation of \\(X\\) among \\(N \\times M\\) of rank \\(K\\) in terms of Frobenius norm.\nWe may obtain the \\(B^\\star, Z^\\star\\) by \\[ B^\\star = U_K, Z^\\star = \\Sigma_K V^T_K \\]\nIf the data is centered in advance, \\(B^\\star\\) is essentially the principal component in PCA and \\(Z^\\star\\) is the transformed \\(X\\) in the basis formed by \\(B^\\star\\).\nRandom Projection The above methods all minimize the Frobenius norm during reconstruction. This objective may become time-consuming when input dimension becomes large. We may use some randomly-generated vectors as basis to do the projection. This greatly saves time, at the expense of losing accuracy. We can measure such projection by checking whether the structure of the data can be preserved, e.g. the distance between points.\nJohnson-Lindenstrauss Lemma tells that a random function \\(f(x) = \\frac{1}{\\sqrt{K}}Ax\\), where \\(A\\) is a \\(K \\times N\\) matrix with i.i.d. entries sampled from a standard Gaussian, can preserve the distance between any two points within error \\(\\epsilon\\): \\[ (1 - \\epsilon)||x^{(i)} - x^{(j)}||^2_2 \\le ||f(x^{(i)}) - f(x^{j})||^2_2 \\le (1 + \\epsilon)||x^{(i)} - x^{(j)}||^2_2 \\] with the probability at least \\(\\frac{1}{M}\\) as long as \\[ K \\ge \\frac{8\\log M}{\\epsilon^2} \\] And usually this requirement is quite conservative.\nNon-linear Dimensionality Reduction We can introduce the feature mapping to handle the …","date":1641559196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559196,"objectID":"28bb7a94a07b3b6cdf6c93acf718c2e7","permalink":"https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/dimensionality-reduction/","section":"notes","summary":"Unsupervised Dimension Reduction Dimensionality reduction\n reduces computational cost; de-noises by projecting onto lower-dimensional space and back to original space; makes results easier to understand by reducing the collinearity.  Compared to feature selection,","tags":null,"title":"Dimension Reduction","type":"book"},{"authors":null,"categories":null,"content":" Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the data (compared with directional derivative, this is directional variance) and project data onto those directions.\nLinear PCA Let \\(v \\in \\mathbb R^{N \\times 1}\\) and \\(||v||^2 = 1\\), the variance in the direction \\(v\\) is given by \\[ \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - \\mu)^2, \\text{where }\\mu = \\frac{1}{M}\\sum_{i=1}^Mv^Tx^{(i)} = v^T\\bar x \\] Under the assumption that data is pre-centered so that \\(\\bar x = \\frac{1}{M}\\sum_{i=1}^Mx^{(i)} = 0\\): \\[ \\begin{aligned} \u0026amp;\\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - \\mu)^2 = \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)} - v^T\\bar x)^2 \\\\ \u0026amp;= \\frac{1}{M}\\sum_{i=1}^Mv^T(x^{(i)} - \\bar x)(x^{(i)} - \\bar x)^Tv \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M(x^{(i)} - \\bar x)(x^{(i)} - \\bar x)^T)v \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\\\ \u0026amp;\\Downarrow_{\\text{by column-row expansion}} \\\\ \u0026amp;= \\frac{1}{M}v^TXX^Tv \\\\ \\end{aligned} \\] Suppose we want to identify the direction \\(v\\) of the maximum variance, we can formulate the problem as \\[ \\begin{gather} \\max_v \\quad \\frac{1}{M}v^TXX^Tv \\\\ s.t.\\quad v^Tv = 1 \\end{gather} \\] Let \\(\\Sigma = \\frac{1}{M}XX^T\\), which is real symmetric, we can form the Lagrangian function: \\[ L(v, \\lambda) = v^T\\Sigma v + \\lambda (1 - v^Tv) \\] We represent the constraint as \\(1 - v^Tv = 0\\) instead of \\(v^Tv - 1 = 0\\). The reason is if we take derivative w.r.t \\(v\\), we have \\[ \\frac{\\partial L}{\\partial v} = 2\\Sigma v - 2\\lambda v \\] which is more intuitive than \\(\\frac{\\partial L}{\\partial v} = 2\\Sigma v + 2\\lambda v\\). Let the derivative be \\(0\\) to have \\(\\Sigma v = \\lambda v\\). Since \\(v^tv = 1\\), \\(v \\ne 0\\), this means the optimal solution \\((\\lambda^\\star, v^\\star)\\) must be a pair of eigenvalue of eigenvector of \\(\\Sigma\\):\n\\[ v^\\star = v_i, \\lambda^\\star = \\lambda_i \\] where \\(v_i\\) is rescaled such that \\(v_i^Tv_i = 1\\). Substitute the result back to the objective to give\n\\[ \\begin{aligned} \\frac{1}{M}v^TXX^Tv \u0026amp;= (v_i)^T\\Sigma v_i \\\\ \u0026amp;= (v_i)^T\\lambda_iv_i \\\\ \u0026amp;= \\lambda_i(v_i)^Tv_i \\\\ \u0026amp;= \\lambda_i \\end{aligned} \\] So the \\(N\\) largest directions of variance will be the \\(\\Sigma\\)’s eigenvectors \\(v_1, v_2, ..., v_N\\), corresponding to the eigenvalues \\(\\lambda_1 \u0026gt; \\lambda_2 \u0026gt; ... \u0026gt; \\lambda_N\\).\nCompared with SVD Intuitively, principal components can be obtained by spectral decomposition of \\(X\\)’s covariance matrix. But in practice, principal components are usually solved with SVD, which is more efficient in computation and can handle sparse representation. One thing worth notice is that, before applying SVD, PCA centers the data firstly. That said, we cannot equalize PCA and SVD. For a more detailed discussion on the relation between PCA and SVD, please refer to this blog and this post.\nAnother view on PCA Finding \\(K\\) different \\(v\\) to \\[ \\max_{v} \\frac{1}{M}\\sum_{i=1}^M(v^Tx^{(i)})^2 \\\\ \\] is equivalent to finding \\(K\\) different \\(v\\) to \\[ \\min_{v} \\frac{1}{M}\\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2 \\] both subject to \\(||v||^2 = 1\\) and inter-orthogonality. This second notation is essentially the projection of \\(x^{(i)}\\) on \\(v\\) because \\(||v|| = 1\\) and thus the objective indicates minimizing the overall approximation error.\nKernel PCA We can map the data to a higher-dimension space: \\(x^{(i)} \\to \\phi(x^{(i)})\\). Let \\(\\mathcal{X} = [\\phi(x^{(1)}), \\phi(x^{(2)}), ..., \\phi(x^{(M)})]\\). Kernel tricks allow us not to explicitly define \\(\\phi\\), but to only focus on the inner product of mapped data: \\(\\mathcal K(x^{(i)}, x^{(j)}) = \\phi(x^{i})^T\\phi(x^{j})\\).\nSimilarly, the variance along direction \\(v\\), where \\(v^Tv=1\\), is given by \\[ \\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - \\mu)^2, \\text{where }\\mu = \\frac{1}{M}\\sum_{i=1}^Mv^T\\phi (x^{(i)}) = v^T\\bar\\phi(x) \\] Under the assumption that data is pre-centered so that \\(\\bar\\phi(x) = \\frac{1}{M}\\sum_{i=1}^M\\phi(x^{(i)}) = 0\\): \\[ \\begin{aligned} \u0026amp;\\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - \\mu)^2 = \\frac{1}{M}\\sum_{i=1}^M(v^T\\phi (x^{(i)}) - v^T\\bar\\phi(x))^2 \\\\ \u0026amp;= \\frac{1}{M}\\sum_{i=1}^Mv^T(\\phi(x^{(i)}) - \\bar{\\phi(x)})(\\phi (x^{(i)}) - \\bar\\phi(x))^Tv \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M(\\phi(x^{(i)}) - \\bar\\phi(x))(\\phi (x^{(i)}) - \\bar\\phi(x))^T)v \\\\ \u0026amp;= \\frac{1}{M}v^T(\\sum_{i=1}^M\\phi(x^{(i)})(\\phi(x^{(i)})^T)v \\\\ \u0026amp;\\Downarrow_{\\text{by column-row expansion}} \\\\ \u0026amp;= \\frac{1}{M}v^T\\mathcal{X}\\mathcal{X}^Tv \\\\ \\end{aligned} \\] Then comes the standard form of linear PCA. Let \\(\\Sigma = \\frac{1}{M}\\mathcal{X}\\mathcal{X}^T\\), we would like to solve \\[ \\begin{gather} \\max_v\\quad \\frac{1}{M}v^T\\mathcal{X}\\mathcal{X}^Tv \\\\ s.t.\\quad v^T v=1 \\end{gather} \\] This is equivalent to solving \\(\\Sigma\\)’s eigenvectors. Unfortunately, this cannot be directly solved like in linear PCA since we don’t know \\(\\phi\\). However note that\n\\[ \\begin{gather} \\begin{aligned} \\Sigma v \u0026amp;= \\lambda v \\\\ v \u0026amp;= \\frac{1}{\\lambda}\\Sigma v \\end{aligned} \\\\ …","date":1641917451,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641917451,"objectID":"ac0ca64bf6bb10436e2a600fb709eb7a","permalink":"https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/principal-component-analysis/","section":"notes","summary":"Given a data matrix \\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \\in \\mathbb R^{N \\times M}\\), the goal of PCA is to identify the directions of maximum variance contained in the","tags":null,"title":"Principal Component Analysis","type":"book"},{"authors":null,"categories":null,"content":" Notations and Concepts An ensemble \\(X\\) (we specifically use the random variable symbol to denote the ensemble) is a triplet \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(X\\) denotes the random variable, which takes on values from \\(\\A_X = \\{a_1, a_1, \\dots \\}\\), that has probability \\(\\P_X = \\{p_1, p_2, \\dots \\}\\).\nThe Shannon information content of an outcome \\(x\\) is: \\[ h(x) = \\log_2 \\frac{1}{P(x)} \\] The raw bit content of \\(X\\) is \\[ H_0(X) = \\log |\\mathcal A_X| \\] The smallest \\(\\delta\\)-sufficient subset \\(S_\\delta\\) of \\(\\mathcal A_X\\) is the smallest subset satisfying \\[ P(X \\in S_\\delta) \\ge 1 - \\delta \\] The essential bit content of \\(X\\) is \\[ H_\\delta(X) = \\log |S_\\delta| \\]\nShannon’s Source Coding Theorem  Let \\(X\\) be an ensemble with entropy \\(H(X) = H\\) bits, and \\(X^N\\) be the ensemble composed of \\(N\\) such i.i.d. random variables. Given \\(\\epsilon \u0026gt; 0\\) and \\(0 \u0026lt; \\delta \u0026lt; 1\\), there exists a positive integer \\(N_0\\) such that for \\(N \u0026gt; N_0\\), \\[ |\\frac{1}{N} H_\\delta (X^N) - H| \u0026lt; \\epsilon \\]\n Or put it in a verbal way,\n \\(N\\) i.i.d. random variables each with entropy \\(H(X) = H\\) can be compressed into more than \\(NH\\) bits with negligible information loss as \\(N \\to \\infty\\); conversely if they are compressed fewer than \\(NH\\) bits, it is virtually certain that there is information loss.\n  Proof: Let the random variable \\(\\frac 1 N \\log \\frac 1 {P(Y)}\\) be defined for the ensemble \\(Y = X^N\\), which composes of \\(N\\) i.i.d. ensembles \\(X_1 \\dots X_N\\). \\(\\frac 1 N \\log \\frac 1 {P(Y)}\\) can be re-written as the average of \\(N\\) information contents \\(h_i = \\log \\frac 1 {P(X_i)}, i \\in 1,2,\\dots,N\\): \\[ \\frac 1 N \\log \\frac 1 {P(Y)} = \\frac 1 N \\log \\frac 1 {P(X_1) \\dots P(X_N)} = \\frac 1 N (\\log \\frac{1}{P(X_1)} + \\dots + \\log \\frac{1}{P(X_N)}) \\] Each of these information contents is in turn a random variable with mean \\(\\bar h_i = H(X) = H\\) and variance \\(\\sigma_{h_i}^2 = \\sigma^2\\).\nA long string of \\(N\\) symbols would usually contain roughly \\(p_1 N\\) occurrences of symbol \\(a_1\\), \\(p_2 N\\) occurrences of symbol \\(a_2\\)… The probability of those elements is roughly \\((p_1)^{p_1 N} (p_2)^{p_2 N} \\dots\\) The information content of each such element is thus roughly \\(N \\sum_i p_i \\log \\frac{1}{p_i} = N H\\).\nWe define the typical elements of \\(\\mathcal{A}_X^N\\) to be just those element that have probability close to \\(2^{-NH}\\). Note that interestingly, the most probable string is not usually typical because its probability is far away from \\(2^{-NH}\\).\nWe introduce another parameter \\(\\beta\\) to control how close the probability has to be to \\(2^{-NH}\\) for an element to be typical. The set of typical elements is called typical set \\(T_{N \\beta}\\) and is defined as \\[ T_{N \\beta} = \\left\\{y \\in \\mathcal A_X^N: [\\frac 1 N \\log \\frac{1}{P(y)} - H]^2 \u0026lt; \\beta^2 \\right\\} \\] By the Weak Law of Large Numbers, \\[ P \\left( \\left( \\frac 1 N \\log \\frac{1}{P(Y)} - H \\right)^2 \\ge \\beta^2 \\right) \\le \\frac{\\sigma^2}{\\beta^2 N} \\] and thus \\[ P(Y \\in T_{N \\beta}) \\ge 1 - \\frac{\\sigma^2}{\\beta^2 N} \\] As \\(N\\) increases, the probability that \\(y\\) falls in \\(T_{N \\beta}\\) draws near to \\(1\\). We need to relate this to the theorem that for any given \\(\\epsilon, \\delta\\), there is a sufficiently-large \\(N\\) such that \\(H_\\delta(X^N) \\simeq NH\\).\n\\(H_\\delta(X^N) \u0026lt; N(H + \\epsilon)\\)\nThe set \\(T_{N \\beta}\\) is not the best sufficient subset for compression (because it doesn’t include those most probable). Therefore, \\(\\log |T_{N \\beta}|\\) upper-bounds the \\(H_\\delta(X^N)\\) for any \\(\\delta\\). On the other hand, for all \\(y \\in T_{N \\beta}\\), \\(P(y) \u0026gt; 2^{-N(H + \\beta)}\\), thus \\[ \\begin{align*} |T_{N \\beta}| 2^{-N(H + \\beta)} \u0026amp;\u0026lt; \\sum_{y \\in T_{N \\beta}} P(y) \u0026lt; 1 \\\\ \u0026amp;\\Downarrow \\\\ |T_{N \\beta}| \u0026amp;\u0026lt; 2^{N(H + \\beta)} \\\\ \\end{align*} \\] If we set \\(\\beta = \\epsilon\\) and set \\(N \\ge N_0\\) in a way such that \\(\\frac{\\sigma^2}{\\epsilon^2 N_0} \\le \\delta\\) and thus \\(P(Y \\in T_{N \\epsilon}) \\ge 1 - \\delta\\), \\(T_{N \\epsilon}\\) is a \\(\\delta\\)-sufficient subset. Then, \\(H_\\delta(X^N) \\le \\log |T_{N \\epsilon}| \\le N(H + \\epsilon)\\) holds for any \\(N \\ge N_0\\).\n \\(H_\\delta(X^N) \u0026gt; N(H - \\epsilon)\\)\nThis part is reached by contradiction. Suppose instead there exists a \\(\\delta\u0026#39;\\) such that there exists a sufficiently large \\(N_0\\) which results in \\(H_\\delta(X^{N}) \\le N(H - \\epsilon)\\) for arbitrary \\(\\epsilon \u0026gt; 0\\) and \\(N \u0026gt; N_0\\). Let \\(\\beta = \\epsilon/2\\), we now have \\[ H_\\delta(X^N) \\le N_0(H - 2\\beta) \\] Denote the associated subset by \\(S\u0026#39;\\). We are to disprove \\(S\u0026#39;\\) with \\(|S\u0026#39;| \\le 2^{N(H - 2\\beta)}\\) can achieve \\(P(Y \\in S\u0026#39;) \\ge 1 - \\delta\\). We break this probability into two parts: \\[ P(Y \\in S\u0026#39;) = P(Y \\in S\u0026#39; \\cap T_{N \\beta}) + P(Y \\in S\u0026#39; \\cap \\overline{T_{N \\beta}}) \\] For the first part, we have \\[ |S\u0026#39; \\cap T_{N \\beta}| \\le |S\u0026#39;| \\le 2^{N(H - 2\\beta)} \\] Thus, the maximum value of the first part is obtained when \\(S\u0026#39; \\cap T_{N \\beta}\\) contains \\(2^{N(H - …","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"3bc13385375660b31d7e8b8a3eb72889","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/","section":"notes","summary":"Notations and Concepts An ensemble \\(X\\) (we specifically use the random variable symbol to denote the ensemble) is a triplet \\((X, \\newcommand{A}{\\mathcal A} \\newcommand{P}{\\mathcal P} \\A_X, \\P_X)\\), where \\(X\\) denotes","tags":null,"title":"Source Coding Theorem","type":"book"},{"authors":null,"categories":null,"content":" Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.\nThe authentic proof entails Gamma function and Laplace’s method. However in integer case, Stirling’s approximation can be approached with Poisson distribution. Start from a Poisson distribution with mean \\(\\lambda\\): \\[ P(r ; \\lambda) = e^{-\\lambda} \\frac{\\lambda^r}{r!} \\] When \\(X \\sim P(\\lambda_1)\\), \\(Y \\sim P(\\lambda_2)\\), and suppose \\(X\\) and \\(Y\\) are independent, \\(X + Y \\sim P(\\lambda_1 + \\lambda_2)\\).\n Proof \\[ \\begin{aligned} \u0026amp; \\Pr(X + Y = r) = \\sum_{k=0}^r \\Pr(X = k) \\Pr(Y = r-k) \\\\ \u0026amp;= \\sum_{k=0}^r e^{-\\lambda_1} \\frac{\\lambda_1^k}{k!} e^{-\\lambda_2} \\frac{\\lambda_2^{r-k}}{(r-k)!} \\\\ \u0026amp;= \\frac{e^{-(\\lambda_1 + \\lambda_2)}}{r!} \\sum_{k=0}^r \\frac{r!}{k! (r-k)!} \\lambda_1^k \\lambda_2^{r-k} \\\\ \u0026amp;= e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^r}{r!} \\\\ \u0026amp;= P(r; \\lambda_1 + \\lambda_2) \\end{aligned} \\]  Therefore, a random variable \\(X \\sim P(\\lambda)\\) (with integer \\(\\lambda\\)) can be treated as the addition of \\(\\lambda\\) independent \\(Y_i \\sim P(1)\\). By the central limit theorem, for a large \\(\\lambda\\), \\[ \\mathbb \\Pr(\\frac{\\underbrace{\\sum_i Y_i}_X - \\lambda}{\\sqrt{\\lambda}} \\le x) \\simeq \\Phi(x) \\] Or put it another way, the mass of the Poisson distribution \\(X\\) follows is well approximated by the density of the Gaussian distribution with mean \\(\\lambda\\) and variance \\(\\lambda\\): \\[ \\begin{aligned} P(x;\\lambda) \u0026amp;\\simeq N(x; \\lambda, \\lambda) \\\\ e^{-r} \\frac{\\lambda^r}{r!} \u0026amp;\\approx \\frac{1}{\\sqrt{2\\pi \\lambda}} e^{-\\frac{(r - \\lambda)^2}{2\\lambda}} \\end{aligned} \\] Plug \\(r = \\lambda\\) into this formula and rearrange it to have \\[ \\begin{aligned} e^{-\\lambda} \\frac{\\lambda^\\lambda}{\\lambda!} \u0026amp;\\approx \\frac{1}{\\sqrt{2\\pi \\lambda}} \\\\ \\lambda! \u0026amp;\\approx \\sqrt{2\\pi \\lambda} \\left( \\frac{\\lambda}{e} \\right)^\\lambda \\end{aligned} \\]\n","date":1652125189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652125189,"objectID":"d578addef391da2c6991619f0bd4741a","permalink":"https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/numerical-analysis/stirlings-approximation/","section":"notes","summary":"Stirling’s Approximation Stirling’s approximation, which states that \\(\\Gamma(n+1) \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n\\) as \\(n \\to \\infty\\), is useful when estimating the order of \\(n!\\). Notably, it is quite accurate even when \\(n\\) is small.","tags":null,"title":"Stirling's Approximation","type":"book"},{"authors":null,"categories":null,"content":" Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric. Another disadvantage of KL-divergence is that it is not bounded from above. Jenson-Shannon divergence, on the other hand, overcomes these two problems of KL-divergence.\nGiven two distributions \\(p\\) and \\(q\\) defined on the same sample space, Jenson-Shannon divergence is defined as \\[ \\newcommand{\\JSD}{\\mathop{\\text{JSD}}} \\JSD(p;q) \\coloneq \\frac{1}{2} (D_\\text{KL}(p || m) + D_\\text{KL}(q || m)), \\text{where $m = \\frac{p+q}{2}$} \\] Since the JSD is the addition of two KL-divergences, it is non-negative by the non-negativity of KL-divergence and it reaches \\(0\\) when \\(p=q\\). On the other hand, \\[ \\begin{aligned} \\JSD(p;q) \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{p(x)}{(p(x)+q(x))/2} + \\E_{x \\sim q} \\log \\frac{q(x)}{(p(x)+q(x))/2} \\big) \\\\ \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{2}{( 1 + e^{\\ln \\frac{q(x)}{p(x)}} )} + \\E_{x \\sim q} \\log \\frac{2}{( 1 + e^{\\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \\end{aligned} \\] Due to the concavity of \\(f(x) = \\log \\frac{2}{1 + e^x}\\), \\[ \\begin{aligned} \\JSD(p;q) \u0026amp;= \\frac{1}{2} \\big( \\E_{x \\sim p} \\log \\frac{2}{( 1 + e^{\\ln \\frac{q(x)}{p(x)}} )} + \\E_{x \\sim q} \\log \\frac{2}{( 1 + e^{\\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \u0026amp;\\le \\frac{1}{2} \\big( \\log \\frac{2}{( 1 + e^{\\E_{x \\sim p} \\ln \\frac{q(x)}{p(x)}} )} + \\log \\frac{2}{( 1 + e^{\\E_{x \\sim q} \\ln \\frac{p(x)}{q(x)}} )} \\big) \\\\ \u0026amp;\\le \\frac{1}{2} \\big( 2 \\log \\frac{2}{( 1 + e^{( \\E_{x \\sim p} \\ln \\frac{q(x)}{p(x)} + \\E_{x \\sim q} \\ln \\frac{p(x)}{q(x)} ) / 2} )} \\big) \\\\ \u0026amp;= \\log \\frac{2}{( 1 + e^{-\\frac{1}{2} ( D_\\text{KL}(p||q) + D_\\text{KL}(q||p) )} )} \\end{aligned} \\] This upper bound is attributed to Crooks. Since the KL-divergence can go to positive infinity, we can conclude that \\(\\JSD(p;q)\\) is upper-bounded by \\(\\log 2\\). The \\(\\newcommand{\\J}{\\mathop{\\text{J}}} \\J(p;q) \\coloneq \\frac{1}{2} ( D_\\text{KL}(p||q) + D_\\text{KL}(q||p) )\\) term is also known as Jeffreys divergence (the coefficient \\(\\frac{1}{2}\\) may be ignored in some other place). Even more accurately, the upper bound can be rewritten as (in this note)\n\\[ \\JSD(p;q) \\le \\min (\\frac{1}{4} \\J(p;q), \\log \\frac{2}{1 + e^{-\\J(p;q)}}) \\] A lower bound in terms of Jeffreys divergence can be derived as (in this note)\n\\[ \\JSD(p;q) \\ge \\frac{1}{4} \\ln(1 + 2\\J(p;q)) \\]\nRemarkably, the square root of JSD between two distributions satisfies the metric axioms.\nRelation with Entropy By definition, \\[ \\begin{aligned} \u0026amp;\\JSD(p;q) = \\frac{1}{2} (D_\\text{KL}(p || \\frac{p+q}{2}) + D_\\text{KL}(q || \\frac{p+q}{2})) \\\\ \u0026amp;= \\frac{1}{2} [H(p||\\frac{p+q}{2}) - H(p) + H(q||\\frac{p+q}{2}) - H(q)] \\\\ \u0026amp;= \\begin{aligned}[t] \u0026amp;-\\frac{1}{2} [\\E_{x \\sim p} \\log \\frac{p+q}{2}(x) + \\E_{x \\sim q} \\log \\frac{p+q}{2}(x)] \\\\ \u0026amp;- \\frac{1}{2} [H(p) + H(q)] \\\\ \\end{aligned} \\\\ \u0026amp;= -\\E_{x \\sim \\frac{p+q}{2}} \\frac{p+q}{2}(x) - \\frac{1}{2} [H(p) + H(q)] \\\\ \u0026amp;= H(\\frac{p+q}{2}) - \\frac{1}{2} [H(p) + H(q)] \\end{aligned} \\]\n","date":1651590621,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651590621,"objectID":"c1c10226a5a23918fda252703ca375bb","permalink":"https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/jenson-shannon-divergence/","section":"notes","summary":"Jenson-Shannon Divergence In probability theory and statistics, Jenson-Shannon divergence is another method of measuring the distance between two distributions. It is based on KL-divergence with some notable differences. KL-divergence does not make a good measure of distance between distributions, since in the first place it is not symmetric.","tags":null,"title":"Jenson-Shannon Divergence","type":"book"},{"authors":null,"categories":null,"content":" Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).\nIf the probability mass function is known, \\[ \\E[g(X)] = \\sum_x g(x) p(x) \\] If the probability density function is known, \\[ \\E[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)p(x)\\ \\d x \\] If the cumulative distribution function is known, \\[ \\E[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)\\ \\d F(x) \\]\nWiki\nMarginal Expectation If the joint distribution of two random variables \\(X\\) and \\(Y\\) is known, then the expectation of one component can be calculated as \\[ \\E[X] = \\int_{-\\infty}^{+\\infty} x p_X(x)\\; \\d x = \\int_{-\\infty}^{+\\infty} x \\int_{-\\infty}^{+\\infty} p(x,y)\\; \\d y\\; \\d x = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} xp(x,y)\\ \\d y\\ \\d x \\] On the other hand, \\[ \\E [X] = \\E_{y \\sim p_Y} [\\E_{x \\sim p(X|Y=y)}] = \\int_{-\\infty}^{+\\infty} p(y) \\bigg( \\int_{-\\infty}^{+\\infty} x p(x|y)\\ \\d x \\bigg) \\d y \\] StackExchange Discussion\nExpectation of Non-negative Random Variables If \\(X\\) is a random variable whose value is non-negative, and its expectation exists, and\n when \\(X\\) is continuous, \\[ \\begin{aligned}[b] \\E (X) \u0026amp;= \\int_{0}^{+\\infty} x p(x)\\ \\d x = \\int_{0}^{+\\infty} x\\ \\d \\big( P(x) - 1 \\big) \\\\ \u0026amp;= [x \\big( P(x) - 1 \\big)]\\bigg|_{x=0}^{+\\infty} - \\int_0^{+\\infty} \\big( P(x) - 1 \\big)\\ \\d x \\end{aligned} \\] Because the expectation exists, the above expression and especially the \\([x \\big( P(x) - 1 \\big)]\\bigg|_{x=0}^{+\\infty}\\) term must converge: \\[ \\begin{gather} [x \\big( P(x) - 1 \\big)]\\bigg|_{x=0} = 0 \\\\ [P(x) - 1]\\bigg|_{x \\to +\\infty} = 0 \\Rightarrow [x \\big( P(x) - 1 \\big)]\\bigg|_{x \\to +\\infty} = 0 \\end{gather} \\] Therefore, \\[ \\E(X) = \\int_{0}^{+\\infty} \\big (1 - P(x) \\big)\\ \\d x \\]\n when \\(X\\) is discrete and \\(X\\) only takes on integer values, supposing the max value of \\(X\\) is \\(N\\), \\[ \\begin{aligned} \u0026amp;\\E(X) = \\sum_{k=0}^{N} [k P(X = k)] \\\\ \u0026amp;= \\sum_{k=0}^{N} [(\\sum_{j=0}^{k-1} 1) P(X = k)] \\\\ \u0026amp;= \\sum_{k=0}^{N} [\\sum_{j=0}^{k-1} P(X = k)] \\\\ \u0026amp;= \\sum_{j=0}^{N-1} [\\sum_{k=j+1}^{N} P(X = k)] \\\\ \u0026amp;= \\sum_{j=0}^{N-1} P(X \u0026gt; j) \\end{aligned} \\]\n  StackExchange Discussion || Summation by Parts\nExpectation and Quantile Function Let \\(f\\) be the PDF and \\(F\\) be the CDF of a random variable \\(X\\). Let \\(Q = F^{-1}\\) be the inverse of \\(F\\). \\(Q\\) is called the quantile function of \\(X\\), and \\[ \\int_0^1 Q(p)\\ \\d p \\stackrel{p=F(x)}{\\Longrightarrow} = \\int_{-\\infty}^{+\\infty} x f(x)\\ \\d x = \\E(X) \\] StackExchange Answer\n","date":1651575054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651575054,"objectID":"98772df8c803c49e6d7e354aae83a4a2","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/","section":"notes","summary":"Law of the Unconscious Statistician In probability theory and statistics, the law of the unconscious statistician (LOTUS), is a theorem used to calculate the expected value of a function \\(g(X)\\) of a random variable \\(X\\) when one knows the probability distribution of \\(X\\) but one does not know the distribution of \\(g(X)\\).","tags":null,"title":"Unconscious Statistics","type":"book"},{"authors":null,"categories":null,"content":" 以下以二维随机变量为例，展示协方差以及相关系数的概念。虽然协方差和相关系数相对期望、方差来说显得复杂，但是他们依旧是随机变量的数字特征。\n协方差 设\\([X, Y]\\)为一组二维随机变量，如果\\(\\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\}\\)存在，则称 \\[ \\notag \\mathrm {Cov}(X, Y) \\triangleq \\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\} \\] 为随机变量\\(X\\)和\\(Y\\)的协方差。在实际中计算协方差时，更多的是使用以下公式：\n\\[ \\begin{aligned} \u0026amp;\\mathrm {Cov}(X, Y) = \\mathrm E\\{[X - \\mathrm E(X)][Y - \\mathrm E(Y)]\\} \\\\ \u0026amp;= \\mathrm E[XY - X\\mathrm E(Y) - \\mathrm E(X)Y + \\mathrm E(X) \\mathrm E(Y)] \\\\ \u0026amp;= \\mathrm E(XY) - \\mathrm E(X)\\mathrm E(Y) - \\mathrm E(X)\\mathrm E(Y) + \\mathrm E(X) \\mathrm E(Y) \\\\ \u0026amp;= \\mathrm E(XY) - \\mathrm E(X) \\mathrm E(Y) \\end{aligned} \\] 而二维随机变量[X, Y]对应的协方差矩阵即为\n\\[ \\Sigma = \\begin{bmatrix} \\Cov(X,X) \u0026amp; \\Cov(X,Y) \\\\ \\Cov(Y,X) \u0026amp; \\Cov(Y,Y) \\\\ \\end{bmatrix} \\]\n相关系数 协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们可以首先将随机变量标准化： \\[ X^\\star = \\frac{X - \\E(X)}{\\sqrt{\\Var(X)}}，Y^\\star = \\frac{Y - \\E(Y)}{\\sqrt{\\Var(Y)}} \\] 再求协方差\\(\\Cov(X^\\star, Y^\\star)\\)，这便是随机变量\\(X\\)和\\(Y\\)的相关系数： \\[ \\rho(X, Y) = \\mathrm{Cov}(X^\\star, Y^\\star) = \\frac{\\Cov(X, Y)}{\\sqrt{\\Var(X) \\Var(Y)}} \\]\n","date":1651401675,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651401675,"objectID":"64051c078bf20447141ce8e7a654b55f","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/","section":"notes","summary":"以下以二维随机变量为例，展示协方差以及相关系数的概念。虽然协","tags":null,"title":"协方差与相关系数","type":"book"},{"authors":null,"categories":null,"content":" Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gathered} \\text{$U$ is $m \\times m$, $V$ is $n \\times n$} \\\\ U U^T = I \\\\ V V^T = I \\\\ \\Sigma = \\diag_{m \\times n} (\\sigma_1, \\sigma_2, ..., \\sigma_r) \\\\ \\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r \\ge 0 \\\\ r = \\rank A \\end{gathered} \\]\nProof This is shown by construction.\nConstruction of \\(V\\) and \\(\\Sigma\\)\n\\(A^T A\\) is an \\(n \\times n\\) real symmetric matrix, so it can be diagonalized by an orthogonal matrix. Let \\(V\\) be this matrix and \\(\\Lambda\\) be the corresponding diagonal matrix consisting of \\(A^TA\\)’s eigenvalues: \\[ V^{-1} (A^T A) V = \\Lambda \\] Let \\(\\Lambda\\) and \\(V\\) be arranged such that corresponding eigenvalues of \\(V\\) are decreasing. Because \\(V\\) consists of orthonormal basis, we have \\(V^T V = I\\). Therefore, \\[ V^T (A^T A) V = \\Lambda \\] Because \\(\\rank(A) = r\\), \\(\\rank(A^T A) = r\\). And because \\(A^T A\\) is positive semi-definite, it has \\(n\\) non-negative real eigenvalues, \\(r\\) of which are positive. Let \\(\\sigma_i = \\sqrt{\\lambda_i}, i = 1, 2, ..., n\\) (which are defined as \\(A\\)’s the singular values).\n\\[ \\begin{gather} \\lambda_1, \\lambda_2, ..., \\lambda_r \u0026gt; 0, \\lambda_r, \\lambda_{r+1}, ..., \\lambda_{n} = 0 \\\\ \\sigma_1, \\sigma_2, ..., \\sigma_r \u0026gt; 0, \\sigma_r, \\sigma_{r+1}, ..., \\sigma_{n} = 0 \\end{gather} \\] Let \\(V_1 = [v_1, v_2, ..., v_r], V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\\). Also let\n\\[ \\Sigma_1 = \\begin{bmatrix} \\sigma_1 \\\\ \u0026amp; \\sigma_2 \\\\ \u0026amp; \u0026amp; \\ddots \\\\ \u0026amp; \u0026amp; \u0026amp; \\sigma_r \\end{bmatrix} \\] Complement \\(\\Sigma_1\\) with \\(0\\) to get the \\(m \\times n\\) matrix: \\[ \\Sigma = \\begin{bmatrix} \\Sigma_1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Since \\(\\rank(A^T A) = r\\), \\(\\Nul(A^T A)\\) is of dimension \\(n-r\\). Because \\(V_2\\) comprises of \\(A^TA\\)’s eigenvectors whose eigenvalues are \\(0\\), \\(V_2\\) has \\(n - r\\) independent columns and \\(A^T A V_2 = 0\\). Therefore, \\(V_2\\) spans \\(\\Nul(A^TA)\\) and thus \\(\\Nul(A)\\). \\[ \\begin{gather} I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1 V_1^T + V_2 V_2^T \\\\ A = A I = A V_1 V_1^T + A V_2 V_2^T = A V_1 V_1^T \\end{gather} \\]\n Construction of \\(U\\)\nLet \\[ \\begin{gather} u_i \\coloneq \\frac{1}{\\sigma_i} A v_i, i = 1, 2, ..., r \\\\ U_1 \\coloneq [u_1, u_2, ..., u_r] \\end{gather} \\] Thus \\(AV_1 = U_1\\Sigma_1\\). Also, \\(U_1\\)’s columns are orthonormal: \\[ \\begin{aligned} u_i^T u_j \u0026amp;= (\\frac{1}{\\sigma_i} A v_i)^T (\\frac{1}{\\sigma_j} A v_j) \\\\ \u0026amp;= \\frac{1}{\\sigma_i\\sigma_j} v_i^T A^T A v_j \\\\ \u0026amp;= \\frac{1}{\\sigma_i\\sigma_j} v_i^T \\lambda_j v_j \\\\ \u0026amp;= \\frac{\\sigma_j}{\\sigma_i} v_i^T v_j \\\\ \u0026amp;= \\begin{cases} 0 \u0026amp; i \\ne j \\\\ 1 \u0026amp; i = j \\end{cases} \\end{aligned} \\]\nSince \\(u_i\\)’s are within \\(\\Col(A)\\) and are orthonormal as shown, plus that \\(\\rank(A) = r\\), \\(u_i\\)’s span the \\(\\Col(A)\\).\nLet \\(\\Col(A)^\\perp\\) be \\(\\Col(A)\\)’s complement, we have \\(\\Col(A)^\\perp = \\Nul(A^T)\\). Let \\(\\{ u_{r+1}, u_{r+2}, \\dots, u_{m} \\}\\) be an orthonormal basis of \\(\\Nul(A^T)\\) such that they are orthogonal to \\(U_1\\)’s column vectors. We construct \\(U\\) by:\n\\[ \\begin{gather} U_2 \\coloneq [u_{r+1}, u_{r+2}, ..., u_{m}] \\\\ U \\coloneq [U_1, U_2] \\end{gather} \\]\n Proof of \\(U \\Sigma V^T = A\\) \\[ \\begin{aligned} U \\Sigma V^T \u0026amp;= [U_1, U_2] \\begin{bmatrix} \\Sigma_1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} V_1^T \\\\ V_2^T \\\\ \\end{bmatrix} \\\\ \u0026amp;= [U_1 \\Sigma_1, 0] \\begin{bmatrix} V_1^T \\\\ V_2^T \\\\ \\end{bmatrix} \\\\ \u0026amp;= U_1 \\Sigma_1 V_1^T \\\\ \u0026amp;= A V_1 V_1^T \\\\ \u0026amp;= A \\end{aligned} \\]\n  Note Note that SVD reveals that \\(A\\) is essentially the addition of \\(r\\) rank-\\(1\\) matrix: \\[ U \\Sigma V^T = \\sum_{i=1}^r \\sigma_i u_i v_i^T \\] We have shown the construction process of \\(A\\) as \\(U \\Sigma V^T\\) in the above proof, i.e. the existence of such decomposition, with which we can investigate into \\(U\\), \\(\\Sigma\\) and \\(V\\). Notice that \\[ \\begin{aligned} A A^T \u0026amp;= U \\Sigma V^T V \\Sigma U^T \\\\ A A^T \u0026amp;= U \\Sigma^2 U^T \\\\ A A^T U \u0026amp;= U \\Sigma^2 U^T U \\\\ A A^T U \u0026amp;= U \\Sigma^2 \\end{aligned} \\] Since \\(\\Sigma\\) is a diagonal matrix, we can conclude that \\(U\\) contains the eigenvectors of \\(A A^T\\). \\(\\Sigma^2\\) contains the eigenvalues of \\(A A^T\\) as its diagonal entries. Similarly \\[ \\begin{aligned} A^T A \u0026amp;= V \\Sigma U^T U \\Sigma V^T \\\\ A^T A \u0026amp;= V \\Sigma^2 V^T \\\\ A^T A V \u0026amp;= V \\Sigma^2 V^T V \\\\ A^T A V \u0026amp;= V \\Sigma^2 \\end{aligned} \\] \\(V\\) contains the eigenvectors of \\(A^T A\\). \\(\\Sigma^2\\) contains the eigenvalues of \\(A^T A\\) as its diagonal entries.\nIt is easy to verify that \\(A A^T\\) and \\(A^T A\\) actually have the same eigenvalues. And if \\(v\\) is an eigenvector of \\(A^T A\\), \\(A v\\) is an eigenvector of \\(A A^T\\) with the same eigenvalue. The columns of \\(U\\) are called the left-singular vectors of \\(A\\) and the columns of \\(V\\) are called the right-singular vectors of \\(A\\).\nThe time complexity of SVD is \\(O(\\min\\{ m^2 n, n^2 m \\})\\), depending on whether \\(A A^T\\) or \\(A^T A\\) is used to solve \\(\\Sigma^2\\).\nEckart-Young-Mirsky Theorem Given an \\(m \\times n\\) matrix \\(X\\) of rank \\(r \\le \\min(m,n)\\) and …","date":1641560132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560132,"objectID":"5d1c7485522774dc14e6a6b95481f2e1","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/singular-value-decomposition/","section":"notes","summary":"Theorem Any \\(m \\times n\\) matrix \\(A\\) can be decomposed into \\(U\\Sigma V^T\\), where \\[ \\begin{gathered} \\text{$U$ is $m \\times m$, $V$ is $n \\times n$} \\\\ U U^T =","tags":null,"title":"Singular Value Decomposition","type":"book"},{"authors":null,"categories":null,"content":" Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\).\nGiven a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define\n\\[ \\begin{aligned} \\newcommand{\\v}{\\mathrm{v}} \\v_1 \u0026amp;= \\x_1 \\\\ \\v_2 \u0026amp;= \\x_2 - \\frac{\\x_2 \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 \\\\ \\v_3 \u0026amp;= \\x_3 - \\frac{\\x_3 \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 - \\frac{\\x_3 \\cdot \\v_2}{\\v_2 \\cdot \\v_2} \\v_2 \\\\ \u0026amp;\\vdots \\\\ \\v_p \u0026amp;= \\x_p - \\frac{\\x_p \\cdot \\v_1}{\\v_1 \\cdot \\v_1} \\v_1 - \\frac{\\x_p \\cdot \\v_{p-1}}{\\v_{p-1} \\cdot \\v_{p-1}} \\v_{p-1} \\end{aligned} \\] Then \\(\\{ \\v_1, \\dots, \\v_p \\}\\) is an orthogonal basis for \\(W\\). In addition, \\[ \\newcommand{\\span}[1]{\\mathrm{Span}\\{#1\\}} \\span{\\v_1, \\dots,\\v_k} = \\span{\\x_1, \\dots, \\x_k} \\text{\\quad for $1 \\le k \\le p$} \\]\nQR Factorization If \\(A\\) is an \\(m \\times n\\) matrix with linearly independent columns, then \\(A\\) can be factored as \\(A = QR\\), where \\(Q\\) is an \\(m \\times n\\) matrix whose columns form an orthonormal basis for \\(\\mathop{\\mathrm{Col}}A\\) and \\(R\\) is an \\(n \\times n\\) upper triangular invertible matrix with positive entries on its diagonal.\nSuch factorization can be realized by Gram-Schmidt orthogonalization. The columns of \\(A\\), say denoted as \\(\\x_1, \\dots, \\x_n\\), form the basis of \\(\\Col A\\). Apply the Gram-Schmidt process to construct an orthogonal basis \\(\\{ \\v_1, \\dots, \\v_n \\}\\) for \\(\\Col A\\) and let \\[ Q = [\\v_1, \\dots, \\v_n] \\] For every \\(k = 1, \\dots, n\\), \\(\\x_k\\) is in \\(\\span{\\x_1, \\dots, \\x_k} = \\span{\\v_1, \\dots, \\v_k}\\). So there are constants \\(r_{1k}, \\dots, r_{kk}\\) such that \\[ \\x_k = [\\v_1, \\dots, \\v_k] \\begin{bmatrix} r_{1k} \\\\ \\vdots \\\\ r_{kk} \\end{bmatrix} \\] \\(r_{kk}\\) is nonzero or else \\(\\x_k\\) is in \\(\\span{\\x_1, \\dots, \\x_{k-1}}\\), which violates the linear independence condition of columns of \\(A\\). We can assume \\(r_{kk} \u0026gt; 0\\); otherwise we can multiply both \\(r_{kk}\\) and \\(\\v_k\\) by \\(-1\\) without compromising previous conditions. Let \\[ \\newcommand{\\r}{\\mathrm{r}} \\r_k = \\begin{bmatrix} r_{1k} \\\\ \\vdots \\\\ r_{kk} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\] We have \\(\\x_k = Q \\r_k\\) for \\(k = 1, \\dots, n\\). Then \\[ A = [\\v_1, \\dots, \\v_n] [\\r_1, \\dots, \\r_n] = Q R \\] The fact that \\(R\\) is invertible follows easily the fact that \\(Q\\)’s columns are linearly independent. Because \\(k\\)-th element of \\(k\\)-th column, i.e. \\(r_{kk}\\), is positive, \\(R\\)’s diagonal entries are positive.\n","date":1641560025,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560025,"objectID":"8d5e4410527b7138a117b40c7e3e0044","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/","section":"notes","summary":"Gram-Schmidt Orthogonalization The Gram-Schmidt process is a simple algorithm for producing orthogonal basis for any nonzero subspace of \\(\\R^n\\).\nGiven a basis \\(\\{ \\x_1, \\dots, \\x_p \\}\\) for a nonzero subspace \\(W\\) of \\(\\R^n\\), define","tags":null,"title":"Gram-Schmidt Orthogonalization","type":"book"},{"authors":null,"categories":null,"content":" Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, ..., \\sigma_K)\\), when distance between \\(X\\) and \\(Y\\) is defined as Frobenius norm: \\[ ||X - Y||_F = \\sqrt{\\sum_{ij}(X - Y)_{ij}^2} \\] Firstly we prove that, if \\(U\\) is orthogonal, then \\(||UA||_F = ||AU||_F = ||A||_F\\): \\[ \\begin{aligned} ||UA||_F^2 \u0026amp;= tr((UA)^TUA) \\\\ \u0026amp;= tr(A^TUUA) \\\\ \u0026amp;= tr(A^TA) \\\\ \u0026amp;= ||A||_F^2 \\end{aligned} \\] The same can be derived for \\(||AU||_F\\).\nFor any \\(m \\times n\\) matrix \\(Y\\) of rank \\(K \\le R\\),\n\\[ \\begin{aligned} ||X - Y||_F^2 \u0026amp;= ||U\\Sigma V^T - Y||_F^2 \\\\ \u0026amp;= ||U^TU\\Sigma V^TV - U^TYV||_F^2 \\\\ \u0026amp;= ||\\Sigma - U^TYV||_F^2 \\\\ \\end{aligned} \\] Let \\(Z = U^TYV\\), \\(Z\\) is also of rank \\(K\\). \\[ \\begin{aligned} ||X - Y||_F^2 \u0026amp;= ||\\Sigma - Z||_F^2 \\\\ \u0026amp;= \\sum_{ij}(\\Sigma_{ij} - Z_{ij})^2 \\\\ \u0026amp;= \\sum_{i=1}^R(\\sigma_{i} - Z_{ii})^2 + \\sum_{i\u0026gt;R}^{\\min\\{M,N\\}}Z_{ii}^2 + \\sum_{i \\ne j}Z_{ij}^2 \\\\ \\end{aligned} \\] The minimum is achieved if \\[ \\begin{gather} Z_{ii} = \\sigma_{i}, i = 1, 2, \\dots, K \\\\ Z_{ii} = \\sigma_{i}, i = K, K + 1, \\dots, R - 1 \\\\ Z_{ii} = 0, i = R, R + 1, \\dots, \\min\\{M,N\\} \\\\ Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \\dots, N, i \\ne j \\end{gather} \\] Such \\(Z\\) exists when \\(Y = U\\Sigma_KV^T\\).\nEckart-Young-Mirsky Theorem also holds for spectral norm .\n","date":1641559250,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641559250,"objectID":"b8c76cf40262b9a1aceb4e7c878e7cb7","permalink":"https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/eckart-young-mirsky-theorem/","section":"notes","summary":"Given an \\(M \\times N\\) matrix \\(X\\) of rank \\(R \\le \\min\\{M,N\\}\\) and its SVD \\(X = U\\Sigma V^T\\), where \\(\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_R)\\), among all the \\(M \\times N\\) matrices of rank \\(K \\le R\\), the best approximation to \\(X\\) is \\(Y^\\star = U\\Sigma_K V^T\\), where \\(\\Sigma_K = diag(\\sigma_1, \\sigma_2, .","tags":null,"title":"Eckart-Young-Mirsky Theorem","type":"book"},{"authors":null,"categories":null,"content":" Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in \\R^{N \\times 1}\\), independent component analysis finds the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\). Columns of \\(A\\) are the propagation weights from a signal source to observers. Therefore, they are considered independent (and are thus called independent components).\n\\(A\\) is called mixing matrix and \\(W = A^{-1}\\) is called unmixing matrix because \\(S = WX\\).\nSuppose \\(A = U\\Sigma V^T\\) by SVD. Assume observations are pre-centered:\n\\[ \\sum_{i=1}^Mx^{(i)} = 0 \\] Assume that signals are independent and the variance of each signal is \\(1\\) (taking the advantage of scale/sign ambiguity): \\[ \\frac{1}{M}\\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I \\] Then the covariance of \\(X\\) can be calculated as \\[ \\begin{aligned} C \u0026amp;= \\frac{1}{M}x^{(i)}(x^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}As^{(i)}(As^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}U\\Sigma V^Ts^{(i)}(U\\Sigma V^Ts^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}U\\Sigma V^Ts^{(i)}(s^{(i)})^TV\\Sigma^T U^T \\\\ \u0026amp;= U\\Sigma V^TIV\\Sigma^T U^T \\\\ \u0026amp;= U\\Sigma^2U^T \\text{ (by property of SVD, note that $\\Sigma$ is $n \\times n$ diagonal)} \\\\ \\end{aligned} \\]\nIf our assumptions are correct, then the \\(U\\) is the concatenated eigenvectors of the matrix \\(CC^T\\). \\(\\Sigma^2\\) is the diagonal matrix consisting of corresponding eigenvalues of the matrix \\(C^TC\\). Since \\(C\\) is symmetric, \\(CC^T = C^TC = C^2\\).\nIn other words, \\(U\\) and \\(\\Sigma\\) can be solved by eigen decomposition. Define \\(\\hat x^{(i)} = \\Sigma^{-1}U^Tx^{(i)}\\): \\[ \\begin{aligned} \\hat C \u0026amp;= \\frac{1}{M}\\hat x^{(i)}(\\hat x^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}\\Sigma^{-1}U^Tx^{(i)}(\\Sigma^{-1}U^Tx^{(i)})^T \\\\ \u0026amp;= \\frac{1}{M}\\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\\Sigma^{-1} \\\\ \u0026amp;= \\Sigma^{-1}U^T(\\frac{1}{M}x^{(i)}(x^{(i)})^T)U\\Sigma^{-1} \\\\ \u0026amp;= \\Sigma^{-1}U^TU\\Sigma^2U^TU\\Sigma^{-1} \\\\ \u0026amp;= I \\\\ \\end{aligned} \\]\n\\[ S = WX = A^{-1}X = V\\Sigma^{-1}U^TX = V\\hat X \\]\nThe remaining job is to find the \\(V\\). We resort to multi-information, a generalization of mutual-information from Information Theory to judge how close a distribution is to statistical independence for multiple variables. \\[ I(s) = \\sum_{s \\in \\mathcal{S}}p(s) \\log \\frac{p(s)}{\\prod_jp(s_j)} \\] It is a non-negative quantity that reaches the minimum if and only if all variables are statistically independent.\nThe multi-information can be written as a function of entropy, which is defined as \\[ H(s) = -\\sum_{s \\in \\mathcal{S}}p(s) \\log p(s) \\] The multi-information can be written as the difference between the sum of entropies of marginal distribution and the entropy of joint distribution \\[ \\begin{aligned} I(s) \u0026amp;= \\sum_jH(s_j) - H(s) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - H(V\\hat x) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - (H(\\hat x) + log|V|) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - (H(\\hat x) + log1) \\\\ \u0026amp;= \\sum_jH((V\\hat x)_j) - H(\\hat x) \\\\ \\end{aligned} \\] Because we are assuming signal are independent from each other, \\[ \\begin{aligned} V^\\star \u0026amp;= \\arg \\min_V\\sum_jH((V\\hat x)_j) - H(\\hat x) \\\\ \u0026amp;= \\arg \\min_V\\sum_jH((V\\hat x)_j) \\end{aligned} \\] Calculating entropy and then taking derivative is no easy task and therefore ICA algorithms focus on approximations or equivalences to the above equation. For example, it can be approximated by finding the rotation that maximizes the expected log-likelihood of the observed data by assuming that the source distributions are known.\nNon-Gaussian and PCA The key assumption of ICA is that signals are non-Gaussian. We are trying to recover the \\(S\\) (by finding \\(W\\)) to be as non-Gaussian as possible. \\(\\hat X\\) consists of independent components because its covariance matrix \\(\\hat C\\) is shown to be an identity matrix (and thus diagonal). \\(S\\) is reconstructed by the linear combination of independent components. Thus it will be the most non-Gaussian when each variable is formed by exactly one component of \\(\\hat X\\) by Central Limit Theorem, i.e. \\(S\\)’s components are independent.\nConsider the case when \\(S\\) consists of \\(N\\) independent Gaussian, which invalidate the above analysis. If we forcibly calculate \\(V^\\star\\), due to the property of multi-variate Gaussian that its iso-density maps are spherical, then any \\(N \\times N\\) rotation matrix will be a solution to Equation (9), including the left singular matrix of \\(X\\), i.e. the concatenated eigenvectors of \\(XX^T\\), which is exactly the projection basis obtained in PCA. If any \\(N \\times N\\) rotation matrix can be a solution, there are infinite many solutions to \\(A\\), and thus ICA will just fail.\nThe conclusion drawn above is based on the ideal case, i.e. many enough observations. Even in the real case where signals are Gaussian, we may get a unique solution to \\(V^\\star\\).\nExternal Materials Independent Component Analysis: A Tutorial (tkk.fi)\n","date":1639909003,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639909003,"objectID":"1afb1b9362a37f571fdcad44487772cb","permalink":"https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/independent-component-analysis/","section":"notes","summary":"Assumption and Derivation Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis \\(i\\)) \\(\\mathcal{D} = \\{x^{(i)}\\}_{i=1}^M, x^{(i)} \\in \\R^{N \\times 1}\\), independent component analysis finds the \\(N \\times N\\) mixing matrix \\(A\\) such that \\(X = AS\\).","tags":null,"title":"Independent Component Analysis","type":"book"},{"authors":null,"categories":null,"content":" Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to robustly fit the model in the presence of outliers.\nRANSAC does the following:\n randomly sample a subset of data of an fairly enough amount for training; fit a model to the this subset; determine data points in the whole data set as inliers or outliers by comparing the residuals (prediction errors) to a threshold. The set of inliers is called a consensus set; repeat above for some iterations and retrain the final model with the largest consensus set (since inliers should be the majority).  Parameters of RANSAC include:\n \\(s\\): number of points to fit the model;\n \\(t\\): threshold of the residual;\n \\(e\\): proportion the outliers;\n \\(\\delta\\): probability of success (at least one iteration is finished with no outlier);\n \\(T\\): number of iterations to be determined.\n  Then,\n \\(p\\text{(training subset has no outliers)} = (1 - e)^s\\) \\(p\\text{(training subset has at least one outlier)} = 1 - (1 - e)^s\\) \\(p\\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T\\)  We want \\[ \\begin{gather} p\\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T \u0026lt; 1 - \\delta \\\\ T \u0026gt; \\log\\frac{1 - \\delta}{1 - (1 - e)^s} \\end{gather} \\] The threshold \\(t\\) is usually set as the median absolute deviation of \\(y\\).\nExternal Material 随机抽样一致算法（Random sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)\n","date":1650472711,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650472711,"objectID":"87967618cd135ba7baa60243cf5eea00","permalink":"https://chunxy.github.io/notes/articles/machine-learning/ransac/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/ransac/","section":"notes","summary":"Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. RANdom SAmple Consensus is a methodology to","tags":null,"title":"RANSAC","type":"book"},{"authors":null,"categories":null,"content":" Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)). We seek to obtain a scalar \\(z\\) by projecting \\(x\\) onto a unit vector \\(v\\): \\(z^{(i)} = v^Tx^{(i)}\\). Of all the possibilities we would like to select the one that maximizes the separability of the scalars between classes.\nThe mean of each class in the \\(x\\)-space and the \\(z\\)-space is \\[ \\mu_c = \\frac{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]x^{(i)}}{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]} \\\\ \\tilde \\mu_c = \\frac{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]v^T x^{(i)}}{\\sum_{i=1}^M\\mathbb I[y^{(i)} = c]} = v^T\\mu_c\\\\ \\] The scatter of each class in the \\(x\\)-space and the \\(z\\)-space is defined as \\[ \\begin{aligned} S_c \u0026amp;= \\sum_{i=1}^M \\mathbb I[y^{(i)} = c](x^{(i)} - \\mu_c)(x^{(i)} - \\mu_c)^T \\\\ \\tilde s^2_c \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c](z^{(i)} - \\tilde u_c)^2 \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)} - u_c) \\\\ \u0026amp;= \\sum_{i=1}^M\\ \\mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} - u_c)^Tv \\\\ \u0026amp;= v^TS_cv \\end{aligned} \\] FLD suggests in \\(z\\)-space maximizing the distance among the means of classes (inter-class scatter) and minimizing the variance over each class (within-class scatter), i.e. \\[ \\begin{aligned} \\max_v J(v) \u0026amp;\\coloneq \\frac{(\\tilde \\mu_1 - \\tilde \\mu_2)^2}{\\tilde s^2_1 + \\tilde s^2_2} \\\\ \u0026amp;= \\frac{(v^T\\mu_1 - v^T\\mu_2)^2}{v^TS_1v + v^TS_2v} \\\\ \u0026amp;= \\frac{v^T(\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^Tv}{v^T(S_1 + S_2)v} \\\\ \\end{aligned} \\] \\(S_W = S_1 + S_2\\) is called within-class scatter, \\(S_B = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^T\\) is called inter-class scatter. Then, \\[ J(v) = \\frac{v^TS_Bv}{v^TS_Wv} \\\\ \\]\nTake derivative w.r.t. \\(v\\) and make it zero to give \\[ \\begin{aligned} \\frac{\\partial J}{\\partial v} \u0026amp;= v^TS_Wv \\frac{\\partial v^TS_Bv}{\\partial v} - v^TS_Bv \\frac{\\partial v^TS_Wv}{\\partial v}\u0026amp; \\\\ \u0026amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv \\\\ \\end{aligned} \\]\n\\[ \\begin{aligned} (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv \u0026amp;= 0 \\\\ S_Bv - \\frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv \u0026amp;= 0 \\\\ S_Bv \u0026amp;= J(v) S_Wv \\end{aligned} \\]\nIf \\(S_W\\) is invertible, \\(v\\) is some eigenvector of \\(S^{-1}_WS_B\\): \\[ S^{-1}_WS_Bv = J(v) v \\]\n\\(S_Bv = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^Tv = \\alpha(\\mu_1 - \\mu_2)\\) always points to the same direction as \\((\\mu_1 - \\mu_2)\\). Thus we can immediately give a \\(v\\) and verify: \\[ v = S^{-1}_W(\\mu_1 - \\mu_2) \\] \\[ S^{-1}_WS_B\\underbrace{S^{-1}_W(\\mu_1 - \\mu_2)}_v = S^{-1}_W\\alpha(\\mu_1 - \\mu_2) = \\underbrace{\\alpha}_{J(v)} \\underbrace{S^{-1}_W(\\mu_1 - \\mu_2)}_v \\\\ \\]\nMulti-class ","date":1641563438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641563438,"objectID":"da6eeb056fd655413d43b9cc90763227","permalink":"https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/fishers-linear-discriminant/","section":"notes","summary":"Two-class Assume we have a set of \\(N\\)-dimensional samples \\(D = \\{x^{(i)}\\}^M_{i=1}\\) , \\(M_1\\) of which belong to Class \\(1\\), and \\(M_2\\) to Class \\(2\\) (\\(M_1 + M_2 = M\\)).","tags":null,"title":"Fisher's Linear Discriminant","type":"book"},{"authors":null,"categories":null,"content":" The notation used is as follows:\n    Symbol Notation    \\(\\mathcal D\\) the dataset  \\(x\\) the sample  \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise  \\(y\\) the real value of \\(x\\)  \\(\\bar y\\) the mean of the real values  \\(f\\) the model learned with \\(\\mathcal D\\)  \\(f(x)\\) the prediction of \\(f\\) with \\(x\\)  \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\)  \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error    By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026amp;[l(f(x), y_\\mathcal D)] = E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026amp;\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y)^2] + E[(y - y_\\mathcal D)^2] + 2E[(\\bar f(x) - y)(y - y_\\mathcal D)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026amp;\\quad + 2E[\\bar f(x) - y] \\underbrace{E[y - y_\\mathcal D]}_0 \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] + 2E[(\\bar f(x) - \\bar y)(\\bar y - y)] \\\\ \u0026amp;= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] \\\\ \u0026amp;\\quad + 2E[\\bar f(x) - \\bar y]\\underbrace{E[\\bar y - y]}_0 \\\\ \u0026amp;= \\underbrace{E[(f(x) - \\bar f(x))^2]}_{variance} + \\underbrace{E[(\\bar f(x) - \\bar y)^2]}_{bias^2} + \\underbrace{E[(y - y_\\mathcal D)^2]}_{noise} + \\underbrace{E[(\\bar y - y)^2]}_{scatter} \\\\ \\end{aligned} \\] 5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium\n","date":1641562759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641562759,"objectID":"3dc8edadde35832274ff1a6f19b70d23","permalink":"https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/bias-variance-decomposition/","section":"notes","summary":"The notation used is as follows:\n    Symbol Notation    \\(\\mathcal D\\) the dataset  \\(x\\) the sample  \\(y_\\mathcal D\\) the observation of \\(x\\) in \\(\\mathcal D\\), affected by noise  \\(y\\) the real value of \\(x\\)  \\(\\bar y\\) the mean of the real values  \\(f\\) the model learned with \\(\\mathcal D\\)  \\(f(x)\\) the prediction of \\(f\\) with \\(x\\)  \\(\\bar f(x)\\) the expectation of prediction of \\(f\\) with \\(x\\)  \\(l(f(x), y_\\mathcal D)\\) the loss function, chosen to be squared error    By assuming that the observation errors averages to \\(0\\), the expectation of the error will be \\[ \\begin{aligned} E_{x \\sim \\mathcal D}\u0026[l(f(x), y_\\mathcal D)] = E[(f(x) - y_\\mathcal D)^2] = E\\{[(f(x) - \\bar f(x) + (\\bar f(x) - y_\\mathcal D)]^2\\} \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y_\\mathcal D)^2] + 2E[(f(x) - \\bar f(x))(\\bar f(x) - y_\\mathcal D)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E\\{[(\\bar f(x) - y) + (y - y_\\mathcal D)]^2\\} \\\\ \u0026\\quad + 2\\underbrace{E[f(x) - \\bar f(x)]}_0 E[\\bar f(x) - y_\\mathcal D] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(\\bar f(x) - y)^2] + E[(y - y_\\mathcal D)^2] + 2E[(\\bar f(x) - y)(y - y_\\mathcal D)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026\\quad + 2E[\\bar f(x) - y] \\underbrace{E[y - y_\\mathcal D]}_0 \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E\\{[(\\bar f(x) - \\bar y) + (\\bar y - y)]^2\\} \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] + 2E[(\\bar f(x) - \\bar y)(\\bar y - y)] \\\\ \u0026= E[(f(x) - \\bar f(x))^2] + E[(y - y_\\mathcal D)^2] + E[(\\bar f(x) - \\bar y)^2] + E[(\\bar y - y)^2] \\\\ \u0026\\quad + 2E[\\bar f(x) - \\bar y]\\underbrace{E[\\bar y - y]}_0 \\\\ \u0026= \\underbrace{E[(f(x) - \\bar f(x))^2]}_{variance} + \\underbrace{E[(\\bar f(x) - \\bar y)^2]}_{bias^2} + \\underbrace{E[(y - y_\\mathcal D)^2]}_{noise} + \\underbrace{E[(\\bar y - y)^2]}_{scatter} \\\\ \\end{aligned} \\] 5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium","tags":null,"title":"Bias-variance Decomposition","type":"book"},{"authors":null,"categories":null,"content":" 设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，那么\\(f(X)\\)的期望以及分布情况会是什么样的呢？\n我们这里只讨论\\(f\\)是单调函数的情况，令\\(Y = f(X)\\)，那么 \\[ P_Y(y) = P_Y(Y \\le y) = P_X(f(X) \\le y) \\]\n 若\\(f\\)单调递增， \\[ \\begin{gather} P_Y(f(X) \\le y) = P_X(X \\le f^{-1}(y)) = P_X(f^{-1}(y)) \\\\ \\nonumber \\\\ \\begin{split} p_Y(y) \u0026amp;= \\frac{\\partial P_Y(Y \\le y)}{\\partial y} \\\\ \u0026amp;= \\frac{\\partial P_X(f^{-1}(y))}{\\partial y} \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot (f^{-1})^\\prime (y) \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)| \\\\ \\end{split} \\end{gather} \\]\n 若\\(f\\)单调递减， \\[ \\begin{gather} P_Y(f(X) \\le y) = P_X(X \\ge f^{-1}(y)) = 1 - P_X(X \\le f^{-1}(y)) = 1 - P_X(f^{-1}(y)) \\\\ \\nonumber \\\\ \\begin{split} p_Y(y) \u0026amp;= \\frac{\\partial P_Y(Y \\le y)}{\\partial y} \\\\ \u0026amp;= \\frac{\\partial [1 - P_X(f^{-1}(y))]}{\\partial y} \\\\ \u0026amp;= -p_X(f^{-1}(y)) \\cdot (f^{-1})^\\prime (y) \\\\ \u0026amp;= p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)| \\\\ \\end{split} \\end{gather} \\]\n  总而言之，\\(p_Y(y) = p_X(f^{-1}(y)) \\cdot |(f^{-1})^\\prime (y)|\\)。\n","date":1648721304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648721304,"objectID":"c1f19050491a0688b653fc4bc963d7c1","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/","section":"notes","summary":"设\\(X\\)为一一维随机变量，\\(f: \\R \\to \\R\\)为一函数，","tags":null,"title":"随机变量的函数","type":"book"},{"authors":null,"categories":null,"content":" Real Symmetric Matrix Let \\(A\\) be an \\(n \\times n\\) real-valued symmetric matrix. We have its properties as follows.\nReal-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued. Suppose by contradiction that \\(A\\) has some imaginary eigenvalue \\(\\lambda\\) and the corresponding imaginary eigenvector \\(x\\). We have \\[ \\begin{aligned} A x \u0026amp;= \\lambda x \\\\ A(x_\\text{real} + x_\\text{img}) \u0026amp;= (\\lambda_\\text{real} + \\lambda_\\text{img})(x_\\text{real} + x_\\text{img}) \\\\ A x_\\text{real} + A x_\\text{img} \u0026amp;= (\\lambda_\\text{real} x_\\text{real} + \\lambda_\\text{img} x_\\text{img}) + (\\lambda_\\text{real} x_\\text{real} + \\lambda_\\text{img} x_\\text{real}) \\\\ \\end{aligned} \\] Denoting \\(\\lambda\\)’s and \\(x\\)’s complex conjugate by \\(\\bar \\lambda\\) and \\(\\bar x\\) respectively, we have \\[ \\begin{gather} \\left. \\begin{aligned} \u0026amp;A\\bar x = A x_\\text{real} - A x_\\text{img} \\\\ \u0026amp;= (\\lambda_\\text{real} x_\\text{real} + \\lambda_\\text{img} x_\\text{img}) \\\\ \u0026amp;\\quad- (\\lambda_\\text{real} x_\\text{real} + \\lambda_\\text{img} x_\\text{real}) \\\\ \u0026amp;= \\bar \\lambda \\bar x \\end{aligned} \\right\\} \\Rightarrow \\begin{aligned} (A\\bar x)^T \u0026amp;= (\\bar \\lambda \\bar x)^T \\\\ \\bar x^T A^T \u0026amp;= \\bar \\lambda \\bar x^T \\\\ \\bar x^T A \u0026amp;= \\bar \\lambda \\bar x^T \\end{aligned} \\end{gather} \\] Left-multiply \\(\\bar x^T\\) on both sides of \\(Ax = \\lambda x\\) to give: \\[ \\bar x^TAx = \\bar x^T \\lambda x = \\lambda \\bar x^T x \\] Right-multiply \\(x\\) on both side of \\(\\bar x^TA = \\bar \\lambda \\bar x^T\\) to give: \\[ \\bar x^TAx = \\bar \\lambda \\bar x^T x \\] Therefore \\(\\bar \\lambda \\bar x^T x = \\lambda \\bar x^T x\\). Since \\(\\bar x^Tx\\) is real-value, \\(\\bar \\lambda = \\lambda\\). In other words, \\(\\lambda\\) is real-valued and thus so is \\(x\\).\nSum of Real Symmetric Matrices Let \\(A\\) and \\(B\\) be two real symmetric matrices. Let \\(\\lambda^-, \\lambda^+\\) be \\(A\\)’s smallest and largest eigenvalue of \\(A\\), \\(\\mu^-, \\mu^+\\) be \\(B\\)’s smallest and largest eigenvalue. Denote \\(\\gamma^-, \\gamma^+\\) as \\(A+B\\)’s smallest and largest eigenvalue. Then it can be derived that \\[ \\lambda^- + \\mu^- \\le \\gamma^- \\le \\gamma^+ \\le \\lambda^+ + \\mu^+ \\] ### Orthogonal Eigenvectors\nIts eigenvectors corresponding to different eigenvalues are orthogonal. Arbitrarily taking \\(A\\)’ s two eigenvectors \\(v_1, v_2\\) and their eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_1 \\ne \\lambda_2\\), we have\n\\[ \\begin{align} \u0026amp;\\begin{aligned} (Av_1)^Tv_2 \u0026amp;= v_1^TA^Tv_2 \\\\ \u0026amp;= v_1^TAv_2 \\\\ \u0026amp;= v_1^T \\lambda_2v_2 \\\\ \u0026amp;= \\lambda_2v_1^Tv_2 \\\\ \\end{aligned} \\\\ \u0026amp;\\begin{aligned} (Av_1)^Tv_2 \u0026amp;= \\lambda_1v_1^Tv_2 \\end{aligned} \\end{align} \\]\nTherefore, \\[ \\begin{aligned} \\lambda_1v_1^Tv_2 \u0026amp;= \\lambda_2v_1^Tv_2 \\\\ (\\lambda_1 - \\lambda_2)v_1^Tv_2 \u0026amp;= 0 \\end{aligned} \\]\nSince \\(\\lambda_1 \\ne \\lambda_2\\), we have \\(v_1^Tv_2 = 0\\) and thus \\(v_1\\) and \\(v_2\\) are orthogonal.\nDiagonalizable It has \\(n\\) independent eigenvectors and thus diagonalizable. To show it, eigenvectors in different eigenspaces are orthogonal and thus linearly independent; and eigenvectors in the same eigenspace are also linearly independent because they form the basis of this eigenspace.\n“Easily Invertible” Further on the diagonalizable property, suppose \\(A = P\\Lambda P^{-1}\\). If \\(A\\) is not invertible, by the relation between the matrix rank and the eigenvalues, some of \\(\\Lambda\\)’s entries on the diagonal are zero. By adding \\(A\\) with \\(\\lambda I\\), we have \\[ \\begin{aligned} A^\\prime \u0026amp;= P\\Lambda P^{-1} + \\lambda PIP^{-1} \\\\ \u0026amp;= P(\\Lambda + \\lambda I)P^{-1} \\end{aligned} \\] where \\(\\lambda I\\) complements all the zero entries on the diagonal of \\(\\Lambda\\). Thus \\(A^\\prime\\) has \\(n\\) nonzero eigenvalues and is invertible. A singular symmetric matrix \\(A\\) becomes invertible by adding \\(\\lambda I\\).\nOrthogonally Diagonalizable Its diagonalization can be in the form of \\(A = P\\Lambda P^T\\), where \\(P^TP = I\\), by properly selecting the orthonormal eigenvectors.\nEigenvectors from different eigenspaces are already orthogonal. Eigenvectors from the same eigenspace are independent but not necessarily orthogonal. However, the linear combination of these homo-spatial independent eigenvectors is still an eigenvector. Thus we can apply the Gram-Schmidt process to these eigenvectors and obtain the orthogonal basis for this eigenspace.\nFinally, we pull together all the orthogonal eigenvectors, normalize them to unit vector, and get the orthonormal matrix \\(P\\).\nOrthogonal diagonalization is diagonalization, as well as SVD. In fact, an \\(n \\times n\\) matrix \\(A\\) is orthogonally diagonalizable if and only if \\(A\\) is a symmetric matrix. Such orthogonal diagonalization is also referred to as spectral decomposition.\nCommutativity If the product of two symmetric matrices \\(A\\) and \\(B\\) is symmetric, then \\(A\\) and \\(B\\) commute, i.e. \\(AB = BA\\). This is simply because \\[ \\begin{aligned} A B \u0026amp;= (A B)^T \u0026amp;\u0026amp;\\iff \\\\ A B \u0026amp;= B^T A^T \u0026amp;\u0026amp;\\iff \\\\ A B \u0026amp;= B A \\end{aligned} \\]\nCovariance Matrix Covariance matrix is a special kind of real symmetric matrix. …","date":1643550372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643550372,"objectID":"d8eac528181390c4eaaa3a11fc1441e9","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/","section":"notes","summary":"Real Symmetric Matrix Let \\(A\\) be an \\(n \\times n\\) real-valued symmetric matrix. We have its properties as follows.\nReal-valued Eigenvalues and Eigenvectors Its eigenvalues and thus eigenvectors are real-valued.","tags":null,"title":"Real Symmetric Matrix","type":"book"},{"authors":null,"categories":null,"content":" Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\). By best approximation we mean to minimize the \\(||Ax - b||\\) over all \\(x \\in \\R^n\\).\nThe best approximation can be achieved when \\(Ax = \\hat b = \\mathop{proj}_{Col(A)}b\\).\nInstead of finding a orthogonal basis for \\(A\\), computing \\(\\hat b\\) and then solving \\(Ax = \\hat b\\), we can derive \\(x\\) in this way: \\[ \\begin{gather} (b - \\hat b) \\perp Col(A) \\iff (b - \\hat b) \\in Nul(A^T)\\iff A^T(b - \\hat b) = 0 \\\\ A^T(b - Ax) = 0 \\\\ A^TAx = A^Tb \\label{solution} \\end{gather} \\]\nWe will show that if columns of \\(A\\) are independent, then the least-square solution \\(\\hat x\\) is uniquely given by \\((A^TA)^{-1}A^Tb\\)\nFirstly, \\(Nul(A) = Nul(A^TA)\\). This is because \\[ \\begin{gather} Ax = 0 \\Rightarrow A^TAx = A^T0 = 0 \\\\ A^TAx = 0 \\iff x^TA^TAx = 0 \\iff (Ax)^TAx = 0 \\Rightarrow Ax = 0 \\end{gather} \\] When columns of \\(A\\) are independent, \\(Nul(A) = 0\\) so that \\(Nul(A^TA) = 0\\), which indicates that equation \\(\\eqref{solution}\\) has the unique solution. Conversely, as an aside, when \\(A^TA\\) is invertible, \\(Nul(A^TA) = 0\\) so that \\(Nul(A) = 0\\), which indicates that columns of \\(A\\) are independent.\n","date":1641560025,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641560025,"objectID":"53c0e3d273d6c8d7d6004f30e0a2c177","permalink":"https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/linear-algebra-and-its-applications/least-squares/","section":"notes","summary":"Suppose we are solving the \\(Ax = b\\) problem. \\(b\\) does not always lie in the column space of \\(A\\). However, we can try to find within \\(A\\)’s column space a vector \\(\\hat x\\) such that \\(A\\hat x\\) best approximates \\(b\\).","tags":null,"title":"Least Squares","type":"book"},{"authors":null,"categories":null,"content":" 前置知识 Chebyshev不等式  定理\n设随机变量\\(X\\)的期望\\(\\E(X)\\)及方差\\(\\Var(X)\\)存在，则对于任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ P(|X-\\E(X)| \\ge \\epsilon) \\le \\frac{\\Var(X)}{\\epsilon^2} \\]\n 证明\n\\(X\\)为连续型随机变量 \\[ \\begin{aligned} \u0026amp;P(|X-\\E(X)| \\ge \\epsilon) = \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} p(x)dx \\\\ \u0026amp;\\le \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} \\bigg( \\frac{X - \\E(x)}{\\epsilon} \\bigg)^2 p(x)dx \\\\ \u0026amp;= \\frac{1}{\\epsilon^2} \\mathop \\int_{|x - \\E(x)| \\ge \\epsilon} \\big( X - \\E(x) \\big)^2 p(x)dx \\\\ \u0026amp;\\le \\frac{1}{\\epsilon^2} \\mathop \\int_{x \\in X} \\big( X - \\E(x) \\big)^2 p(x)dx \\\\ \u0026amp;= \\frac{\\Var(X)}{\\epsilon^2} \\\\ \\end{aligned} \\]\n \\(X\\)为离散型随机变量 \\[ \\begin{aligned} \u0026amp;P(|X-\\E(X)| \\ge \\epsilon) = \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} P(x) \\\\ \u0026amp;\\le \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} \\bigg( \\frac{x - \\E(x)}{\\epsilon} \\bigg)^2 P(x) \\\\ \u0026amp;= \\frac{1}{\\epsilon^2} \\mathop \\sum_{|x - \\E(x)| \\ge \\epsilon} \\big( x - \\E(x) \\big)^2 P(x) \\\\ \u0026amp;\\le \\frac{1}{\\epsilon^2} \\mathop \\sum_{x \\in X} \\big( x - \\E(x) \\big)^2 P(x) \\\\ \u0026amp;= \\frac{\\Var(X)}{\\epsilon^2} \\\\ \\end{aligned} \\]\n   Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是作为证明其它大数定理的基础工具。\n依概率收敛 随机变量序列即是由随机变量构成。对于一个普通数列\\(\\{x_n\\}\\)来说，若其收敛于\\(c\\)，则当\\(n\\)充分大时，\\(x_n\\)和\\(c\\)的距离可以达到任意小。而随机变量序列\\(X_1, X_2, \\dots\\)的极限却不能按照这样定义，因为\\(X_n\\)取值不确定，不可能和某个数字\\(c\\)的距离任意小。\n随机变量是事件的映射，当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。\n 定义\n设\\(X_1, X_2, \\dots,\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \u0026lt; \\epsilon) = 1\\)，则称该随机变量序列依概率收敛于\\(c\\)，记作\\(X_n \\stackrel{P}{\\to} c\\)。或者，对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \\ge \\epsilon) = 0\\)。 ### Markov不等式\n  Chebyshev不等式其实是Markov不等式的一个特例。令\\(X\\)为一非负随机变量、\\(\\alpha\\)为一非负实数，Markov不等式描述的是以下关系： \\[ P(X \\ge \\alpha) \\le \\frac{\\E(X)}{\\alpha} \\] 以连续型随机变量为例，证明如下： \\[ P(X \\ge \\alpha) = \\int_{x \\ge \\alpha} p(x) \\d x \\le \\int_{x \\ge \\alpha} \\frac{x}{\\alpha} p(x) \\d x \\le \\int_x \\frac{x}{\\alpha} p(x) \\d x = \\frac{\\E(X)}{\\alpha} \\] 由于\\(|X - \\E(x)| \\ge \\epsilon \\iff (X - \\E(X))^2 \\ge \\epsilon^2\\)，将Markov不等式中的的\\(X\\)替换为\\((X - \\E(X))^2\\)、\\(\\alpha\\)替换为\\(\\epsilon^2\\)，即可得到Chebyshev不等式。不过由于Markov不等式有随机变量非负的要求，适用范围就小了一些；而且同样，Markov不等式的这种估计也是很粗糙的。\n弱大数定律（Weak Law of large numbers） Chebyshev大数定律  定理\n设随机变量序列\\(X_1,X_2,\\dots\\)两两不相关，若存在常数\\(c\\)，使得\\(\\Var(X_i) \\le c \\ne +\\infty, i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \u0026lt; \\epsilon) = 1 \\] 亦即\\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{P}{\\to} \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)\\)。\n 证明\n由于该随机序列两两不相关，故根据期望及方差的性质， \\[ \\E(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n} \\sum_{i=1}^n \\E(X_i),\\quad \\Var(\\frac{1}{n} \\sum_{i=1}^N X_i) = \\frac{1}{n^2} \\sum_{i=1}^n \\Var(X_i) \\le \\frac{c}{n} \\] 根据Chebyshev不等式， \\[ \\begin{gather} 0 \\le P(|\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) \u0026lt; \\frac{\\Var(\\frac{1}{n} \\sum_{i=1}^N X_i)}{\\epsilon^2} \\le \\frac{c}{n \\epsilon} \\\\ \\underbrace{\\lim_{n \\to \\infty} 0}_0 \\le \\lim_{n \\to \\infty} P(\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) \\le \\underbrace{\\lim_{n \\to \\infty} \\frac{c}{n \\epsilon}}_0 \\Rightarrow\\\\ \\lim_{n \\to \\infty} P(\\frac{1}{n} \\sum_{i=1}^n X_i - \\frac{1}{n} \\sum_{i=1}^n \\E(X_i)| \\ge \\epsilon) = 0 \\end{gather} \\]\n  Khinchin大数定律 相互独立同分布大数定律  设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu, \\Var(X_i) = \\sigma^2 \\ne \\infty, i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P (|\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu| \u0026lt; \\epsilon) = 1 \\]  相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家Khinchin证明该定律依然成立，即：\n 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P (|\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu| \u0026lt; \\epsilon) = 1 \\]  Bernoulli大数定律  随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(X_i \\sim B(1,p), i=1,2,\\dots\\)，则对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\frac{1}{n} \\sum_{i=1}^n X_i - p| \u0026lt; \\epsilon) = 1 \\]  显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。\n中心极限定理 注意，在本节中，我们用\\(\\Phi\\)表示标准正态分布的分布函数。\nLindburg-Levy中心极限定理 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，若\\(\\E(X_i) = \\mu, \\Var(X_i) = \\sigma^2 \\ne \\infty, i=1,2,\\dots\\)，则对任意实数\\(x\\)，有 \\[ \\lim_{n \\to \\infty} P(\\frac{\\sum_{i=1}^n X_i - n \\mu}{\\sqrt n \\sigma} \\le x) = \\Phi(x) \\]\nde Moivre-Laplace中心极限定理 设随机变量序列\\(X_1, X_2, \\dots\\)相互独立且同分布，且\\(X_i \\sim B(1,p), i=1,2,\\dots\\)，则对任意实数\\(x\\)，有 \\[ \\lim_{n \\to \\infty} P(\\frac{\\sum_{i=1}^n X_i - np}{\\sqrt{np(1-p)}} \\le x) = \\Phi(x) \\] 显然de Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。\n前面的Bernoulli大数定律告诉我们可以用\\(\\frac{1}{n} \\sum_{i=1}^n X_i\\)（频率）近似\\(p\\)（概率），而至于近似程度如何，却不得而知。de Moivre-Laplace中心极限定理则告诉我们当\\(n\\)足够大时，近似程度如何： \\[ P(|\\frac{1}{n}\\sum_{i=1}^n X_i - p| \\le \\epsilon) = P(|\\frac{\\sum_{i=1}^n X_i - np}{\\sqrt{np(1-p)}}| \\le \\frac{\\sqrt n \\epsilon}{\\sqrt{p(1-p)}}) \\simeq 2\\Phi(\\frac{\\sqrt n \\epsilon}{\\sqrt{p(1-p)}}) - 1 \\]\n上式实际是在用正态分布近似二项分布（多个伯努利分布随机变量加和为伯努利分布），比如在Galton Board游戏中，我们就可以应用de Moivre-Laplace中心极限定理来近似实际概率。\n","date":1653038853,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653038853,"objectID":"2452e7b6b1863a32e0e9c71664055c9b","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/","section":"notes","summary":"前置知识 Chebyshev不等式 定理 设随机变量\\(X\\)的期","tags":null,"title":"大数定律和中心极限定理","type":"book"},{"authors":null,"categories":null,"content":" \\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由正态分布衍生而来，常见统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。\n前置知识 Gamma函数 \\[ \\Gamma (x) = \\int_0^{+\\infty} e^{-t} t^{x-1} dt \\quad (x \u0026gt; 0) \\]\n\\(\\Gamma\\)函数具有\\(\\Gamma(x + 1) = x \\Gamma(x)\\)的性质：\n\\[ \\Gamma (x+1) = \\int_0^{+\\infty} e^{-t} t^{x} dt = [-e^{-t} t^x] \\bigg|^{+\\infty}_{t=0} - \\int_0^{+\\infty} -e^{-t} xt^{x-1} dt \\] 根据洛必达法则，\\(\\lim_{t \\to +\\infty} = \\frac{-t^x}{e^t} = \\lim_{t \\to +\\infty} \\frac{x!}{e^t} = 0\\)，故 \\[ \\Gamma (x+1) = 0 + x\\int_0^{+\\infty} e^{-t} t^{x-1} dt = x \\Gamma(x) \\]\n \\(\\Gamma(1) = \\int_0^{+\\infty} e^{-t} dt = 1\\)，故\\(x\\)为正整数时，\\(\\Gamma(x) = x!\\)；\n \\(\\Gamma(1/2) = \\sqrt \\pi\\)，故\\(x = 2k + 1\\)为正奇数时，\\(\\Gamma(\\frac{x}{2}) = \\sqrt \\pi \\prod_{i=0}^{k-1} \\frac{2 * i + 1}{2}\\)：\n\\[ \\begin{gather} \\Gamma (\\frac{1}{2}) = \\int_0^{+\\infty} e^{-t} t^{-\\frac{1}{2}} \\d t \\stackrel{u = t^\\frac{1}{2}}{\\Longrightarrow} \\int_0^{+\\infty} e^{-u^2} u^{-1} \\;2u \\d u = 2 \\int_0^{+\\infty} e^{-u^2} \\d u = \\int_{-\\infty}^{+\\infty} e^{-u^2} \\d u \\\\ \\notag \\\\ \\begin{aligned} \u0026amp;\\Gamma^2(\\frac{1}{2}) = (\\int_{-\\infty}^{+\\infty} e^{-u^2} \\d u)^2 \\\\ \u0026amp;= (\\int_{-\\infty}^{+\\infty} e^{-u^2} du)(\\int_{-\\infty}^{+\\infty} e^{-v^2} \\d v) \\\\ \u0026amp;= \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} e^{-(u^2+v^2)} \\d u \\d v \\\\ \u0026amp;\\downarrow_{u = r\\sin\\theta, v = r\\cos\\theta} \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} e^{-r^2}\\; r \\d r \\d \\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} [-\\frac{1}{2}e^{-r^2}] \\bigg|_{r=0}^{+\\infty} \\d \\theta \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\frac{1}{2} \\d \\theta \\\\ \u0026amp;= \\pi \\\\ \u0026amp;\\Gamma (\\frac{1}{2}) = \\sqrt \\pi \\end{aligned} \\end{gather} \\]\n  参考 The Gamma Function || The derivation of \\(\\Gamma(1/2)\\)\n\\(\\chi^2\\)分布 设\\(X_1,\\dots,X_n\\)为相互独立的标准正态分布随机变量，即\\(X_i \\sim N(0,1)\\)则称\\(Y = X_1^2 + \\dots + X_n^2\\)服从自由度为\\(n\\)的\\(\\chi^2\\)分布，记作\\(Y \\sim \\chi^2(n)\\)，其中\\(\\E[Y] = n, \\Var[Y] = 2n\\)。\n \\(n=1\\)时，容易得到\\(\\forall y \\le 0, P_Y(y) = 0, p_Y(y)= 0\\)， $$ \\[\\begin{align} \\begin{split} \\forall y \u0026gt; 0, P_Y(y) \u0026amp;= 2P_X(\\sqrt y) - 1 \\\\ \\end{split} \\\\ \\begin{split} \u0026amp;p_Y(y) = 2P_X\u0026#39;(\\sqrt y) \\frac 1 {2 \\sqrt y} \\\\ \u0026amp;= \\frac 1 {\\sqrt {2\\pi y}} e^{-\\frac 1 2 y} \\\\ \u0026amp;= \\frac 1 {2^\\frac{1}{2} \\Gamma(\\frac 1 2)} y^{\\frac 1 2 - 1} e^{-\\frac y 2} \\end{split} \\end{align}\\] \\[ $\\chi^2(1)$分布的密度函数为： \\] p_Y(y) =\n\\[\\begin{cases} \\frac 1 {2^\\frac{1}{2} \\Gamma(\\frac 1 2)} y^{\\frac 1 2 - 1} e^{-\\frac y 2}, \u0026amp;y \u0026gt; 0 \\\\ 0, \u0026amp; y \\le 0 \\end{cases}\\] $$\n \\(n=k\\)时，令\\(X_1,\\dots,X_k\\)表示一个\\(k\\)维空间中的点， \\[ \\begin{aligned} p_Y(y) = P_Y(Y \\le y) \u0026amp;= \\int_\\mathcal V \\prod_{i=1}^k N(0,1,x_i)\\; \\d x_1 \\dots \\d x_k \\\\ \u0026amp;= \\int_\\mathcal V \\frac{e^{-\\frac 1 2 (x_1^2 + \\dots + x_k^2)}} {(2\\pi)^{k / 2}}\\ \\d x_1 \\dots \\d x_k \\\\ \\end{aligned} \\] 其中\\(\\mathcal V\\)表示\\(\\sum_{i=1}^k x_i^2 \\le y\\)的积分区域。可以看出，\\(\\mathcal V\\)对应一个\\(k\\)维球体，且其半径\\(R = \\sqrt y\\)。对此，作高维球坐标变换： \\[ \\begin{aligned} \u0026amp;P_Y(Y \\le y) = \\int_\\mathcal V \\prod_{i=1}^k N(0,1,x_i)\\; \\d x_1 \\dots \\d x_k = \\int_\\mathcal V \\frac{e^{-\\frac 1 2 (x_1^2 + \\dots + x_k^2)}} {(2\\pi)^{k / 2}}\\ \\d x_1 \\dots \\d x_k \\\\ \u0026amp;= \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\int_0^\\sqrt{y} \\\\ \u0026amp;\\quad\\quad\\quad\\frac{e^{-\\frac 1 2 (r^2\\cos^2 \\varphi_1 + r^2\\sin^2 \\varphi_1 \\cos^2 \\varphi_2 + \\dots + r^2\\sin^2 \\varphi_1 \\dots \\sin^2 \\varphi_{k-2} \\cos^2 \\varphi_{k-1} + r^2\\sin^2 \\varphi_1 \\dots \\sin^2 \\varphi_{k-2} \\sin^2 \\varphi_{k-1})} } {(2\\pi)^{k / 2}}\\\\ \u0026amp;\\quad\\quad\\quad r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\ \\d r\\ \\d \\varphi_1 \\dots \\d \\varphi_k \\\\ \u0026amp;= \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\int_0^\\sqrt{y} \\frac{e^{-\\frac 1 2 r^2}} {(2\\pi)^{k / 2}} r^{k-1} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2}) \\ \\d r\\ \\d \\varphi_1 \\dots \\d \\varphi_k \\\\ \u0026amp;= \\int_0^\\sqrt{y} \\underbrace{ \\int_0^{2\\pi} \\underbrace{\\int_0^\\pi \\dots \\int_0^\\pi}_{k-2} \\frac{1} {(2\\pi)^{k / 2}} \\sin(\\varphi_1)^{k-2} \\sin(\\varphi_2)^{k-3} \\dots \\sin(\\varphi_{k-2})\\ \\d \\varphi_1 \\dots \\d \\varphi_k}_{c_k} e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \u0026amp;= c_k \\int_0^\\sqrt{y} e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \\end{aligned} \\]\n其中\\(c_k\\)是和\\(k\\)相关的常数项，并且由于\\(P_Y(Y \\le \\infty) = 1\\)，有 \\[ \\begin{aligned} 1 \u0026amp;= c_k \\int_0^\\infty e^{-\\frac 1 2 r^2} r^{k-1} \\d r \\\\ \u0026amp;\\Downarrow_{r = \\sqrt{2t}} \\\\ 1 \u0026amp;= c_k \\int_0^\\infty e^{-t} \\sqrt{2t}^{k-1} \\frac{1}{\\sqrt{2t}} \\d t \\\\ 1 \u0026amp;= 2^{(k-2)/2} c_k \\int_0^\\infty e^{-t} t^{(k-2)/2} \\d t \\\\ 1 \u0026amp;= 2^{(k-2)/2} c_k \\Gamma(\\frac{k}{2}) \\\\ c_k \u0026amp;= \\frac{1} {2^{(k-2)/2} \\Gamma(\\frac{k}{2})} \\end{aligned} \\]\n故可得密度函数： \\[ \\begin{aligned} \u0026amp;p_Y(y) = \\frac{\\d P_Y(Y \\le y)}{\\d y} \\\\ \u0026amp;= \\frac{\\d [c_k \\int_0^\\sqrt{y} e^{-\\frac 1 2 r^2} r^{k-1} \\d r]}{\\d y} \\\\ \u0026amp;= \\frac{1} {2^{(k-2)/2} \\Gamma(\\frac{k}{2})} e^{-\\frac y 2} y^\\frac{k-1}{2} \\frac{1}{2\\sqrt y} \\\\ \u0026amp;= \\frac{1} {2^{\\frac k 2} \\Gamma(\\frac{k}{2})} e^{-\\frac y 2} y^{\\frac{k}{2} - 1} \\end{aligned} \\]\n最终可得密度函数如下： \\[ p_Y(y) = \\begin{cases} \\frac 1 {2^\\frac{n}{2} \\Gamma(\\frac n 2)} e^{-\\frac y 2} y^{\\frac n 2 - 1}, \u0026amp;y \u0026gt; 0 \\\\ 0, \u0026amp; y \\le 0 \\end{cases} \\] 另外，该密度函数也可以通过数学归纳法验证。\n  参考 Chi Squared Distribution || Generating Function of Chi Squared …","date":1657405282,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657405282,"objectID":"fce05e30e309e1e125615c419259b0b9","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/","section":"notes","summary":"\\(\\chi^2\\)分布、\\(t\\)分布、\\(F\\)分布都由","tags":null,"title":"三大分布与正态总体的抽样分布","type":"book"},{"authors":null,"categories":null,"content":" Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix. Data whitening usually has two steps: the decorrelation step and the standardization step.\nTo do it, we shall first apply the orthogonal diagonalization to \\(X\\)’s covariance matrix \\(\\Sigma_X\\): \\[ \\Sigma_X \\Phi = \\Phi \\Lambda \\] where \\(\\Phi\\) contains the normalized eigenvectors and \\(\\Phi^{-1} = \\Phi^T\\), \\(\\Lambda\\) is diagonal and contains the eigenvalues. Now let \\(Y = \\Phi^T X\\), we can verify that \\[ \\begin{aligned} \\Sigma_Y \u0026amp;= \\E \\{ \\Phi^T (\\x - \\mu_X) [\\Phi^T (\\x - \\mu_X)]^T \\} \\\\ \u0026amp;= \\E [\\Phi^T (\\x - \\mu_X) (\\x - \\mu_X)^T \\Phi] \\\\ \u0026amp;= \\Phi^T \\E [(\\x - \\mu_X) (\\x - \\mu_X)^T] \\Phi \\\\ \u0026amp;= \\Phi^T \\Sigma_X \\Phi = \\Phi^T \\Phi \\Lambda = \\Lambda \\end{aligned} \\] \\(\\Sigma_Y\\) is diagonal and we finish the decorrelation step. To further make it an identity matrix (the standardization step), we apply \\(Z = \\Lambda^{-1/2} Y = \\Lambda^{-1/2} \\Phi^T X\\) to give \\[ \\begin{aligned} \\Sigma_Z \u0026amp;= \\E \\{\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X) [\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X)]^T \\} \\\\ \u0026amp;= \\E [\\Lambda^{-1/2} \\Phi^T (\\x - \\mu_X) (\\x - \\mu_X)^T \\Phi \\Lambda^{-1/2}] \\\\ \u0026amp;= \\Lambda^{-1/2} \\Phi^T \\Sigma_X \\Phi \\Lambda^{-1/2} = I \\end{aligned} \\] The inverse of data whitening can be used to derive density function of first-order correlated random variables, e.g. for Gaussian case. Data whitening looks a lot like PCA: they both compute the eigen pairs; they both project the original data onto the basis formed by eigenvectors; they both can be solved with SVD. But unlike PCA, data whitening uses all the eigenvectors as the basis instead of \\(K\\) most prominent ones. Therefore, data whitening does not reduce the data’s dimensionality as PCA does.\n","date":1660240322,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660240322,"objectID":"decbace4652c6f21d2d203edd7df5350","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/whitening/","section":"notes","summary":"Whitening Data whitening is the process of converting a random vector \\(X\\) with only first-order correlation into a new random vector \\(Z\\) such that the covariance matrix of \\(Z\\) is an identity matrix.","tags":null,"title":"Whitening","type":"book"},{"authors":null,"categories":null,"content":" Other than block code where symbols are encoded in chunks, symbol code will assign each symbol a unique codeword. Among the codeword schemes, we prefer those where no codeword is a prefix of any other codeword. These are called prefix code.\nThe problem is how to give such scheme, or does it really exist?\nKraft-McMillan Inequality Kraft-McMillan inequality reveals the relation\nDenote the length of each symbol code as \\(l_i\\) and suppose there are \\(I\\) symbols. If \\(\\sum_{i=1}^I 2^{-l_i} \\le 1\\), then there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes’ lengths.\n Proof: The proof is done by construction. The number of codes of length \\(l\\) should be less than \\(2^{l}\\), or else the inequality will be violated. Therefore \\(\\forall l = 0,1,\\dots\\), we can loosely arrange all the codes of length \\(l\\) to be unique (since there are \\(2^l\\) many distinct bit strings of length \\(l\\)). Then the uniqueness condition is checked.\nDenote the number of codes of length \\(l\\) by \\(C_l\\). For any two consecutive lengths \\(l\\) and \\((l+1)\\), we have \\[ \\begin{aligned} 2^{-l} C_l + 2^{-(l+1)} C_{l+1} \u0026amp;\\le\\sum_{i=1}^I 2^{-l_i}\\le 1 \\\\ C_{l+1} \u0026amp;\\le 2^{l+1} - 2 C_l \\\\ C_{l+1} \u0026amp;\\le 2(2^l - C_l) \\\\ \\end{aligned} \\] This means we can append these unused \\((2^l - C_l)\\) codes of length \\(l\\) with \\(0\\) and \\(1\\) to suit the number of codes of length \\(l+1\\). Construction completes.\n  Suppose we have a set of uniquely-decodable prefix coding. Denote the length of each symbol code as \\(l_i\\) and there are \\(I\\) symbols. Then, \\[ \\sum_{i=1}^I 2^{-l_i} \\le 1 \\]\n Proof: Let \\(S = \\sum_{i=1}^I 2^{-l_i} \\le 1\\), then, \\[ \\begin{aligned} S^N \u0026amp;= (\\sum_{i=1}^I 2^{-l_i})^N \\\\ \u0026amp;= \\sum_{i_1=1}^I \\dots \\sum_{i_N=1}^I 2^{-(l_{i_1} + \\dots + l_{i_N})} \\end{aligned} \\] The \\((l_{i_1} + \\dots + l_{i_N})\\) term can be treated as the length of encoding of \\(a_{i_1} \\dots a_{i_N}\\) of arbitrary length \\(N\\). Let \\(l_\\min = \\min_i l_i, l_\\max = \\max_i l_i\\), the above can be re-written as \\[ S^N = \\sum_{l = Nl_\\min}^{Nl_\\max} 2^{-l}C_l \\] where \\(C_l\\) represents the number of symbol codes of length \\(l\\). Since the coding is uniquely-decodable, \\(C_l \\le 2^l\\). Therefore, \\[ S^N \\le \\sum_{l = Nl_\\min}^{Nl_\\max} 2^{-l} 2^l \\le Nl_\\max \\] If \\(S \u0026gt; 1\\), the above cannot hold for arbitrary \\(N\\). Therefore \\(S \\le 1\\).   Source Coding Theorem for Symbol Code For an ensemble \\(X\\), there exists a prefix code \\(C\\) with expected length satisfying \\[ H(X) \\le L(C,X) \u0026lt; H(X) + 1 \\]\n Proof: We define the implicit probabilities \\(q_i = 2^{-l_i} / z\\) where \\(z = \\sum_i 2^{-l_i}\\). Then, \\[ \\begin{aligned} L(C,X) \u0026amp;= \\sum_i p_i l_i = -\\sum_i [p_i \\log (q_iz)] \\\\ \u0026amp;=\\sum_i [p_i \\log 1/q_i] - \\log z \\\\ \u0026amp;\\ge H(X) \\end{aligned} \\] The equality holds when \\(z = 1\\) (the code is complete) and \\(q = p\\) (\\(l_i = \\log 1/p_i\\)).\nFrom another perspective, suppose the coding is complete but not optimal, \\[ \\begin{aligned} L(C,X) \u0026amp;= -\\sum_i [p_i \\log (q_iz)] = -\\sum_i [p_i \\log (q_i)] \\\\ \u0026amp;= -\\sum_i [p_i \\log (p_i)] + \\sum_i [p_i \\log (p_i)] -\\sum_i [p_i \\log (q_i)] \\\\ \u0026amp;= H(X) + D_{KL}(p || q) \\end{aligned} \\] where the cost is the extra \\(D_{KL}(p || q)\\) bits, which is brought by instead treating \\(q\\) as the real distribution. \\(D_{KL(p||q)}\\) is termed as relative entropy or the KL-divergence.\n  ","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"a9b5c5488078789d400efa072e78190e","permalink":"https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/","section":"notes","summary":"Other than block code where symbols are encoded in chunks, symbol code will assign each symbol a unique codeword. Among the codeword schemes, we prefer those where no codeword is a prefix of any other codeword.","tags":null,"title":"Symbol Code","type":"book"},{"authors":null,"categories":null,"content":" Both Cross Entropy and KL-divergence describe the relationship between two distributions.\nBoth conditional entropy and mutual information, as well as joint entropy, describe the relationship between two random variables.\nSince the notion of entropy is the basis of all other concepts in this section and the entropy is more well-defined on discrete random variable, they are more meaningful to be applied in discrete case, though they are usually trivially extended to continuous case.\nKL-divergence, Entropy and Cross Entropy By definition, \\[ \\begin{align} D_\\text{KL}(p||q) \u0026amp;= -\\mathrm{E}_{x \\sim p} \\log \\frac{q(x)}{p(x)} \\\\ \u0026amp;= -\\mathrm{E}_{x \\sim p} \\log q(x) - (-\\mathrm{E}_{x \\sim p} \\log {p(x)}) \\\\ \u0026amp;= H(p||q) - H(p) \\end{align} \\] By the convexity of \\(-\\log\\), \\[ \\begin{align} D_\\text{KL}(p||q) \u0026amp;= \\mathrm{E}_{x \\sim p} [-\\log \\frac{q(x)}{p(x)}] \\\\ \u0026amp;\\ge -\\log(\\mathrm{E}_{x \\sim p} \\frac{q(x)}{p(x)}) \\\\ \u0026amp;= 0 \\end{align} \\] That is \\[ H(p||q) \\ge H(p) \\] The equality holds if and only if \\(p = q\\).\nKL-divergence and Mutual Information Given that \\(X\\) and \\(Y\\) have the same dimension, \\[ \\begin{aligned} \u0026amp;I(X;Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\\\ \u0026amp;= \\sum_{x,y} p(x) p(y|x) \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= \\sum_{x} \\sum_{y} p(x) p(y|x) \\log \\frac{p(y|x)}{p(y)} \\\\ \u0026amp;= \\sum_{x} p(x) D_{KL}[p(y|x), p(y)] \\\\ \u0026amp;= \\E_{x} D_{KL}[p(y|x), p(y)] \\\\ \u0026amp;= \\E_{y} D_{KL}[p(x|y), p(x)] \\\\ \\end{aligned} \\]\nJoint Entropy and Conditional Entropy Joint entropy is just entropy, but with the random variable usually broken into two separate components. \\[ \\begin{aligned} \u0026amp;H(X, Y) = \\sum_{x,y} p(x,y) \\log p(x,y) \\\\ \u0026amp;= \\sum_{x,y} p(x,y) \\log p(y|x) p(x) \\\\ \u0026amp;= \\sum_{x,y} p(x,y) \\log p(y|x) \\\\ \u0026amp;\\quad + \\sum_{x,y} p(x,y) \\log p(x) \\\\ \u0026amp;= H(Y|X) + H(X) \\end{aligned} \\]\n","date":1650662010,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650662010,"objectID":"c117287a25f16f15da190d218940051d","permalink":"https://chunxy.github.io/notes/articles/information-theory/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/information-theory/overview/","section":"notes","summary":"Both Cross Entropy and KL-divergence describe the relationship between two distributions.\nBoth conditional entropy and mutual information, as well as joint entropy, describe the relationship between two random variables.","tags":null,"title":"Overview","type":"book"},{"authors":null,"categories":null,"content":" Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form:\n\\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]\nTo solve it, we try to find the eigenvalues and eigenvectors of \\(A\\): \\[ A - \\lambda I = \\left[ \\begin{array} \\\\ a_{t-1} - \\lambda \u0026amp; a_{t-2} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; -\\lambda \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; -\\lambda \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; -\\lambda \\end{array} \\right] \\] Apply Laplacian expansion along the first column to get its determinant as \\[ \\det (A - \\lambda I) = (a_{t-1} -\\lambda) (-\\lambda)^{t-1} - \\det B_{t-1} \\] where \\[ B_{t-1} = \\underbrace{ \\begin{bmatrix} a_{t-2} \u0026amp; a_{t-3} \u0026amp; \\cdots \u0026amp; a_1 \u0026amp; a_0 \\\\ 1 \u0026amp; -\\lambda \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; -\\lambda \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \u0026amp; -\\lambda \\end{bmatrix} }_\\text{$t-1$ columns} \\] Apply Laplacian expansion along the first column of \\(B_{n-1}\\) to give the following recurrence relation: \\[ \\left. \\begin{array} \\\\ \\det B_{t-1} = a_{t-2} (-\\lambda)^{t-2} - \\det B_{t-2} \\\\ \\det B_1 = a_0 \\end{array} \\right\\} \\Rightarrow \\det B_{t-1} = (-1)^t (a_{t-2} \\lambda^{t-2} + a_{t-3} \\lambda^{t-3} + \\dots + a_0) \\] In all, \\[ \\begin{aligned} \\det (A - \\lambda I) \u0026amp;= (a_{t-1} - \\lambda)(-\\lambda)^{t-1} - (-1)^t (a_{t-2} \\lambda^{t-2} + a_{t-3} \\lambda^{t-3} + \\dots + a_0) \\\\ \u0026amp;= (-1)^t (\\lambda^t - a_{t-1} \\lambda^{t-1} - a_{t-2} \\lambda^{t-2} - \\dots - a_0) \\end{aligned} \\]\nAfter solving the eigenvalues \\(\\lambda_1 \\ge \\dots \\ge \\lambda_t\\) and corresponding eigenvectors \\(\\newcommand{\\v}{\\mathrm{v}} \\v_1, \\dots, \\v_t\\) from above equation, we can rewrite the vector formed by the initial \\(t\\) terms as the linear combination of \\(\\v_1, \\dots, \\v_t\\): \\[ \\x_0 = \\left[ x_{t-1}, \\cdots, x_0 \\right]^T = c_1 \\v_1 + \\dots c_t \\v_t \\] Then for every \\(n \\ge t\\), \\[ x_n = \\left[ A^{n-t+1} \\x_0 \\right]_0 = \\left[ \\lambda_1^{n-t+1} c_1 \\v_1 + \\dots + \\lambda_t^{n-t+1} c_t \\v_t\\right]_0 \\] \\(x_n\\) will be asymptotic to \\(\\lambda_1^{n}\\) as \\(n \\to \\infty\\).\n","date":1640462913,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640462913,"objectID":"92ec8bc2f4c577d51cece5567f970741","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/difference-equation/","section":"notes","summary":"Difference Equation To solve difference equation like \\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \\dots + a_0\\), we first rewrite it into the matrix form:\n\\[ \\left[ \\begin{array} \\\\ x_t \\\\ x_{t-1} \\\\ \\vdots \\\\ x_2 \\\\ x_1 \\end{array} \\right] = \\underbrace{ \\left[ \\begin{array} \\\\ a_{t-1} \u0026 a_{t-2} \u0026 \\cdots \u0026 a_1 \u0026 a_0 \\\\ 1 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 \\cdots \u0026 0 \u0026 \\vdots \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\cdots \u0026 1 \u0026 0 \\end{array} \\right] }_{A} \\left[ \\begin{array} \\\\ x_{t-1} \\\\ x_{t-2} \\\\ \\vdots \\\\ x_1 \\\\ x_0 \\end{array} \\right] \\]","tags":null,"title":"Difference Equation","type":"book"},{"authors":null,"categories":null,"content":"  定义：设\\((X_1,\\dots,X_n)\\)为取自总体的一组样本，若函数\\(g(X_1,\\dots,X_n)\\)不包含总体分布中的任何参数，则称\\(g(X_1,\\dots,X_n)\\)为统计量。\n 样本均值和样本方差 \\[ \\begin{gather} \\text{样本均值：}\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\\\ \\text{样本方差：}S^2 = \\frac{1}{n-1} \\sum_{i=1}^N (X_i - \\bar X)^2 = \\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2) \\end{gather} \\]\n令\\(m_k = \\frac{1}{n} \\sum_{i=1}^n X_i^k\\)为样本的\\(k\\)阶原点矩，\\(a_k = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^k\\)为样本的\\(k\\)阶中心矩，这些也都是统计量。特别地，当\\(k=2\\)时，我们令\\(S_n^2 \\triangleq a_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2\\)。\n由于统计量是随机变量的函数，故统计量也是随机变量。设总体\\(X\\)的期望\\(\\E(X) = \\mu\\)，方差\\(\\Var(X) = \\sigma^2\\)，关于统计量有如下定理： \\[ \\begin{gather} \u0026amp; \\E(\\bar X) = \\mu, \\Var(\\bar X) = \\frac{\\sigma^2}{n} \\\\ \\notag \\\\ \u0026amp; \\E(S^2) = \\sigma^2, \\E(S_n^2) = \\frac{n-1}{n} \\sigma^2 \\\\ \\notag \\\\ \u0026amp; \\bar X \\stackrel{P}{\\to} \\mu, S^2 \\stackrel{P}{\\to} \\sigma^2, S_n^2 \\stackrel{P}{\\to} \\sigma^2 \\end{gather} \\] 有关证明如下： \\[ \\begin{gather} \\E(\\bar X) = \\E (\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n} \\sum_{i=1}^n \\E(X_i) = \\mu \\\\ \\Var(\\bar X) = \\Var(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n^2} \\sum_{i=1}^n \\Var(X_i) = \\frac{\\sigma^2}{n} \\end{gather} \\]\n\\[ \\begin{gather} \\begin{aligned}[t] \\E(S^2) \u0026amp;= \\E [\\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{1}{n-1} \\big( \\sum_{i=1}^n \\E (X_i^2 ) - n \\E(\\bar X^2) \\big) \\\\ \u0026amp;\\Downarrow_ {\\E(X_i^2) = \\Var(X_i) + \\E^2(X_i) = \\sigma^2 + \\mu^2, \\E(\\bar X^2) = \\Var(\\bar X) + \\E^2(\\bar X) = \\frac{\\sigma^2}{n} + \\mu^2} \\\\ \u0026amp;= \\frac{1}{n-1} \\big( \\sum_{i=1}^n (\\sigma^2 + \\mu^2) - n (\\frac{\\sigma^2}{n} + \\mu^2) \\big) \\\\ \u0026amp;= \\sigma^2 \\end{aligned} \\begin{aligned}[t] \\E(S_n^2) \u0026amp;= \\E [\\frac{n-1}{n} \\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{n-1}{n} \\E [\\frac{1}{n-1} (\\sum_{i=1}^n X_i^2 - n \\bar X^2)] \\\\ \u0026amp;= \\frac{n-1}{n} \\sigma^2 \\end{aligned} \\end{gather} \\]\n归根结底，样本方差使用\\(\\frac{1}{n-1}\\)而不是\\(\\frac{1}{n}\\)的原因是，其使用的“均值”为\\(\\bar X\\)而不是\\(\\mu\\)，这导致了一个自由度的缺失。而假设\\(\\mu\\)已知，我们定义一个新的统计量\\(S\u0026#39;^2 = \\frac{1}{n} \\sum_{i=1}^N (X_i - \\mu)^2 = \\frac{1}{n} (n \\mu^2 - 2n\\mu \\bar X + \\sum_{i=1}^n X_i^2)\\)，我们会发现\\(\\E(S\u0026#39;^2) = \\sigma^2\\)： \\[ \\begin{aligned} \u0026amp;\\E(S\u0026#39;^2) = \\frac{1}{n} \\E(n \\mu^2 - 2n\\mu \\bar X + \\sum_{i=1}^n X_i^2) \\\\ \u0026amp;= \\frac{1}{n} (n \\mu^2 - 2n\\mu \\E(\\bar X) + \\sum_{i=1}^n \\E (X_i^2)) \\\\ \u0026amp;= \\frac{1}{n} (n \\mu^2 - 2n\\mu^2 + \\sum_{i=1}^n (\\sigma^2 + \\mu^2)) \\\\ \u0026amp;= \\sigma^2 \\end{aligned} \\] 至于三个统计量的依概率收敛证明，根据相互独立同分布大数定律，有 \\[ \\begin{gather} \\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{P}{\\to} \\mu \\\\ \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\stackrel{P}{\\to} \\frac{1}{n} \\sum_{i=1}^n \\E (X_i^2) = \\sigma^2 + \\mu^2 \\end{gather} \\]\n对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)，存在\\(N_1, N_2 \u0026gt; 0\\)，使得当\\(n \u0026gt; \\max(N_1, N_2)\\)时，始终有 \\[ \\begin{gather} 0 \u0026lt; P(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| \\ge \\epsilon / 2) \u0026lt; \\delta / 2 \\\\ 0 \u0026lt; P(|\\mu^2 - \\bar X^2| \\ge \\epsilon / 2) \u0026lt; \\delta / 2 \\\\ \\end{gather} \\] 记事件\\(A\\)为\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| \\ge \\epsilon / 2\\)、事件\\(B\\)为\\(|\\bar X^2 - \\mu^2| \\ge \\epsilon / 2\\)、事件\\(C\\)为\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 - \\sigma^2| \\ge \\epsilon / 2\\)。由于\\(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - (\\sigma^2 + \\mu^2)| + |\\mu^2 - \\bar X^2| \\ge |\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 - \\sigma^2|\\)，则事件\\(C\\)发生时，事件\\(A\\)、\\(B\\)至少发生其中之一，即事件\\(C\\)是事件\\(A\\)与事件\\(B\\)并集的子集。故\n\\[ 0 \u0026lt; P(\\text{事件$C$}) \\le P(\\text{事件$A$ 或 事件$B$}) \\le P(\\text{事件$A$}) + P(\\text{事件$B$}) \u0026lt; \\delta \\] 即\\(0 \u0026lt; P(|\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2 \\ - \\sigma^2| \\ge \\epsilon) \u0026lt; \\delta\\)。又由于对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)该结论都成立，故 \\[ \\begin{gathered} \\lim_{n \\to \\infty} P(|\\underbrace{\\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\bar X^2}_{S_n^2} - \\sigma^2| \\ge \\epsilon) = 0 \\iff \\\\ S_n^2 \\stackrel{P}{\\to} \\sigma^2 \\end{gathered} \\] 运用类似的\\(\\epsilon, \\delta\\)语言，我们可以证明\\(S^2 = \\frac{n}{n-1} S_n^2 \\stackrel{P}{\\to} \\sigma^2\\)。\n次序统计量 令\\((X_{(1)}, \\dots, X_{(n)})\\)为样本\\((X_1, \\dots, X_n)\\)排序后的结果，则\\(X_{(1)} = \\min (X_1, \\dots, X_n), X_{(n)} = \\max (X_1, \\dots, X_n)\\)亦是统计量。\n记\\(X_{(1)}, X_{(n)}\\)的概率密度函数分别为\\(p_{X_{(1)}}, p_{X_{(n)}}\\)，则 \\[ \\begin{gather} p_{X_{(1)}}(u) = n \\big( 1 - P_X(u) \\big)^{n-1} p_X(u) \\\\ p_{X_{(n)}}(u) = n \\big( P_X(u) \\big)^{n-1} p_X(u) \\end{gather} \\] 记\\(X_{(k)}\\)的概率密度函数为\\(p_{X_{(k)}}\\)，则…\n","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"f7bebf8b361cf293130b4fb34d56e9ce","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/","section":"notes","summary":"定义：设\\((X_1,\\dots,X_n)\\)为取自总体的一","tags":null,"title":"统计量","type":"book"},{"authors":null,"categories":null,"content":" 点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知，但其参数\\(\\theta\\)未知。设\\(X_1, \\dots, X_n\\)为总体的一组样本，若用一个统计量\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)来估计\\(\\theta\\)，则称\\(\\hat \\theta\\)为参数\\(\\theta\\)的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。\n矩估计 矩估计的思想就是就是替换思想，即用样本矩替换总体矩。设总体的\\(k\\)阶原点矩（origin moment）\\(\\mu_k = \\E[X^k]\\)，总体的\\(k\\)中心矩（central moment）\\(\\alpha_k = \\E[(X - \\mu)^k]\\)；样本的\\(k\\)阶原点矩为\\(m_k = \\frac 1 n \\sum_{i=1}^n X_i^k\\)，样本的\\(k\\)阶中心矩为\\(a_k = \\frac 1 n \\sum_{i=1}^n (X_i - \\bar X)^k\\)。\n仅以中心矩为例，如果未知参数\\(\\theta = \\varphi(\\mu_1, \\dots, \\mu_p)\\)，则其估计量\\(\\hat \\theta = \\varphi(m_1, \\dots, m_p)\\)，这种估计总体未知参数的方法叫作矩估计法。矩估计往往不唯一，如设\\(X \\sim P(\\lambda)\\)，则由于\\(\\E(X) = \\lambda\\)，\\(\\hat \\lambda\\)可写作\\(\\bar X\\)；又\\(\\Var(X) = \\lambda\\)，\\(\\hat \\lambda\\)可写作\\(\\frac 1 n \\sum_{i=1}^n X_i^2 - \\bar X^2\\)。此时往往采用较低阶的矩来估计未知参数。\n最大似然估计 设总体有分布律\\(X \\sim P(X=x;\\theta)\\)或密度函数\\(X \\sim p(x;\\theta)\\)，\\(x_1, \\dots, x_n\\)为取自总体的一组样本观测值，将样本的联合分布律或联合密度函数看作\\(\\theta\\)的函数： \\[ L(\\theta) = \\prod_{i=1}^n P(X=x_i;\\theta)\\ \\text或 \\ L(\\theta) = \\prod_{i=1}^n p(x_i;\\theta) \\] \\(L(\\theta)\\)又称作\\(\\theta\\)的似然函数，似然函数满足关系式\\(L(\\hat \\theta) = \\max_{\\theta} L(\\theta)\\)的解\\(\\hat \\theta\\)为\\(\\theta\\)的最大似然估计量。\n由于最大似然估计对样本使用较为充分，通常其方差较小。\n优良性评判 无偏性（unbiasedness） 设\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)是\\(\\theta\\)的一个估计量，\\(\\theta\\)的取值空间为\\(\\Theta\\)，若对任意的\\(\\theta \\in \\Theta\\)，有 \\[ \\E [\\hat \\theta(X_1, \\dots, X_n)] = \\theta \\] 则称\\(\\hat \\theta\\)是\\(\\theta\\)的一个无偏估计（量），否则则称作有偏估计（量）。如果有 \\[ \\lim_{n \\to \\infty} \\E [\\hat \\theta(X_1, \\dots, X_n)] = \\theta \\] 则称\\(\\hat \\theta\\)是\\(\\theta\\)的一个渐进无偏估计（量）。渐进无偏亦记作\\(\\hat \\theta \\stackrel{L_1}{\\to} \\theta\\)。\n估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值也许偏大，取另一些样本时估计量也许偏小，但多次取样本进行估计，平均来讲偏差为\\(0\\)。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即系统误差。\n有趣的是，有一些估计虽然不满足无偏性，但满足一致性，所以我们依旧会采用这些估计。比如在估计正态总体的方差时，令\\(S_n^2 \\triangleq a_2\\)，则最大似然估计为\\(S_n^2\\)，该估计不满足无偏性，但满足一致性；\\(S_n^2/k\\)形式（\\(k\\)为待定系数）的最小均方差估计为\\(n S_n^2/(n+1)\\)（参此），也不满足无偏性，但满足一致性。\n最小方差（minimum-variance） 设\\(\\hat \\theta_1 = \\hat \\theta_2\\)是\\(\\theta\\)的两个估计量，\\(\\theta\\)的取值空间为\\(\\Theta\\)，若对任意的\\(\\theta \\in \\Theta\\)，有\\(\\Var(\\hat \\theta_1) \\le \\Var(\\hat \\theta_2)\\)，且至少有一个\\(\\theta \\in \\Theta\\)使得该不等式严格成立，则称\\(\\hat \\theta_1\\)比\\(\\hat \\theta_2\\)有效。\n一致性（consistency） 设\\(\\hat \\theta = \\hat \\theta(X_1, \\dots, X_n)\\)是\\(\\theta\\)的一个估计量，若对任意\\(\\epsilon \u0026gt; 0\\)，有 \\[ \\lim_{n \\to \\infty} P(|\\hat \\theta - \\theta| \u0026gt; \\epsilon) = 0 \\\\ \\equiv \\\\ \\lim_{n \\to \\infty} P(|\\hat \\theta - \\theta| \\le \\epsilon) = 1 \\] 则称估计量\\(\\hat \\theta\\)具有一致性，一致性描述的是一个估计量依概率收敛到真实值的过程，一致性亦记作\\(\\hat \\theta \\stackrel{P}{\\to} \\theta\\)。\n一致性是一个很基本（“基本”不是指“一致性是其他两条性质的必要条件”）的要求：随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。\nCramer-Rao不等式 实际上，点估计量不仅仅可以估计未知参数\\(\\theta\\)本身（假设为一元情况），更可以估计未知参数的某个函数\\(g(\\theta)\\)，即给定总体的一组样本\\(X_1, \\dots, X_n\\)，用统计量\\(\\hat g = \\hat g(X_1, \\dots, X_n)\\)估计\\(g(\\theta)\\)。估计量最好的效果便是达到最小方差无偏（minimum-variance unbiased \u0026lt;MVU\u0026gt;）估计，Cramer-Rao不等式给出了点估计量\\(\\hat g\\)方差的一个下界。 \\[ \\begin{equation} \\label{cr} \\Var(\\hat g) \\ge (g\u0026#39;(\\theta))^2 / (nI(\\theta)) \\end{equation} \\] 其中，\\(I(\\theta) = \\int [(\\frac{\\partial p(x;\\theta)}{\\partial \\theta})^2 / p(x;\\theta)] \\d x\\)为Fisher Information。当\\(g(\\theta) = \\theta\\)，即只估计未知参数本身时，有\\(\\Var(\\hat g) \\ge 1 / (nI(\\theta))\\)。\n\\(\\eqref{cr}\\)成立有一定的条件，其本身就暗含了\\(\\frac{\\partial p(x;\\theta)}{\\partial \\theta}\\)存在及\\(g\u0026#39;(\\theta)\\)存在的条件。记 \\[ S = S(X_1, \\dots, X_n, \\theta) = \\sum_{i=1}^n \\frac{\\partial \\ln p(X_i;\\theta)} {\\partial \\theta} = \\sum_{i=1}^n [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\] \\(\\int p(x;\\theta)\\ \\d x = 1\\)，此式两边同时对\\(\\theta\\)求导，并假定此处求导可以移至积分号内部，可得到\\(\\int \\frac{\\partial p(x;\\theta)}{\\partial \\theta} \\d x = 0\\)。根据LOTUS， \\[ \\E [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] = \\int [\\frac{\\partial p(x;\\theta)} {\\partial \\theta} / p(x;\\theta)] p(x;\\theta)\\ \\d x = \\int \\frac{\\partial p(x;\\theta)} {\\partial \\theta}\\d x = 0 \\]\n由于\\(X_1, \\dots, X_n\\)的独立性， \\[ \\begin{aligned} \\Var(S) \u0026amp;= \\sum_{i=1}^n \\Var [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\\\ \u0026amp;= \\sum_{i=1}^n \\{ \\E [\\big (\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta) \\big)^2] - \\E^2 [\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta)] \\} \\\\ \u0026amp;= \\sum_{i=1}^n \\E [\\big (\\frac{\\partial p(X_i;\\theta)} {\\partial \\theta} / p(X_i;\\theta) \\big)^2] \\\\ \u0026amp;= n \\int \\big (\\frac{\\partial p(x;\\theta)} {\\partial \\theta} / p(x;\\theta) \\big)^2 p(x;\\theta)\\ \\d x \\\\ \u0026amp;= n I(\\theta) \\end{aligned} \\]\n根据协方差的性质， \\[ \\begin{equation} \\label{cov_prop} [\\Cov(\\hat g, S)]^2 \\le \\Var(\\hat g) \\Var(S) = \\Var(\\hat g) n I(\\theta) \\end{equation} \\]\n又\\(\\E(S) = 0\\)， \\[ \\begin{aligned} \\Cov(\\hat g, S) = \\E (\\hat g S) \u0026amp;= \\int \\dots \\int \\hat g(x_1, \\dots, x_n) \\sum_{i=1}^n [\\frac{\\partial p(x_i;\\theta)} {\\partial \\theta} / p(x_i;\\theta)] \\prod_{i=1}^n p(x_1;\\theta)\\ \\d x_1 \\dots \\d x_n \\\\ \u0026amp;= \\int \\dots \\int \\hat g(x_1, \\dots, x_n) \\frac{\\partial p(x_1;\\theta) \\dots p(x_n;\\theta)} {\\partial \\theta}\\ \\d x_1 \\dots \\d x_n \\end{aligned} \\] 假定此处对\\(\\theta\\)求导可以移至积分号外部， \\[ \\begin{aligned} \\Cov(\\hat g, S) \u0026amp;= \\frac \\partial{\\partial \\theta} \\int \\dots \\int \\hat …","date":1657191973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657191973,"objectID":"ce2db8f3089e450f05ac119bf4d9c6b6","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","section":"notes","summary":"点估计 设总体\\(X \\sim p(x;\\theta)\\)的分布形式已知","tags":null,"title":"参数估计","type":"book"},{"authors":null,"categories":null,"content":" A useful matrix identity: \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] It can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026amp;= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026amp;= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form: \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] It can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026amp;= A(I_M+BA)^{-1} \\\\ \\iff A \u0026amp;= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026amp;= A \\end{aligned} \\]\n","date":1640188302,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640188302,"objectID":"571a787a063764a5f4b1c74a1ec25dbe","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/matrix-identity/","section":"notes","summary":"A useful matrix identity: \\[ (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1} \\] It can be proved with \\[ \\begin{aligned} (P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} \u0026= PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\\\ \\iff B^TR^{-1} \u0026= B^TR^{-1} \\\\ \\end{aligned} \\] Its reduced form: \\[ (I_N+AB)^{-1}A = A(I_M+BA)^{-1} \\] It can be proved with \\[ \\begin{aligned} (I_N+AB)^{-1}A \u0026= A(I_M+BA)^{-1} \\\\ \\iff A \u0026= (I_N + AB)A(I_M + BA)^{-1} \\\\ \\iff A \u0026= (A + ABA)(I_M + BA)^{-1} \\\\ \\iff A \u0026= A(I_M + BA)(I_M + BA)^{-1} \\\\ \\iff A \u0026= A \\end{aligned} \\]","tags":null,"title":"Matrix Identity","type":"book"},{"authors":null,"categories":null,"content":" 贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j) P(A|B_j)} \\] 这便是贝叶斯公式。贝叶斯公式中的项目也有它们在贝叶斯学派中相应的称呼： \\[ \\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{evidence}} \\]\n贝叶斯估计 在参数估计问题中，记\\(D = \\{ X_1, \\dots, X_n \\}\\)为样本、\\(\\theta\\)为参数，并用将\\(D\\)代入后验概率中的事件\\(A\\)、\\(\\theta\\)代入后验概率中的\\(B_i\\)，我们得到：\n\\[ P(\\theta | D) = \\frac{P(\\theta) P(D|\\theta)} {\\sum_j P(\\theta_j) P(D|\\theta_j)} \\\\ \\] 取决于\\(\\theta\\)和单个样本的取值是连续型或是离散型，上式中的\\(P\\)可代表密度函数或分布律，而分母中的求和运算应当在\\(\\theta\\)取值连续的时候被替换在\\(\\theta\\)所有可行范围内的积分运算。比较值得注意的一点是，贝叶斯推断为参数\\(\\theta\\)引入了先验分布，而在频率学派中，参数\\(\\theta\\)是不存在什么先验分布的。\n由于贝叶斯学派假设参数服从某个分布，在先验、似然已知的情况下，我们可以求得后验的解析解、近似解，或者通过Metropolis-Hastings等算法对后验直接采样。无论采取哪种方式，我们都可以获得后验的（近似）密度函数。如此一来，贝叶斯估计其实天然是一种区间估计。除开直接获取后验密度函数，我们再讨论一些贝叶斯方法中常见的其他的估计方法。\n最大后验估计 最大后验估计（maximum a posteriori estimation）得到的点估计是以下： \\[ \\hat \\theta = \\arg \\max_{\\theta} \\frac{P(\\theta) P(D | \\theta)}{\\int P(\\theta\u0026#39;) P(D|\\theta\u0026#39;) \\d \\theta\u0026#39;} = \\arg \\max_{\\theta} {P(\\theta) P(D | \\theta)} \\]\n最小均方差估计 最小均方差估计（minimum mean squared error estimation）得到的点估计是以下： \\[ \\theta^\\star = \\arg \\min_{\\hat \\theta} \\E_{\\theta \\sim \\text{posterior}} [(\\hat \\theta - \\theta)^2] \\] 换言之，此时的到的点估计\\(\\theta^\\star\\)即为\\(\\E_{\\theta \\sim \\text{posterior}} \\theta\\)。\n可信区间 可信区间（credible interval），或者叫最大后验密度（highest posterior density）得到的是一个区间，该区间是使得随机变量落在该区间内的概率大于某一个数字（常用的有95%、98%）的最小区间。\n案例：抛硬币 在具体的抛硬币案例中（抛\\(N\\)次硬币，其中\\(n\\)次正面朝上，未知参数为正面朝上概率\\(p\\)）， \\[ \\begin{gather} \\text{likelihood: } P(n|p) = {N \\choose n} p^n (1-p)^{N-n} \\\\ \\text{prior: } \\rho(p) = 1 \\\\ \\text{evidence: } \\int_0^1 P(n|p) \\rho(p) \\d p \\\\ \\text{posterior: } \\rho(p|n) \\end{gather} \\]\n\\[ \\begin{aligned} \u0026amp; \\rho(p|n) = \\frac{P(n|p) \\rho(p)}{\\int_0^1 P(n|p) \\rho(p) \\d p} \\\\ \u0026amp;= \\frac{{N \\choose n} p^n (1-p)^{N-n} \\rho(p)}{\\int_0^1 {N \\choose n} x^n (1-x)^{N-n} \\rho(x) \\d x} \\\\ \u0026amp;= \\frac{p^n (1-p)^{N-n} 1}{\\int_0^1 x^n (1-x)^{N-n} 1 \\d x} \\\\ \u0026amp;= \\frac{p^{n+1-1} (1-p)^{N-n+1-1}} {\\underbrace{\\int_0^1 x^{n+1-1} (1-x)^{N-n+1-1} \\d x}_{\\mathrm{Beta}(n+1, N-n+1)}} \\\\ \u0026amp;= \\mathrm{Beta}(p|n+1,N-n+1) \\end{aligned} \\]\n其中\\(\\mathrm{Beta}(n+1, N-n+1)\\)表示Beta函数在\\((n+1, N-n+1)\\)处的取值；\\(\\mathrm{Beta}(p|\\underbrace{n+1}_{a\u0026gt;0}, \\underbrace{N-n+1}_{b\u0026gt;0})\\)表示参数\\(a =n+1, b=N-n+1\\)时的Beta分布。\n需要注意的是\\(\\mathrm{Beta}(p|1,1)\\)等价于\\([0,1]\\)之间的均匀分布： \\[ \\mathrm{Beta}(p|1,1) = \\text{Uniform}(p|0,1) \\]\n在prior为Beta分布、likelihood为二项分布时，得到的posterior依旧是Beta分布（不过该Beta分布的参数和prior中Beta分布的参数有所不同），此时的prior和likelihood称作conjugate distributions，此时的prior称作likelihood的conjugate prior。\nBeta分布是二项分布的conjugate prior；高斯分布是高斯分布的conjugate prior。\n","date":1684088833,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684088833,"objectID":"9a02d38a8ecdf812d564db25204f3ad1","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/","section":"notes","summary":"贝叶斯公式 在全概率公式之下，有 \\[ P(B_i | A) = \\frac{P(A B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)}{P(A)} = \\frac{P(B_i) P(A|B_i)} {\\sum_j P(B_j)","tags":null,"title":"贝叶斯推断","type":"book"},{"authors":null,"categories":null,"content":" Quadratic Form Quadratic form involves many concepts like real symmetric matrix, positive definiteness and singular value decomposition. It can be quite helpful to glue these things together.\nA quadratic function \\(f\\) of \\(n\\) variables, or say a vector \\(\\x\\) of length \\(n\\), is the sum of second-order terms: \\[ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j \\]\nThe above summation can be simplified as matrix product \\(\\x^T A \\x\\) where \\(A\\) is \\(n \\times n\\) and \\(a_{ij} = c_{ij}\\). \\(\\x^T A \\x\\) is called the quadratic form of \\(A\\).\nIn essence, there is a nicer formulation for \\(A\\). Firstly define the \\(n \\times n\\) matrix \\(A\\) such that \\(a_{ij} = \\frac{1}{2} (c_{ij} + c_{ji})\\). It suffices to show \\(A\\) is a real symmetric matrix and \\[ f(\\x) = \\x^T A \\x \\]\nThus the discussion of quadratic form usually encompasses the real symmetric matrix.\nPositive Definiteness Let \\(A\\) be a real symmetric matrix. \\(A\\) is positive definite if and only if the quadratic form of \\(A\\) is positive. Specifically, for every \\(\\x \\ne 0\\), \\(\\x^T A \\x \u0026gt; 0\\).\n Theorem\nA real symmetric matrix \\(A\\) is positive definite if and only if \\(A\\)’s eigenvalues are positive.\n Proof\n Necessity\nFor every \\(A\\)’s eigenpair \\((\\lambda, \\v)\\), we have \\(\\v^T A \\v = \\lambda \\v^T \\v \u0026gt; 0 \\Rightarrow \\lambda \u0026gt; 0\\).\n Sufficiency\nTake \\(A\\)’s spectral decomposition as \\(A = Q \\Lambda Q^T\\) where \\(Q Q^T = I\\). For every \\(\\x \u0026gt; 0\\), we have \\[ \\x^T A \\x = \\overbrace{\\x^T Q}^{y^T} \\Lambda \\overbrace{Q^T \\x}^{y} \u0026gt; 0 \\]\n   One possible reason why we would like to define positive definiteness on real symmetric matrix is that, any real matrix can be easily decomposed into the addition of a real symmetric matrix and a real skew-symmetric matrix whose quadratic form is zero: \\[ A = \\frac{A + A^T}{2} + \\frac{A - A^T}{2} \\] Since \\(\\frac{A - A^T}{2}\\)’s quadratic form is zero and it makes no contribution to \\(A\\)’s, the only component of interest will be the real symmetric \\(\\frac{A + A^T}{2}\\). So why not just focus on the real symmetric matrix?\n","date":1683282072,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683282072,"objectID":"ab0a4a551cc9fdaf813df443a8f01282","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/quadratic-form/","section":"notes","summary":"Quadratic Form Quadratic form involves many concepts like real symmetric matrix, positive definiteness and singular value decomposition. It can be quite helpful to glue these things together.\nA quadratic function \\(f\\) of \\(n\\) variables, or say a vector \\(\\x\\) of length \\(n\\), is the sum of second-order terms: \\[ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j \\]","tags":null,"title":"Quadratic Form","type":"book"},{"authors":null,"categories":null,"content":" 依概率收敛（convergence in probability） 随机变量序列即是由随机变量构成的序列。对于一个普通数列\\(\\{x_n\\}\\)来说，若其收敛于\\(c\\)，则意味着当\\(n\\)充分大时，\\(x_n\\)和\\(c\\)的距离可以达到任意小。而随机变量序列\\(X_1, X_2, \\dots\\)的极限却不能按照这样定义，因为\\(X_n\\)取值不确定，不可能总和某个数字\\(c\\)的距离任意小。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \u0026lt; \\epsilon) = 1\\)，抑或是，对于任意\\(\\epsilon \u0026gt; 0\\)，都有\\(\\lim_{n \\to \\infty} P(|X_n - c| \\ge \\epsilon) = 0\\)），则称该随机变量序列依概率收敛于\\(c\\)，记作\\(X_n \\stackrel{P}{\\to} c\\)。\n换言之，对于任意\\(\\epsilon, \\delta \u0026gt; 0\\)，都存在\\(N \u0026gt; 0\\)，使得\\(n \u0026gt; N\\)时，始终有 \\[ 1 - \\delta \u0026lt; P(|X_n - c| \u0026lt; \\epsilon) \\le 1 \\]\n  依概率收敛的一个例子便是Bernoulli大数定律，即当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。\n几乎必然收敛（almost-sure convergence） 在某些情况下，若随机变量序列能够和某个数字\\(c\\)几乎接近，我们说它几乎必然收敛。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，如果存在一个常数\\(c\\)，使得\\(P(\\lim_{n \\to \\infty} X_n = c) = 1\\)，则称该随机变量序列几乎必然收敛于\\(c\\)，记作\\(X_n \\stackrel{a.s.}{\\to} c\\)。\n换言之，对于任意\\(\\epsilon \u0026gt; 0\\)，都存在\\(N \u0026gt; 0\\)，使得\\(n \u0026gt; N\\)时，始终有 \\[ P(|X_n - c| \u0026lt; \\epsilon) = 1 \\]\n  需要注意的是，几乎必然收敛和依概率收敛是不等价的，因为\\(\\lim_{n \\to \\infty} f(x_n)\\)中的极限符号不总是能够交换到函数\\(f\\)内部，举个简单的例子： \\[ \\begin{gathered} \\{ x_n \\} = -\\frac{1}{n}, \\ f(x) = \\begin{cases} x^2 - 1, \u0026amp; -1 \\le x \u0026lt; 0 \\\\ x, \u0026amp; x \\ge 0 \\end{cases} \\\\ \\lim_{n \\to \\infty} f(x_n) = \\lim_{n \\to \\infty}(\\frac{1}{n^2}-1) = -1 \\ne f(\\lim_{n \\to \\infty} x_n) = f(0) = 0 \\end{gathered} \\] 注意\\(f\\)是右连续的，这也意味着，我们可以找到类似的右连续的分布函数\\(P\\)，使得极限符号不能被移至\\(P\\)内部。也就是说，几乎必然收敛和依概率收敛是不等价的，而显然，几乎必然收敛是强于依概率收敛的。\n\\(L_p\\)收敛（convergence in \\(L_p\\)）  定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，对于某个\\(p \u0026gt; 0\\)，如果存在一个常数\\(c\\)，使得\\(\\lim_{n \\to \\infty} \\E(|| X_n - c||_p^p) = 0\\)，则称该随机变量序列\\(L_p\\)收敛于\\(c\\)，记作\\(X_n \\stackrel{L_p}{\\to} c\\)。\n  均方收敛 当\\(p=2\\)时，\\(L_p\\)收敛又称作均方收敛。根据Chebyshev不等式， \\[ P(|X_n-\\E(X_n)| \\ge \\epsilon) \\le \\frac{\\Var(X_n)}{\\epsilon^2} = \\frac{\\E[(X_n - \\E(X_n))^2]}{\\epsilon^2} \\] 在两边取\\(n \\to \\infty\\)可以得到 \\[ \\lim_{n \\to \\infty} P(|X_n-\\E(X_n)| \\ge \\epsilon) \\le \\lim_{n \\to \\infty} \\frac{\\E[(X_n - \\E(X_n))^2]}{\\epsilon^2} = 0 \\] 即均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也强于依概率收敛；但均方收敛和几乎必然收敛之间并没有推导关系。\n依分布收敛（convergence in distribution） 前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。\n 定义\n设\\(X_1, X_2, \\dots\\)是一个随机变量序列，让\\(F_n\\)表示\\(X_n\\)的分布函数，如果存在一个分布函数\\(F\\)，使得\\(\\lim_{n \\to \\infty} F_n(x) = F(x)\\)，则称该随机变量序列依分布收敛于\\(F\\)，记作\\(X_n \\stackrel{d}{\\to} F\\)。\n  “收敛到随机变量” 除了上述讨论的收敛到值、收敛到函数的情况外，另外一个比较有趣的话题是“收敛到随机变量”，或者说“两个随机变量相等”是一个怎样的概念？\n我们讨论概率的时候，会涉及到两个函数：一个是概率函数，另一个是随机变量这一从事件到数字的映射。方便起见我们令\\(X\\)和\\(Y\\)为两个随机变量，随机变量相等，则意味着这两个从事件到数字的映射相等，进而\\(P(X = Y) = 1\\)。\n映射相等，意味着定义域、值域、映射关系完全相等。如果我有两个骰子，令\\(X\\)表示第一个骰子掷出的点数、\\(Y\\)表示第二个骰子掷出的点数，那么\\(X = Y\\)吗？答案是不，因为这两个随机变量的定义域不相等：\\(X\\)的定义域表示第一个骰子的所有可能事件，\\(Y\\)的定义域表示第二个骰子的所有可能事件；虽然两个骰子掷出的点数都只能是1、2、3、4、5、6，但这代表的仅是值域相同，而“第一个骰子掷出一”（注意这里避免使用任何数字，以表示它是一个事件）这个事件和“第二个骰子掷出一”是不一样的，因为\\(X\\)不会因为第二个骰子掷出一而取为1。\n参考资料 随机变量的收敛\n","date":1657723265,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657723265,"objectID":"cea954f6f69fb48b3f027376f5dd667e","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/","section":"notes","summary":"依概率收敛（convergence in probability）","tags":null,"title":"随机变量的收敛","type":"book"},{"authors":null,"categories":null,"content":" 假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间估计中，我们还能得到一个这个参数置信区间和相应的置信水平。而在假设检验中，我们要做的则是根据某个假设（hypothesis）以及给定的样本，决定是否接受这个假设。注意我们特地将假设和估计（estimation）区分开，因为这个假设并不是由样本得到的一个估计量，而是一条已有的断言（assertion）；我们要做的，则是给出在什么样的情况下（对应置信区间），我们能够以多高的信心否定这个断言（对应置信水平）。\n至于假设检验和区间估计的关系，其实我们会发现，如果本身就能对未知参数做有效的区间估计，那其实对它的假设检验设计，自然迎刃而解。\n一般检验方法 一般检验方法指Neyman-Pearson方法。\n建立假设 对要检验的问题，一般有一个原假设\\(H_0\\)（也叫零假设，null hypothesis）以及一个备择假设\\(H_1\\)（alternative hypothesis）。原假设一般是总体的某个未知参数\\(\\theta\\)等于某个具体值\\(\\theta_0\\)，即 \\[ H_0: \\theta = \\theta_0 \\] 这种只包含一个假设值（即\\(\\theta_0\\)）的原假设又叫作简单原假设（simple hypothesis null）。而备择假设一般和原假设互斥，它通常有以下三种形式：\n\\(H_1: \\theta \\ne \\theta_0\\)，此时\\(H_0\\)与\\(H_1\\)为对立关系，我们要检验\\(\\theta\\)落在\\(\\theta_0\\)两侧的可能，这样的检测问题也称为双边检验（two-sided test）； \\(H_1: \\theta \u0026gt; \\theta_0\\)，此时我们要检验\\(\\theta\\)落在\\(\\theta_0\\)右侧的可能，这样的检测问题也称为右侧的单边检验； \\(H_1: \\theta \u0026lt; \\theta_0\\)，此时我们要检验\\(\\theta\\)落在\\(\\theta_0\\)左侧的可能，这样的检测问题也称为左侧的单边检验；  选择否定域形式 根据已有的样本，我们能够给出未知参数的点估计量\\(\\hat \\theta\\)（在假设检验中又称作检验统计量\u0026lt;test statistic\u0026gt;），如果\\(\\hat \\theta\\)和\\(\\theta_0\\)的距离小于某个临界值\\(c \u0026gt; 0\\)（critical value），我们就可以接受原假设（即便\\(\\hat \\theta\\)与\\(\\theta_0\\)不完全相等），否则则否定原假设。使得原假设被接受的样本所在的区域就被称作接受域（acceptance region）；使得原假设被否定的样本所在的区域就被称作否定域（也叫拒绝域，rejection region）；一般我们习惯先构造否定域\\(W\\)，则剩余区域就为接受域\\(\\overline W\\)： \\[ \\begin{gather} W = \\{ (x_1, \\dots, x_n) \\big | |\\hat \\theta(x_1, \\dots, x_n) - \\theta_0| \u0026gt; c \\} \\\\ \\overline W = W^c \\end{gather} \\] 对于某些参数，可能本身越小越好（比如故障率），所以我们仅需要进行右侧的单边检测，此时对应的否定域为 \\[ W = \\{ (x_1, \\dots, x_n) \\big | \\hat \\theta(x_1, \\dots, x_n) - \\theta_0 \u0026gt; c \\} \\\\ \\] 此时可以等价认为\\(H_0: \\theta \\le \\theta_0\\)，这种情况下，\\(H_0\\)是一个复合原假设（composite hypothesis null），因为其包含的假设值不止一个。\n对于另外一些参数，可能本身越大越好（比如身高均值），所以我们仅需要左侧的单边检测，此时对应的否定域为 \\[ W = \\{ (x_1, \\dots, x_n) \\big | \\hat \\theta(x_1, \\dots, x_n) - \\theta_0 \u0026lt; c \\} \\\\ \\] 同理，此时可以等价认为\\(H_0: \\theta \\ge \\theta_0\\)，这种情况下，\\(H_0\\)也是一个复合原假设。\n设定显著性水平 给定假设，我们已经可以根据样本属于接受域还是否定域，做出接受或是否定假设的决策了。但和点估计中的问题一样，我们依然是基于样本提供的不完全信息做出的判断，所以我们的判断不总是正确的。这种判断会有四种结果：\n   判断：接受\\(H_0\\) 判断：否定\\(H_0\\)    实际：\\(H_0\\)成立 判断正确 第一类错误  实际：\\(H_1\\)成立 第二类错误 判断正确    通常较低的第一类错误风险\\(P(\\text{否定}H_0; H_0\\text{成立})\\)和较低的第二类错误风险\\(P(\\text{接受}H_0; H_1\\text{成立})\\)不可兼得，因为在检验统计量确定后，这两个概率主要是由临界值\\(c\\)导出的否定域大小来控制的。而我们更希望降低第一类错误发生的风险。也就是说我们一旦否定，\\(H_0\\)很大概率确实是不成立的；尽管这意味着我们在接受\\(H_0\\)时，\\(H_0\\)有可能不成立——不过虚惊一场总好过后知后觉。所以实际应用中，\\(H_0\\)往往对应了比较严重的结果，我们不希望在\\(H_0\\)成立时，我们却没有发现（即否定\\(H_0\\)）；或者\\(H_0\\)本身就对应了我们比较想否定的结果，这样我们否定时，它确实不成立的概率也更高。\n我们会将第一类错误发生的概率限制在\\(\\alpha\\)之内，这个\\(\\alpha\\)便是显著性水平（significance level）。显著性水平其实代表了我们对小概率事件的接受程度，即我们认为概率小于\\(\\alpha\\)的事件应该是小概率事件，并且是不应该被正好碰上的；而此时在\\(H_0\\)成立的假设下，“给定的一组样本属于否定域”正是这样的一个小概率事件，如果碰上了这样的小概率事件，则有理由怀疑\\(H_0\\)不成立。\n确定临界值 在确定显著性水平后，我们便可以进一步确定临界值，从而给出完整的否定域。此时我们调整临界值\\(c\\)，从而使得 \\[ \\begin{gathered} P(\\text{否定}H_0; H_0\\text{成立}) = P( (X_1, \\dots, X_n) \\in W;\\theta = \\theta_0) \\\\ = P(|\\hat \\theta(X_1, \\dots, X_n) - \\theta_0| \u0026gt; c; \\theta = \\theta_0) \\le \\alpha \\end{gathered} \\] 问题就变成了一个简单的分布问题。此处需要指出的是，\\(P(\\text{否定}H_0; H_0\\text{成立})\\)不应写作\\(P( (X_1, \\dots, X_n) \\in W | \\theta = \\theta_0)\\)，因为此处讨论的是频率学派中的假设检验，频率学派中的未知参数\\(\\theta\\)并没有先验分布。\n\\(P( (X_1, \\dots, X_n) \\in W;\\theta = \\theta_0)\\)又叫作功效函数（power function），记作\\(\\beta_W(\\cdot)\\)，它表示在未知参数取特定值时，一组随机样本属于否定域的概率。 前面我们之所以说单边检验等价于原假设对应某个形式的复合假设（比如\\(H_0: \\theta \\le \\theta_0\\)或\\(H_0: \\theta \\ge \\theta_0\\)），是因为这种情况下，有 \\[ \\max_{h \\in H_0} \\beta_W(h) = \\beta_W(\\theta_0) \\le \\alpha \\]\n\\(p\\)值和\\(p\\)值检验法 \\(p\\)值检验法指的是数学家Fisher提出的检验方法。\n假设检验的\\(p\\)值是在原假设\\(H_0\\)成立的情况下，检验统计量\\(\\hat \\theta(X_1, \\dots, X_n)\\)出现其具体观测值\\(z = \\hat \\theta(x_1,\\dots,x_n)\\)或者比之更极端的值的概率，即\\(p = P(\\hat \\theta = z; \\theta = \\theta_0)\\)（类似likelihood）。\\(p\\)值检验中，我们检验\\(p\\)值是否足够小，如果\\(p\\)值小到一定程度，我们还是会否定\\(H_0\\)，即\n 如果\\(p \\le \\alpha\\)，则我们在显著性水平\\(\\alpha\\)下否定原假设\\(H_0\\)； 如果\\(p \u0026gt; \\alpha\\)，则我们在显著性水平\\(\\alpha\\)下接受原假设\\(H_0\\)。  参考 hypothesis testing - Are type I error \u0026amp; FWER both conditional probabilities? - Cross Validated (stackexchange.com)\n第 3 章 假设检验 | 数理统计讲义 (bookdown.org)\n","date":1670353333,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670353333,"objectID":"6069d02896d73e792ecc9429414214b4","permalink":"https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/","section":"notes","summary":"假设检验 参数估计是根据样本值得出参数的一个估计量，并且在区间","tags":null,"title":"假设检验","type":"book"},{"authors":null,"categories":null,"content":" 贝叶斯分类器 贝叶斯分类器（Bayes classifier）基于“能够根据标签\\(y\\)预测特征\\(\\x\\)”的思想，在训练环节，它学习\\(p(\\x|y)\\)和\\(p(y)\\)；在预测环节，它给出\\(\\arg \\max_y p(y|\\x) = \\arg \\max_y p(\\x|y) p(y)\\)。可以看出，贝叶斯分类器的两个关键部分便是“似然”和“先验”，其预测环节采用了最大后验估计的思想，从这个角度或许能够理解为什么它叫做“贝叶斯”分类器。\n朴素贝叶斯分类器 贝叶斯分类器在训练环节学习\\(p(\\x|y)\\)和\\(p(y)\\)，\\(p(y)\\)可以直接由频率估计得来，而\\(p(\\x|y)\\)这个条件概率则不太好求，尤其当\\(\\x\\)为多元随机变量时，其各个成分之间的关系难以捕捉。\n为简化问题，朴素贝叶斯分类器（naive Bayes classifier）做出这样的假设：\\(\\x\\)的各个成分在\\(Y=y\\)给定的情况下，是相互独立的，此时有\\(p(\\x|y) = p(x_1|y) \\dots p(x_n|y)\\)。进一步，为了求解各个成分在\\(Y=y\\)给定时的条件概率分布，我们还可以对条件概率的分布形式作出假设，比如假设\\(p(x_i|y)\\)服从伯努利分布、多项分布、高斯分布（分别对应伯努利朴素贝叶斯分类器\u0026lt;Bernoulli naive Bayes classifier\u0026gt;、多项分布朴素贝叶斯分类器\u0026lt;multinomial naive Bayes classifier\u0026gt;、高斯分布朴素贝叶斯\u0026lt;Gaussian naive Bayes classifier\u0026gt;）等等。\n贝叶斯最优分类器 给定贝叶斯分类器\\(f \\in \\mathcal{H}\\)，给定样本\\((\\x,y)\\)，0-1损失函数\\(l\\)定义为， \\[ \\newcommand{\\I}{\\mathbb{I}} l(f(\\x), y) = \\I[f(\\x) \\ne y] = 1 - \\I[f(\\x) = y] \\] 我们的目标是最小化\\(l(f) \\triangleq \\E_{\\x,y} l(f(\\x), y)\\)： \\[ \\begin{aligned} \u0026amp;l(f) = \\sum_{\\x} \\sum_{y} p(\\x,y) l(f(\\x), y) \\\\ \u0026amp;= \\sum_x p(\\x) [\\sum_y p(y|\\x) l(f(\\x), y)] \\\\ \u0026amp;= \\E_\\x [\\sum_y p(y|\\x) l(f(\\x), y)] \\end{aligned} \\] 用\\(f^\\star\\)表示贝叶斯最优分类器（Bayes optimal classifier），则\\(f^\\star = \\arg \\min_{f \\in \\mathcal{H}} \\E_\\x [\\sum_y p(y|\\x) l(f(\\x), y)]\\)；故对于每一个样本\\(\\x\\)，都应该有： \\[ \\begin{aligned} \u0026amp;f^\\star(\\x) = \\arg \\min_{f \\in \\mathcal{H}} [\\sum_y p(y|\\x) l(f(\\x), y)] \\\\ \u0026amp;= \\arg \\min_{f \\in \\mathcal{H}} [\\sum_y p(y|\\x) (1 - \\I[f(\\x) = y])] \\\\ \u0026amp;= \\arg \\min_{f \\in \\mathcal{H}} [\\sum_y p(y|\\x) - \\sum_y p(y|\\x) \\I[f(\\x) = y]] \\\\ \u0026amp;= \\arg \\min_{f \\in \\mathcal{H}} [1 - \\sum_y p(y|\\x) \\I[f(\\x) = y]] \\\\ \u0026amp;= \\arg \\min_{f \\in \\mathcal{H}} [- \\sum_y p(y|\\x) \\I[f(\\x) = y]] \\\\ \u0026amp;= \\arg \\max_{f \\in \\mathcal{H}} [\\sum_y p(y|\\x) \\I[f(\\x) = y]] \\\\ \u0026amp;= \\arg \\max_{y \\in \\mathcal{Y}} p(y|\\x) \\end{aligned} \\]\n以\\(Y\\)仅有两种取值举例： \\[ \\begin{aligned} f^\\star(x) \u0026amp;= \\arg \\max_{f \\in \\mathcal{H}} [p(y_1|\\x) \\I[f(\\x) = y_1] + p(y_2|\\x) \\I[f(\\x) = y_2] ] \\\\ \u0026amp;= \\arg \\max_{y \\in \\mathcal{Y}} p(y|\\x) \\end{aligned} \\] 以上结论在\\(\\x\\)为连续型随机变量或者\\(y\\)为连续型随机变量时也成立。\n参考资料 Artificial Intelligence - foundations of computational agents – 7.3.3 Bayesian Classifiers (artint.info)\nmachine learning - What does it mean for the Bayes Classifier to be optimal? - Cross Validated (stackexchange.com)\n","date":1684748595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684748595,"objectID":"52df35f010dacea76a20a42e698ed754","permalink":"https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","section":"notes","summary":"贝叶斯分类器 贝叶斯分类器（Bayes classifier）基","tags":null,"title":"贝叶斯分类器","type":"book"},{"authors":null,"categories":null,"content":" 隐马尔可夫模型 隐马尔可夫模型（Hidden Markov Model）的对象是序列类型的数据，其基本假设为：该序列的观测值实际上是来自于完全关于当前时序下的状态（state，也可叫作隐变量\u0026lt;latent variable\u0026gt;）取值的一个条件概率分布，而状态取值亦会随着时序发生变化，且其状态变化遵从马尔可夫过程。\n令\\(Y_1, \\dots, Y_T\\)表示\\(T\\)个时序下的观测值，\\(X_1, \\dots, X_T\\)表示\\(T\\)个时序下的状态取值，对于任意\\(i = 1, \\dots, T\\)，有\\(Y_i \\sim p(\\cdot | X_i)\\)；一般来说，隐马尔科夫模型中假设状态取值和观测取值都是离散的，假设共有\\(N\\)种状态取值\\(x_1, \\dots, x_N\\)和\\(M\\)种观测取值\\(y_1, \\dots, y_M\\)，令状态转移矩阵（transition matrix）为\\(N \\times N\\)的方阵\\(A\\)，令观测概率矩阵（emission matrix）为\\(N \\times M\\)的矩阵\\(B\\)；令\\(\\pi\\)为初始状态概率向量。\n隐马尔可夫模型即是由\\(\\pi, A, B\\)三者决定，所以其可以用三元组表示： \\[ \\lambda = (\\pi, A, B) \\]\n隐马尔可夫模型有三类基本问题： 1. 概率问题。给定模型\\(\\lambda = (\\pi, A, B)\\)和观测序列\\(\\Y = (y_{o_1}, \\dots, y_{o_T})\\)，计算模型\\(\\lambda\\)产生观测序列\\(\\Y\\)的概率。 2. 学习问题。给定观测序列\\(\\Y = (y_{o_1}, \\dots, y_{o_T})\\)，估计模型参数\\(\\lambda = (\\pi, A, B)\\)。 3. 预测问题（也叫解码\u0026lt;decoding\u0026gt;问题）。给定模型\\(\\lambda = (\\pi, A, B)\\)和观测序列\\(\\Y = (y_{o_1}, \\dots, y_{o_T})\\)，求使得条件概率\\(P(\\X | \\Y)\\)最大的状态序列\\(\\X\\)。\n概率问题 暴力解法 在预测问题中，最直接的想法便是首先枚举出所有的状态序列，然后根据状态序列计算观测序列的概率。某个状态序列\\(\\X = (x_{i_1}, \\dots, x_{i_T})\\)出现的概率为 \\[ P(\\X) = \\pi_{i_1} a_{i_1, i_2} a_{i_2, i_3} \\dots a_{i_{T-1}, i_T} \\] 给定这个状态序列\\(\\X\\)的情况下，观测序列\\(\\Y\\)出现的概率为 \\[ P(\\Y | \\X) = b_{i_1, o_1} b_{i_2, o_2} \\dots b_{i_T, o_T} \\] 两者同时出现的联合概率为 \\[ P(\\X, \\Y) = P(\\X) \\times P(\\Y | \\X) = \\pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \\dots a_{i_{T-1}, i_T} b_{i_T, o_T} \\] 然后对上式关于所有可能的状态序列求和，得到 \\[ P(\\Y) = \\sum_\\X P(\\X, \\Y) = \\sum_{i_1, i_2, \\dots, i_T} \\pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \\dots a_{i_{T-1}, i_T} b_{i_T, o_T} \\] 该式共有\\(N^T\\)项，每一项有\\(O(T)\\)次相乘，故整体复杂度为\\(O(T N^T)\\)，难以接受。\n前向算法 首先引入前向概率的概念。给定隐马尔可夫模型\\(\\lambda\\)及观测序列\\(\\Y = (y_{o_1}, \\dots, y_{o_T})\\)，定义到时刻\\(t\\)的观测序列为给定的\\((y_{o_1}, \\dots, y_{o_t})\\)且此时状态为\\(x_{i}\\)的概率为前向概率，记作 \\[ \\alpha_t(i) = P(Y_1 = y_{o_1}, \\dots, Y_T = y_{o_t}, X_T = x_i; \\lambda) \\] 初始情况下，即\\(t=1\\)时，有 \\[ \\alpha_1(i) = \\pi_i a_{i, o_1} \\] 对\\(t = 2, \\dots, T\\)，有 \\[ \\alpha_t(i) = \\left[ \\sum_{j=1}^N \\alpha_{t-1}(j) a_{j, i} \\right] b_{i, o_t} \\] 最终， \\[ P(\\Y) = \\sum_{i=1}^N \\alpha_T(i) \\] 前向算法主要利用了状态变化的序列结构，记录了状态变化的中间过程，从而节省了计算时间。整个算法最耗时的步骤为递推这一步，\\(\\alpha_t(i)\\)对应的加和表达式包含了\\(O(N)\\)个项，而对于每一个时刻\\(t\\)，有\\(N\\)个这样的加和表达式，故\\(T\\)个时刻整体耗时为\\(O(N^2 T)\\)。\n学习问题 有监督学习 有监督学习是较好处理的一种问题，在这种情况下，\\(\\X, \\Y\\)都已知，需要我们估计模型参数\\(\\pi, A, B\\)。\n设样本中状态由\\(i\\)转移到\\(j\\)的频数为\\(A_{i, j}\\)，则\\(a_{i, j}\\)的估计为 \\[ \\hat a_{i, j} = \\frac{A_{i, j}} {\\sum_{k=1}^N A_{i, k}} \\] 设样本中状态为\\(i\\)且观测为\\(j\\)的频数是\\(B_{i, j}\\)，则\\(b_{i, j}\\)的估计为 \\[ \\hat b_{i, j} = \\frac{B_{i, j}} {\\sum_{k=1}^N B_{i, k}} \\] 至于初始状态向量\\(\\pi\\)，若样本中由多条时序链，\\(\\pi\\)亦可由相应频次估计而来。\n无监督学习 多数情况下，状态——或者说隐变量\\(\\X\\)——是没有办法观测到的，而这时就可以采用EM算法，通过优化似然函数的下界，找出最好的模型参数。EM算法在隐马尔科夫模型中具体实现由Baum和Welch提出，故实际求解算法被称作Baum-Welch算法。\n此问题的似然函数以及对数似然函数分别为： \\[ \\begin{aligned} P(\\Y; \\lambda) \u0026amp;= \\sum_{\\X} P(\\X, \\Y; \\lambda) \\\\ \\log P(\\Y; \\lambda) \u0026amp;= \\log \\sum_{\\X} P(\\X, \\Y; \\lambda) \\end{aligned} \\] 根据EM算法， \\[ \\begin{aligned} \u0026amp;\\log P(\\Y; \\lambda) = \\log \\E_{\\X \\sim P(\\cdot | \\Y; \\lambda^t)} P(\\X, \\Y; \\lambda) \\\\ \u0026amp;\\ge \\E_{\\X \\sim P(\\cdot | \\Y; \\lambda^t)} \\log P(\\X, \\Y; \\lambda) \\triangleq Q(\\lambda^t, \\lambda) \\\\ \\end{aligned} \\]\n而下一轮迭代中，新的估计\\(\\lambda^{t+1}\\)为： \\[ \\lambda^{t+1} = \\arg \\max_\\lambda Q(\\lambda^t, \\lambda) \\]\n又由于 \\[ \\begin{aligned} \u0026amp;Q(\\lambda^t, \\lambda) = \\E_{\\X \\sim P(\\cdot | \\Y; \\lambda^t)} \\log P(\\X, \\Y; \\lambda) \\\\ \u0026amp;= \\sum_\\X P(\\X | \\Y; \\lambda^t) \\log P(\\X, \\Y; \\lambda) \\\\ \u0026amp;= \\sum_\\X \\frac{P(\\X, \\Y; \\lambda^t)}{P(\\Y; \\lambda^t)} \\log P(\\X, \\Y; \\lambda) \\\\ \u0026amp;= \\frac{\\sum_\\X P(\\X, \\Y; \\lambda^t) \\log P(\\X, \\Y; \\lambda)}{P(\\Y; \\lambda^t)} \\\\ \\end{aligned} \\] 其中的\\(\\frac{1}{P(\\Y; \\lambda^t)}\\)与\\(\\lambda\\)无关，所以 \\[ \\lambda^{t+1} = \\arg \\max_\\lambda \\sum_\\X P(\\X, \\Y; \\lambda^t) \\log P(\\X, \\Y; \\lambda) \\]\n注意在接下来的讨论中，我们会令\\(\\X = (x_{i_1}, \\dots, x_{i_T})\\)，但为了整体简洁，我们不会在每一个\\(\\X\\)出现的地方将此式展开。对上式做进一步展开， \\[ \\begin{aligned} \u0026amp;\\sum_\\X P(\\X, \\Y; \\lambda^t) \\log P(\\X, \\Y; \\lambda) = \\sum_\\X P(\\X, \\Y; \\lambda^t) \\log \\pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \\dots a_{i_{T-1}, i_T} b_{i_T, o_T} \\\\ \u0026amp;= \\sum_{\\X} P(\\X, \\Y; \\lambda^t) \\log \\pi_{i_1} + \\sum_{\\X} P(\\X, \\Y; \\lambda^t) \\sum_{t=1}^{T-1} a_{i_t, i_{t+1}} + \\sum_{\\X} P(\\X, \\Y; \\lambda^t) \\sum_{t=1}^T \\log b_{i_t, o_t} \\end{aligned} \\]\n我们可以对其中的三项分别最大化，以达到整体最大化。\n注意到\\(\\pi\\)满足约束条件\\(\\sum_{j=1}^N \\pi_j = 1\\)，写出其拉格朗日函数： \\[ \\begin{aligned} L_\\pi (\\pi, \\gamma) \u0026amp;= \\sum_{\\X} P(\\X, \\Y; \\lambda^t) \\log \\pi_{i_1} + \\gamma (\\sum_{j=1}^N \\pi_j - 1) \\\\ \u0026amp;= \\sum_{j=1}^N P(\\Y, i_1 = j; \\lambda^t) \\log \\pi_{j} + \\gamma (\\sum_{j=1}^N \\pi_j - 1) \\end{aligned} \\] 对上式关于\\(\\pi\\)求导并令其为\\(0\\)，得到对于任意\\(j = 1, \\dots, N\\)， \\[ \\begin{align} \\frac{P(\\Y, i_1 = j; \\lambda^t)}{\\pi_j} + \\gamma \u0026amp;= 0 \\\\ P(\\Y, i_1 = j; \\lambda^t) + \\pi_j \\gamma \u0026amp;= 0 \\label{pi-eq} \\end{align} \\] 对上式关于\\(j\\)求和，得到 \\[ \\sum_{j=1}^N P(\\Y, i_1 = j; \\lambda^t) + \\sum_{j=1}^N \\pi_j \\gamma = 0 \\\\ \\gamma = -P(\\Y; \\lambda^t) \\] 重新代入\\(\\eqref{pi-eq}\\)得到 \\[ \\pi_j = \\frac{P(\\Y, i_1=j; \\lambda^t)}{P(\\Y; \\lambda^t)} \\]\n 注意到\\(A\\)满足约束条件\\(\\forall j = 1, \\dots, N, \\sum_{k=1}^N a_{j, k} = 1\\)，写出其拉格朗日函数： \\[ \\begin{aligned} L_A (A, \\gamma) \u0026amp;= \\sum_{\\X} P(\\X, \\Y; \\lambda^t) \\sum_{t=1}^{T-1} \\log a_{i_t, i_{t+1}} + \\sum_{j=1}^N \\gamma_j …","date":1684748595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684748595,"objectID":"587887e7cbf6ca44c1fa63048698197d","permalink":"https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/","section":"notes","summary":"隐马尔可夫模型 隐马尔可夫模型（Hidden Markov Model）的对","tags":null,"title":"隐马尔可夫模型","type":"book"},{"authors":null,"categories":null,"content":" 定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),g(x)\\)，如果它们各阶导数相等的越多，它们就越相似，换言之 \\[ \\text{各阶导数都相同} \\Rightarrow f(x) = g(x) \\] 可以说，函数的各阶导数即是它们的特征。\n对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即 \\[ \\text{各阶矩都相同} \\Rightarrow \\text{随机变量对应的分布相同} \\] 对于随机变量\\(X\\)，其特征函数定义为 \\[ \\varphi(t) = \\E[e^{itX}] \\] \\(e^{itX}\\)的泰勒级数为 \\[ e^{itX} = 1 + \\frac{itX}{1!} - \\frac{t^2X^2}{2!} + \\dots + \\frac{(itX)^n}{n!} \\] 代入特征函数可得 \\[ \\begin{aligned} \\varphi(t) \u0026amp;= \\E[1 + \\frac{itX}{1!} - \\frac{t^2X^2}{2!} + \\dots + \\frac{(itX)^n}{n!}] \\\\ \u0026amp;= \\E[1] + \\E[\\frac{itX}{1!}] - \\E[\\frac{t^2X^2}{2!}] + \\dots + \\E[\\frac{(itX)^n}{n!}] \\\\ \u0026amp;= 1 + \\frac{it \\overbrace{\\E[X]}^\\text{一阶矩} }{1!} - \\frac{t^2 \\overbrace{\\E[X^2]}^\\text{二阶矩} }{2!} + \\dots + \\frac{(it)^n \\overbrace{\\E[矩} }{n!} \\\\ \\end{aligned} \\] 可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。\n理性认知 \\[ \\varphi(t) = \\E[e^{itX}] = \\int_{-\\infty}^{+\\infty} e^{itx} p(x)\\; dx \\]\n而对\\(p(x)\\)进行逆傅里叶变换可得 \\[ F(t) = \\int_{-\\infty}^{+\\infty} p(x) e^{-itx} dx \\] 可见二者互为共轭关系： \\[ \\varphi(t) = \\overline{F(t)} \\]\n应用 通过求\\(t = 0\\)时的各阶导数，可以快速求得各阶矩： \\[ \\varphi^{(k)}(0) = i^k \\E[X^k] \\]\n参考 特征函数的理解\n","date":1652097068,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652097068,"objectID":"abdc103820e24c5967145fdb79e8db20","permalink":"https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/","section":"notes","summary":"定义 感性认知 根据泰勒级数我们可以得知，两个函数\\(f(x),","tags":null,"title":"特征函数","type":"book"},{"authors":null,"categories":null,"content":"Implementation Trick1 GEMM General Matrix Multiplication describes the implementation tricks that speeds up computation in neural network. Matrix multiplication is a classical, fundamental and established field in both math and computer science. And this is the reason why much effort and interest have been put into how to further speed up it and how to convert other kinds of operations into it.\nIm2Col Single Feature Map and Kernel A normal convolution operation will slide a window of the same size as kernel ($F: k_h \\times k_w$) through the feature map ($I: i_h \\times i_w$) in a row-major order (for simplicity, we will take that stride is $1$ and padding is $0$). This causes problem because numbers in a single convolution operation will span multiple columns, due to which the spatial locality cannot be exploited.\nSince the convolution operation is in essence doing the “sum of products”, we may just as well treat the convolution as dot product between two vectors.\nTo realize it, we can squeeze the kernel into a $k_h k_w \\times 1$ column vector and each window on the feature map to a $1 \\times k_h k_w$ row vector (in memory, a column vector v[N][1] is no difference from a row vector v[1][N]). Then we stack these row vectors vertically in the order as their original window would appear in the convolution. This newly synthesized matrix $L$ is usually called lowered matrix.\nAs for implementation, we don’t do this “window by window”, because usually the feature map has a large width, which still introduce the same issue of not exploiting the spatial locality when accessing across rows. For each position in the feature map, we can identify all the positions that it will appear in the lowered matrix in one off. Since kernel size is usually small, accessing this lowered matrix across rows causes less cache misses than that in the input.\nTreating $L$ as $l_h \\times l_w \\times k_h \\times k_w$, we fill up its entries in following way:\nconst int i_h = 9, i_w = 9; const int k_w = 3, k_h = 3; // For simplicity, set s_w = s_h = 1, padding = 0. const int l_h = i_h - k_h + 1, l_w = i_w - k_w + 1; void im2lower(double I[i_h][i_w], double L[l_h][l_w][k_h][k_w]) { for (int h = 0; h \u0026lt; i_h; h++) { for (int w = 0; w \u0026lt; i_w; w++) { for (int i = -k_h + 1; i \u0026lt; 1; i++) { for (int j = -k_w + 1; j \u0026lt; 1; j++) { if (h + i \u0026lt; 0 || h + i \u0026gt;= l_h) continue; if (w + j \u0026lt; 0 || w + j \u0026gt;= l_w) continue; L[h + i][w + j][-i][-j] = I[h][w]; } } } } } Normal Input and Filter By far we only consider the case of a single convolution. More often that not, the real-world convolution is done in batch with multiple channels, meaning that the input $I$ is of shape $i_n \\times i_c \\times i_w \\times i_w$ and filter $F$ is of shape $f_c \\times i_c \\times k_h \\times k_w$. Each of $f_c$ output channels is obtained as the sum of the one-on-one convolution between each of the $i_c$ kernels and each of the $i_c$ channels (which is in accordance with PyTorch’s Conv2d).\nNow we consider the case where there are multiple channels. We simplify a bit by still setting $i_n = 1$. As a result, the input $I$ is of shape $i_c \\times i_h \\times i_w$ and filter $F$ still remains $f_c \\times i_c \\times k_h \\times k_w$. Suppose each feature map contains $d = o_h \\times o_w$ unique kernel windows. Then each transformed channel should be of shape $d \\times k_h k_w$ (the order of symbols in shortened multiplication matters too; in this case, $k_h k_w$ indicates we squeeze by row); each transformed kernel should be of shape $k_h k_w \\times 1$.\nThe input contains $i_c$ such transformed $d \\times k_h k_w$ channels. Instead of doing matrix multiplication $i_c$ times, we can concatenate these $i_c$ matrices horizontally to give a single $d \\times (i_c k_h k_w)$ lowered matrix $L$.\nThe filter contains $f_c \\times i_c$ such transformed $k_h k_w \\times 1$ kernels. For each output channel, we can concatenate corresponding $i_c$ kernels vertically to facilitate the one-on-one convolution, which gives a single $(i_c k_h k_w) \\times f_c$ transformed filter $F’$.\nNow $I \\circledast F$ becomes $L \\times F’$. The output shape is $d \\times f_c$. Each input image becomes a $d \\times 1$ column vector finally, which is exactly what “Im2Col” means. This $d \\times 1$ vector can be transposed and then reshaped into $o_h \\times o_w$ to recover the convolution result (no actual transformation has to be done, just to interpret it this way). Then by applying the Im2Col trick repeatedly, we can chain up and handle consecutive convolutional layers.\nconst int i_h = 7, i_w = 7, i_c = 3; const int k_h = 3, k_w = 3, f_c = 9; // For simplicity, set s_w = s_h = 1, padding = 0. const int l_h = i_h - k_h + 1, l_w = i_w - k_w + 1; void im2lower(double I[i_c][i_h][i_w], double L[l_h][l_w][i_c][k_h][k_w]) { for (int c = 0; c \u0026lt; i_c; c++) { for (int h = 0; h \u0026lt; i_h; h++) { for (int w = 0; w \u0026lt; i_w; w++) { for (int i = -k_h + 1; i \u0026lt; 1; i++) { for (int j = -k_w + 1; j \u0026lt; 1; j++) { if (h + i \u0026lt; 0 || h + i \u0026gt;= l_h) continue; if (w + j \u0026lt; …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7a63bd6c7668795983e5878c1541acda","permalink":"https://chunxy.github.io/courses/energy-efficient-computing/notes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/energy-efficient-computing/notes/","section":"courses","summary":"Implementation Trick1 GEMM General Matrix Multiplication describes the implementation tricks that speeds up computation in neural network. Matrix multiplication is a classical, fundamental and established field in both math and computer science.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Problem Formulation The standard optimization problem will be in the form: $$ \\inf_{x \\in X} f(x) \\ $$ where $X$ is the feasible/constraint region/set, $f: \\R^n \\mapsto \\R$ is the objective function.\n Definition: We say that $x^$ is an optimal solution to the problem if $v^ = f(x^)$. In this case, we also say that $x^$ attains optimal value $v^*$.\n  Definition: We say that $x’$ is a local minimizer if $\\exists \\epsilon \u0026gt; 0, \\forall x \\in B(x’, \\epsilon), f(x) \\ge f(x’)$.\n Types of Problems   Unconstrained: $X = \\R^n$\n  Discrete programming\n$X$ is a discrete set, which means $\\forall x \\in X, \\exists \\epsilon \u0026gt; 0, X \\cap B(x, \\epsilon) = { x }$.\n Note: Every feasible solution to discrete optimization problem is a local minimizer.\n   Linear programming\n$X = { x \\in \\R^n | (a^i)^T x \\le c_i, i=1,2,\\dots,m }$ is a set defined by a finite number of linear inequalities.\n$f = b_1 x_1 + b_2 x_2 + \\dots + b_n x_n = b^T x$.\n  Quadratic programming\n$X$ is the same as that in linear programming.\n$f(x) = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j = x^T A x$, where $A = [a_{ij}] \\in R^{n \\times n}$.\n Remark: This form does not include any linear term. Generally, a quadratic function takes on the form $f(x) = x^T A x + b^T x$.\n  Remark: We may assume $A$ is symmetric, since even it is not, we can have $A’ = \\frac{A + A^T}{2}$ and $$ x^T A x = x^T A^T x \\to x^T A x = x^T A’ x $$\nThe right hand side is obvious because $x^T A^T x$ is a number, and it equals to its transpose $x^T A x$.\n   Semi-definite programming\n Definition: Consider $Q \\in \\mathcal{S}^{n}$, where $\\mathcal{S}^n$ is the set of $n \\times n$ symmetric matrix. The following is equivalent:\n $Q$ is positive semi-definite (short as PSD, denoted as $Q \\succcurlyeq 0$). $\\forall x \\in \\R^n, x^T Q x \\ge 0$. All eigenvalues of $Q$ are non-negative.   Let $C, A_1, \\dots, A_m \\in S^n$ and $b \\in \\R^m$ be given, the semi-definite programming is $$ \\begin{aligned} \\inf_{x \\in \\R^n} \\quad \u0026amp; b^T x \\ \\textrm{s.t.} \\quad \u0026amp; C - \\underbrace{\\sum_{i=1}^m x_i A_i}_{M(x)} \\succcurlyeq 0 \\end{aligned} $$\n Remark: The constraint is called a linear matrix inequality. Observe that $M: \\R^m \\mapsto S^n$ satisfies $$ M(\\alpha x + \\beta y) = \\alpha M(x) + \\beta M(y) $$ $M$ is a linear map.\n  Remark: Compare between linear programming and positive semi-definite programming: $$ \\begin{gathered}\n\\begin{aligned}[t] \\inf_{x \\in \\R^n} \\quad \u0026amp; b^T x \\ \\textrm{s.t.} \\quad \u0026amp; (a^i)^T x \\le c_i, \\ \u0026amp; i = 1,\\dots,m \\end{aligned}\n\\quad \\quad\n\\begin{aligned}[t] \\inf_{x \\in \\R^n} \\quad \u0026amp; b^T x \\ \\textrm{s.t.} \\quad \u0026amp; C - \\sum_{i=1}^m x_i A_i \\succcurlyeq 0 \\end{aligned}\n\\end{gathered} $$ In linear programming, construct matrices: $$ C’ = \\begin{pmatrix} c_1 \u0026amp; \u0026amp; \u0026amp; \\ \u0026amp; c_2 \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \u0026amp; c_m \\end{pmatrix},\nA_i’ = \\begin{pmatrix} a_1^i \u0026amp; \u0026amp; \u0026amp; \\ \u0026amp; a_2^i \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \u0026amp; a_m^i \\ \\end{pmatrix} $$ Then $$ C’ - \\sum_{i=1}^m x_i A_i’ = \\begin{pmatrix} c_1 - \\sum_{j=1}^m x_j a_1^j \u0026amp; \u0026amp; \u0026amp; \\ \u0026amp; c_2 - \\sum_{j=1}^m x_j a_2^j \u0026amp; \u0026amp; \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \u0026amp; c_m - \\sum_{j=1}^m x_j a_m^j \u0026amp; \\ \\end{pmatrix}\n\\succcurlyeq 0 $$ because a diagonal matrix is PSD if and only if all of its diagonal entries are non-negative.\nIn this sense, linear programming is special case of semi-definite programming where $C$ and $A_i$’s are all diagonal.\n   Examples of Problems   Air traffic control\n $n$ airplanes are arriving. $i$-th airplane arrives within $[a_i, b_i]$. Assume airplanes arrive and land in order. Let $t_i$ be the landing time assigned to $i$-th airplane $i$. The metering time is defined to be the time difference between two consecutive airplane landings.  The implicit constraints derived from above conditions is that $a_i \\le t_i \\le b_i$ and $ t_i \\le t_{i+1}$. For safety, we want to the minimum metering time to be maximized. That is, $$ \\begin{aligned} \\max_{t} \\quad \u0026amp; f(t) \\triangleq \\min_{1 \\le i \\le n-1} t_{i+1} - t_i \\ \\textrm{s.t.} \\quad \u0026amp; a_i \\le t_i \\le b_i, i=1,\\dots,n \\ \u0026amp; t_i \\le t_{i+1}, i=1,\\dots,n-1 \\end{aligned} $$ The constraints are all linear. The min operation in $f$, however, doesn’t comfort us. We can introduce a new variable $z$ and convert the original problem to an equivalent one: $$ \\begin{aligned} \\max_{t, z} \\quad \u0026amp; z \\ \\textrm{s.t.} \\quad \u0026amp; z = \\min_{1 \\le i \\le n-1} t_{i+1} - t_i \\ \u0026amp; a_i \\le t_i \\le b_i, i=1,\\dots,n \\ \u0026amp; t_i \\le t_{i+1}, i=1,\\dots,n-1 \\end{aligned} $$ Further, since the objective is to maximize and $z$’s coefficient is positive, the problem can be converted to $$ \\begin{aligned} \\max_{t, z} \\quad \u0026amp; z \\ \\textrm{s.t.} \\quad \u0026amp; z \\le t_{i+1} - t_i, i=1,\\dots,n-1\\ \u0026amp; a_i \\le t_i \\le b_i, i=1,\\dots,n \\ \u0026amp; t_i \\le t_{i+1}, i=1,\\dots,n-1 \\end{aligned} $$ Now the problem becomes a linear one and is easy to solve.\n  Data fitting problem\nGiven data points $(a_i, b_i) \\in \\R^n \\times \\R$, a typical choice to fit those data would be an affine function $f(x) = y^T x + t$. Other than the choice of function, the choice of objective function matters too.\n  Least squares\nThe objective …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"caf4342983f09ca4246db72def9ff018","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/1-optimization-problem/","section":"courses","summary":"Problem Formulation The standard optimization problem will be in the form: $$ \\inf_{x \\in X} f(x) \\ $$ where $X$ is the feasible/constraint region/set, $f: \\R^n \\mapsto \\R$ is the objective function.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Convex Set Given $x^1, \\dots, x^k \\in \\R^n$, we say that $y = \\sum_{i=1}^k \\alpha_i x^{i}$ is\n a linear combination of $x^1, \\dots, x^k$ if $\\alpha_1, \\dots, \\alpha_k \\in \\R$; an affine combination of $x^1, \\dots, x^k$ if $\\sum_{i=1}^k \\alpha_i = 1$; a convex combination of $x^1, \\dots, x^k$ if $\\sum_{i=1}^k \\alpha_i = 1$ and $0 \\le \\alpha_1, \\dots, \\alpha_k$.  Let $S \\in \\R^n$,\n $S$ is a linear subspace if $\\forall x,y \\in S, \\alpha,\\beta \\in \\R, \\alpha x + \\beta y \\in S$; $S$ is an affine subspace if $\\forall x,y \\in S, \\alpha \\in \\R, \\alpha x + (1-\\alpha) y \\in S$; $S$ is a convex set if $\\forall x,y \\in S, 0 \\le \\alpha \\le 1, \\alpha x + (1 - \\alpha) y \\in S$.   Proposition: The following statements are equivalent:\n $S$ is affine. Any affine combination of a finite number of points in $S$ belongs to $S$. $S$ can be written as $S = {x} + V \\triangleq { x + v: v \\in V }$. Note that though $V$ is unique, $x$ is not.    Proposition: The following statements are equivalent:\n $S$ is convex. Any convex combination of a finite number of points in $S$ belongs to $S$.   Examples of Convex Set   Nonnegative orthant (in 2-D, an orthant is called a quadrant): $\\R^n_+ \\triangleq { x \\in \\R^n: \\forall i, x_i \\ge 0 }$\n  Hyperplane\nGiven $w \\in \\R^n, b \\in R$, the hyperplane $H(s, c)$ is the set ${ x \\in \\R^n: s^T x = c }$\n  Half-space\nGiven $w \\in \\R^n, b \\in R$, the upper half-space $H^+(s, c)$ is the set ${ x \\in \\R^n: s^T x \\ge c }$; the lower half-space $H^-(s, c)$ is the set ${ x \\in \\R^n | s^T x \\le c }$.\nNote that hyperplane $H(s, c)$ is the intersection of $H^+(s, c)$ and $H^-(s, c)$.\n  Euclidean ball\nGiven the center $\\bar x \\in \\R^n$ and the radius $r \u0026gt; 0$, the Euclidean ball $B(\\bar x, r)$ is the set ${ x \\in \\R^n: ||x - \\bar x||_2 \\le r }$.\nA generalization of Euclidean ball would be to extend the norm to other numbers that are larger than 1. For $q \\ge 1$ $$ B_q(\\bar x, r) = { x \\in \\R^n: ||x - \\bar x||q \\le r } $$ is also a convex set. Note that $||z||\\infty = \\lim_{q \\to \\infty} (\\sum_i z_i^q)^{1/q} = \\max_i z_i$.\n  Convex cone\n Definition: A set $K \\in \\R^n$ is called a cone if $\\forall x \\in K, \\alpha \u0026gt; 0, \\alpha x \\in K$.\n Linear subspace is a cone. Affine subspace is not necessarily so.\n Definition: A convex cone is a cone that is convex.\n Some examples of convex cone include $\\R^n_+$, and $\\mathcal{S}_+^n$, which is the set of $n \\times n$ PSD matrices.\n  Convexity-preserving Operations For any two convex sets $S_1$ and $S_2$, there are some binary operators that will preserve the convexity after being applied.\n  Set operations\n$S_1 \\cup S_2$ is not necessarily convex. $S_1 \\cap S_2$ is always convex.\n  Affine transformations\n Definition: We say that $A: \\R^n \\mapsto \\R^m$ is affine if $\\forall x,y \\in \\R^n, \\alpha \\in \\R$, $$ A(\\alpha x + (1 - \\alpha) y) = \\alpha A(x) + (1 - \\alpha) A(y) $$\n Let $A: \\R^n \\mapsto \\R^m$ be affine, $S \\subseteq \\R^n$ be convex. Then $A(S) \\triangleq { A(x): x \\in S }$ is convex.\nThere are two types of affine transformation worth noting.\n  Rotation: $A(x) = U x$ where $U \\in \\R^{n \\times n}$ and $U U^T = U^T U= I$.\n  Projection: $A(x) = P x$ where $P \\in \\R^{n \\times n}$ and $P^2 = P$.\nFor orthogonal projection, its projection matrix further satisfies $P^T = P$.\n  As an example of affine transformation, given center $\\bar x$ and axes $Q$ which is positive definite, the ellipsoid $E(\\bar x, Q)$ is the set ${ x \\in \\R^n | (x - \\bar x)^T Q (x - \\bar x) \\le 1 }$. Note that $B(\\bar x, r) = E(\\bar x, I/r^2)$.\n Remark: When $Q$ is positive semi-definite, it might occur that ${ x \\in \\R^n | (x - \\bar x)^T Q (x - \\bar x) \\le 1 }$ will degenerate into two parallel lines. Just consider the case when $Q = [1,2] [1,2]^T$.\n  Proposition: There always exists an affine transformation $A: \\R^n \\mapsto \\R^n$ such that $A(B(0, 1)) = E(\\bar x, Q)$. Note that $B(\\bar x, r) = E(\\bar x, I/r^2)$. See this handout for the construction of such transformation.\n Therefore, ellipsoid is also a convex set (Problem 2 of Homework 1 proves this from the first principle).\n  Topological Preparation Basic Topology  Definition: Given a set $S \\in \\R^n, S \\ne \\emptyset$ and a point $x \\notin S$, we want to find a point in $S$ that is closest (in terms of Euclidean distance) to $x$. Formally, $\\hat x = \\arg\\min_{z \\in S} ||z - x||_2$ is called the projection of $x$ onto $S$, denoted as $\\hat x = \\Pi_S(x)$.\n Note that this projection does not necessarily exist. Neither the projection is unique. Under what conditions can we guarantee the existence and uniqueness of projection? Before that, some concepts are needed. Let $S \\subseteq \\R^n$ be a set.\n Definition: $x$ is an interior point of $S$ if $\\exists \\epsilon \u0026gt; 0, B(x, \\epsilon) \\subseteq S$. The collection of all interior points of $S$ is called the interior of $S$, denoted as $\\mathop{\\mathrm{int}} S$.\n We say that $S$ is open if $S = \\mathop{\\mathrm{int}} S$. We say that $S$ is closed if $\\R^n \\setminus S$ is open. Note that it can be the case that a set …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cd35823bcdd172ade30a180ccaeaec5d","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/2-convex-set/","section":"courses","summary":"Convex Set Given $x^1, \\dots, x^k \\in \\R^n$, we say that $y = \\sum_{i=1}^k \\alpha_i x^{i}$ is\n a linear combination of $x^1, \\dots, x^k$ if $\\alpha_1, \\dots, \\alpha_k \\in \\R$; an affine combination of $x^1, \\dots, x^k$ if $\\sum_{i=1}^k \\alpha_i = 1$; a convex combination of $x^1, \\dots, x^k$ if $\\sum_{i=1}^k \\alpha_i = 1$ and $0 \\le \\alpha_1, \\dots, \\alpha_k$.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Convex Function  Definition: Let $f: \\R^n \\mapsto \\R_+$, where $\\R_+ = \\R \\cup { +\\infty }$ (in convex discussion, usually only $+\\infty$ is included) be an extended real-valued function. Note that $f$ shouldn’t trivially be $+\\infty$ everywhere. We say that $f$ is convex if $\\forall x_1, x_2 \\in \\R^n, \\alpha \\in [0,1]$, $$ f(\\alpha x_1 + (1-\\alpha) x_2) \\le \\alpha f(x_1) + (1-\\alpha) f(x_2) $$ Note that for $x \\in \\R$, $x \u0026lt; +\\infty, x + \\infty = \\infty + x = \\infty, +\\infty \\le +\\infty$.\n Interestingly, convex functions on an open domain are always continuous.\n Definition: The epigraph of a function $f: \\R^n \\mapsto \\R$ is the set $\\epi f \\triangleq { (x, t) \\in \\R^n \\times \\R: f(x) \\le t }$.\nDefinition: The effective domain of $f$ is the set $\\dom f \\triangleq { x \\in \\R^n: f(x) \u0026lt; \\infty }$.\n Note that the real line does not contain $\\infty$. Therefore, the effective domain of $f(x) = x$ is still $\\R$.\n Definition: Let $S \\subseteq \\R^n$ be a set. The indicator of $S$ is the function $$ \\mathbb 1_S (x) = \\begin{cases} 0, \u0026amp; x \\in S \\ +\\infty, \u0026amp; \\text{otherwise} \\end{cases} $$\n Using the indicator, we have $$ \\underset{\\text{constrained}}{\\inf_{x \\in S} f(x)} \\iff \\underset{\\text{unconstrained}}{\\inf_{x \\in \\R^n} f(x) + \\mathbb 1_S(x)} $$ That is, we convert a constrained problem to a unconstrained one.\n Proposition (verify it): Let $f: \\R^n \\mapsto \\R_+$. Then, $f$ is convex (as a function) if and only if $\\epi f$ is convex (as a set). Moreover, let $S \\subseteq \\R^n$ be a set. Then $S$ is convex (as a set) if and only if $\\mathbb 1_S$ is convex (as a function).\n The proposition above associates the convexity of a function with that of its epigraph; and the convexity of a set with that of its indicator.\n Corollary: Jensen’s inequality. Let $f: \\R^n \\mapsto \\R_+$. Then $f$ is convex if and only if $f(\\sum_{i=1}^m \\alpha_i x_i) \\le \\sum_{i=1}^m \\alpha_i f(x_i)$ for any $m \\in \\N^+$ and any $x_1, \\dots, x_m \\in \\R^n$ and any $\\alpha_1, \\dots, \\alpha_m \\ge 0$ such that $\\sum_{i=1}^m \\alpha_i = 1$.\nProof:\n  Sufficiency\nSufficiency is easy to show by interpreting the definition of convex function.\n  Necessity\nFor $i=1,\\dots,m$, we have $$ \\big( x_i, f(x_i) \\big) \\in \\epi f $$ Since $\\epi f$ is convex, $$ \\sum_{i=1}^m \\alpha_i \\big( x_i, f(x_i) \\big) = \\big( \\sum_{i=1}^m \\alpha_i x_i, \\sum_{i=1}^m \\alpha_i f(x_i) \\big) \\in \\epi f $$ which means $$ f(\\sum_{i=1}^m \\alpha_i x_i) \\le \\sum_{i=1}^m \\alpha_i f(x_i) $$\n   Convexity-preserving Operations   Non-negative combination\nLet $f_1, \\dots, f_m$ be convex functions, $\\alpha_1, \\dots, \\alpha_m \\ge 0$ be non-negative scalars. Then, $$ f \\triangleq \\sum_{i=1}^m \\alpha_i f_i \\text{ is convex.} $$\n  Pointwise supremum\nLet $I$ be an index set (either finite or infinite) and ${ f_i: i \\in I }$ be a collection of convex functions. Then $$ f \\triangleq \\sup_{i \\in I} f_i \\text{ is convex.} $$ To show it, let $x_1, x_2 \\in \\R^n$ and $\\alpha \\in [0, 1]$. Then, $$ \\begin{aligned} \u0026amp;f(\\alpha x_1 + (1-\\alpha) x_2) = \\sup_{i \\in I} f_i(\\alpha x_1 + (1-\\alpha) x_2) \\ \u0026amp;\\le \\sup_{i \\in I} \\big( \\alpha f_i(x_1) + (1-\\alpha) f_i(x_2) \\big) \\ \u0026amp;\\le [\\alpha \\sup_{i \\in I} f_i(x_1)] + [(1-\\alpha) \\sup_{i \\in I} f_i(x_2)] \\ \u0026amp;= \\alpha f(x_1) + (1-\\alpha) f(x_2) \\end{aligned} $$\nGeometrically, pointwise supremum is intersecting the epigraphs of $f_i$’s.\n Example: Consider the mapping $f: \\R^{m \\times n} \\supseteq X \\to ||X||$ where $||X||$ is the largest singular value of $X$. Show that $f$ is convex.\nBy the Courant-Fischer theorem, $$ \\begin{aligned} ||X|| \u0026amp;= \\max_{u \\in \\R^m, v \\in \\R^n} u^T X v \\ \\text{s.t.} \u0026amp;\\quad ||u||2 = 1, ||v||2 = 1 \\end{aligned} $$ Let $f{u,v}(X) = u^T X v$ and $I = { (u,v) \\in \\R^m \\times \\R^n: ||u||2 = 1, ||v||2 = 1 }$. Then, $$ f(X) = \\max{(u,v) \\in I} f{u,v}(X) $$ Note that $f{u,v}(X)$ is linear and thus convex in $X$. It follows directly from the pointwise supremum principle that $f$ is convex.\n   Composition with increasing function\nLet $g: \\R^n \\mapsto \\R$ be convex, $h: \\R \\mapsto \\R$ be convex. $h \\circ g$ is not generally convex. To verify it, take $$ h(x) = -x, g(x) = x^2 $$ But if $h$ is convex as well as increasing, then $h \\circ g$ is convex.\n  Restriction on lines\nGiven a point $x_0 \\in \\R^n$ and a direction $h \\in \\R^n \\setminus { 0 }$, we call the set $$ { x_0 + t h: t \\in \\R } $$ a line through $x_0$ in the direction $h$. Let $f: \\R^n \\mapsto \\R$ be a function. Define\n$$ \\tilde f_{x_0, h}(t) \\triangleq f(x_0 + t h) $$ as the restriction of $f$ on the line ${ x_0 + t h: t \\in \\R }$.\n  Then, $f$ is convex if and only if $\\tilde f_{x_0, h}$ is convex for any $x_0 \\in \\R^n$ and any $h \\in \\R^n \\setminus { 0 }$.\nDifferentiable Convex Function  Theorem: Let $f: \\R^n \\mapsto \\R$ be differentiable, i.e. $\\nabla f$ exists. Then, $f$ is convex if and only if for every $x, y \\in \\R^n$, $$ f(x) \\ge f(y) + (\\nabla f(y))^T (x - y) \\tag{gradient inequality} $$\n For a fixed $y$, the right-hand side of the gradient inequality is in essence an affine function of $x$. …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"635ed3e7bcda49b3d6c4d6b193e40f5a","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/3-convex-function/","section":"courses","summary":"Convex Function  Definition: Let $f: \\R^n \\mapsto \\R_+$, where $\\R_+ = \\R \\cup { +\\infty }$ (in convex discussion, usually only $+\\infty$ is included) be an extended real-valued function. Note that $f$ shouldn’t trivially be $+\\infty$ everywhere.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Linear Programming Recall that the linear programming problem is $$ \\label{lp} \\begin{aligned} \\min_{x} \\quad\u0026amp; c^T x \\ \\text{s.t.} \\quad\u0026amp; a_i^T x \\le b_i, i=1,\\dots,m \\end{aligned} \\tag{LP} $$ where $c \\in \\R^n$ and $a_i \\in \\R^{n}, b_i \\in \\R, i=1,\\dots,m$.\n Definition: A polyhedron is the intersection of a finite set of halfspaces. A bounded polyhedron is called a polytope.\n Therefore, the feasible set of LP is a polyhedron. Interestingly, all polyhedrons are convex. That is, the feasible set of LP is convex.\nStandard Form The standard form of LP is $$ \\label{standard-lp} \\begin{aligned} \\min_{x \\in \\R_+^n} \\quad \u0026amp; c^T x \\ \\text{s.t.} \\quad \u0026amp; A x = b \\ \\end{aligned} \\tag{Standard LP} $$ where $c \\in \\R^n, A \\in \\R^{m \\times n}, b \\in \\R^m$. Standard LP has standard solutions. Thus, the next question is how to convert an ordinary LP problem to a standard one.\nNow write $x = x^+ - x^-$, where $x^+, x^- \\in \\R_+^n$. Introduce another slack variable $s \\in \\R_+^m$. $\\eqref{lp}$ is equivalent to $$ \\begin{aligned} \\min_{x^+, x^- \\ge 0, s \\ge 0} \\quad \u0026amp; c^T (x^+ - x^-) \\ \\text{s.t.} \\quad \u0026amp; A (x^+ - x^-) + s = b \\ \\end{aligned} $$ Let $$ A’ = \\left[\\begin{array}{c:c} A \u0026amp; -A \u0026amp; I \\end{array}\\right] \\in \\R^{m \\times (2n+m)} \\ x’ = \\begin{bmatrix} x^+ \\ \\hdashline x^- \\ \\hdashline s \\end{bmatrix} \\in \\R_+^{2n+m}, c’ = \\begin{bmatrix} c \\ \\hdashline -c \\ \\end{bmatrix} \\in \\R_+^{2n} $$ The problem becomes $$ \\begin{aligned} \\min_{x’ \\in \\R_+^{2n+m}} \\quad \u0026amp; (c’)^T x’ \\ \\text{s.t.} \\quad \u0026amp; A’ x’ = b \\ \\end{aligned} $$ which observes the formality of standard LP. Here lays down again the standard LP problem:\n$$ \\label{primal} \\begin{aligned} v_p^* = \\min_{x \\in \\R_+^n} \\quad \u0026amp; c^T x \\ \\text{s.t.} \\quad \u0026amp; A x = b \\ \\end{aligned} \\tag{P} $$\nThe first question to ask is whether there is a optimal solution to it. The answer is:\n Proposition: If $\\eqref{primal}$ is feasible, then either 1) the optimal value $v^* = -\\infty$ (no optimal solution),or 2) it has an optimal solution.\n Farkas’ Lemma But the second question arises: how to certify that $\\eqref{primal}$ is infeasible? We meet the similar dilemma to that when dealing with set-set separation: it is easy to test the feasibility of any $x$; but it is prohibitive to test the feasibility of all $x$’s just to show that the original problem is infeasible.\nThe idea is that, given the feasible polyhedron $P \\triangleq { x \\in \\R_+^n: Ax = b }$ of the original problem, we construct another auxiliary polyhedron $Q$ such that exactly one of the following holds:\n $P \\ne \\emptyset, Q = \\emptyset$; $P = \\emptyset, Q \\ne \\emptyset$.   Theorem: Farkas’ lemma. Let $A \\in \\R^{m \\times n}$ and $b \\in \\R^m$ be given. Then exactly one of the following systems is solvable:\n $A x = b, x \\ge 0$; $A^T y \\le 0, b^T y \u0026gt; 0$.  Interpretation:\n$A x = b, x \\ge 0$ means that $b$ is a non-negative linear combinations of $a_1, \\dots, a_n$. $A^T y \\le 0$ means that $y$ forms an obtuse angle to $a_1, \\dots, a_n$; $b^T y \u0026gt; 0$ means $y$ forms an acute angle with $b$.\nProof:\nWe claim that the above two statements cannot be solvable at the same time. or else this gives rise to the contradiction for some $x_0 \\ge 0, y_0 \\in \\R^m$: $$ \\underbrace{x_0^T}{\\ge 0} \\underbrace{A^T y_0}{\\le 0} = y_0^T A x = \\underbrace{y_0^T b}_{\u0026gt; 0} $$ We then claim that the above two statements cannot be unsolvable at the same time. Suppose $1$ is unsolvable.\nIn this case, $b \\notin A^+ \\triangleq { Ax: x \\ge 0 }$. It can be verified that $A^+$ is non-empty ($0 \\in A^+$), closed (closeness is not generally preserved under affine transformation; but in this case it is \u0026lt;refer to this handout\u0026gt;) and convex (because convexity is preserved under affine transformation $A^+ = A \\R_+^n$). Then by point-set separation theorem, for any $x \\ge 0$, there exists a $y_0 \\in \\R^m$ such that $$ \\max_{x \\ge 0} y_0^T A x \u0026lt; y_0^T b $$ Take $x = 0$, we have $$ y_0^T b \u0026gt; 0 $$ We claim that $A^T y_0 \\le 0$. Suppose on the contrary that there exists an $i$ such that $(A^T y_0)i$ is positive. Then for any $\\lambda \\ge 0$, take $x = \\lambda e_i$ and give $$ y_0^T A x = x^T A^T y_0 = \\underbrace{\\lambda}{\\ge 0} \\underbrace{(A^T y_0)i}{\u0026gt; 0} \u0026lt; y_0^T b $$ which is impossible when $\\lambda \\to \\infty$.\nTherefore, $2$ is solvable when $1$ is unsolvable. $1$ and $2$ cannot be unsolvable at the same time.\n We construct $Q’$ as ${ y \\in \\R^m: A^T y \\le 0, b^T y \u0026gt; 0 }$. Note that $Q’$ is not a polyhedron yet, because ${ y \\in \\R^m: b^T y \u0026gt; 0 }$ is open and is not a half-space. However, observe that (verify it) $$ A^T y \\le 0, b^T y \u0026gt; 0 \\text{ is solvable} \\iff A^T y \\le 0, b^T y = 1 \\text{ is solvable} $$ ${ y \\in \\R^m: b^T y = 1 }$ can be rewritten as ${ y \\in \\R^m: b^T y \\ge 1, b^T y \\le 1 }$ which is an intersection of halfspaces. Therefore, $Q \\triangleq { y \\in \\R^m: A^T y \\le 0, b^T y \\ge 1, b^T \\le 1 }$ is non-empty if and only if $Q’$ is non-empty. Moreover, $Q$ is an intersection of half-spaces as desired.\nNow, we convert the infeasibility of $P$ into the …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1e1d7154c495e75f5dd9e5a6a3758086","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/4-linear-programming/","section":"courses","summary":"Linear Programming Recall that the linear programming problem is $$ \\label{lp} \\begin{aligned} \\min_{x} \\quad\u0026 c^T x \\ \\text{s.t.} \\quad\u0026 a_i^T x \\le b_i, i=1,\\dots,m \\end{aligned} \\tag{LP} $$ where $c \\in \\R^n$ and $a_i \\in \\R^{n}, b_i \\in \\R, i=1,\\dots,m$.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Though some problems can be relaxed to accommodate the linear programming form, applications of LP are still limited due to its linear constraints. To extend, a natural idea is to gradually allow for other kinds of non-linear constraints. But that would go too far away from the established theories for linear programming.\nIn this post, we study the conic linear programming, which is a phased open-up to non-linear problems.\nConcept Preparation “Good” Order and “Proper” Cone Before exploring other non-linear problems, we first study the properties of a “good” order $\\succeq$. between vectors. We expect it to have the following basic three properties of a partial-order:\n  Reflexivity: $\\forall u \\in \\R^n, u \\succeq u$;\n  Anti-symmetry $$ \\forall u,v \\in \\R^n, u \\succeq v, v \\succeq u \\to u = v $$\n  Transitivity $$ \\forall u,v,w \\in \\R^n, u \\succeq v, v \\succeq w \\to u \\succeq w $$\n  Other than them, we expect $\\succeq$ to further possess the following two arithmetic properties:\n  Homogeneity $$ \\forall u,v \\in \\R^n, u \\succeq v, \\alpha \\succeq 0, \\alpha u \\succeq \\alpha v $$\n  Additivity $$ \\forall u,v,w,z \\in \\R^n, u \\succeq v, w \\succeq z \\to u + w \\succeq v + z $$\n  We say the $\\succeq$ order is “good” if it satisfies the above five properties. The above five is from the algebraic perspective. To illustrate it geometrically, consider the set $K \\triangleq { x \\in \\R^n: x \\ge 0 }$. We say $K$ is a “proper” cone in that it is\n  non-empty and closed under addition $$ \\forall u, v \\in K, u + v \\in K $$\n  conic $$ \\forall u \\in K, \\alpha \u0026gt; 0 \\to \\alpha u \\in K \\ $$\n  pointed $$ u, -u \\in K \\to u = 0 $$\n  We further claim that a proper cone is convex and contains the zero vector (verify it).\n Remark: The pointedness plus closeness (not closeness under addition) implies that there is no complete line in this cone, which equivalently means there is no non-trivial subspace (i.e., except ${ 0 }$ and the universe) inside this cone. To show it, let $K$ be a closed pointed cone, suppose on the contrary there exists $u, v \\in K$ such that for every $\\alpha \\in \\R$, we have $u + \\alpha(v - u) \\in K$. For $t \u0026gt; 1$, consider the sequence ${w_+^t}{t=1}^{\\infty}$ and ${w-^t}{t=1}^{\\infty}$ where $$ w+^t = \\frac{u + t(v-u)}{|u + t(v-u)|2}, w-^t = \\frac{u - t(v-u)}{|u - t(v-u)|2} $$ Now $w+^t, w_-^t \\in K$ for all $t \\ge 1$. But $w_+^t \\to w \\triangleq \\frac{v-u}{|v-u|2}$ and $w-^t \\to -w$ . Since $K$ is closed, we have $w, -w \\in K$. However, since $w$ has unit norm and is not zero vector, it follows that $K$ is not pointed, which is a contradiction.\n  Remark: At times, people also refer to this as the salient property of a cone. A proper cone is variously defined on a subset of these properties (closeness, closeness under addition, pointed, salient, and essentially, conic) depending on the context.\n In fact, the algebraic properties and the geometric properties can derive each other. A good order and a proper cone have a one-to-one relationship. Now we ask, in an arbitrary $n$-d universe (or a finite-dimensional Euclidean space), is $\\ge$ the only good order, or equivalently, is $\\R_+^n$ the only proper cone? The answer is no.\nExamples of Proper Cone   Lorentz cone / second-order cone / ice cream cone $$ \\mathcal{Q}^{n+1} \\triangleq { (t, x) \\in \\R \\times \\R^n: t \\ge ||x||_2 } $$ $\\mathcal{Q}^{n+1}$ is a closed proper cone. Then what is the good order associated with this proper cone?\n  Semi-definite cone $$ \\mathcal{S}+^n \\triangleq { X \\in \\mathcal{S}^n: u^T X u \\ge 0, \\forall u \\in \\R^n } $$ $\\mathcal{S}+^n$ is a closed proper cone. Then what is the good order associated with this proper cone?\n  Zero cone: ${ 0 }$\n  New cones from old ones by Cartesian product\nLet $K_1, \\dots, K_m$ be closed proper cones (with non-empty interior). Then $$ K \\triangleq K_1 \\times \\dots \\times K_m = { (x_1, \\dots, x_m): x_i \\in K_i, \\forall i=1,\\dots,m } $$ is a closed proper cone with non-empty interior (with non-empty interior).\n Remark: $\\R_+^n, \\mathcal{Q}^{n+1}, \\mathcal{S}+^n$ have non-empty interior: $$ \\intr(\\R+^n) = \\R_{++}^n \\ \\intr(\\mathcal{Q}^{n+1}) = { (t, x) \\in \\R \\times \\R^n: t \u0026gt; ||x||2 } \\ \\intr(\\mathcal{S}+^n) = \\mathcal{S}_{++}^n \\ $$\n   Conic Linear Programming Formulation Recall that linear programming is\n$$ \\begin{align*} \\min \u0026amp; \\quad c^T x \\ \\text{s.t.} \u0026amp; \\quad b - A x \\ge 0 \\end{align*} $$ Now replacing $\\ge$ with the good order $\\succeq$ gives the conic linear programming. $$ \\begin{align*} \\tag{CLP} \\min \u0026amp; \\quad c^T x \\ \\text{s.t.} \u0026amp; \\quad b - A x \\succeq 0 \\end{align*} $$ The next question is, can we recover Farkas’ lemma and strong duality in this setting? The answer is yes. And the good aspect of the good order is that we can recover the conclusions in linear programming verbatim. Next we generalize the LP problem and show the same result applies.\nLet $E$ be a finite-dimensional Euclidean space (e.g. $\\R^n, \\mathcal{S}^n$) and $\\langle \\cdot, \\cdot \\rangle$ be the inner product on $E$ (e.g., on $\\R^n$, $\\langle x, …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4d0759e282b38c952274d74cda3e8aee","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/5-conic-linear-programming/","section":"courses","summary":"Though some problems can be relaxed to accommodate the linear programming form, applications of LP are still limited due to its linear constraints. To extend, a natural idea is to gradually allow for other kinds of non-linear constraints.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Firstly consider that in the previous (conic) linear programming discussion, $$ \\tag{Problem} \\begin{aligned} \\min \\quad \u0026amp; c^T x \\ \\text{s.t.} \\quad \u0026amp; \\hat a_i^T x \\le \\hat b_i, i=1,2,\\dots,m \\end{aligned} $$ The coefficients $\\hat a_i, \\hat b, \\hat c$ are given data. They correspond to the measurements in the real world. But it is highly likely that these measurements are not 100% accurate. How do we take into account these uncertainties?\nWithout loss of generality, we may assume that $\\hat c$ is deterministic, since the above is equivalent to $$ \\begin{aligned} \\min \\quad \u0026amp; t \\ \\text{s.t.} \\quad \u0026amp; \\hat c^T x \\le t \\ \u0026amp; \\hat a_i^T x \\le \\hat b_i, i=1,2,\\dots,m \\end{aligned} $$ In this new problem, we bring all uncertainties into the constraint and the coefficient $[0,\\dots,0,1]$ for the variable $[x^T, t]$ is deterministic.\nThe problem remains how to handle the uncertainty in the constraint in $\\text{(Problem)}$. There are several viable methods:\n  Stochastic optimization\nStochastic optimization assumes that data follow a probability distribution $\\mathbb{P}$ (unknown). The constraint becomes $$ \\Pr(\\hat a_i^T x \\le \\hat b_i) \\ge 1 - \\delta $$ for some $\\delta \u0026gt; 0$. This method is non-trivial, due to the integral of multi-dimensional probability distribution function.\n  Robust optimization\nThe assumption is that data is drawn from a ambiguity set $\\mathcal{U}$. We require that $$ \\hat a_i^T x \\le \\hat b_i, \\forall i=1,2,\\dots, \\forall (\\hat a_i, \\hat b_i) \\in \\mathcal{U} $$\n  Distributionally-robust optimization\nThis is kind of the combination of the above two methods. The assumption is that data follow a probability distribution $\\mathbb{P}$, which in turn belongs to some ambiguity set $\\mathcal{U}$. The constraint becomes $$ \\inf_{\\mathbb{P} \\in \\mathcal{U}} \\Pr(\\hat a_i^T x \\le \\hat b_i) \\ge 1 - \\delta, \\forall i=1,2,\\dots $$ for some $\\delta \u0026gt; 0$.\n  Robust Linear Programming Consider the problem $$ \\label{rp} \\tag{R} \\begin{aligned} \\min \\quad \u0026amp; c^T x \\ \\text{s.t.} \\quad \u0026amp; \\underbrace{[\\hat \\alpha_i^T, \\hat b_i]}_{\\hat a_i^T} \\underbrace{[x^T, x’]^T}z \\le 0, \\ \u0026amp; \\quad \\hat a_i \\in \\mathcal{U}i, i=1,\\dots,m \\ \u0026amp; \\underbrace{x’}{z{n+1}} = -1 \\end{aligned} $$ where $\\mathcal{U}i \\triangleq { y \\in \\R^{n+1}: y = u_i + B_i v, B_i \\in \\mathcal{S}{++}^{n+1}, |v| \\le 1 }$ is an ellipsoid.\nNote that $\\eqref{rp}$ is not LP actually, as it may contain infinitely-many linear constraints. This is usually hard. Just imagine its dual problem. For a primal that has infinitely-many constraints, its dual has infinitely-many variables.\nThe key question now is how to tackle $\\hat a_i^T z \\le 0, \\forall \\hat a_i \\in \\mathcal{U}i$ (ignoring $z{n+1} = -1$ for now). In essence, it is equivalent to $$ \\begin{aligned} \\left[ \\sup_{\\hat a_i \\in \\mathcal{U}i} \\hat a_i^T z \\right] \u0026amp;\\le 0 \\iff \\ \\left[ \\sup{|v| \\le 1} (u_i + B_i v)^T z \\right] \u0026amp;\\le 0 \\iff \\ u_i^T z + \\left[ \\sup_{|v| \\le 1} v^T B_i z \\right] \u0026amp;\\le 0 \\iff \\ y_i^T z + |B_i z|_2 \u0026amp;\\le 0 \\end{aligned} $$ This is exactly an SOCP constraint.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e5f39173423ff063c28de68fb3743bb4","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/","section":"courses","summary":"Firstly consider that in the previous (conic) linear programming discussion, $$ \\tag{Problem} \\begin{aligned} \\min \\quad \u0026 c^T x \\ \\text{s.t.} \\quad \u0026 \\hat a_i^T x \\le \\hat b_i, i=1,2,\\dots,m \\end{aligned} $$ The coefficients $\\hat a_i, \\hat b, \\hat c$ are given data.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Quadratically Constrained Quadratic Programming Consider the quadratically constrained quadratic programming: $$ \\label{qcqp} \\tag{QCQP} \\begin{aligned} \\inf \\quad \u0026amp; x^T Q x \\ \\text{s.t.} \\quad \u0026amp; x^T A_i x \\ge b_i, \\forall i=1,\\dots,m \\end{aligned} $$ where $Q, A_1, \\dots, A_m \\in \\mathcal{S}^n$. By far, no convexity is not assumed. But on the other hand, we can apply the semi-definite relaxation technique to transform the $\\eqref{qcqp}$.\nObserve that $x^T Q x = \\tr(x^T Q x) = \\tr(Q x x^T) = Q \\bullet x x^T$, which is linear in $x x^T$. We can apply the same trick to the constraints so that $\\eqref{qcqp}$ is equivalent to $$ \\begin{aligned} v^* = \\inf \\quad \u0026amp; Q \\bullet X \\ \\text{s.t.} \\quad \u0026amp; A_i \\bullet X \\ge b_i, \\forall i=1,\\dots,m \\ \u0026amp; \\exists x \\in \\R^n, X = x x^T \\ \\text{ non-convex??} \\end{aligned} $$ Note that $\\exists x \\in \\R^n, X = x x^T \\iff X \\in \\mathcal{S}_+^n, \\rank(X) \\le 1$. That is. $$ \\begin{aligned} \\inf \\quad \u0026amp; Q \\bullet X \\ \\text{s.t.} \\quad \u0026amp; A_i \\bullet X \\ge b_i, \\forall i=1,\\dots,m \\ \u0026amp; X \\succeq 0 \\ \u0026amp; \\rank(X) \\le 1 \\end{aligned} $$ The $\\rank$ function is not convex. We may as well drop the rank constraint so that the semi-definite relaxation of $\\eqref{qcqp}$ is $$ \\label{relaxed-qcqp} \\tag{Relaxed QP} \\begin{aligned} v_R^* = \\inf \\quad \u0026amp; Q \\bullet X \\ \\text{s.t.} \\quad \u0026amp; A_i \\bullet X \\ge b_i, \\forall i=1,\\dots,m \\ \u0026amp; X \\succeq 0 \\end{aligned} $$ Observe that $v^* \\ge v_R^*$ and $\\eqref{relaxed-qcqp}$ is a SDP.\nMax-cut Problem Let $G = (V,E)$ be an undirected graph and $w: E \\mapsto \\R_+$ be a weight function on $E$. A subset $S \\subseteq V$ defines a cut and the value of a cut is $$ w(S) \\triangleq \\sum_{(i,j) \\in E, i \\in S, j \\notin S} w_{ij} $$ The goal is to find $S \\subseteq V$ such that $w(S)$ is maximized. The minimization problem is trivial, simply choosing $S$ as $V$ or $\\emptyset$ gives the minimum value $0$. Let $x_i \\in { -1, +1 }$ be a binary variable indicating whether vertex $i$ is in the cut ($+1$) or not ($-1$). Then, $$ \\label{mc} \\tag{Max-cut} \\begin{aligned} v^* = \\max \\quad \u0026amp; \\sum_{(i,j) \\in E} \\frac{1}{2} w_{ij} (1 - x_i x_j) \\ \\text{s.t.} \\quad \u0026amp; x_i^2 = 1, \\forall i=1,\\dots,n \\ \\end{aligned} $$ Apply the SDR technique (there is a middle step to convert the above to a QP) to get $$ \\label{relaxed-mc} \\tag{Relaxed Max-cut} \\begin{aligned} v_\\text{sdr}^* = \\max \\quad \u0026amp; \\sum_{(i,j) \\in E} \\frac{1}{2} w_{ij} (1 - X_{ij}) \\ \\text{s.t.} \\quad \u0026amp; X_{ii} = 1, \\forall i=1,\\dots,n \\ \u0026amp; X \\succeq 0 \\end{aligned} $$ Note that $v^* \\le v_\\text{sdr}^$. If $\\eqref{relaxed-mc}$ is solved with a rank-one matrix $X^$, we can automatically decompose it to give the optimal solution to $\\eqref{mc}$. The crux is how to preserve the optimality when $X^*$ is of rank higher than one. Here is an algorithm for it:\n Solve $\\eqref{relaxed-mc}$ to get an optimal solution $X^$. Let $X^ = U^T U$ where $U \\in \\R^{n \\times n}$. Let $u_i \\in \\R^n$ be the $i$-th column of $U$. We have $|u_i|2^2 = u_i^T u_i = X{ii}^* = 1$. Let $r \\in \\R^n$ be a random vector uniformly distributed on the sphere $S^n = { x \\in \\R^n: |x|_2 = 1 }$ (this can be done by normalizing the samples from standard Gaussian in $\\R^n$). Let $x_i’ = \\sign(u_i^T r)$ where $\\sign(z)$ is $+1$ if $z \\ge 0$ or $-1$ otherwise. Return ${ x_i’: i=1,\\dots,n }$ as a feasible solution to $\\eqref{mc}$. This is also referred to as the hyperplane rounding.  In the first place, $x_i’$s are feasible. Let $v’$ be the objective value associated with the cut ${ x_i’: i=1,\\dots,n }$. Clearly, $v’ \\le v^$. To analyze its approximation bound, firstly note that ${ x_i’: i=1,\\dots,n }$ is random. We can only consider the $\\E[v’]$. $$ \\begin{aligned} \u0026amp;\\E[v’] = \\frac{1}{2} \\E[\\sum_{(i,j) \\in E} w_{ij} (1 - x_i’ x_j’)] \\ \u0026amp;= \\sum_{(i,j) \\in E} w_{ij} \\E[\\frac{1 - x_i’ x_j’}{2}] \\ \u0026amp;= \\sum_{(i,j) \\in E} w_{ij} \\Pr[\\sign(u_i^T r) \\ne \\sign(u_j^T r)] \\ \\end{aligned} $$ Let $u, v \\in S^n$ be arbitrary, $r$ be uniformly distributed on $S^{n-1}$. Then, $$ \\Pr[\\sign(u^T r) \\ne \\sign(v^T r)] = \\frac{\\arccos(u^T v)}{\\pi} $$ Under the above setting, for any $z \\in [-1, 1]$ and $\\theta$ such that $\\cos \\theta = z$, $$ \\frac{\\arccos z}{\\pi} = \\frac{2 \\theta}{\\pi(1 - \\cos \\theta)} \\frac{1}{2} (1 - z) \\ \\ge \\alpha \\cdot \\frac{1}{2} (1-z) $$ where $\\alpha = \\min_{0 \\le \\theta \\le \\pi} \\frac{2 \\theta}{\\pi (1 - \\cos \\theta)} \u0026gt; 0.878$. As a result, $$ \\begin{aligned} \u0026amp;\\E[v’] = \\sum_{(i,j) \\in E} w_{ij} \\frac{\\arccos u_i^T u_j}{\\pi} \\ \u0026amp;\\ge \\sum_{(i,j) \\in E} w_{ij} \\alpha \\cdot \\frac{1}{2} (1 - \\underbrace{u_i^T u_j}{X{ij}^}) \\ \u0026amp;= \\alpha \\sum_{(i,j) \\in E} \\frac{1}{2} w_{ij} (1 - X_{ij}^) \\ \u0026amp;= \\alpha v_\\text{sdr}^ \\ge \\alpha v^* \\end{aligned} $$\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f903c36e56c878811ca496c499c4e61b","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/","section":"courses","summary":"Quadratically Constrained Quadratic Programming Consider the quadratically constrained quadratic programming: $$ \\label{qcqp} \\tag{QCQP} \\begin{aligned} \\inf \\quad \u0026 x^T Q x \\ \\text{s.t.} \\quad \u0026 x^T A_i x \\ge b_i, \\forall i=1,\\dots,m \\end{aligned} $$ where $Q, A_1, \\dots, A_m \\in \\mathcal{S}^n$.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Nonlinear Programming Recall the unconstrained optimization problem: $$ \\inf_{x \\in \\R^n} \\quad f(x) $$\n Proposition: Let $f: \\R^n \\mapsto \\R$ be a continuously differentiable function, $\\bar x \\in \\R^n$ be an arbitrary point. If there exists s direction $d \\in \\R^n \\setminus { 0 }$ such that $\\nabla f(\\bar x)^T d \u0026lt; 0$, then there exists $\\alpha_0 \u0026gt; 0$ such that $$ f(\\bar x + \\alpha d) \u0026lt; f(\\bar x), \\forall \\alpha \\in (0, \\alpha_0] $$ Here, $d$ is called a descent direction of $f$ at $\\bar x$.\n As a result, a necessary condition for $\\bar x$ to be a local minima is that $\\nabla f(\\bar x) = 0$ (first-order necessary condition).\n Proposition: Let $f: \\R^n \\mapsto \\R$ be a convex and continuously differentiable function. Then, $\\bar x$ is a global minima of $f$ if and only if $\\nabla f(\\bar x) = 0$.\n  Proposition: second-order sufficient condition. Let $f: \\R^n \\mapsto \\R$ be a twice continuously differentiable function. If $\\nabla f(\\bar x) = 0$ and $\\nabla^2 f(\\bar x) \\succ 0$, then $\\bar x$ is a local minima.\n Constrained Optimization In constrained case, simple conditions does not apply, e.g. $\\inf_{x \\ge 1} x^2$ and $\\inf_{x \\ge -1} x^2$. Consider the following constrained problem: $$ \\label{primal} \\tag{P} \\begin{aligned} \\inf_{x \\in \\R^n} \\quad \u0026amp; f(x) \\ \\text{s.t.} \\quad \u0026amp; g_i(x) \\le 0, i=1,\\dots,r \\ \u0026amp; h_j(x) = 0, j=1,\\dots,s \\end{aligned} $$ where $f, g_i, h_j$ are continuously differentiable.\n Theorem: Fritz John necessary condition. Let $\\bar x$ be a local minimum of $\\eqref{primal}$. Then there exist multipliers $u \\in \\R$, $v_1, \\dots, v_r \\in \\R$, $w_1, \\dots, w_s \\in \\R$ such that $$ \\begin{gather} u \\nabla f(\\bar x) + \\sum_{i=1}^r v_i \\nabla g_i(\\bar x) + \\sum_{j=1}^s w_j \\nabla h_j(x) = 0 \\tag{vanishing gradient} \\ [u, v_1, \\dots, v_r, w_1, \\dots, w_s] \\ne 0 \\tag{non-trivial solution} \\ u, v_i \\ge 0, i=1,\\dots,r \\tag{non-negativity} \\ v_i g_i(\\bar x) = 0, i=1,\\dots,r \\tag{complementarity} \\ \\end{gather} $$ $v_i$ tells the importance of the inequality constraint $g_i(x)$; $v_i = 0$ implies that $g_i(x) \\le 0$ is well-fulfilled (strictly less than zero).\n The implication is that, at a local minima, we should not be able to find a direction that decreases the objective value as well as maintains the feasibility. For simplicity of discussion, we drop the equality constraints below.\n  Linear non-independence\nThe vanishing gradient together with the non-trivial solution in essence rules out the possibility of linear independence among the gradients. This is easy to interpret. If $\\nabla f(\\bar x), \\nabla g_1(\\bar x), \\dots, \\nabla g_r(\\bar x)$ are linearly independent, it is easy to find a direction $d$ in the $\\Col^\\perp(\\nabla g_1(\\bar x), \\dots, \\nabla g_r(\\bar x))$ that is acute to $-\\nabla f(\\bar x)$. Then moving along $d$ at $\\bar x$ will decrease the objective function value but maintain the inequality constraints, which contradicts that $\\bar x$ is a local minima.\n  Non-negativity + complementarity\nThese two components should be discussed together and are a bit intriguing. Please refer to this paper.\n  Surely, the premise is that $u$ is nonzero. When the only solution to $u$ is zero, the corresponding solution $\\bar x$ to $x$ will be a garbage point: it will not be a local minima. In this case, $\\nabla f(\\bar x)$ must have a component that is orthogonal to $\\nabla g_i(\\bar x)$’s. We can walk along this component (or its opposite) to decrease $f$ without compromising inequality constraints. This motivates the study of constraint qualification, which aims to ensure a nonzero $u$.\nNote that in the above discussion, we ignore the effect of $\\nabla h_j(\\bar x)$’s. They only make the choice of direction stricter, since the direction has to be orthogonal to them.\nKKT Conditions and Constraint Qualification One observation is that, if the constraint gradients are linearly independent (though required dependent), there is no way to have $u = 0$ or otherwise $v_1, \\dots, v_r, u_1, \\dots, u_s$ has to be zero due to the linear independence, which in turn violates the nonzero multiplier condition. How to “obtain” the contradicting linear independence expectation and nonzero multiplier condition at the same time?\n Theorem: Karush-Kuhn-Tucker conditions. Let $\\bar x$ be a local minima of $\\eqref{primal}$. Let $$ I(\\bar x) = { i: g_i(\\bar x) = 0 } $$ be the index set on the active inequality constraint. Suppose that ${ \\nabla g_i(\\bar x) }{i \\in I} \\cup { \\nabla h_j(\\bar x) }{j=1}^s$ are linearly independent (linear-independence constraint qualification). Then, there exists $v \\in \\R^r$ and $w \\in \\R^s$ such that $$ \\nabla f(\\bar x) + \\sum_{i=1}^r \\nabla g_i(x) + \\sum_{j=1}^s \\nabla h_j(x) = 0 \\ v_i \\ge 0, i=1,\\dots,r \\ v_i g_i(\\bar x) = 0, i=1,\\dots,r $$\n  Example: importance of CQ. Consider the problem $$ \\begin{aligned} \\inf \\quad \u0026amp; f(x_1, x_2) = x_1 \\ \\text{s.t.} \\quad \u0026amp; g_1(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - 1 \\le 0 \\ \u0026amp; g_2(x_1, x_2) = (x_1 - 1)^2 + (x_2 + 1)^2 - 1 \\le 0 \\ \\end{aligned} $$ The only …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"73ae6a3cc8668c696634befa7218f6d2","permalink":"https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/foundations-of-optimization/8-nonlinear-programming/","section":"courses","summary":"Nonlinear Programming Recall the unconstrained optimization problem: $$ \\inf_{x \\in \\R^n} \\quad f(x) $$\n Proposition: Let $f: \\R^n \\mapsto \\R$ be a continuously differentiable function, $\\bar x \\in \\R^n$ be an arbitrary point.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"The Precision The typical precision and recall definition in a binary classification is\n$$\n\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n$$\nPrecision determines, among all the samples that are identified as positive, how many are really positive. Recall determines, among all the samples that are positive, how many are successfully identified.\nBinary classification task will assign each case a score that shows how confident the model is in the case is indeed positive. We can arrange all the samples in descending order of this score. Then beginning with an empty set, we add the sample one by one into this set. Every time we add in a new sample, we can calculate the precision and the recall within this set.\nDuring this process, recall will increase monotonically; but precision may go up and down. We can draw a plot with regard to this two numbers and this is the precision-recall curve (PRC). The area under PRC usually indicates the goodness of the model, as that of a perfect model will be 1.\nAverage Precision The “average precision” term usually appears in document retrieval and object detection scenario. For document retrieval task,\n$$\n\\text{precision} = \\frac{{ \\text{relevant documents} } \\cap { \\text{retrieved documents} } } {{ \\text{retrieved documents} }}\n$$\nThe question is where the “average” comes from. Similar to the plotting of PRC, we may rank the retrieved documents according to the relevance. Starting from an empty set of documents predicted as relevant, we add these retrieved documents one by one. Then we can have a series of precisions to average upon.\nOn the other hand, in object detection task, predicted anchors are firstly ranked according to its predicted objectness score. After that, each anchor will be assigned a objectness label. The “positivity” of a anchor is mainly determined by its intersection over union (IoU) with the ground-truth bounding boxes. The rules for determining positivity is complex. But as a result, we will have positive anchors, negative anchors and those neither positive nor negative. During training, only positive and negative anchors will contribute gradient. But in precision, non-positive anchors are treated as “negative”. For a detailed discussion of positivity in object detection, please refer here.\nWe can rank the anchors according to the objectness score. Then similarly beginning with an empty set, we add the anchor one by one into this set. Every time we add in a new anchor, we can calculate the precision. By doing so, we obtain a series of precisions to average.\nMean Average Precision We can also obtain a series of average precisions for different classes of objects, yielding the concept of mean average precision.\n“Average Mean Average Precision” Moreover, we may change the IoU threshold to obtain a series of mean average precisions to average upon; in some sense this is the “average mean average precision”.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2ca8125e3bc3f0ed7d4d893148228c0","permalink":"https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/machine-learning/mean-average-precision/","section":"notes","summary":"The Precision The typical precision and recall definition in a binary classification is\n$$\n\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n$$\nPrecision determines, among all the samples that are identified as positive, how many are really positive.","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":"Positive semi-definite matrix involves many concepts like quadratic form, real symmetric matrix and singular value decomposition. It can be quite helpful to glue these things together here.\nQuadratic Form A quadratic function $f$ of $n$ variables, or say a vector $\\x$ of length $n$, is the sum of second-order terms: $$ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j $$\nThe above summation can be simplified as matrix product $\\x^T A \\x$ where $A$ is $n \\times n$ and $a_{ij} = c_{ij}$. $\\x^T A \\x$ is called the quadratic form of $A$.\nIn essence, there is a nicer formulation for $A$. Firstly define the $n \\times n$ matrix $A$ such that $a_{ij} = \\frac{1}{2} (c_{ij} + c_{ji})$. It suffices to show $A$ is a real symmetric matrix and $$ f(\\x) = \\x^T A \\x $$\nThus the discussion of quadratic form usually encompasses the real symmetric matrix.\nPositive Semi-definiteness Let $A$ be a real symmetric matrix. $A$ is positive definite if and only if the quadratic form of $A$ is positive. Specifically, for every $\\x \\ne 0$, $\\x^T A \\x \u0026gt; 0$. $A$ is positive semi-definite if and only if the quadratic form of $A$ is non-negative. Specifically, for every $\\x \\ne 0$, $\\x^T A \\x \\ge 0$.\nOne possible reason why we would like to define positive definiteness on real symmetric matrix is that, any real matrix can be easily decomposed into the addition of a real symmetric matrix and a real skew-symmetric matrix whose quadratic form is zero: $$ A = \\frac{A + A^T}{2} + \\frac{A - A^T}{2} $$ Since $\\frac{A - A^T}{2}$’s quadratic form is zero and it makes no contribution to $A$’s, the only component of interest will be the real symmetric $\\frac{A + A^T}{2}$. So why not just focus on the real symmetric matrix?\nNote that there is also “PSD” matrix that is not symmetric. It is not very easy to find a such matrix. As a guideline, drop the idea to find such a matrix whose eigenvalues are all real. But for an example, $$ \\x^T \\begin{bmatrix} 1 \u0026amp; 1 \\ -1 \u0026amp; 1 \\end{bmatrix} \\x = x_1^2 + x_2^2 \\ge 0 $$\nInterestingly, the real part of such matrix’s eigenvalues must be positive. Refer to this.\nPSD and Eigenvalues  Theorem: A real symmetric matrix $A$ is positive semi-definite if and only if $A$’s eigenvalues are non-negative.\nProof:\n  Necessity\nFor every $A$’s eigenpair $(\\lambda, \\v)$, we have $\\v^T A \\v = \\lambda \\v^T \\v \\ge 0 \\Rightarrow \\lambda \\ge 0$.\n  Sufficiency\nTake $A$’s spectral decomposition as $A = Q \\Lambda Q^T$ where $Q Q^T = I$. For every $\\x \u0026gt; 0$, we have $$ \\x^T A \\x = \\overbrace{\\x^T Q}^{y^T} \\Lambda \\overbrace{Q^T \\x}^{y} \\ge 0 $$\n   Similarly we have that a real symmetric matrix $A$ is positive definite if and only if $A$’s eigenvalues are positive. Then it follows that a positive definite matrix is invertible, because its eigenvalues are positive and thus its determinant (product of eigenvalues) is positive.\nNon-negative Diagonals The diagonal entries of a PSD matrix $A$ must be non-negative. This is because for the basis vector $e_i = [0, \\dots, \\underset{i\\text{-th}}{1}, \\dots, 0]$, we have $e_i^T A e_i = a_{ii} \\ge 0$. Furthermore, a PSD matrix is PD if and only if its main diagonal entries are positive.\nSpecifically, if one diagonal entry of $A$ is zero, then the row and the column to which this diagonal entry belongs to must be zero.\nTo show it, we firstly argue that $a_{ij}^2 \\le a_{ii} a_{jj}$. For every $\\lambda \\in \\R$, we have $$ \\begin{gathered} (e_i + \\lambda e_j)^T A (e_i + \\lambda e_j) = e_i^T A e_i + \\lambda (e_i^T A e_j + e_j^T A e_i) + \\lambda^2 e_j^T A e_j \\ = a_{ii} + 2 a_{ij} \\lambda + a_{jj} \\lambda^2 \\ge 0 \\end{gathered} $$ Note the formula above is a quadratic function in $\\lambda$. This quadratic form has at most one real root. Hence, $4 a_{ij}^2 \\le 4 a_{ii} a_{jj} \\Rightarrow a_{ij}^2 \\le a_{ii} a_{jj}$. Therefore, if $j$-th diagonal entry of $A$ is zero, for any $i = 1,\\dots,n$, we have $a_{ij}^2 \\le 0 \\Rightarrow a_{ij} = 0$.\nAnother implication is that any $2 \\times 2$ submatrix $\\begin{bmatrix} a_{ii} \u0026amp; a_{ij} \\ a_{ji} \u0026amp; a_{jj} \\end{bmatrix}$ obtained from $A$ is always PSD. In fact, any submatrix obtained by removing the row and column of one of the main diagonal entry of a PSD matrix is PSD. Let $A_{kk}$ be the matrix obtained by removing $A$’s $k$-th row and column. To show it, let $\\x \\in \\R^n$ and let $\\bar \\x = \\x$ except that $\\bar \\x_k = 0$. $A_{kk}$’s quadratic form can be written as $$ \\sum_{i=1, i \\ne k}^{n} \\sum_{j=1, j \\ne k}^n a_{ij} x_i x_j = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j \\mathbb{1}[i,j \\ne k] = \\bar \\x^T A \\bar x \\ge 0 $$\nDecomposition Definitive Decomposition A positive semi-definite matrix $A$ can be decomposed into the product of a square matrix $Q$ and this matrix’s transpose $Q^T$. In fact, a real symmetric matrix is positive semi-definite if and only if it can be decomposed this way.\nConsider that a real symmetric matrix can be orthogonally diagonalized: $$ A = P \\Lambda P^T $$ When $A$ is PSD, its eigenvalues are all non-negative. Thus, $\\Lambda$ can be written as …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"09e8163cb929839f8c4971706a8b5c37","permalink":"https://chunxy.github.io/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/","section":"notes","summary":"Positive semi-definite matrix involves many concepts like quadratic form, real symmetric matrix and singular value decomposition. It can be quite helpful to glue these things together here.\nQuadratic Form A quadratic function $f$ of $n$ variables, or say a vector $\\x$ of length $n$, is the sum of second-order terms: $$ f(\\x) = \\sum_{i=1}^n \\sum_{j=1}^n c_{ij} x_i x_j $$","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":"Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair $(f,f^)$ has the following property: $$ f(x) + f^(t) \\ge x^T \\cdot t $$ As a conjugate, $f^{} = f$: $$ \\begin{aligned} f^{}(t) \u0026amp;= \\sup_x [x^T \\cdot t - f^*(x)] \\ \u0026amp;= \\sup_x [x^T \\cdot t - \\sup_y [y^T \\cdot x - f(y)]] \\ \u0026amp;= \\sup_x [x^T \\cdot t + \\inf_y [f(y) - y^T \\cdot x]] \\ \u0026amp;= \\sup_x \\inf_y [x^T (t-y) + f(y)]\t\\ \u0026amp;\\Downarrow_\\text{Mini-max Theorem} \\ \u0026amp;= \\inf_y \\sup_x [x^T (t-y) + f(y)] \\end{aligned} $$ The above reaches the infimum only if $y=t$. Otherwise, $\\sup_x [x^T (t-y) + f(y)]$ can make it to infinity. Therefore, $$ f^{**}(t) = \\inf_y \\sup_x [f(t)] = f(t) $$ Wiki || 凸优化-凸共轭\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"57d01174ce8319ba1023ce0b56087404","permalink":"https://chunxy.github.io/notes/articles/optimization/convex-conjugate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/articles/optimization/convex-conjugate/","section":"notes","summary":"Convex Conjugate For a convex function $f$, its convex conjugate $f^$ is defined as $$ f^(t) = \\sup_x [x^T \\cdot t - f(x)] $$ By definition, a Convex Conjugate pair","tags":null,"title":"","type":"notes"}]