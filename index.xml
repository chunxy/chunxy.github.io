<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/</link>
      <atom:link href="https://chunxy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Chunxy&#39; Website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 22 May 2023 09:43:15 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Chunxy&#39; Website</title>
      <link>https://chunxy.github.io/</link>
    </image>
    
    <item>
      <title>Determinant</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</link>
      <pubDate>Tue, 16 May 2023 11:42:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</guid>
      <description>

&lt;h2 id=&#34;derivation-of-determinant&#34;&gt;Derivation of Determinant&lt;/h2&gt;
&lt;p&gt;Determinant may be the most infamous concept in linear algebra, in
terms of its odd definition and computation. Sometimes, we may wonder
why there has to be a determinant.&lt;/p&gt;
&lt;p&gt;So instead of giving its definition directly, we first lay down some
properties we expect the determinant to have. Then we try to construct
the determinant from the ground up and prove its existence and
uniqueness.&lt;/p&gt;
&lt;p&gt;Determinant in essence captures the volume of the parallelepiped
formed by vectors of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt;
matrix. When &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, it is the length;
when &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, it is the area of
parallelogram; when &lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt;, it is the
volume of parallelepiped. What about when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; goes larger?&lt;/p&gt;
&lt;p&gt;As said, there are some basic properties that we expect the
determinant (the volume) to have, and that indeed hold for cases &lt;span class=&#34;math inline&#34;&gt;\(n = 1,2,3\)&lt;/span&gt;. In the following, let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix and denote &lt;span class=&#34;math inline&#34;&gt;\(\det(\v_1, \dots, \v_n)\)&lt;/span&gt; as the
determinant of the matrix formed by a system of vectors &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;basic-properties&#34;&gt;Basic properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Linearity in each argument: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\alpha \v_k + \beta \u_k}_k, \dots, \v_n)
= \alpha \det(\v_1, \dots, \underset{k}{\v_k}, \dots, \v_n) + \beta
\det(\v_1, \dots, \underset{k}{\u_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anti-symmetry: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underset{j}{\v_j}, \dots, \underset{k}{\v_k}, \dots,
\v_n) = -\det(\v_1, \dots, \underset{j}{\v_k}, \dots,
\underset{k}{\v_j}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normalization: &lt;span class=&#34;math display&#34;&gt;\[
\det(I) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these basic properties, we can derive some advanced properties
for determinant.&lt;/p&gt;
&lt;h3 id=&#34;derived-properties&#34;&gt;Derived properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Preservation under column replacement: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\v_j + \alpha \v_k}_{j}, \dots,
\underset{k}{\v_k}, \dots, \v_n) = \det(\v_1, \dots, \underset{j}{\v_j},
\dots, \underset{k}{\v_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;zero-determinants&#34;&gt;Zero determinants&lt;/h4&gt;
&lt;ol start=&#34;2&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has a zero column, then
&lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has two equal columns,
then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If one column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the
multiple of another, then &lt;span class=&#34;math inline&#34;&gt;\(\det A =
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are linearly
dependent, then &lt;span class=&#34;math inline&#34;&gt;\(\det A =
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;diagonal-matrices-and-triangular-matrices&#34;&gt;Diagonal matrices and
triangular matrices&lt;/h4&gt;
&lt;ol start=&#34;6&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;diagonal matrix&lt;/strong&gt; equal the product
of the diagonal entries.&lt;/li&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;triangular matrix&lt;/strong&gt; equal the
product of the diagonal entries.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;transpose-and-product&#34;&gt;Transpose and product&lt;/h4&gt;
&lt;ol start=&#34;8&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det A^T = \det A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This property implies that, all the statements above about columns,
can be applied to rows. Thus, to compute the determinant of a matrix, we
can apply row operations to transform it to reduced row echelon form
first, and then obtain the result by computing the product of the
diagonal entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det (AB) = (\det A)(\det
B)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;construction&#34;&gt;Construction&lt;/h3&gt;
&lt;p&gt;Now with these properties on hand, how can we find the definition of
the determinant and how can we know that the definition is unique over
these properties?&lt;/p&gt;
&lt;p&gt;Consider an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix
&lt;span class=&#34;math inline&#34;&gt;\(A = \{ a_{jk} \}_{j,k=1}^n\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt; be its columns
and &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\e}{\mathrm{e}} \e_1, \dots,
\e_n\)&lt;/span&gt; be the unit vectors. We have &lt;span class=&#34;math display&#34;&gt;\[
\v_k = a_{1, k} \e_1 + a_{2, k} \e_2 + \dots + a_{n, k} \e_{n} =
\sum_{j=1}^n a_{j, k} \e_j
\]&lt;/span&gt; By the linearity in each argument, expand the first column to
give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \det(\sum_{j=1}^n a_{j, 1} \e_i, \v_2, \dots,
\v_n) = \sum_{j=1}^n a_{j, 1} \det(\e_j, \v_2, \dots, \v_n)
\]&lt;/span&gt; Further expand the second column to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1},
\v_2, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1}, \sum_{j_2=1}^n a_{j_2,
2} e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \sum_{j_2=1}^n a_{j_2, 2}
\det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n \sum_{j_2=1}^n a_{j_1, 1} a_{j_2, 2}
\det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n)
\end{aligned}
\]&lt;/span&gt; Expand the remaining columns to give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n \sum_{j_2=1}^n \dots
\sum_{j_n=1}^n a_{j_1, 1} a_{j_2, 2} \dots a_{j_n, n} \det(\e_{j_1},
e_{j_2}, \dots, \e_{j_n})
\]&lt;/span&gt; This yields &lt;span class=&#34;math inline&#34;&gt;\(n^n\)&lt;/span&gt; terms!
But luckily, many terms are zero, as long as any two of &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots, j_n\)&lt;/span&gt; coincides. To eliminate
zero terms, consider the permutation of &lt;span class=&#34;math inline&#34;&gt;\(\{
1, \dots, n \}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots,
j_n\)&lt;/span&gt; are chosen to be a permutation, &lt;span class=&#34;math inline&#34;&gt;\(\det(\e_{j_1}, e_{j_2}, \dots, \e_{j_n})\)&lt;/span&gt;
is nonzero. We may use a function &lt;span class=&#34;math inline&#34;&gt;\(\sigma: \{
1, \dots, n \} \to \{ 1, \dots, n \}\)&lt;/span&gt; to denote a permutation.
Let the set of all permutations of set &lt;span class=&#34;math inline&#34;&gt;\(\{ 1,
\dots n \}\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Perm}(n)\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{\sigma \in \mathrm{Perm}(n)}
a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n}
\det(e_{\sigma(1)}, e_{\sigma(2)}, \dots, e_{\sigma(n)})
\]&lt;/span&gt; The matrix with columns &lt;span class=&#34;math inline&#34;&gt;\(e_{\sigma(1)}, e_{\sigma(2)}, \dots,
e_{\sigma(n)}\)&lt;/span&gt; can be obtained from identity matrix by finitely
many column exchanges. So &lt;span class=&#34;math inline&#34;&gt;\(\det(e_{\sigma(1)}, e_{\sigma(2)}, \dots,
e_{\sigma(n)})\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or
&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; depending on the number of
column exchanges. We informally define the sign of function &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; if an even number of column exchanges
are needed to permute &lt;span class=&#34;math inline&#34;&gt;\(1, \dots, n\)&lt;/span&gt;
to &lt;span class=&#34;math inline&#34;&gt;\(\sigma(1), \dots, \sigma(n)\)&lt;/span&gt;; and
the signa of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; if the number of exchanges is odd.&lt;/p&gt;
&lt;p&gt;The necessary condition of the definition of determinant requires us
define it like &lt;span class=&#34;math display&#34;&gt;\[
\det A \coloneq \sum_{\sigma \in \mathrm{Perm}(n)} a_{\sigma(1), 1}
a_{\sigma(2), 2} \dots a_{\sigma(n), n} \mathrm{sign}(\sigma)
\]&lt;/span&gt; If we define it in this way, we can verify that it indeed
satisfies the basic properties, concluding the construction of
determinant.&lt;/p&gt;
&lt;h2 id=&#34;cofactor-expansion&#34;&gt;Cofactor Expansion&lt;/h2&gt;
&lt;p&gt;For an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(A_{j,k}\)&lt;/span&gt; denote &lt;span class=&#34;math inline&#34;&gt;\((n-1) \times (n-1)\)&lt;/span&gt; matrix obtained by
crossing out the row &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and column
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The determinant of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be expanded in the row number &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
\det A = a_{j, 1} (-1)^{j+1} \det A_{j, 1} + a_{j, 2} (-1)^{j+2} \det
A_{j, 2} + \dots + a_{j, n} (-1)^{j+n} \det A_{j, n}
\]&lt;/span&gt; The numbers &lt;span class=&#34;math inline&#34;&gt;\(C_{j,k} = (-1)^{j+k}
\det A_{j,k}\)&lt;/span&gt; are called the &lt;strong&gt;cofactors&lt;/strong&gt; of
matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The matrix &lt;span class=&#34;math inline&#34;&gt;\(C = \{ C_{j,k} \}_{j=1,k=1}^n\)&lt;/span&gt; is called
the &lt;strong&gt;cofactor matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an invertible matrix, then &lt;span class=&#34;math display&#34;&gt;\[
A^{-1} = \frac{1}{\det A} C^T
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C^T\)&lt;/span&gt; is sometimes denoted
as &lt;span class=&#34;math inline&#34;&gt;\(A^*\)&lt;/span&gt;, called the &lt;strong&gt;adjugate
matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cramers-rule&#34;&gt;Cramer’s Rule&lt;/h3&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and
an equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt;, there is a
unique solution that &lt;span class=&#34;math display&#34;&gt;\[
x = A^{-1} b = \frac{1}{\det A} C^T b
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; is obtained by
multiplying the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of the
cofactor matrix with &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which is
equivalent to the determinant of the matrix obtained by replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. This property is known as
&lt;strong&gt;Cramer’s rule&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, the
entry &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the solution of the
equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt; is given by the
formula &lt;span class=&#34;math display&#34;&gt;\[
x_k = \frac{\det B_k}{\det A}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt; is obtained by
replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;algebraic-properties&#34;&gt;Algebraic Properties&lt;/h2&gt;
&lt;p&gt;In blockwise matrix multiplication, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det \begin{pmatrix}
A &amp;amp; 0 \\
C &amp;amp; D
\end{pmatrix} &amp;amp;= \det \left( \begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; B \\
C &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; 0 \\
0 &amp;amp; D
\end{pmatrix} \right) \\
\\
&amp;amp;= \det(A) \det(D) \\
\\
\det \begin{pmatrix}
A &amp;amp; B \\
0 &amp;amp; D
\end{pmatrix} &amp;amp;= \det \left( \begin{pmatrix}
I &amp;amp; 0 \\
0 &amp;amp; D
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; B \\
0 &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; I
\end{pmatrix} \right)
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is invertible, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det \begin{pmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{pmatrix}
&amp;amp;= \det \left( \begin{pmatrix}
I &amp;amp; 0 \\
C A^{-1} &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; D - C A^{-1} B
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; A^{-1} B \\
0 &amp;amp; I
\end{pmatrix} \right) \\
&amp;amp;= 1 \cdot \det A \det (D - C A^{-1} B) \cdot 1 \\
&amp;amp;= \det A \det (D - C A^{-1} B)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Law of Total Variance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/</link>
      <pubDate>Wed, 05 Apr 2023 09:06:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/</guid>
      <description>

&lt;h2 id=&#34;conditional-expectation&#34;&gt;Conditional Expectation&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be two discrete random variables. The
&lt;strong&gt;conditional probability function of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\Pr(X=x|Y=y) = \frac{\Pr(X=x, Y=y)}{P(Y=y)}
\]&lt;/span&gt; Thus the &lt;strong&gt;conditional expectation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given that &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\E(X|Y=y) \coloneq \sum_x x \Pr(X=x|Y=y)
\]&lt;/span&gt; Clearly the &lt;strong&gt;conditional expectation&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\E(X|Y)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, or put it another way, a random
variable depending on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, instead of
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conditional-variance&#34;&gt;Conditional Variance&lt;/h2&gt;
&lt;p&gt;Conditional variance can be similarly defined. &lt;span class=&#34;math inline&#34;&gt;\(\Var(X|Y=y)\)&lt;/span&gt; is the conditional variance
of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Var(X|Y)\)&lt;/span&gt; is a random variable depending
on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Var(X|Y) \coloneq \E[(X - \mu_{X|Y})^2 | Y] = \E(X^2|Y) - \E(X|Y)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;laws-of-total-expectation-and-variance&#34;&gt;Laws of Total
Expectation and Variance&lt;/h2&gt;
&lt;p&gt;If all the expectations below exist, then for any random variable
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \E_{y \sim p_Y} [\E(X|Y=y)] \quad \textbf{Law of Total
Expectation}
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\Var(X) = \E_{y \sim p_Y} [\Var(X|Y=y)] + \Var_{y \sim p_Y} [\E(X|Y=y)]
\quad \textbf{Law of Total Variance}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>总览</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</link>
      <pubDate>Tue, 08 Nov 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</guid>
      <description>

&lt;h2 id=&#34;概率论与统计总览&#34;&gt;概率论与统计：总览&lt;/h2&gt;
&lt;p&gt;随机变量其实是一个从事件（概率空间的子集）到数字的映射，是一个具体事件数字化的过程。概率论和统计的一个共同的主要话题就是随机变量，可以说它们像是随机变量的一体两面。&lt;/p&gt;
&lt;p&gt;概率论更加注重随机变量取值集合相对应事件的概率。为此，概率论需要讨论事件所有可能的试验结果、事件的运算、事件的独立性、随机变量的取值范围、随机变量的概率分布等话题。&lt;/p&gt;
&lt;p&gt;统计中的随机变量来自于对总体的随机抽样，这个过程中，我们会得到一组样本，每个样本在被观测之前，都是服从总体分布的随机变量；观测（observation）之后，它们便有了一个具体的观测值（realization）。我们可以将观测行为类比概率论中的试验，而这种观测行为将会导致一个随机变量坍缩成为一个具体的观测值。&lt;/p&gt;
&lt;p&gt;可以这样理解概率论与统计：概率论是根据事件总体的属性，正向推导所有事件对应概率分布的分布函数；统计是根据某个概率分布（即总体分布）采样得到的结果，反向推导该分布的属性。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;概率论&lt;/th&gt;
&lt;th&gt;统计&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;概率空间（事件总体）&lt;/td&gt;
&lt;td&gt;样本空间（总体）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;试验&lt;/td&gt;
&lt;td&gt;样本&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;随机变量的数字特征&lt;/td&gt;
&lt;td&gt;样本的数字特征（统计量）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;渐进理论&lt;/td&gt;
&lt;td&gt;统计推段&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;统计推断是统计的终极话题。从任务角度，统计推断主要包括&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/&#34;&gt;参数估计&lt;/a&gt;和&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/&#34;&gt;假设检验&lt;/a&gt;；从方法角度，统计推断分为频率学派和贝叶斯学派。由于笔者习惯采用频率学派视角，故将&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/&#34;&gt;贝叶斯推断&lt;/a&gt;单独抽出，另放它处。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Generating Covariance Matrix</title>
      <link>https://chunxy.github.io/blogs/generating-covariance-matrix/</link>
      <pubDate>Sat, 13 Aug 2022 11:23:21 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/generating-covariance-matrix/</guid>
      <description>

&lt;p&gt;Covariance matrix of a random vector can usually be deduced from the
distribution’s property, or estimated from samples. But how to generate
an arbitrary covariance matrix? How should we populate the entries in a
square matrix so that it makes a legitimate covariance matrix?&lt;/p&gt;
&lt;h2 id=&#34;first-method&#34;&gt;First Method&lt;/h2&gt;
&lt;p&gt;In general, we construct the target covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; by giving its eigenvalues and its
orthonormal eigenvectors (&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Orthogonal Eigenvectors&#34;&gt;any real
symmetric matrix, including the covariance matrix of course, can be
constructed in this way&lt;/a&gt;). A diagonal matrix of eigenvalues (denoted
as &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;) are easy to synthesize. It
remains that how to synthesize a square matrix that has orthonormal
column vectors.&lt;/p&gt;
&lt;p&gt;Let the dimension of the target covariance matrix be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Given an arbitrary square matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, we can decompose &lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;, which is a real symmetric matrix,
into &lt;span class=&#34;math inline&#34;&gt;\(U \Lambda U^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the orthonormal matrix consisting of
&lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is the diagonal matrix populated
with &lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;’s eigenvalues, due to
the property of real symmetric matrix.&lt;/p&gt;
&lt;p&gt;Then we define &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
(M M^T)^{1/2} &amp;amp;\coloneqq U \Lambda^{1/2} U^T \\
(M M^T)^{-1/2} &amp;amp;\coloneqq U \Lambda^{-1/2} U^T \\
\end{align}
\]&lt;/span&gt; We take &lt;span class=&#34;math inline&#34;&gt;\(E = (M M^T)^{-1/2}
M\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; will contain the
orthonormal column vectors as expected. To verify, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;E E^T = \left[ (M M^T)^{-1/2} M \right] \left[ M^T ((M
M^T)^{-1/2})^T \right] \\
&amp;amp;= \left[ (M M^T)^{-1/2} \right] \left[ M M^T \right] \left[ ((M
M^T)^{-1/2})^T \right] \\
&amp;amp;= \left[ U \Lambda^{-1/2} U^T \right] \left[ U \Lambda U^T \right]
\left[ U \Lambda^{-1/2} U^T \right] \\
&amp;amp;= U \Lambda^{-1/2} \underbrace{\left[ U^T U \right]}_{I} \Lambda
\underbrace{\left[ U^T U \right]}_{I} \Lambda^{-1/2} U^T \\
&amp;amp;= U \Lambda^{-1/2} \Lambda \Lambda^{-1/2} U^T = U U^T = I
\end{aligned}
\]&lt;/span&gt; Thus, the targeting covariance matrix can be constructed as
&lt;span class=&#34;math inline&#34;&gt;\(\Sigma = E D E^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;second-method&#34;&gt;Second Method&lt;/h2&gt;
&lt;p&gt;The easiest way to generate a legitimate covariance matrix would be
to arbitrarily synthesize a square matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, and take &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = A A^T\)&lt;/span&gt; (see &lt;a href=&#34;https://stats.stackexchange.com/questions/215497/how-to-create-an-arbitrary-covariance-matrix&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;By doing so, we can obtain a covariance matrix very fast. But you
lose the control over it. Since &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
is totally arbitrary, you can tell little about &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’ s eigenvalues, eigenvectors,
etc.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/introduction/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/introduction/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;The fundamental problem of communication is that of reproducing at
one point either exactly or approximately a message selected at another
point (Claude Shannon, 1948).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similar to that stated above, the fundamental subject of machine
learning is about with best effort recovering the original data using
the so-called model or algorithm.&lt;/p&gt;
&lt;p&gt;The recovery can be done by memoizing. In communication system, this
is exactly what we want: we need to try our best to convey the message
with the achievable highest fidelity. But it may be overwhelmingly
costly when the volume of data becomes larger, such as in the case faced
by machine learning. Thus, a better way would be to summarizing,
i.e. using a function to reveal the correlation (here I don’t mean the
mathematical term correlation) among data.&lt;/p&gt;
&lt;p&gt;This book reviews the classic concepts and methods of the recovery
process in communication system field. But it would also be interesting
for machine learning folks, as the two fields meet the same capacity
limit of the recovery.&lt;/p&gt;
&lt;h2 id=&#34;a-system-view&#34;&gt;A System View&lt;/h2&gt;
&lt;p&gt;Let’s make the life easier by introducing a few concepts. In
communication, we have &lt;strong&gt;source&lt;/strong&gt; message to convey. After
being passed to and encoded by the &lt;strong&gt;encoder&lt;/strong&gt;, the message
travels along the &lt;strong&gt;channel&lt;/strong&gt; to be finally decoded by the
&lt;strong&gt;decoder&lt;/strong&gt; and received by the receiver.&lt;/p&gt;
&lt;p&gt;The encoding process includes transforming the data into digital form
and introducing redundant information for the actual transmission. The
redundancy is necessary to keep the underlying data from the
&lt;strong&gt;noisy&lt;/strong&gt; channel, which may alter bits, or add/drop bits
(these two cases are very rare though) to the bit flow transmitting on
it.&lt;/p&gt;
&lt;p&gt;Information theory is concerned with the theoretical limit and
potential for such systems. The coding theory is concerned with
designing such a coding theme as to reach the limit/potential.&lt;/p&gt;
&lt;h2 id=&#34;error-correcting-codes&#34;&gt;Error-correcting Codes&lt;/h2&gt;
&lt;p&gt;By the name of “error-correcting”, we mean that we want to be able to
detect and correct errors; the retransmission is not an option.&lt;/p&gt;
&lt;p&gt;The way we do this, as stated above, is to add redundancy. But that
the redundancy needs to be cost-effective. The &lt;strong&gt;rate&lt;/strong&gt; is
the ratio between the amount of source data and that of the transmitted
data; the higher the better.&lt;/p&gt;
&lt;p&gt;There are some examples of coding scheme in this genre:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repetition code&lt;/li&gt;
&lt;li&gt;block code
&lt;ul&gt;
&lt;li&gt;Hamming code&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-best-performance&#34;&gt;The Best Performance?&lt;/h2&gt;
&lt;p&gt;There seems to be a tradeoff between the error probability (which we
want to minimize) and the rate (which we want to maximize), which can be
seen mathematically in the case of repetition code and Hamming code.&lt;/p&gt;
&lt;p&gt;It seems that the error-rate curve, no matter for which kind of
coding scheme, would pass the &lt;span class=&#34;math inline&#34;&gt;\((0,
0)\)&lt;/span&gt; point: in order to achieve a vanishingly small error
probability, one would have to reduce the rate correspondingly to
zero.&lt;/p&gt;
&lt;p&gt;However, Shannon proved that this curve may intersect with the rate
axis at some nonzero point! And this maximum rate at which the
communication is possible with arbitrarily small error probability is
called the &lt;strong&gt;capacity&lt;/strong&gt; of the channel. The capacity is
only related to the noise level of this channel. For any noisy binary
symmetric channel with noise level of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (i.e., the probability that a bit is
flipped), its capacity is &lt;span class=&#34;math display&#34;&gt;\[
C(f) = 1 - H_2(f) = 1 - \left[ f \log \frac{1}{f} + (1-f) \log
\frac{1}{1-f} \right]
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>椭圆曲线加密算法</title>
      <link>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</link>
      <pubDate>Thu, 23 Jun 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;本文为&lt;a href=&#34;https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/&#34;&gt;Elliptic
Curve
Cryptography&lt;/a&gt;系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#加密算法分支&#34;&gt;加密算法分支&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线&#34;&gt;椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#群group&#34;&gt;群（Group）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积scalar-multiplication&#34;&gt;标量积（Scalar
Multiplication）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看群&#34;&gt;再看群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积与子群&#34;&gt;标量积与子群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete
Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve
Diffie-Hellman&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve
Digital Signature Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看离散对数运算&#34;&gt;再看离散对数运算&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ecc与rsa&#34;&gt;ECC与RSA&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;加密算法分支&#34;&gt;加密算法分支&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于椭圆曲线&lt;/p&gt;
&lt;p&gt;基于椭圆曲线的加密算法包括ECC（Elliptic Curve
Cryptography）、ECDH和ECDSA。ECDH与ECDSA是基于ECC发展而来。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于模余运算&lt;/p&gt;
&lt;p&gt;基于模余运算的加密算法包括RSA、DSA、DH以及其他衍生算法。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/h2&gt;
&lt;h3 id=&#34;椭圆曲线&#34;&gt;椭圆曲线&lt;/h3&gt;
&lt;p&gt;一条椭圆曲线就是一组满足&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax +
b\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne
0\)&lt;/span&gt;的二维平面点集。&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne
0\)&lt;/span&gt;的条件是为了保证曲线不存在&lt;strong&gt;奇点（singularity）&lt;/strong&gt;；&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax +
b\)&lt;/span&gt;又被称作椭圆曲线的&lt;strong&gt;Weierstrass normal
form&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;除了这条曲线上的点，我们还需要一个无穷远处的点，我们用&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;这个特殊的符号来表示这个点，所以椭圆曲线更准确的表达式为
&lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 椭圆曲线的一条显而易见的性质是，它是关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴对称的。&lt;/p&gt;
&lt;h3 id=&#34;群group&#34;&gt;群（Group）&lt;/h3&gt;
&lt;p&gt;一个集合&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;加上一个二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，若满足以下条件，就构成了数学上的一个&lt;strong&gt;群&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;封闭性（closure）：&lt;span class=&#34;math inline&#34;&gt;\(a \in G, b \in G \to
a \oplus b \in G\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;结合律（associativity）：&lt;span class=&#34;math inline&#34;&gt;\((a + b) + c = a
+ (b + c)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;存在一个单位元（identity element）&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(a + 0
= 0 + a =
a\)&lt;/span&gt;，即单位元与任何元素进行运算，不改变该元素的值；&lt;/li&gt;
&lt;li&gt;每个数都存在一个逆元（inverse）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若该群进一步满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;交换律（commutativity）：&lt;span class=&#34;math inline&#34;&gt;\(a + b = b +
a\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则称该群为&lt;strong&gt;阿贝尔群（Abelian group）&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/h3&gt;
&lt;p&gt;对于我们定义的椭圆曲线集合，我们&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义无穷远处的&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;为单位元；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义逆元为该点关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴另一侧的对称点；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;如下：&lt;/p&gt;
&lt;p&gt;若一条直线与椭圆曲线的三个交点分别为&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(P
\oplus Q \oplus R =
0\)&lt;/span&gt;，我们称这三个点是&lt;strong&gt;对齐的（aligned）&lt;/strong&gt;。在此处我们没有规定三个点之间的顺序，即三个点之间可以任意交换位置，也就是说我们的定义的二元运算是满足交换律的，我们定义的群是一个阿贝尔群。&lt;/p&gt;
&lt;p&gt;给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q
= (x_q, y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R
= P \oplus Q\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= m^2 - x_P - x_Q \\
y_R &amp;amp;= y_P + m(x_R - x_P) \\
&amp;amp;= y_Q + m(x_R - x_Q)
\end{align}
\]&lt;/span&gt; 其中： &lt;span class=&#34;math display&#34;&gt;\[
m = \begin{cases}
\frac{y_P - y_Q}{x_P - xQ}, &amp;amp; P \ne Q \\
\frac{3x_P^2 + a}{2y_P}, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;标量积scalar-multiplication&#34;&gt;标量积（Scalar
Multiplication）&lt;/h3&gt;
&lt;p&gt;给定之前的二元加法运算，我们可以定义出相应的群中元素与标量之间的乘法运算：
&lt;span class=&#34;math display&#34;&gt;\[
n P  = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 这样的乘法运算可以在&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;时间内完成。&lt;/p&gt;
&lt;h3 id=&#34;对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/h3&gt;
&lt;p&gt;给定&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们可以很高效地完成标量积运算&lt;span class=&#34;math inline&#34;&gt;\(Q = nP\)&lt;/span&gt;；但如果给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们如何计算出对数运算（虽然这里是除法，但是为了和密码学中的标记保持一致，这里使用了对数）&lt;span class=&#34;math inline&#34;&gt;\(n = Q \div P\)&lt;/span&gt;呢？&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/h2&gt;
&lt;h3 id=&#34;有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/h3&gt;
&lt;p&gt;有限域首先是一系列元素的集合，比如说由整数模余某个质数&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;得到的集合（通常表示为&lt;span class=&#34;math inline&#34;&gt;\(\Z/p\)&lt;/span&gt;或&lt;span class=&#34;math inline&#34;&gt;\(\newcommand{F}{\mathbb F}
\F_p\)&lt;/span&gt;）；有限域还定义了两种二元运算：加法和乘法，且这两种运算应该满足如下条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在有限域上都是封闭的、满足结合律以及交换律的；&lt;/li&gt;
&lt;li&gt;存在单位元；&lt;/li&gt;
&lt;li&gt;每个元素都存在相应的逆元。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，乘法运算还应该满足分配律（distributive）：&lt;span class=&#34;math inline&#34;&gt;\(x \cdot (y + z) = x \cdot y + x \cdot
z\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;包含了从&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(p-1\)&lt;/span&gt;的所有整数，而加法、乘法操作之后要追加模余（除数为&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;）操作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(a + b = 0 \pmod p\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;加法逆元（additive
inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=-b,
b=-a\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(ab = 1 \pmod o\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;乘法逆元（multiplicative
inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=b^{-1},b=a^{-1}\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(xy^{-1}\)&lt;/span&gt;有时也表示为&lt;span class=&#34;math inline&#34;&gt;\(x/y\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元可以通过Extended Euclidean
Algorithm，其时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以证明，&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;也是一个阿贝尔群。&lt;/p&gt;
&lt;h3 id=&#34;有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/h3&gt;
&lt;p&gt;椭圆曲线本身的定义为： &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 加上有限域的限制之后，变为 &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \F^2 | y^2 = x^3 + ax + b \pmod p, 4a^3 + 27b^2 \ne 0 \pmod
p, a, b \in \F_p \} \cup \{0\}
\]&lt;/span&gt;
由于有限域的限制，此时所有的点全部出现第一象限。该图像关于&lt;span class=&#34;math inline&#34;&gt;\(y = p / 2\)&lt;/span&gt;对称，因为若&lt;span class=&#34;math inline&#34;&gt;\(y_1 + y_2 = p\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_1^2 &amp;amp;= (p - y_2)^2 \\
&amp;amp;= p^2 - 2py_2 + y_2^2 \\
&amp;amp;= y_2^2 \pmod p
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看群&#34;&gt;再看群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于一个点&lt;span class=&#34;math inline&#34;&gt;\(Q = (x_Q,
y_Q)\)&lt;/span&gt;，其逆元&lt;span class=&#34;math inline&#34;&gt;\(-Q\)&lt;/span&gt;定义为&lt;span class=&#34;math inline&#34;&gt;\(-Q = (x_Q, -y_Q \mod p)\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们这样定义有限域上椭圆曲线上的点之间的二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，同之前一样，三个对齐的点（aligned
points）&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;满足 &lt;span class=&#34;math display&#34;&gt;\[
P \oplus Q \oplus R = 0
\]&lt;/span&gt;
只不过这里“对齐”的含义与之前有所不同，之前的对齐指的是几何上的共线，即三个点满足&lt;span class=&#34;math inline&#34;&gt;\(ax + by + c = 0\)&lt;/span&gt;；而这里的对齐指的是：
&lt;span class=&#34;math display&#34;&gt;\[
ax + by + c = 0 \pmod p
\]&lt;/span&gt; 有趣的是，计算加法的公式和之前没有发生太大变化（&lt;a href=&#34;https://arxiv.org/pdf/1710.00214&#34;&gt;证明&lt;/a&gt;）。给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q = (x_q,
y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R = P
\oplus Q\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= (m^2 - x_P - x_Q) \mod p \\
y_R &amp;amp;= (y_P + m(x_R - x_P)) \mod p \\
&amp;amp;= (y_Q + m(x_R - x_Q)) \mod p
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math display&#34;&gt;\[
m =
\begin{cases}
(y_P - y_R)(x_P - x_R)^{-1} \mod p, &amp;amp; P \ne Q \\
(3x_P^2 + a)(2y_P)^{-1} \mod p, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;群中元素的个数叫做群的&lt;strong&gt;秩（order）&lt;/strong&gt;，可以通过&lt;a href=&#34;https://en.wikipedia.org/wiki/Schoof%27s_algorithm&#34;&gt;Schoof’s
algorithm&lt;/a&gt;计算求得。&lt;/p&gt;
&lt;h3 id=&#34;标量积与子群&#34;&gt;标量积与子群&lt;/h3&gt;
&lt;p&gt;标量积依旧遵循之前的定义，给定正整数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和群中的点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
nP = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 标量积其实就是对某个点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;不断做加法，其中一个有趣的性质是，&lt;span class=&#34;math inline&#34;&gt;\(0P, 1P, 2P,
\dots\)&lt;/span&gt;的结果会以某个最小正周期周期&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;循环（&lt;a href=&#34;https://en.wikipedia.org/wiki/Subgroup#Basic_properties_of_subgroups&#34;&gt;证明&lt;/a&gt;）。这也就意味着，群中对加法&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;的倍数是关于加法封闭的（closed under
addition），它们又构成了一个循环子群（cyclic subgroup），&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;又称作这个循环子群的&lt;strong&gt;基点（base
point/generator）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;是这个&lt;strong&gt;循环子群的秩（subgroup
order）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrange%27s_theorem_(group_theory)&#34;&gt;Lagrange’s
theorem&lt;/a&gt;，子群的秩是其父群的秩的约数。&lt;/p&gt;
&lt;h4 id=&#34;寻找基点&#34;&gt;寻找基点&lt;/h4&gt;
&lt;p&gt;在ECC算法中，我们一般会先计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，找出它一个比较大的约数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，让&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;作为子群的秩，&lt;span class=&#34;math inline&#34;&gt;\(h = N /
n\)&lt;/span&gt;称作这个子群的余因子（cofactor），再根据这个子群的秩去找这个子群的基点。一般来说，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;会从&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子中选取，基本算法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，从大到小排列进行试验：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;计算余因子&lt;span class=&#34;math inline&#34;&gt;\(h = N /
n\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;随机选择椭圆曲线上的一点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(G = hP\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，则重新选择&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;进行试验；否则这意味着&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;就是秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的子群的基点。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，ECC算法能够运行的前提是，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;必须是&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子。&lt;/p&gt;
&lt;h3 id=&#34;离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete
Logarithm）&lt;/h3&gt;
&lt;p&gt;现在我们解答之前提出的对数运算问题，给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，目前没有算法能够在多项式时间之内求解满足&lt;span class=&#34;math inline&#34;&gt;\(Q = kP\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。这个问题有点类似于给定整数&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，如何求解满足&lt;span class=&#34;math inline&#34;&gt;\(b = a^k \pmod p\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;？这两个问题目前都没有算法能在多项式时间之内求解，这也是ECC算法安全的根本。&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/h2&gt;
&lt;p&gt;寻找到之前秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的子群后，我们就可以生成私钥和公钥了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;私钥是从&lt;span class=&#34;math inline&#34;&gt;\(\{1,\dots,n-1\}\)&lt;/span&gt;中随机抽取的数字&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;公钥是点&lt;span class=&#34;math inline&#34;&gt;\(H = dG\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面介绍两个基于ECC的公钥加密算法。&lt;/p&gt;
&lt;h3 id=&#34;elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve
Diffie-Hellman&lt;/h3&gt;
&lt;p&gt;ECDH是DH算法在椭圆曲线中的变体，它实际上是一种密钥交换算法，而不是加密算法。它的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice和Bob各自随机生成私钥和公钥：&lt;span class=&#34;math inline&#34;&gt;\(H_A =
d_A G, H_B = d_B G\)&lt;/span&gt;，注意，Alice和Bob使用了相同的基点；&lt;/li&gt;
&lt;li&gt;Alice和Bob在非安全信道上交换各自的公钥，即使中间人拦截到了&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，如果他不能求解出对数运算问题，他也不会知道Alice和Bob的私钥；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_A
H_B\)&lt;/span&gt;，Bob计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_B
H_A\)&lt;/span&gt;，根据子群对加法的封闭性，二者应该得到相同的结果；&lt;/li&gt;
&lt;li&gt;中间人即使知道&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，也无法得到密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，Alice和Bob便可以通过密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;加密内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve
Digital Signature Algorithm&lt;/h3&gt;
&lt;p&gt;ECDSA是一种公钥加密算法，可以用于数字签名。ECDSA的作用对象是消息的哈希值，而不是消息本身，所以在使用ECDSA时，也要选取一个安全的哈希函数。消息的哈希值在签名过程中会被截断，使得该剩余哈希值的比特位数等于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的比特位数，我们用&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;来表示剩余哈希值所代表的整数。ECDSA的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice从&lt;span class=&#34;math inline&#34;&gt;\(\{1, \dots,
n\}\)&lt;/span&gt;中随机抽取数字&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(P = kG\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod
n\)&lt;/span&gt;，如果&lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + rd_A) \mod
n\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(d_A\)&lt;/span&gt;是Alice的私钥，&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;关于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元（我们前面选取&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;为质因子的目的就在于，保证这里的&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;一定存在），如果&lt;span class=&#34;math inline&#34;&gt;\(s=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;元组&lt;span class=&#34;math inline&#34;&gt;\((r,s)\)&lt;/span&gt;就是Alice对应的签名。Bob拿到这样的签名之后，作以下验证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_1 = s^{-1}z \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_2 = s^{-1}r \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算点&lt;span class=&#34;math inline&#34;&gt;\(P = u_1G + u_2H_A\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;仅当&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod
n\)&lt;/span&gt;时，Bob可以验证这确实是Alice的签名。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;验证过程的正确性证明如下， &lt;span class=&#34;math display&#34;&gt;\[
\label{P} \begin{aligned}
P &amp;amp;= u_1 G + u_2 H_A \\
&amp;amp;= u_1 G + u_2 d_A G \\
&amp;amp;= (s^{-1} z + s^{-1} r d_A) G \\
&amp;amp;= s^{-1}(z + r d_A) G
\end{aligned}
\]&lt;/span&gt; 之前我们定义&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + r d_A)
\mod n\)&lt;/span&gt;，将两边同乘&lt;span class=&#34;math inline&#34;&gt;\(ks^{-1}\)&lt;/span&gt;，我们可以得到&lt;span class=&#34;math inline&#34;&gt;\(k = s^{-1}(z + r d_A) \mod
n\)&lt;/span&gt;，将此式代入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{P}\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
P = kG
\]&lt;/span&gt; 这也就是Alice签名过程中得到的&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，证毕。&lt;/p&gt;
&lt;h4 id=&#34;k的选取&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的选取&lt;/h4&gt;
&lt;p&gt;在使用ECDSA时，我们必须注意不能使用相同的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密多份消息，也不能暴露我们选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的方式（比如说随机数生成方式），否则就会有很大的私钥泄露风险。比如说我们用同一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密两份消息，Bob就可以通过这两次签名过程得到&lt;span class=&#34;math inline&#34;&gt;\((r, s_1), (r,
s_2)\)&lt;/span&gt;，如果Bob还有额外途径获取两次消息的哈希&lt;span class=&#34;math inline&#34;&gt;\(z_1, z_2\)&lt;/span&gt;，那么： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
s_1 = k^{-1}(z_1 + r d_A), s_2 = k^{-1}(z_2 + r d_A) \to \\
(s_1 - s_2) = k^{-1}(z_1 - z_2) \mod n \to \\
k = (z_1 - z_2)(s_1 - s_2)^{-1}
\end{gather}
\]&lt;/span&gt; 再根据&lt;span class=&#34;math inline&#34;&gt;\(s_1 = k^{-1}(z_1 + r d_A)
\mod n\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_A &amp;amp;= r^{-1}(s_1k - z_1) \mod n \\
&amp;amp;= r^{-1}(s_1(z_1 - z_2)(s_1 - s_2)^{-1} - z_1) \mod n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看离散对数运算&#34;&gt;再看离散对数运算&lt;/h3&gt;
&lt;p&gt;给定秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的椭圆曲线子群，以及该子群上的两点&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，离散对数运算求解的是满足&lt;span class=&#34;math inline&#34;&gt;\(Q = xP\)&lt;/span&gt;的整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。我们接下来了解两个求解离散对数运算的算法。&lt;/p&gt;
&lt;h4 id=&#34;baby-step-giant-step&#34;&gt;Baby-step-giant-step&lt;/h4&gt;
&lt;p&gt;首先任意一个整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，都可以写成&lt;span class=&#34;math inline&#34;&gt;\(x = am + b\)&lt;/span&gt;，由&lt;span class=&#34;math inline&#34;&gt;\(a,m,b\)&lt;/span&gt;这三个满足关系的任意整数表示，那么，我们就可以考虑这样解决离散对数运算问题：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q &amp;amp;= xP \\
Q &amp;amp;= (am + b)P \\
Q - amP &amp;amp;= bP
\end{aligned}
\]&lt;/span&gt;
Baby-step-giant-step算法采取了从两边夹逼的方式解决问题，过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(m = \lceil \sqrt n
\rceil\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots,
m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，计算&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，并将&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;存储到哈希表中；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots,
m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(Q - amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;检查哈希表中是否存在某个&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(Q -
amP = bP\)&lt;/span&gt;，如果存在，就意味着我们找到了一个解。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;的计算对应着baby-step，&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;的计算对应着giant-step，该算法的合理性在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=0\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(0, P,
\dots, mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(mP, P
+ mp, \dots, mP + mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=2\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(2mP, P
+ 2mp, \dots, mP + 2mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=m-1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\((m-1)mP, P + (m-1)mp, \dots, mP +
(m-1)mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，我们检查了&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(m^2P=nP\)&lt;/span&gt;之间的所有点，也就是所有可能的点。而检查的过程我们并不需要做实际的加法运算，只需要检查哈希表中有没有对应的差值。在baby-step中，我们需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加法，在giant-step中，由于哈希表查询速度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;，并且至多需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加减法，所以整体上该算法的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;，而哈希表带来的空间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;。尽管看上去这个多项式时间的算法还不错，但是由于一般&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;非常大，这个算法实际需要的时间成本以及存储成本远远超出当前计算机的水平。&lt;/p&gt;
&lt;h4 id=&#34;pollards-rho&#34;&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法的时间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;，但是它的空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;。和Baby-step-giant-step算法一样，我们实际解决的问题与原问题稍微有所不同，在Pollard’s
&lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中，给定&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，我们想要找到整数&lt;span class=&#34;math inline&#34;&gt;\(a,b,A,B\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
aP + bQ = AP + BQ
\]&lt;/span&gt; 找到这四个整数之后，我们代入&lt;span class=&#34;math inline&#34;&gt;\(Q =
xP\)&lt;/span&gt;来求解&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
aP + bxP &amp;amp;= AP + BxP \\
(a-A)P &amp;amp;= (b-B)xP \\
&amp;amp;\Downarrow \\
(a-A) &amp;amp;= (b-B)x \pmod n \\
x &amp;amp;= (a-A)(b-B)^{-1} \mod n
\end{aligned}
\]&lt;/span&gt; Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法思路是这样的：我们生成一系列伪随机点&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(X_i = a_iP +
b_iQ\)&lt;/span&gt;。这样的序列可以由一个伪随机函数&lt;span class=&#34;math inline&#34;&gt;\(f(X_i) = (a_{i+1},
b_{i+1})\)&lt;/span&gt;生成，也就是说下一点是由当前点决定的，而&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部如何工作并不重要。通过这样的&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;产生序列，我们的序列迟早会出现一个回环，也就是说&lt;span class=&#34;math inline&#34;&gt;\(X_j =
X_i\)&lt;/span&gt;，而这时我们也就能够找到相应的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。出现回环的原因也很好理解：我们点的个数是有限的，问题其实在于如何找到回环入口。&lt;/p&gt;
&lt;h5 id=&#34;龟兔赛跑&#34;&gt;龟兔赛跑&lt;/h5&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中的回环入口查找，其实类似单向链表中的回环入口查找：在链表开头设置一快一慢两个指针，我们让快指针每次前进两步，慢指针每次前进一步；二者相遇时，从相遇点和起点再设置两个新的慢指针，这两个新的慢指针相遇之处即为环的入口。&lt;/p&gt;
&lt;h4 id=&#34;量子计算shors-algorithm&#34;&gt;量子计算：Shor’s Algorithm&lt;/h4&gt;
&lt;p&gt;理论上，Shor’s Algorithm的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron((\log n)^3)\)&lt;/span&gt;，空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;，但是目前的量子计算机还不能进行像Shor’s
Algorithm这样复杂的运算。&lt;/p&gt;
&lt;h3 id=&#34;ecc与rsa&#34;&gt;ECC与RSA&lt;/h3&gt;
&lt;p&gt;RSA的密钥长度在数量级上大于ECC的密钥长度，这不仅意味着更多的内存占用，还意味着更慢的计算速度。这其中的原因在于，RSA算法的离散对数运算是快于ECC算法的离散对数运算（参考&lt;a href=&#34;https://en.wikipedia.org/wiki/General_number_field_sieve&#34;&gt;General
number field
sieve&lt;/a&gt;），这也就意味着RSA算法不得不采用更长的密钥来加大破解难度。更少的内存占用，更快的计算速度，这就是在已经有了成熟的RSA算法的情况下，ECC仍被提出的原因。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/entropy/</link>
      <pubDate>Thu, 28 Apr 2022 22:47:16 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/entropy/</guid>
      <description>

&lt;p&gt;The entropy of discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (probability mass function) is defined
as &lt;span class=&#34;math display&#34;&gt;\[
H(p) = -\mathrm{E}_{x \sim p}\log p(x)
\]&lt;/span&gt; The entropy reaches its maximum when the underlying
distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a uniform
distribution. The maximum value is &lt;span class=&#34;math inline&#34;&gt;\(\log
k\)&lt;/span&gt; if the support is finite and has &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; many distinct values. This can be
derived with the Jenson’s inequality and understood via the level of
chao of a distribution. From an analysis point of view, the entropy is
defined on a &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-tuple whose domain
is compact so that the maxima must exist; we can always find a larger
entropy if any two of &lt;span class=&#34;math inline&#34;&gt;\(p_1,
\dots,p_k\)&lt;/span&gt; are not equal.&lt;/p&gt;
&lt;p&gt;The entropy of continuous distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; (probability density function) is
usually similarly defined as &lt;span class=&#34;math display&#34;&gt;\[
H(q) = -\E_{x\sim q}\log q(x) = -\int q(x)\log q(x)\d x
\]&lt;/span&gt; This is actually the &lt;strong&gt;differential entropy&lt;/strong&gt;
introduced by Shannon. In fact, it is not a good continuous analog of
discrete entropy and it was not rigorously derived. For example, this
formula can be negative. Therefore, in the case of entropy, the random
variable had better be discrete, despite the wide usage of differential
entropy.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;The entropy of a &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional
&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(x) = \frac{e^{-\frac 1 2 (x-\mu)^T
\Sigma^{-1}(x-\mu)}}{\sqrt{|2\pi\Sigma|}}\)&lt;/span&gt; can be derived as
follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
H(p) &amp;amp;\triangleq -\int p(x) \log p(x) \d x = -\int p(x) [-\frac 1 2
(x-\mu)^T \Sigma^{-1} (x-\mu) - \frac 1 2 \log |2\pi\Sigma|] \d x \\
&amp;amp;= \frac 1 2 \int p(x) (x-\mu)^T \Sigma^{-1}(x-\mu) \d x + \frac 1 2
\log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 \int p(x) x^T \Sigma^{-1} x \d x + \frac 1 2 \int p(x)
\mu^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad - \frac 1 2 \int p(x) \mu^T \Sigma^{-1} x \d x - \frac 1 2
\int p(x) x^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= [\frac 1 2 \tr(\Sigma^{-1} \Sigma) + \frac 1 2 \mu^T \Sigma^{-1}
\mu] + \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad - \frac 1 2 \mu^T \Sigma^{-1} \mu - \frac 1 2 \mu^T
\Sigma^{-1} \mu \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 n + \frac 1 2 \log |2\pi\Sigma|
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>0-intro</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/0-intro/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/0-intro/</guid>
      <description>

&lt;h2 id=&#34;roadmap&#34;&gt;Roadmap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What is the difference between machine learning and
optimization?&lt;/p&gt;
&lt;p&gt;Machine learning training is exactly an optimization process. But
machine learning additionally takes into consideration the adaptation of
the trained model from training data to unknown test data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical model (multi-variate central limit theorem
exemplified with Gaussian and &lt;a href=&#34;https://online.stat.psu.edu/stat504/book/export/html/667&#34;&gt;multinomial
distribution&lt;/a&gt;) and its limitation in machine learning analytics&lt;/p&gt;
&lt;p&gt;The analysis mostly focuses on asymptotic scenarios, and does not
provide non-asymptotic guarantees.&lt;/p&gt;
&lt;p&gt;The analysis requires a well-behaved statistical model, e.g. a normal
or multinomial distribution. However, real-world image, text, and sound
distributions could be more complex.&lt;/p&gt;
&lt;p&gt;The analysis aims to find the entire probability distribution, which
could be too costly to compute and analyze. An approach to directly
analyze the error could be more feasible for large-scale machine
learning models and datasets. For example, an approximation bound
guarantee other than a statistical guarantee is also useful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is machine learning theory about?&lt;/p&gt;
&lt;p&gt;Analysis of finite training data falls under probability, statistics
and information theory.&lt;/p&gt;
&lt;p&gt;Analysis of learning models falls under functional analysis and
signal processing.&lt;/p&gt;
&lt;p&gt;Analysis of computing algorithms falls under optimization and
computation theory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is this course about?&lt;/p&gt;
&lt;p&gt;Theory of &lt;strong&gt;model-based&lt;/strong&gt; statistical learning:
exponential families, maximum likelihood, method of moments, maximum
entropy principle&lt;/p&gt;
&lt;p&gt;Theory of &lt;strong&gt;model-free&lt;/strong&gt; machine learning: uniform
convergence bounds, VC dimension, Rademacher complexity, covering
numbers&lt;/p&gt;
&lt;p&gt;Theory of representation: kernel functions and methods, approximation
in deep learning&lt;/p&gt;
&lt;p&gt;Theory of convergence: optimization and generalization in deep
learning, convex vs. non-convex machine learning problems&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>1-optimization-problem</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</guid>
      <description>

&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h2&gt;
&lt;p&gt;The standard optimization problem will be in the form: &lt;span class=&#34;math display&#34;&gt;\[
\inf_{x \in X} f(x) \\
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the
&lt;strong&gt;feasible/constraint region/set&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; is the objective
function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; is
an &lt;strong&gt;optimal solution&lt;/strong&gt; to the problem if &lt;span class=&#34;math inline&#34;&gt;\(v^* = f(x^*)\)&lt;/span&gt;. In this case, we also say
that &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; attains optimal value
&lt;span class=&#34;math inline&#34;&gt;\(v^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;
is a &lt;strong&gt;local minimizer&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\exists \epsilon &amp;gt; 0, \forall x \in B(x&amp;#39;,
\epsilon), f(x) \ge f(x&amp;#39;)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;types-of-problems&#34;&gt;Types of Problems&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Unconstrained: &lt;span class=&#34;math inline&#34;&gt;\(X =
\R^n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discrete programming&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a discrete set, which means
&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in X, \exists \epsilon &amp;gt; 0, X
\cap B(x, \epsilon) = \{ x \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Every feasible solution to discrete optimization problem is a
local minimizer.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X = \{ x \in \R^n | (a^i)^T x \le c_i,
i=1,2,\dots,m \}\)&lt;/span&gt; is a set defined by a &lt;strong&gt;finite&lt;/strong&gt;
number of linear inequalities.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f = b_1 x_1 + b_2 x_2 + \dots + b_n x_n =
b^T x\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quadratic programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same as that in linear
programming.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}
x_i x_j = x^T A x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A =
[a_{ij}] \in R^{n \times n}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: This form does not include any linear term. Generally, a
quadratic function takes on the form &lt;span class=&#34;math inline&#34;&gt;\(f(x) =
x^T A x + b^T x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: We may assume &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
symmetric, since even it is not, we can have &lt;span class=&#34;math inline&#34;&gt;\(A&amp;#39; = \frac{A + A^T}{2}\)&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
x^T A x = x^T A^T x \to x^T A x = x^T A&amp;#39; x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The right hand side is obvious because &lt;span class=&#34;math inline&#34;&gt;\(x^T A^T x\)&lt;/span&gt; is a number, and it equals to
its transpose &lt;span class=&#34;math inline&#34;&gt;\(x^T A x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Semi-definite programming&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Consider &lt;span class=&#34;math inline&#34;&gt;\(Q \in
\mathcal{S}^{n}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}^n\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; symmetric matrix. The
following is equivalent:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is &lt;strong&gt;positive
semi-definite&lt;/strong&gt; (short as PSD, denoted as &lt;span class=&#34;math inline&#34;&gt;\(Q \succcurlyeq 0\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in \R^n, x^T Q x \ge
0\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;All eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are
non-negative.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(C, A_1, \dots, A_m \in S^n\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R^m\)&lt;/span&gt; be given, the
semi-definite programming is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; C - \underbrace{\sum_{i=1}^m x_i A_i}_{M(x)}
\succcurlyeq 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The constraint is called a linear matrix inequality. Observe
that &lt;span class=&#34;math inline&#34;&gt;\(M: \R^m \mapsto S^n\)&lt;/span&gt; satisfies
&lt;span class=&#34;math display&#34;&gt;\[
M(\alpha x + \beta y) = \alpha M(x) + \beta M(y)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is a linear map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: Compare between linear programming and positive semi-definite
programming:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; (a^i)^T x \le c_i, \\
&amp;amp; i = 1,\dots,m
\end{aligned}
\quad \quad
\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; C - \sum_{i=1}^m x_i A_i \succcurlyeq 0
\end{aligned}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In linear programming, construct matrices:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[
C&amp;#39; =
\begin{pmatrix}
c_1 &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; c_2 &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp; \\
&amp;amp; &amp;amp; &amp;amp; c_m
\end{pmatrix},
A_i&amp;#39; =
\begin{pmatrix}
a_1^i &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; a_2^i &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp; \\
&amp;amp; &amp;amp; &amp;amp; a_m^i \\
\end{pmatrix}
\]&lt;/span&gt; Then $$ C’ - _{i=1}^m x_i A_i’ =
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
c_1 - \sum_{j=1}^m x_j a_1^j &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; c_2 - \sum_{j=1}^m x_j a_2^j &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp;  \\
&amp;amp; &amp;amp; &amp;amp; c_m - \sum_{j=1}^m x_j a_m^j &amp;amp;  \\
\end{pmatrix}\]&lt;/span&gt;
&lt;p&gt;$$ because a diagonal matrix is PSD if and only if all of its
diagonal entries are non-negative.&lt;/p&gt;
&lt;p&gt;In this sense, linear programming is special case of semi-definite
programming where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt;’s are all diagonal.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples-of-problems&#34;&gt;Examples of Problems&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Air traffic control&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; airplanes are arriving.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th airplane arrives within
&lt;span class=&#34;math inline&#34;&gt;\([a_i, b_i]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Assume airplanes arrive and land in order.&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; be the landing time
assigned to &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th airplane &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The metering time is defined to be the time difference between two
consecutive airplane landings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implicit constraints derived from above conditions is that &lt;span class=&#34;math inline&#34;&gt;\(a_i \le t_i \le b_i\)&lt;/span&gt; and $ t_i t_{i+1}$.
For safety, we want to the minimum metering time to be maximized. That
is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t} \quad &amp;amp; f(t) \triangleq \min_{1 \le i \le n-1} t_{i+1} -
t_i \\
\textrm{s.t.} \quad &amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; The constraints are all linear. The &lt;code&gt;min&lt;/code&gt; operation
in &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, however, doesn’t comfort us.
We can introduce a new variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;
and convert the original problem to an equivalent one: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \\
\textrm{s.t.} \quad &amp;amp; z = \min_{1 \le i \le n-1} t_{i+1} - t_i \\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; Further, since the objective is to maximize and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;’s coefficient is positive, the problem
can be converted to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \\
\textrm{s.t.} \quad &amp;amp; z \le t_{i+1} - t_i, i=1,\dots,n-1\\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; Now the problem becomes a linear one and is easy to
solve.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data fitting problem&lt;/p&gt;
&lt;p&gt;Given data points &lt;span class=&#34;math inline&#34;&gt;\((a_i, b_i) \in \R^n
\times \R\)&lt;/span&gt;, a typical choice to fit those data would be an
affine function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = y^T x + t\)&lt;/span&gt;.
Other than the choice of function, the choice of objective function
matters too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Least squares&lt;/p&gt;
&lt;p&gt;The objective minimizes the sum of the squares of errors for all data
points: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} (y^T a_i - b_i)^2 \\
\end{aligned}
\]&lt;/span&gt; This is a quadratic programming problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum absolute deviation &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} |y^T a_i - b_i| \\
\end{aligned}
\]&lt;/span&gt; Using the same trick of reparameterization, the problem is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \\
\textrm{s.t.} \quad &amp;amp; z_i = |y^T a_i - b_i|, i=1,\dots,m \\
\end{aligned}
\]&lt;/span&gt; Further, the trick of relaxation applies: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \\
\textrm{s.t.} \quad &amp;amp; y^T a_i - b_i \le z_i, i=1,\dots,m \\
&amp;amp; y^T a_i - b_i \ge -z_i, i=1,\dots,m \\
\end{aligned}
\]&lt;/span&gt; This becomes a linear programming problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key takeaway from these examples are the &lt;strong&gt;problem
equivalence&lt;/strong&gt; trick, which includes introducing new variables and
relaxing &lt;span class=&#34;math inline&#34;&gt;\(\max\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\min\)&lt;/span&gt; to inequalities.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier</title>
      <link>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</link>
      <pubDate>Fri, 07 Jan 2022 13:00:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</guid>
      <description>

&lt;h2 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h2&gt;
&lt;p&gt;The standard form of the Lagrange multiplier optimization problem is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\inf f_0(x) \\
s.t.\quad &amp;amp; f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp; h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Denote the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; that satisfies all the requirements in
the above problem as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X \subseteq
\R^n\)&lt;/span&gt;. Then the Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(L:\R^n \times \R^r \times \R^s \mapsto \R\)&lt;/span&gt;
(there is an implicit constraint that the variables must reside in the
natural domain of the functions):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^r\lambda_if_i(x) +
\sum_{i=1}^s\mu_ih_i(x)
\]&lt;/span&gt; Define the function &lt;span class=&#34;math inline&#34;&gt;\(P: \mathcal X
\mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu)
\]&lt;/span&gt; The primal problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\inf_{x \in \mathcal X} P(x) \label{primal}
\end{equation}
\]&lt;/span&gt; It is easy to have &lt;span class=&#34;math inline&#34;&gt;\(P(x)
=f_0(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt;.
Thus the primal problem is equivalent to the original problem. Denote
primal problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;’s
optimal value as &lt;span class=&#34;math inline&#34;&gt;\(p^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define the dual function &lt;span class=&#34;math inline&#34;&gt;\(D: \R^r \times
\R^s \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
D(\lambda, \mu) = \inf_{x \in \mathcal \R^n}L(x, \lambda, \mu)
\]&lt;/span&gt; The Lagrangian dual problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\sup_{\lambda,\mu;\lambda_i \ge 0} D(\lambda, \mu) \label{dual}
\end{equation}
\]&lt;/span&gt; Denote the dual problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;’s optimal value as &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt;. We always have &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;For any feasible &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathcal X,
(\lambda, \mu) \in {\R^r}^+ \times \R^s\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda&amp;#39;,\mu&amp;#39;;\lambda&amp;#39;_i \ge
0}L(x,\lambda&amp;#39;,\mu&amp;#39;) \ge L(x,\lambda,\mu) \ge \inf_{x&amp;#39; \in
\R^n}L(x&amp;#39;, \lambda, \mu) =D (\lambda, \mu)
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge
d^\star\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt; is called
weak duality because it always holds. &lt;span class=&#34;math inline&#34;&gt;\(p^\star = d^\star\)&lt;/span&gt; is called strong
duality because it does not hold in general. Assume, though, a strong
duality holds, let &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; be the
primal optima, &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star,
\mu^\star)\)&lt;/span&gt; be the dual optima, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
f_0(x^\star) = p^\star = d^\star = D(\lambda^\star, \mu^\star) = \inf_{x
\in \R^n}L(x,\lambda^\star,\mu^\star) \le
L(x^\star,\lambda^\star,\mu^\star) \\
f_0(x^\star) = p^\star = P(x^\star) = \sup\limits_{\lambda,\mu;\lambda_i
\ge 0}L(x^\star,\lambda,\mu) \ge L(x^\star,\lambda^\star,\mu^\star) \\
L(x^\star,\lambda^\star,\mu^\star) \le f_0(x^\star) \le
L(x^\star,\lambda^\star,\mu^\star)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) =
L(x^\star,\lambda^\star,\mu^\star)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the strong duality holds and &lt;span class=&#34;math inline&#34;&gt;\((x^\star,\lambda^\star,\mu^\star)\)&lt;/span&gt; is the
optima, they must satisfy the KKT conditions, and we can leverage the
KKT conditions to solve the optima and optimal value:&lt;/p&gt;
&lt;h4 id=&#34;karush-kuhn-tucker-conditions&#34;&gt;Karush-Kuhn-Tucker
Conditions&lt;/h4&gt;
&lt;p&gt;In standard optimization problem, KKT conditions refer to the below
four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;primal constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(f_1(x) \dots,f_r(x) \le 0, h_1(x),\dots,h_s(x) =
0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;dual constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\dots,\lambda_r \ge 0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;complementary slackness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1f_1(x),\dots,\lambda_rf_r(x) =
0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;vanishing gradient&lt;/strong&gt; of Lagrangian w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; : &lt;span class=&#34;math display&#34;&gt;\[
\nabla f_0(x) + \sum_{i=1}^r\lambda_i\nabla f_i(x) +
\sum_{i=1}^s\mu_i\nabla h_i(x) = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that solutions satisfying KKT conditions do not imply a strong
duality or an optimal point. For a better discussion between strong
duality and KKT conditions, please go to &lt;a href=&#34;https://math.stackexchange.com/questions/3616646/question-about-kkt-conditions-and-strong-duality&#34;&gt;this
discussion&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;when-to-apply&#34;&gt;When-to-apply&lt;/h3&gt;
&lt;h4 id=&#34;slater-condition&#34;&gt;Slater Condition&lt;/h4&gt;
&lt;p&gt;Strong duality does not hold generally. But it does hold in &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/convex-optimization/#Standard Form&#34;&gt;standard convex optimization
problem&lt;/a&gt;. In such case, KKT conditions are also sufficient for strong
duality provided that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is an
interior point of the feasible region.&lt;/p&gt;
&lt;h4 id=&#34;general-case&#34;&gt;General Case&lt;/h4&gt;
&lt;p&gt;In cases where we cannot tell strong duality directly, we may still
try to apply Lagrangian multiplier to convert the primal problem to the
less-constrained dual problem (&lt;span class=&#34;math inline&#34;&gt;\(\lambda \ge
0\)&lt;/span&gt; is much looser than the constraints in the original problem).
That is, we solve &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt; first,
and then check if &lt;span class=&#34;math inline&#34;&gt;\(d^\star =
p^\star\)&lt;/span&gt;. We do so with the following process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;firstly take derivative of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;
w.r.t. the unconstrained &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and make
it zero (vanishing gradient) to obtain the closed-form expression of
&lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;find &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star \ge 0\)&lt;/span&gt; (dual
constraint) and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt; that
maximizes the &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt; and
solve &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;finally verify that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;
satisfies the constraints (primal constraint and the implied
complementary slackness) in the original problem and &lt;span class=&#34;math inline&#34;&gt;\(f_0(x^\star) = d^\star\)&lt;/span&gt; (strong
duality).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we can successfully go through the above process, we can still
solve the problem with Lagrangian multiplier.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bullet Points</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bullet-points/</link>
      <pubDate>Sun, 19 Dec 2021 13:59:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bullet-points/</guid>
      <description>

&lt;h2 id=&#34;bullet-points&#34;&gt;Bullet Points&lt;/h2&gt;
&lt;p&gt;This post lists out various topics under the machine learning
subject.&lt;/p&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;data modalities
&lt;ul&gt;
&lt;li&gt;numbers&lt;/li&gt;
&lt;li&gt;texts&lt;/li&gt;
&lt;li&gt;images&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;audios&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data cleaning&lt;/li&gt;
&lt;li&gt;imbalanced data&lt;/li&gt;
&lt;li&gt;data normalization (one &lt;a href=&#34;https://cs231n.github.io/neural-networks-2/#:~:text=Common%20pitfall.&#34;&gt;pitfall&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;standardization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data augmentation&lt;/li&gt;
&lt;li&gt;data splitting
&lt;ul&gt;
&lt;li&gt;cross validation&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/ransac/&#34;&gt;RANSAC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;feature extraction/engineering
&lt;ul&gt;
&lt;li&gt;domain expertise&lt;/li&gt;
&lt;li&gt;mathematical methods
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kernel methods&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;neural network self-adaptation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;supervised vs. unsupervised
&lt;ul&gt;
&lt;li&gt;supervised learning
&lt;ul&gt;
&lt;li&gt;fits towards given target&lt;/li&gt;
&lt;li&gt;includes
&lt;ul&gt;
&lt;li&gt;classification
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/&#34;&gt;logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/&#34;&gt;support vector machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/&#34;&gt;Bayes classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/&#34;&gt;linear discriminant
analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;extrapolation
&lt;ul&gt;
&lt;li&gt;from classification model to regression model&lt;/li&gt;
&lt;li&gt;from binary classification to multi-class classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regression (&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-regression/&#34;&gt;linear&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/&#34;&gt;non-linear&lt;/a&gt; case)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;unsupervised learning
&lt;ul&gt;
&lt;li&gt;discovers inherent structures within/among the data&lt;/li&gt;
&lt;li&gt;includes
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/clustering/&#34;&gt;clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/&#34;&gt;dimensionality
reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;manifold embedding&lt;/li&gt;
&lt;li&gt;latent variable modelling
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/&#34;&gt;hidden Markov model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;conditional random field&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/&#34;&gt;independent component
analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;reinforcement learning
&lt;ul&gt;
&lt;li&gt;adapts to the environment via explore-and-exploit strategy&lt;/li&gt;
&lt;li&gt;depending on the knowledge of the environment, categorizes into
&lt;ul&gt;
&lt;li&gt;model-based&lt;/li&gt;
&lt;li&gt;model-free&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;depending on the learning objective, categorizes into
&lt;ul&gt;
&lt;li&gt;value-based (prediction problem)&lt;/li&gt;
&lt;li&gt;policy-based (control problem)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;discriminative vs. generative&lt;/li&gt;
&lt;li&gt;linear vs. non-linear (depending on whether the decision boundary is
a hyperplane)&lt;/li&gt;
&lt;li&gt;parametric vs. non-parametric (depending on whether the data is
characterized 1) by a distribution 2) with a finite number of
parameters)&lt;/li&gt;
&lt;li&gt;ensemble method
&lt;ul&gt;
&lt;li&gt;bootstrap aggregating
&lt;ul&gt;
&lt;li&gt;random forest&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;gradient boosting
&lt;ul&gt;
&lt;li&gt;least square boosting&lt;/li&gt;
&lt;li&gt;AdaBoost&lt;/li&gt;
&lt;li&gt;LogitBoost&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regularization and overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;training criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean squared error&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;cross entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;impurity
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/entropy/&#34;&gt;entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;testing criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;confusion matrix&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Real&lt;/th&gt;
&lt;th&gt;Positive&lt;/th&gt;
&lt;th&gt;Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Positive&lt;/td&gt;
&lt;td&gt;TP&lt;/td&gt;
&lt;td&gt;FN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Negative&lt;/td&gt;
&lt;td&gt;FP&lt;/td&gt;
&lt;td&gt;TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;F-score&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;precision-recall curve&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/receiver-operator-characteristic/&#34;&gt;receiver operation
characteristics&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/&#34;&gt;mean average
precision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate System and Change of Basis</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</link>
      <pubDate>Sat, 18 Dec 2021 20:47:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</guid>
      <description>
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \{b_1, b_2, ...,
b_n\}\)&lt;/span&gt; be a basis for a vector space &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Then for &lt;span class=&#34;math inline&#34;&gt;\(x
= [x_1, x_2, ..., x_n]^T\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, there exists a unique set of scalars
&lt;span class=&#34;math inline&#34;&gt;\(q_1, q_2, ..., p_n\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
x = q_1b_1 + q_2b_2 + ... + q_nb_n
\]&lt;/span&gt; These scalars are called the coordinates of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{B} =
\begin{bmatrix}
q_1
\cdots
q_n
\end{bmatrix}^T
\]&lt;/span&gt; is the coordinate vectors of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. The mapping &lt;span class=&#34;math inline&#34;&gt;\(x \mapsto [x]_\mathcal{B}\)&lt;/span&gt; is called the
coordinate mapping determined by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{B} = [b_1, b_2, ...,
b_n]\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x =
P_\mathcal{B}[x]_\mathcal{B}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt; both be a basis for an
n-dimensional vector space &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;.
Then there is a unique &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt;
matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C
\leftarrow B}\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{C} = \mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B}
\]&lt;/span&gt; The columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow
B}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;-coordinate vectors of the
vectors in the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\mathop{P}\limits_\mathcal{C \leftarrow B} = [[b_1]_\mathcal{C},
[b_2]_\mathcal{C}, ..., [b_n]_\mathcal{C}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B} =  P_\mathcal{C}\mathop{P}\limits_\mathcal{C
\leftarrow B}
\begin{bmatrix}
q_1
\cdots
q_n
\end{bmatrix}^T \\
&amp;amp;= P_\mathcal{C}(q_1[b_1]_\mathcal{C} + q_2[b_2]_\mathcal{C} + ... +
q_n[b_n]_\mathcal{C}) \\
&amp;amp;= q_1P_\mathcal{C}[b_1]_\mathcal{C} +
q_2P_\mathcal{C}[b_2]_\mathcal{C} + ... +
q_nP_\mathcal{C}[b_n]_\mathcal{C} \\
&amp;amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\
&amp;amp;= x
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} =
\mathcal{I}\)&lt;/span&gt; is the standard basis, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= [[e_1]_\mathcal{C},
[e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;=
P_\mathcal{C}[[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ...,
[e_n]_\mathcal{C}] \\
&amp;amp;= [e_1, e_2, ..., e_n] \\
&amp;amp;= I
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{C}\)&lt;/span&gt; is
invertible, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C
\leftarrow I} = P_\mathcal{C}^{-1}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\([x]_\mathcal{B} = P_\mathcal{B}^{-1}x\)&lt;/span&gt; in
equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \\
[x]_\mathcal{C} &amp;amp;= \mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B} \\
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C} &amp;amp;=
(\mathop{P}\limits_\mathcal{C \leftarrow
B})^{-1}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
[x]_\mathcal{B} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow
B})^{-1}[x]_\mathcal{C}  \\
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{B \leftarrow C} =
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1},
\mathop{P}\limits_\mathcal{B \leftarrow C}\mathop{P}\limits_\mathcal{C
\leftarrow B} = I\)&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>单源最短路径问题</title>
      <link>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 21 Jun 2021 09:33:12 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。&lt;/p&gt;
&lt;p&gt;当然，减少严谨的符号运算，并不意味着完全不出现符号，因为算法本身就是对问题的抽象，剥掉这层抽象，就没办法进行架构在抽象之上的信息传递了。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#问题描述及定义&#34;&gt;问题描述及定义&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决思路&#34;&gt;解决思路&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#正文之前&#34;&gt;正文之前&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#负权边&#34;&gt;负权边&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#负权环路&#34;&gt;负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#非负权环路&#34;&gt;非负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#放缩操作&#34;&gt;放缩操作&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决方法&#34;&gt;解决方法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#dijkstra算法&#34;&gt;Dijkstra算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对比&#34;&gt;对比&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;&lt;img src=&#34;sssp.png&#34;/&gt;&lt;/p&gt;
&lt;h2 id=&#34;问题描述及定义&#34;&gt;问题描述及定义&lt;/h2&gt;
&lt;p&gt;单源最短路径问题，旨在求解&lt;strong&gt;带权有向图&lt;/strong&gt;（weighted
directed graph）中&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;，从某个&lt;strong&gt;点&lt;/strong&gt;（vertex）出发，到图中任意一&lt;strong&gt;点&lt;/strong&gt;的最短距离，某些情况下，还需要找出这一条最短距离的&lt;strong&gt;路径&lt;/strong&gt;，称之为&lt;strong&gt;最短路径&lt;/strong&gt;，若无特殊指明且不致歧义，以下最短路径问题均指代单源最短路径问题。&lt;/p&gt;
&lt;p&gt;更严格一些，设&lt;span class=&#34;math inline&#34;&gt;\(G(V,
E)\)&lt;/span&gt;表示带权有向图，&lt;span class=&#34;math inline&#34;&gt;\(w : E \to
\mathbb{R}\)&lt;/span&gt;表示&lt;strong&gt;权重&lt;/strong&gt;，&lt;strong&gt;路径&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(p = \left &amp;lt;v_0, v_1, ... v_k\right
&amp;gt;\)&lt;/span&gt;的&lt;strong&gt;距离&lt;/strong&gt;定义为： &lt;span class=&#34;math display&#34;&gt;\[
W(p) = \sum\limits_{i = 1}^k w(v_{k-1}, v_k)
\]&lt;/span&gt; 其中， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\forall i \in [0, k], v_i \in V \\
\forall i \in [1, k], (v_{i-1}, v_i) \in E
\end{gather*}
\]&lt;/span&gt; 我们从某一点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;（叫作&lt;strong&gt;起点&lt;/strong&gt;，或者源点）出发，记其到图中任意一&lt;strong&gt;点&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径距离为&lt;span class=&#34;math inline&#34;&gt;\(\delta(v)\)&lt;/span&gt;，&lt;strong&gt;单源最短路径&lt;/strong&gt;求解的就是任意一条从&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(W(p) =
\delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;为了方便，我们为每一个点&lt;span class=&#34;math inline&#34;&gt;\(v \in
V\)&lt;/span&gt;设立一个中间变量&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;，用&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;来表示求解过程中的最短距离的&lt;strong&gt;可行上界&lt;/strong&gt;，也就是说始终有&lt;span class=&#34;math inline&#34;&gt;\(\delta(v) \leq v.d\)&lt;/span&gt;，算法初始化时，&lt;span class=&#34;math inline&#34;&gt;\(v.d =
+\infty\)&lt;/span&gt;，算法运行过程中，我们通过寻找路径使&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;这个上界不断减小，直到&lt;span class=&#34;math inline&#34;&gt;\(v.d = \delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;解决思路&#34;&gt;解决思路&lt;/h2&gt;
&lt;p&gt;最短路径问题（包括多源最短路径问题）都隐含着一个最优子结构（optimal
substructure），即：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;是一条连接两个点的最短路径，那么&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的任意一条&lt;strong&gt;子路径&lt;/strong&gt;，一定也是连接其两个端点的最短路径。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这条性质可由反证法轻松得到，也将是后续寻找最短路径所需要理解的一个重要概念。&lt;/p&gt;
&lt;h2 id=&#34;正文之前&#34;&gt;正文之前&lt;/h2&gt;
&lt;h3 id=&#34;负权边&#34;&gt;负权边&lt;/h3&gt;
&lt;p&gt;负权边指的是图中某些边的权重为负。虽然负权边不会对最短路径的最优子结构性质产生任何影响，但是后面我们会看到，负权边会导致Dijkstra算法失效。&lt;/p&gt;
&lt;h3 id=&#34;负权环路&#34;&gt;负权环路&lt;/h3&gt;
&lt;p&gt;负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;（loop），并且这条环路上的所有边的权重相加结果为负。&lt;/p&gt;
&lt;p&gt;一旦从起点可以到达这个环路上的点，那么最短路径问题就变得没有意义了：我们可以不断重复地走这条环路，然后
“拐出”
环路，到达目标点，使得到达目标点的路径的权重变得任意小（arbitrarily
short），所以也就不存在什么 “最短路径” 了。&lt;/p&gt;
&lt;p&gt;一个成熟的算法应当能够检测出图中是否有可以由&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到达的负权环路，如果没有，则算法照常进行；如果有，应予以通报。&lt;/p&gt;
&lt;h3 id=&#34;非负权环路&#34;&gt;非负权环路&lt;/h3&gt;
&lt;p&gt;非负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;，并且这条环路上的所有边的权重相加的结果大于等于0。&lt;/p&gt;
&lt;p&gt;负权环路会使最短路径问题没有意义，那么非负权环路呢？或者说，最短路径是否包含非负权环路呢？&lt;/p&gt;
&lt;p&gt;答案是否，如果一条最短路径包含了非负权环路，我们大可将这段环路从路径中
“拿掉”，得到的路径和原路径可以达到同样的终点，并且新路径的权重不大于原路径的权重。&lt;/p&gt;
&lt;h3 id=&#34;放缩操作&#34;&gt;放缩操作&lt;/h3&gt;
&lt;p&gt;放缩操作的对象是边，对于边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;，放缩操作将检测能否优化点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界：&lt;/p&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;RELAX(u, v, w):
if v.d &amp;gt; u.d + w(u, v):
    v.d = u.d + w(u, v)
    v.predecessor = u&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即如果路径&lt;span class=&#34;math inline&#34;&gt;\(s \sim u \to
v\)&lt;/span&gt;的长度小于当前&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，我们便可以借此优化&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，并同时通过将的&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;前继设为&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;来记录这一次优化。&lt;/p&gt;
&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;
&lt;h3 id=&#34;bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/h3&gt;
&lt;p&gt;Bellman-Ford算法是最短路径问题中最为robust的一种了，能处理负权边、能检测负权环路、不要求当前图为有向无环图（directed
acyclic graph）。Bellman-Ford算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原图中存在可由起点抵达的负权环路，返回
false，用以告知存在负权环路，最短路径问题无意义&lt;/li&gt;
&lt;li&gt;原图中不存在可由起点抵达的负权环路，返回
true，用以告知最短路径问题已解决，并将结果蕴含在相应的数据结构中&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;// 算法主体
for i = 1 to |V| - 1
    for each edge (u, v) in E
        RELAX(u, v, w)

// 检测是否存在负权回路
for each edge (u, v) in E
    if v.d &amp;gt; u.d + w(u, v)
        return false

return true&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;算法主体&lt;/p&gt;
&lt;p&gt;我们不妨先假设原图中不存在负权环路，先思考Bellman-Ford在解决最短路径问题时的正确性。&lt;/p&gt;
&lt;p&gt;根据以上的讨论，任意一点的最短路径中不存在环，故任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径最多由&lt;span class=&#34;math inline&#34;&gt;\(|V|-1\)&lt;/span&gt;条边，&lt;span class=&#34;math inline&#34;&gt;\(|V|\)&lt;/span&gt;个点构成。设： &lt;span class=&#34;math display&#34;&gt;\[
p=\left &amp;lt;v_0,v_1, ... v_k\right &amp;gt;，其中v_0 = s，v_k = t
\]&lt;/span&gt; 在寻找&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径时，任一&lt;span class=&#34;math inline&#34;&gt;\(v \in \{u | (u,t) \in E, u.d = \delta(s,
u)\}\)&lt;/span&gt;（即此时&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径已找到，且点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;有一条连向点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的边） ，都是&lt;span class=&#34;math inline&#34;&gt;\(v_{k-1}\)&lt;/span&gt;（也就是&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在其最短路径中的前继）的一个候选，我们需要证明的是，&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的实际前继能够在Bellman-Ford算法运行之下被发现，从而被真正地选为&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的前继。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bellman.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;根据前面的讨论，路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的前缀&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right &amp;gt;, \left &amp;lt; v_0,
v_1, v_2\right &amp;gt;...\)&lt;/span&gt;分别是&lt;span class=&#34;math inline&#34;&gt;\(v_1,
v_2, ...\)&lt;/span&gt;的最短路径，在外侧第一轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_0, v_1)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0,
v_1\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_1.d =
\delta(v_1)\)&lt;/span&gt;且将不再变化（因为这已经是最小）；在外侧第二轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_1, v_2)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1,
v_2\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_2.d =
\delta(v_2)\)&lt;/span&gt;且将不再变化（因为这已经是最小）&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ...
v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;负权环路检测&lt;/p&gt;
&lt;p&gt;至于负权环路检测部分的正确性，则不得不引入一些公式，但其实并不复杂。&lt;/p&gt;
&lt;p&gt;假设原图存在可由到达的负权环路&lt;span class=&#34;math inline&#34;&gt;\(c = \left
&amp;lt;v_0, v_1, ... v_k\right &amp;gt;, v_0 = v_k\)&lt;/span&gt;，其中，&lt;span class=&#34;math inline&#34;&gt;\(W(c) = \sum_{i=0}^{k-1}w(v_i, v_{i+1}) &amp;lt;
0\)&lt;/span&gt;。运用反证法，即假设最终不存在&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(v.d &amp;gt; u.d + w(u, v)\)&lt;/span&gt;，则有： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_i.d &amp;amp;\leq v_{i-1}.d + w(v_{i-1}, v_i), \forall i \in [1, k]
\Rightarrow  \\
\sum_{i=1}^k v_i.d &amp;amp;\leq \sum_{i=1}^k (v_{i-1}.d + w(v_{i-1},v_i))
\\
v_k.d + \sum_{i=1}^{k-1} v_i.d &amp;amp;\leq v_0.d + \sum_{i=1}^{k-1}
v_{i}.d + \sum_{i=1}^k w(v_{i-1},v_i) \\
0 &amp;amp;\leq \sum_{i=1}^k w(v_{i-1},v_i) = W(c)
\end{aligned}
\]&lt;/span&gt; 与&lt;span class=&#34;math inline&#34;&gt;\(W(c) &amp;lt;
0\)&lt;/span&gt;矛盾，故得证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dijkstra算法&#34;&gt;Dijkstra算法&lt;/h3&gt;
&lt;p&gt;Dijkstra算法相对于Bellman-Ford算法来说，可以在时间复杂度上有所优化，但是能够处理的情形也就少了一些：Dijkstra算法不能处理负权边（所以更不用提负权环路了）。Dijkstra算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;维持一个点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;由最短路径已确定的点构成；&lt;/li&gt;
&lt;li&gt;不断向中加入能够确定最短路径的点，直到所有中的点都被加入。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;S = {}
Q = G.V
while Q is not empty
    u = EXTRACT-MIN(Q)
    add u to S
    for each v in G.adj[u]
        RELAX(u, v, w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然，难点在于如何根据&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;找出能够确定最短路径的点。寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dijkstra.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in S\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\((u, t) \in E\)&lt;/span&gt;，对&lt;span class=&#34;math inline&#34;&gt;\((u, t)\)&lt;/span&gt;进行放缩后得到的值&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;，都是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的一个备选，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中每加入一个点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;（非&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的点），若边&lt;span class=&#34;math inline&#34;&gt;\((v,t)\)&lt;/span&gt;存在，对该边进行放缩后，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选（也就是放缩后的&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;）就会多一个，而&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;自然是这些备选中最小的那个。而当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径确定后，便可以将&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;不断扩展，直至最终包含整个点集&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;，也就是所有点的最短路径都被找到。&lt;/p&gt;
&lt;p&gt;之前提到过，Dijkstra算法不能处理负权边的情况，但上述
Dijkstra算法的讨论中似乎也没有涉及到负权边，为什么它就不能处理了呢？并且，我们只知道放缩后&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选，那么对&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的放缩要进行到什么时候，才能确认&lt;span class=&#34;math inline&#34;&gt;\(t.d=\delta(t)\)&lt;/span&gt;呢？Dijkstra算法告诉我们，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(t.d \leq u.d\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t) = t.d\)&lt;/span&gt;，也就是当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界小于所有待确认点&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;的上界时，我们就能确定&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径，也就能够将点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中。&lt;/p&gt;
&lt;p&gt;为什么？如果没有负权边，我们可以会发现，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，其上界&lt;span class=&#34;math inline&#34;&gt;\(u.d\)&lt;/span&gt;总是由放缩操作得到的，所以在算法运行过程中，它必然是单调递减的，而且它代表了一条具体的到达&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的路径。但为什么&lt;span class=&#34;math inline&#34;&gt;\(V -
S\)&lt;/span&gt;中的所有点的上界的最小值，却能够成为某个特定点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径呢？&lt;/p&gt;
&lt;p&gt;我们来看看&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为最小上界的之前之后都发生了什么，换言之，在此之前，或者在此之后，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;有没有可能更小？之前是不会更小了，因为我们说过，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;是单调递减的；那么之后呢？如果在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;中的最小上界、从而被加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;之后，我们在新的某一轮中选取另外一个点&lt;span class=&#34;math inline&#34;&gt;\(u \in V - S\)&lt;/span&gt;，作为此轮加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的点，在随后的操作中&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;会不会因为某个由&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;出发的放缩操作继续减小呢？&lt;/p&gt;
&lt;p&gt;不会的，对于某个新加入的点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\delta(u)\)&lt;/span&gt;必然不小于任何一个&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点的最短距离。运用数学归纳法，假设某一时刻&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;按加入顺序排列为&lt;span class=&#34;math inline&#34;&gt;\(\{ u_0, \dots, u_k \}\)&lt;/span&gt;，且有&lt;span class=&#34;math inline&#34;&gt;\(u_0.d \le \dots \le u_k.d\)&lt;/span&gt;。若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;“引荐”进来（也就是说进行了&lt;span class=&#34;math inline&#34;&gt;\(RELAX(u_k, u, w)\)&lt;/span&gt;操作）的，则必有&lt;span class=&#34;math inline&#34;&gt;\(u.d = u_k.d + w(u_k, u) \ge
u_k.d\)&lt;/span&gt;；而若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由非&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;的某个&lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;引荐而来，则也必然应该有&lt;span class=&#34;math inline&#34;&gt;\(u.d \ge u_k.d\)&lt;/span&gt;，运用反证法：如果&lt;span class=&#34;math inline&#34;&gt;\(u.d &amp;lt; u_k.d\)&lt;/span&gt;，则在我们的算法中，&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;至少应该在选择&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;加入的一轮中，因优于&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;被加入，和&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的当前情况矛盾，从而得证。&lt;/p&gt;
&lt;p&gt;而如果有负权边，则不能保证在后续加入的点的最短距离单调递增，故不能用以上论证来证明Dijkstra算法的正确性了。何况，这种情况下，强行使用Dijkstra算法，很轻易地就能举出反例来证明结果的错误，比如对下图以&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;为起点应用Dijkstra算法，就会得到错误结果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;counterexample.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;当然我们也得证明，每一轮加入新点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(t.d =
\delta(t)\)&lt;/span&gt;，因为此时虽然&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;达到整个算法流程中的最小，但这个最小值尚未被证明等于&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;。但正如前面所说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们可以使用数学归纳法，来证明&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;始终是一个最短路径已经确定的点的集合，也就是说，假设此时&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;是一个最短路径已经确定的点的集合，加入&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;后，&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;依然保持它的性质。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;前继的候选无非就是图中所有点，我们已经证明，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;不可能再由于&lt;span class=&#34;math inline&#34;&gt;\(V-S\)&lt;/span&gt;中的点放缩而变小了，所以&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在最短路径中的前继只可能来自于&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正是由&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点放缩而来，故&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正式所有可能中最小的那一个，证毕。&lt;/p&gt;
&lt;h3 id=&#34;有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/h3&gt;
&lt;p&gt;有向无环图，从定义上就排除了负权环路存在的可能，故所有点的最短路径必然存在，问题就在于如何寻找到这些最短路径。&lt;/p&gt;
&lt;p&gt;我们当不能对其直接应用Dijkstra算法，因为有向无环图并不排除负权边存在的可能——那就直接用Bellman-Ford算法咯？&lt;/p&gt;
&lt;p&gt;也不尽然。有向无环图显然只是Bellman-Ford算法能够处理的情况中的一小部分，并且这一小部分具有一些特殊的性质：无环。设任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径为&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1, ... v_k\right
&amp;gt;\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(v_0 = s，v_k =
t\)&lt;/span&gt;，既然不存在环路，从起点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;出发，只要沿着边走，一步一步放缩，必然是先放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right
&amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_1.d =
\delta(v_1)\)&lt;/span&gt;；然后放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left
&amp;lt;v_1, v_2\right &amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_2.d = \delta(v_2)\)&lt;/span&gt;……如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便按顺序寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ...
v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;
&lt;p&gt;“沿着边走”
有一个专业名词，叫做按&lt;strong&gt;拓扑顺序&lt;/strong&gt;遍历。事实上，我们放缩过边&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},
v_i)\)&lt;/span&gt;之后，并不一定要马上放缩边&lt;span class=&#34;math inline&#34;&gt;\((v_i,
v_{i+1})\)&lt;/span&gt;，只要我们能够保证&lt;span class=&#34;math inline&#34;&gt;\((v_i,
v_{i+1})\)&lt;/span&gt;一定在&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},
v_i)\)&lt;/span&gt;之后放缩即可，至于中间是否穿插其他边的放缩操作，都无所谓。而拓扑顺序正是满足上述性质的一组顺序，为了得到一组拓扑顺序，我们需要对原图进行&lt;strong&gt;拓扑排序&lt;/strong&gt;，然后按照得到的拓扑顺序进行放缩。&lt;/p&gt;
&lt;p&gt;如何进行拓扑排序呢？实际很简单，首先对原图进行&lt;strong&gt;深度优先遍历&lt;/strong&gt;（还有一种基于&lt;strong&gt;入度&lt;/strong&gt;的拓扑排序，此处不表），将完成遍历的点依次插入队列的首部，便可得到按照拓扑顺序排列的一个队列，拓扑顺序的实际意义是，如果边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;存在，那么对点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的访问必须先于对点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的访问。我们每次从队列中取出一个点，并对从其出发的所有边进行放缩操作即可。虽说拓扑顺序是对点的一个排序，但从该点出发的边和这个点是关联的，所以，先访问点，也就相当于先访问从这个点出发的边了，我们先&lt;span class=&#34;math inline&#34;&gt;\(v_{i}\)&lt;/span&gt;访问，也就必然先于&lt;span class=&#34;math inline&#34;&gt;\((v_i,v_{i+1})\)&lt;/span&gt;放缩&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},v_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;对比&#34;&gt;对比&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 9%&#34;/&gt;
&lt;col style=&#34;width: 26%&#34;/&gt;
&lt;col style=&#34;width: 34%&#34;/&gt;
&lt;col style=&#34;width: 29%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;条目&lt;/th&gt;
&lt;th&gt;Bellman-Ford&lt;/th&gt;
&lt;th&gt;Dijkstra&lt;/th&gt;
&lt;th&gt;DAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;复杂度&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|E| \log |V|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| + |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;条件&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;无负权边&lt;/td&gt;
&lt;td&gt;有向无环&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;section class=&#34;footnotes footnotes-end-of-document&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;无向图可以很便捷的转换为带权有向图。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</link>
      <pubDate>Thu, 12 May 2022 11:26:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</guid>
      <description>

&lt;h2 id=&#34;three-perspectives-on-nce&#34;&gt;Three Perspectives on NCE&lt;/h2&gt;
&lt;h3 id=&#34;non-parametric-estimation&#34;&gt;Non-parametric estimation&lt;/h3&gt;
&lt;p&gt;The traditional log-likelihood function will be &lt;span class=&#34;math inline&#34;&gt;\(\ell = \sum_x \ln p_\theta(x)\)&lt;/span&gt;. In NCE, we
learn &lt;span class=&#34;math display&#34;&gt;\[
p_\theta(1|x) = \sigma(G(x;\theta) - \gamma) = \frac{1}{1 +
e^{-G(x;\theta) + \gamma}}
\]&lt;/span&gt; And corresponding loss function becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal {L} &amp;amp;= -\E_{x \sim \tilde p(x)} \ln p_\theta(1|x) - \E_{x
\sim q(x)} \ln p_\theta(0|x) \\
&amp;amp;= -\int \tilde p(x) \ln p_\theta(1|x) dx - \int q(x) \ln
p_\theta(0|x) dx \\
&amp;amp;= - \int [\tilde p(x) + q(x)] [\frac{\tilde p(x)}{\tilde p(x) +
q(x)} \ln p_\theta(1|x) + \frac{q(x)}{\tilde p(x) + q(x)} \ln
p_\theta(0|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P(y|x) = \begin{cases}\frac{\tilde
p(x)}{\tilde p(x) + q(x)}, y=1 \\\frac{q(x)}{\tilde p(x) +
q(x)},y=0\end{cases}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\label{loss} \begin{aligned}
\arg \min_{\theta, \gamma} \mathcal{L} &amp;amp;= \arg \min_{\theta, \gamma}
-\int [\tilde p(x) + q(x)][\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln
p_\theta(1|x) + \int \frac{q(x)}{\tilde p(x) + q(x)} \ln
p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][P(1|x) \ln
p_\theta(1|x) + P(0|x)\ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)][P(1|x) \ln
\frac{1}{p_\theta(1|x)} + P(0|x)\ln \frac{1}{p_\theta(0|x)}]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)]
H[P(y|x)||p_\theta(y|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since cross entropy &lt;span class=&#34;math inline&#34;&gt;\(H(p||q) \ge
H(p)\)&lt;/span&gt; and the minimum is reached only when &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;, the global minimum for equation
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is reached when &lt;span class=&#34;math inline&#34;&gt;\(p_\theta(y|x) = P(y|x)\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_\theta(1|x) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}} &amp;amp;=
\frac{\tilde p(x)}{\tilde p(x) + q(x)} = P(1|x) \\
\frac{q(x)}{\tilde p(x)} &amp;amp;= e^{-G(x;\theta) + \gamma} \\
\tilde p(x) &amp;amp;= \frac{q(x) e^{G(x;\theta)}}{e^\gamma}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; are learnt so that &lt;span class=&#34;math inline&#34;&gt;\(q(x) e^{G(x;\theta) - \gamma}\)&lt;/span&gt; fit the
real distribution. It becomes more intuitive when &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a uniform distribution and &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(e^\gamma\)&lt;/span&gt; will be the learnt normalizing
factor.&lt;/p&gt;
&lt;p&gt;https://kexue.fm/archives/5617/comment-page-1&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation-the-original-papers-view&#34;&gt;Maximum
likelihood estimation (the original paper’s view)&lt;/h3&gt;
&lt;p&gt;The model is defined as &lt;span class=&#34;math inline&#34;&gt;\(\ln p_\theta(x) =
\ln p^0_\alpha(x) + c\)&lt;/span&gt;. The MLE will maximize the objective
function &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J_T(\theta) &amp;amp;= \frac{1}{T_d} [\sum_{i=1}^{T_d \\} \ln h(x_i;\theta)
+ \sum_{i=1}^{T_n} \ln (1 - h(y_i;\theta)) \text{, ($x_i$&amp;#39;s are
samples, $y_i$&amp;#39;s are noises, $T_n = \nu T_d$)} \\
&amp;amp;\stackrel{P}\to J(\theta) \triangleq \E_{x \sim \tilde p} \ln
h(x;\theta) + \nu \E_{x \sim q} \ln (1 - h(x;\theta)) \\
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
r_\nu(u) = \frac{1}{1 + \nu e^{-u}} \\
G(x; \theta) = \ln p_\theta(x) - \ln q(x) \\
h(x;\theta) = r_\nu(G(x;\theta)) = \frac{1}{1 + \nu e^{-G(x;\theta)}} \\
\end{gather}
\]&lt;/span&gt; Denote by &lt;span class=&#34;math inline&#34;&gt;\(\tilde J\)&lt;/span&gt; the
objective function &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; seen as a
function of &lt;span class=&#34;math inline&#34;&gt;\(f(.) = \ln p_\theta(.)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f) = \E_{x \sim \tilde p} \ln r_\nu(f(x) - \ln q(x)) + \nu
\E_{x \sim q} \ln (1 - r_\nu[f(x) - q(x)])
\]&lt;/span&gt; For &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; a perturbation of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f + \epsilon \phi) = \E_{x \sim \tilde p} \ln r_\nu(f(x) +
\epsilon \phi(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) +
\epsilon \phi - q(x)])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By Taylor’s expansion,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln r_\nu(u + \epsilon u_1 + \epsilon^2 u_2) &amp;amp;\approx \ln r_\nu (u)
+ r_{\frac{1}{\nu}}(-u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_\nu
(u)r_\frac{1}{\nu}(-u)}{2}(\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron
\big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln r_\nu (u) + \epsilon u_1r_{\frac{1}{\nu}}(-u) +
\epsilon^2(u_2r_{\frac{1}{\nu}}(-u) - \frac{1}{2}u_1^2
r_{\frac{1}{\nu}}(-u)r_\nu (u)) + \Omicron \big(\epsilon^3 \big) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln \big(1 - r_v(u + \epsilon u_1 + \epsilon^2 u_2) \big) &amp;amp;\approx
\ln \big(1 - r_v(u) \big) - r_v(u)(\epsilon u_1 + \epsilon^2 u_2) -
\frac{r_{\frac{1}{\nu}}(-u) r_\nu(u)}{2} (\epsilon u_1 + \epsilon^2
u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln(1 - r_v(u)) - \epsilon u_1 r_v(u) - \epsilon^2 \big( u_2
r_v(u) + \frac{1}{2} u_1^2 r_{\frac{1}{\nu}}(-u) r_\nu(u) \big) +
\Omicron(\epsilon^3)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde J(f + \epsilon \phi) \approx &amp;amp;\E_{x \sim \tilde p} \ln r_\nu
\big(f(x) + \epsilon \phi(x) - \ln q(x) \big) + \nu \E_{x \sim q} \ln (1
- r_\nu[f(x) + \epsilon \phi - q(x)]) \\
= &amp;amp;\E_{x \sim \tilde p} \{\ln r_\nu \big(f(x) - \ln q(x) \big) +
\epsilon \phi(x) r_{\frac{1}{\nu}} \big(\ln q(x) - f(x) \big) \} \\
&amp;amp;+\nu \E_{x \sim q} \{ \ln \big(1 - r_\nu[f(x) -\ln q(x)] \big) -
\epsilon \phi(x) r_\nu \big( f(x) - \ln q(x) \big) \} +
\Omicron(\epsilon^2) \\
= &amp;amp;\tilde J(f) + \epsilon \int \phi(x) \big(r_\frac{1}{\nu} [\ln
q(x) - f(x)] \tilde p(x) - \nu r_\nu[f(x) - \ln q(x)] q(x) \big) +
\Omicron(\epsilon^2)
\end{aligned}
\]&lt;/span&gt; The above equation attains the local maximum at &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; only if the term of order &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) &amp;amp;= \nu r_\nu[f(x) -
\ln q(x)] q(x) \\
\frac{\nu \tilde p(x)}{\nu + e^{f(x) - \ln q(x)}} &amp;amp;= \frac{\nu
q(x)}{1 + \nu e^{\ln q(x) - f(x)}} \\
\frac{\tilde p(x)q(x)}{\nu q(x) + p_\theta(x)} &amp;amp;= \frac{q(x)
p_\theta(x)}{p_\theta(x) + \nu q(x)} \\
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is nonzero, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\tilde p(x)}{\nu q(x) + p_\theta(x)} =
\frac{p_\theta(x)}{p_\theta(x) + \nu q(x)} \iff p_\theta(x) = \tilde
p(x)\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;https://leimao.github.io/article/Noise-Contrastive-Estimation/&lt;/p&gt;
&lt;h2 id=&#34;externals&#34;&gt;Externals&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/334772391&#34;&gt;NCE与语言模型&lt;/a&gt; ||
&lt;a href=&#34;https://github.com/Stonesjtu/Pytorch-NCE&#34;&gt;PyTorch-NCE&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jacobian Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</guid>
      <description>
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \to \R^m\)&lt;/span&gt;, with
input &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt; and output &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f =
\begin{cases}
y_1 = f_1(x_1, x_2, ..., x_n) \\
y_2 = f_2(x_1, x_2, ..., x_n) \\
... \\
y_m = f_m(x_1, x_2, ..., x_n) \\
\end{cases}
\]&lt;/span&gt; Then Jacobian matrix is &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J &amp;amp;=
\begin{bmatrix}
\frac{\partial f}{\partial x_1} &amp;amp; \frac{\partial f}{\partial x_2}
&amp;amp; \cdots &amp;amp; \frac{\partial f}{\partial x_n} \\
\end{bmatrix} \\
&amp;amp;=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp;amp; \frac{\partial f_m}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \\
\end{bmatrix}
\end{aligned}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear
transformation, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y =
Tx\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
J = T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear
transformation and &lt;span class=&#34;math inline&#34;&gt;\(n = m\)&lt;/span&gt;, i.e.,
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; square matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
dy_1 \\
dy_2 \\
\vdots \\
dy_n \\
\end{bmatrix} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}
\begin{bmatrix}
dx_1 \\
dx_2 \\
\vdots \\
dx_n \\
\end{bmatrix}
\]&lt;/span&gt; That is, &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{
\begin{bmatrix}
dy_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dy_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dy_n
\end{bmatrix}}_A
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix} =
\underbrace{
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}}_J
\underbrace{
\begin{bmatrix}
dx_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dx_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dx_n
\end{bmatrix}}_B
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(JB\)&lt;/span&gt; are both diagonal. From above equation
we can find that &lt;span class=&#34;math inline&#34;&gt;\(A = JB\)&lt;/span&gt;. Therefore,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|A| &amp;amp;= |JB| \\
dy_1 \dots dy_n &amp;amp;= |J|dx_1 \dots dx_n \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spherical Coordinates</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</link>
      <pubDate>Mon, 09 May 2022 20:26:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</guid>
      <description>

&lt;p&gt;The conversion between the 2-d Cartesian coordinate system and the
2-d polar coordinate system can be extended to a higher dimension, say
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d. In &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d case, their conversion can be
described as below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Spherical to Cartesian &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
r &amp;amp;= \sqrt{x_1^2 + \dots + x_k^2} &amp;amp;&amp;amp; \\
\varphi_1 &amp;amp;= \arccot \frac{x_1} {\sqrt{x_k^2 + \dots + x_2^2}}
&amp;amp;&amp;amp;=  \arccos \frac{x_1} {\sqrt{x_k^2 + \dots + x_1^2}} \\
\varphi_2 &amp;amp;= \arccot \frac{x_2} {\sqrt{x_k^2 + \dots + x_3^2}}
&amp;amp;&amp;amp;=  \arccos \frac{x_2} {\sqrt{x_k^2 + \dots + x_2^2}} \\
&amp;amp; \vdots &amp;amp;&amp;amp;\vdots\\
\varphi_{k-2} &amp;amp;= \arccot \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}
&amp;amp;&amp;amp;= \arccos \frac{x_{k-2}} {\sqrt{x_k^2 + x_{k-1}^2 +
x_{k-2}^2}} \\
\varphi_{k-1} &amp;amp;= 2 \arccot \frac{x_{k-1} + \sqrt{x_k^2 +
x_{k-1}^2}}{x_k} &amp;amp;&amp;amp;=
\begin{cases}
\arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n \ge 0 \\
2\pi - \arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n &amp;gt;
0\\
\end{cases}
\end{alignat}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cartesian to spherical &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_1 &amp;amp;= r \cos(\varphi_1) \\
x_2 &amp;amp;= r \sin(\varphi_1) \cos(\varphi_2) \\
\notag &amp;amp;\vdots \\
x_{k-1} &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\cos(\varphi_{k-1}) \\
x_k &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\sin(\varphi_{k-1}) \\
\end{align}
\]&lt;/span&gt; The corresponding &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian
Matrix&lt;/a&gt; is &lt;span class=&#34;math display&#34;&gt;\[
J^{(k)} = \left[ \begin{array}{ccccc|c}
\cos (\varphi_1) &amp;amp; -r \sin(\varphi_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots
&amp;amp; 0 \\
\sin(\varphi_1) \cos(\varphi_2) &amp;amp; r \cos(\varphi_1) \cos(\varphi_2)
&amp;amp; -r \sin(\varphi_1) \sin(\varphi_2) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ &amp;amp; \ddots &amp;amp; \vdots \\
\hline
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) &amp;amp;
\cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; -r \sin(\varphi_1) \dots
\sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) &amp;amp;
\cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; r \sin(\varphi_1) \dots
\sin(\varphi_{k-2}) \cos(\varphi_{k-1})
\end{array} \right]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|J^{(2)}|\)&lt;/span&gt; can be easily
derived as &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;; and &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k)}|\)&lt;/span&gt; can be constructed from &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k-1)}|\)&lt;/span&gt;. Comparing &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt;,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra column
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and an extra row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\cos(\varphi_{k-1})\)&lt;/span&gt; term in each element
than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on
row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\sin(\varphi_{k-1})\)&lt;/span&gt; term in each element
than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on
row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; are totally the same on the
region delimited by row &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, row
&lt;span class=&#34;math inline&#34;&gt;\(k-2\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apply the Laplace expansion along column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and combine the property of determinant
to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J^{(k)}| =\ &amp;amp; \underbrace{0 + \dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1})
\sin(\varphi_{k-1}) \big( \sin(\varphi_{k-1}) |J^{(k-1)}| \big)] + \\
&amp;amp; (-1)^{n+n} [r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\cos(\varphi_{k-1}) \big( \cos(\varphi_{k-1}) |J^{(k-1)}| \big)] \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \big(
\sin_{\varphi_{k-1}}^2 + \cos{\varphi_{k-1}}^2 \big) |J^{(k-1)}| \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) |J^{(k-1)}| \\
\end{aligned}
\]&lt;/span&gt; By induction, &lt;span class=&#34;math display&#34;&gt;\[
|J^{(k)}| = r^{k-1} \sin^{k-2}(\varphi_1) \sin^{k-3}(\varphi_2) \dots
\sin(\varphi_{k-2})
\]&lt;/span&gt; Therefore when changing basis from orthogonal coordinate
system to polar coordinate system, &lt;span class=&#34;math display&#34;&gt;\[
\d x_1 \dots \d x_k = r^{k-1} \sin^{k-2}(\varphi_1) \sin^{k-3}
(\varphi_2) \dots \sin(\varphi_{k-2}) \d r \d \varphi_1 \dots \d
\varphi_{k-1}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates&#34;&gt;Wiki&lt;/a&gt;
|| &lt;a href=&#34;https://wuli.wiki//online/SphCar.html&#34;&gt;3d Case Blog 1&lt;/a&gt; ||
&lt;a href=&#34;https://www.cnblogs.com/hans_gis/archive/2012/11/21/2755126.html&#34;&gt;3d
Case Blog 2&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;volume-of-sphere&#34;&gt;Volume of Sphere&lt;/h3&gt;
&lt;p&gt;When changing basis between spherical coordinate and Cartesian, we
may need to compute the volume of sphere of radius &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimension. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_n &amp;amp;= \int_{B_n} 1 \; \d x_1 \d x_2 \dots \d x_n \\
&amp;amp;= \int_{0}^{R} \int_{0}^{2 \pi}
\underbrace{\int_{0}^{\pi} \dots \int_{0}^{\pi}}_{n-2} \\
&amp;amp;\quad r^{n-1} \sin(\varphi_1)^{n-2} \sin(\varphi_2)^{n-3} \dots
\sin(\varphi_{n-2}) \d r \d \varphi_1 \dots \d \varphi_{n-1} \\
&amp;amp;= \int_{0}^{R} r^{n-1} \d r
\int_{0}^{2 \pi} \d \varphi_{n-1} \\
&amp;amp;\quad
\int_{0}^{\pi} \sin(\varphi_{n-2}) \d \varphi_{n-2}
\int_{0}^{\pi} \sin^2(\varphi_{n-3}) \d \varphi_{n-3}
\dots
\int_{0}^{\pi} \sin^{n-2}(\varphi_{1}) \d \varphi_{1} \\
\end{aligned}
\]&lt;/span&gt; Notice that &lt;span class=&#34;math display&#34;&gt;\[
\int_{0}^{\pi} \sin^{n}(x) \d x = \sqrt{\pi} \frac{\Gamma
(\frac{n-1}{2})}{\Gamma (\frac{n}{2})}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_n &amp;amp;= \int_{0}^{R} r^{n-1} \d r
\int_{0}^{2 \pi} \d \varphi_{n-1} \\
&amp;amp;\quad
\int_{0}^{\pi} \sin(\varphi_{n-2}) \d \varphi_{n-2}
\int_{0}^{\pi} \sin^2(\varphi_{n-3}) \d \varphi_{n-3}
\dots
\int_{0}^{\pi} \sin^{n-2}(\varphi_{1}) \d \varphi_{1} \\
&amp;amp;= \frac{R^n}{n} \cdot 2\pi \cdot \sqrt{\pi}
\frac{\Gamma(0)}{\Gamma(1/2)} \cdot \sqrt{\pi}
\frac{\Gamma(1/2)}{\Gamma(2/2)} \cdots \sqrt{\pi}
\frac{\Gamma((n-3)/2)}{\Gamma((n-2)/2)} \\
&amp;amp;= R^n \frac{1}{n/2} \sqrt{\pi^n} \frac{1}{\Gamma((n-2)/2)} \\
&amp;amp;= \frac{R^n \sqrt{\pi^n}}{\Gamma(n/2)}
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1v8411J7WQ/&#34;&gt;《三体》中的数学——为什么很高维度的单位球体积为0？_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</guid>
      <description>

&lt;h2 id=&#34;continuous-time-fourier-transform&#34;&gt;Continuous-time Fourier
Transform&lt;/h2&gt;
&lt;h3 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h3&gt;
&lt;p&gt;In Euclidean space, we usually represent a vector by a set of
independent and orthogonal base vectors (basis). Orthogonality means the
inner product between two basis is zero. Inner product can also be
defined on some common interval between two functions, and thus the
orthogonality.&lt;/p&gt;
&lt;h4 id=&#34;frequency-domain&#34;&gt;Frequency Domain&lt;/h4&gt;
&lt;p&gt;It is intuitive to model after the inner product between vectors.
Function (signal) on its domain can be viewed as an “infinite-dimension”
vector. We represent this infinity in the definition of function inner
product by integration. In particular, given two functions &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, an interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[
\int\limits_{x \in I}s(x)g(x)dx
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are orthogonal on interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; if their inner product &lt;span class=&#34;math inline&#34;&gt;\(\int_{x \in I}s(x)g(x)dx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set of basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt; Euclidean
space contains &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent
orthogonal basis. For an “infinite-dimension” function space, there are
an infinite number of basis, among which a group of sine and cosine
functions satisfy. For integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;
and positive integers &lt;span class=&#34;math inline&#34;&gt;\(m, n\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx = 0, m, n \ge 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m = n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \cos^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 - \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 + \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin(nx) \cos(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin(2nx)}{2}dx \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m \ne n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) + \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) - \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} - \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\sin((n+m)x) + \sin((n-m)x)}{2}dx \\
&amp;amp;= -\frac{\frac{\cos((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\cos((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, we can use the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(x), \sin(x), \cos(2x), \sin(2x),
\dots\)&lt;/span&gt; to fit any &lt;strong&gt;continuous function&lt;/strong&gt; on
interval &lt;span class=&#34;math inline&#34;&gt;\([-\pi, \pi]\)&lt;/span&gt;. Or use &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(2\pi fx), \sin(2\pi fx), \cos(2\pi f2x),
\sin(2\pi f2x), \dots\)&lt;/span&gt; to fit any function on interval &lt;span class=&#34;math inline&#34;&gt;\([\frac{-1}{2f} + \frac{k}{f}, \frac{1}{2f} +
\frac{k}{f}]\)&lt;/span&gt;, which can be any interval by choosing the value
of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (frequency) and the integer
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;
approximated with such series up to level &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; can be written as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{split}
s_N(x) &amp;amp;= a_0 + \sum_{i=1}^N \big(
\underbrace{a_n}_{A_n\sin(\varphi_n)} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\cos(\varphi_n)} \sin(2\pi fnx) \big) \\
&amp;amp;= a_0 + \sum_{n=1}^N \bigg( A_n\sin(2\pi fnx + \varphi_n) \bigg)
\text{, where} \\
\end{split} \\
\notag &amp;amp;A_n = \sqrt{a_n^2 + b_n^2}, \sin(\varphi_n) =
\frac{a_n}{\sqrt{a_n^2 + b_n^2}}, \cos(\varphi_n) =
\frac{b_n}{\sqrt{a_n^2 + b_n^2}}
\end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A_n\)&lt;/span&gt; can be interpreted as
the amplitude, &lt;span class=&#34;math inline&#34;&gt;\(\varphi_n\)&lt;/span&gt; as the
phase, &lt;span class=&#34;math inline&#34;&gt;\(nf\)&lt;/span&gt; as the frequency.&lt;/p&gt;
&lt;h4 id=&#34;complex-frequency-domain&#34;&gt;Complex Frequency Domain&lt;/h4&gt;
&lt;p&gt;By Euler’s Formula we have &lt;span class=&#34;math display&#34;&gt;\[
e^{ix} = \cos x + i\sin x
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(s_N(x)\)&lt;/span&gt; can be
re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
s_N(x) &amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg(
\underbrace{a_n}_{A_n\cos \phi_n} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\sin \phi_n} \sin(2\pi fnx) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( A_n\cos(2\pi fnx - \phi_n)
\bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) + i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) - i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;\Downarrow_\text{by multiplication rule between complex
numbers in polar form}  \\
&amp;amp;= &amp;amp;&amp;amp; \sum_{n=-N}^N c_ne^{i 2\pi fnx} \\
\end{alignat}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
c_n &amp;amp;=
\begin{cases}
\frac{A_n}{2}(\cos \phi_n - i\sin \phi_n) = \frac{1}{2}(a_n - ib_n),
&amp;amp;n &amp;gt; 0 \\
\overline{c_{|n|}} =\frac{A_n}{2}(\cos \phi_n + i\sin \phi_n), &amp;amp;n
&amp;lt; 0 \\
a_0, &amp;amp;n = 0
\end{cases}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(N \to +\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; can be reconstructed as the Fourier
Series: &lt;span class=&#34;math display&#34;&gt;\[
s(x) = \lim_{N \to +\infty} s_N(x) = a_0 + \sum_{n=1}^{+\infty} \bigg(
a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg) \\
\]&lt;/span&gt; The problem comes how &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt;
can be computed. By the orthogonality mentioned before, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0
+ \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx)
\bigg)] \d x = \frac{1}{f} a_0 \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \cos(2\pi fkx) \d x =
\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \cos (2\pi fkx) \d x =
\frac{1}{2f} a_k \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \sin(2\pi fkx) \d x
=\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \sin (2\pi fkx) \d x =
\frac{1}{2f} b_k \\
\end{gather}
\]&lt;/span&gt; The computation of &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; can be combined together by
the Euler’s Formula: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) e^{-i 2\pi fkx} \d x &amp;amp; =
\int_{-1/2f+f/k}^{1/2f+f/k} (a_k \cos(2\pi fkx) + b_k \sin(2\pi fkx))
(\cos(2\pi fkx) - i \sin(2\pi fkx)) \d x \\
&amp;amp;= \frac{1}{2f} (a_k - i b_k) = \frac{1}{f} c_k
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;fourier-transform&#34;&gt;Fourier Transform&lt;/h3&gt;
&lt;p&gt;We have been through representing a continuous function on a certain
interval using the Fourier Series. This can be quite useful for periodic
functions. As long as we figure out the representation on its repeating
interval, we obtain the representation on its whole domain. The problem
is more of computing the factor for each sine and cosine function.&lt;/p&gt;
&lt;p&gt;The process of finding out factors for an arbitrary continuous
function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is called Fourier
Transform. It transforms the function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; from the time domain to the frequency
domain. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is periodic, it can
be easily represented by the Fourier Series as discussed in previous
section. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is not periodic, we
can treat the periodic interval as &lt;span class=&#34;math inline&#34;&gt;\([-\infty,
+\infty]\)&lt;/span&gt;. Its Fourier Transform and the inverse will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\hat s \stackrel{\mathcal F}\Longleftrightarrow s \\
\hat s(f) = \int_{-\infty}^{+\infty} s(t) e^{-i 2\pi f x} \d t \\
s(x) = \int_{-\infty}^{+\infty} \hat s(f) e^{i 2\pi f x} \d f
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;discrete-time-fourier-transform&#34;&gt;Discrete-time Fourier
Transform&lt;/h2&gt;
&lt;p&gt;The domain (time axis) of the function (signal) is continuous in our
discussion by now. When the time axis is discrete (and usually takes on
a series of integers), we are facing the Discrete-time Fourier
Transform. We will be using the term &lt;strong&gt;signal&lt;/strong&gt; instead of
the function from now on.&lt;/p&gt;
&lt;p&gt;For a discrete signal &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, its
Fourier Transform is &lt;span class=&#34;math display&#34;&gt;\[
\hat s(\omega) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i\omega k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the
angular speed. &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is in the
unit of radian/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat
s(\omega)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; between two signal samples. &lt;span class=&#34;math display&#34;&gt;\[
\hat s(f) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i 2\pi f k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the
“frequency”. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the unit of
cycles/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(f)\)&lt;/span&gt; is the
weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; cycles between two signal samples.&lt;/p&gt;
&lt;p&gt;Todo&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Laplace Transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;-transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Fast Fourier Transform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jezzamon.com/fourier/zh-cn.html&#34;&gt;傅里叶变换交互式入门
(jezzamon.com)&lt;/a&gt; || &lt;a href=&#34;https://charlesliuyx.github.io/2018/02/18/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E8%AE%A9%E4%BD%A0%E6%B0%B8%E8%BF%9C%E5%BF%98%E4%B8%8D%E4%BA%86%E7%9A%84%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E8%A7%A3%E6%9E%90/&#34;&gt;傅立叶变换与群&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/1119070975&#34;&gt;如何理解傅里叶变换公式？
- 苗华栋的回答 - 知乎&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/334686351&#34;&gt;如何理解傅里叶变换公式？
- 马同学的回答 - 知乎&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Contrastive Predictive Coding</title>
      <link>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</link>
      <pubDate>Fri, 29 Apr 2022 11:32:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</guid>
      <description>

&lt;h2 id=&#34;rationale&#34;&gt;Rationale&lt;/h2&gt;
&lt;p&gt;CPC was initially proposed in autoregressive models. It enhances the
autoencoder by lifting the lower bound of mutual information between the
encoded representation and the original data. The original data can
either be the data before the encoding, or the future data after various
steps.&lt;/p&gt;
&lt;p&gt;CPC learns the representation by minimizing the following loss
function: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\c}{\mathrm{c}} \label{loss}
\mathcal L_N = -\E_{t \sim \Phi} \log \frac{f_\theta(\x_l,\c)}
{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} =
-\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(t = (\x_1, \dots, \x_{N+1}, \c^*;
\ell)\)&lt;/span&gt; is a tuple of random variables and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the distribution from which &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is drawn. &lt;span class=&#34;math inline&#34;&gt;\((\x:\c)_{1:N+1}\)&lt;/span&gt; are drawn from the joint
distribution &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(\x,\c)\)&lt;/span&gt;. All
&lt;span class=&#34;math inline&#34;&gt;\(\c_i\)&lt;/span&gt;’s but one randomly-chosen
&lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; are trimmed from the original
samples. &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; is known but it is
unknown which sample it is associated with. &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; denotes the index of this unique
sample we are trying to predict. In essence, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is parameterizing the
representation &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. The formal score
function &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt; is simply chosen
to be a deterministic cosine similarity between &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\x_{1:N+1}\)&lt;/span&gt; consists of one
positive sample &lt;span class=&#34;math inline&#34;&gt;\(\x^*\)&lt;/span&gt; that is
matched with &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; and more other
independent negative (noise) samples &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt;’s that are not matched with &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed ratio of the number of
negative samples to the number of positive samples. Let &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i|\x_{1:N+1},\c^*)\)&lt;/span&gt; represent the
probability that &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt; is the
positive sample given &lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots
x_{N+1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\ell=i,
\x_{1:N+1},\c^*)}{\sum_{j=1}^{N+1} P(\ell=j, \x_{1:N+1},\c^*)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i,\x_{1:N+1},\c^*) =
P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\x_i,\c^*) \prod_{j=1,j \ne
i}^{N+1} P(\x_j)}{\sum_{j=1}^{N+1} [P(\x_j,\c^*) \prod_{k=1,k \ne
j}^{N+1} P(\x_k)]} \\
&amp;amp;= \frac{\tilde p(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} \tilde
p_X(\x_j)} {\sum_{j=1}^{N+1} [\tilde p(\x_j,\c^*) \prod_{k=1,k \ne
j}^{N+1} \tilde p_X(\x_j)]} \\
&amp;amp;= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} }
{\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} \\
\end{aligned}
\]&lt;/span&gt; The loss function &lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is in fact the expectation
(the outer &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of the categorical
cross entropy (the inner &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of
identifying the sample as positive or negative. The minimum of loss
function is thus reached when the two categorical distributions are
identical. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_\theta(l = i|x_{1:N+1},\c^*) =
\frac{f_\theta(\x_i,\c)}{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1}
\frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} =
P(\ell=i|\x_{1:N+1},\c^*) \\
f_\theta(\x_i,\c) = \frac{\sum_{\x^\prime \in X}f_\theta(\x^\prime,
\c)}{\sum_{\x^\prime \in X} \frac{\tilde p(\x^\prime|\c)}{\tilde
p_X(\x^\prime)}} \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} \\
f_\theta(\x_i,\c) \propto \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;bounding-the-mutual-information&#34;&gt;Bounding the Mutual
Information&lt;/h2&gt;
&lt;p&gt;CPC helps estimate the lower bound of the mutual information between
the encoded representation and the original data when optimizing the
InfoNCE loss: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\mathcal L_N^{\text{opt}} = -\E_{p(\x_{1:N+1},\c^*)}
\E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde
p_X(\x_l)}} {\sum_{\x&amp;#39; \in X} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde
p_X(\x&amp;#39;)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)} + \sum_{\x&amp;#39; \in
X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}}
{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 +
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} \sum_{\x&amp;#39; \in X,
\x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}
\big) \\
&amp;amp;\approx \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1)
\E_{\tilde p_X(\x&amp;#39;)} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde
p_X(\x&amp;#39;)} \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 +
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;\ge \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big(
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} [\log
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)}] + \log (N - 1) \\
&amp;amp;= -I(\x;\c^*) + \log (N - 1)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(\x;\c^\star) \ge
\log(N-1) - \mathcal L^{\mathrm{opt}}_{N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;externals&#34;&gt;Externals&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://anilkeshwani.github.io/CPC/&#34;&gt;Paper Review&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=zNKMHj1eLa0&#34;&gt;CPC Formulation&lt;/a&gt;
|| &lt;a href=&#34;https://jxmo.io/posts/nce&#34;&gt;NCE and InfoNCE&lt;/a&gt; || &lt;a href=&#34;https://colab.research.google.com/github/google-research/google-research/blob/master/vbmi/vbmi_demo.ipynb#scrollTo=DUd0hFZ5Js8k&#34;&gt;Demo
of Bounding Mutual Information&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Conditional Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</link>
      <pubDate>Wed, 13 Apr 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</guid>
      <description>
&lt;p&gt;The &lt;strong&gt;conditional entropy&lt;/strong&gt; measures the the amount of
information needed to describe the outcome of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that the value of another random
variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is known. The entropy of
&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(Y|X) = -\sum_{(x,y) \in \mathcal{X} \times \mathcal{Y}} p_{(X,Y)}(x,y)
\log \frac{p_{(X,Y)}(x,y)}{p_X(x)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lipschitz Continuity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</link>
      <pubDate>Mon, 31 Jan 2022 00:02:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</guid>
      <description>

&lt;p&gt;For a continuous mapping &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, it
is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous if there
exists a number &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in \dom(f)\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
||f(x) - f(y)|| \le K||x - y||
\]&lt;/span&gt; If the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous, we further
say &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-smooth.&lt;/p&gt;
&lt;h3 id=&#34;lipschitz-constant&#34;&gt;Lipschitz Constant&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the minimum number to
make the above condition hold, then &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is called the &lt;strong&gt;Lipschitz
constant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Lipschitz constant for a general differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will be the maximum spectral norm of
its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt; over its
domain. &lt;span class=&#34;math display&#34;&gt;\[
||f||_{Lip} = \sup_x \sigma[\nabla f(x)] = \sup_x \sup_{||v||=1} \nabla
f(x) \cdot v
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma[\nabla f(x)]\)&lt;/span&gt;
denotes the spectral norm of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s
gradient at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Lipschitz constant for a matrix transformation will be the
matrix’s &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The Lipschitz constant for a &lt;span class=&#34;math inline&#34;&gt;\(\R \mapsto
\R\)&lt;/span&gt; function will be its largest &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;subgradient&lt;/a&gt; over its domain&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;composition-of-functions&#34;&gt;Composition of Functions&lt;/h3&gt;
&lt;p&gt;Suppose two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are Lipschitz continuous
respectively. Then, &lt;span class=&#34;math display&#34;&gt;\[
\nabla (f \circ g)(x) = \nabla f [g(x)] \nabla g(x)
\]&lt;/span&gt; by the chain rule of derivatives. &lt;span class=&#34;math inline&#34;&gt;\(f \circ g\)&lt;/span&gt;’s Lipschitz constant will be
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sigma(\nabla (f \circ g)(x)) &amp;amp;= \sup_{||v|| = 1} ||\{\nabla f[g(x)]
\nabla g(x)\} v|| \\
&amp;amp;=  \sup_{||v|| = 1} \{||\nabla g(x) v||\} \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;\le  \sup_{||v|| = 1} \sigma[\nabla g(x)] \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \sup_{||v|| = 1} \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \cdot \sigma\{\nabla f[g(x)]\}
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(||f \circ
g||_{Lip} = ||f||_{Lip} \cdot ||g||_{Lip}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/&#34;&gt;Lipschitz
Continuity, convexity, subgradients – Marco Tulio Ribeiro –
(washington.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/Lipschitz-gradient&#34;&gt;Lipschitz
continuous gradient · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spectral Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/</link>
      <pubDate>Sat, 29 Jan 2022 21:23:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/</guid>
      <description>
&lt;p&gt;Spectral normalization of an &lt;span class=&#34;math inline&#34;&gt;\(M \times
N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
||A||_2 = \max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} =
\sqrt{\lambda_{\max}(A^TA)} = \sigma_{\max}(A)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\rm z \in \R^N\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\max}(A^TA)\)&lt;/span&gt; is the maximum
eigenvalue of matrix &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;, which is
exactly &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s largest singular value
&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\max}(A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To prove it, firstly note that: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} \iff
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; We may force a constraint on &lt;span class=&#34;math inline&#34;&gt;\(\mathrm z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(||\mathrm z||^2_2 = 1\)&lt;/span&gt;. This is because
&lt;span class=&#34;math display&#34;&gt;\[
\frac{||Ac\mathrm z||^2_2}{||c\mathrm z||^2_2} = \frac{c^2||A\mathrm
z||^2_2}{c^2||\mathrm z||^2_2} = \frac{||A\mathrm z||^2_2}{||\mathrm
z||^2_2}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2} =
||A\mathrm z||^2_2 = \mathrm z^TA^TA\mathrm z \\
s.t. ||\mathrm z||^2_2 = 1
\end{gather}
\]&lt;/span&gt; This can be solved by Lagrange multiplier, where the
Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
L(\mathrm z, \lambda) = \mathrm z^TA^TA\mathrm z + \lambda(||\mathrm
z||^2_2 - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extrapolation of the spectral normalization will be related to
Rayleigh quotient.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://math.stackexchange.com/questions/2723629/why-is-the-maximum-rayleigh-quotient-equal-to-the-maximum-eigenvalue&#34;&gt;matrices
- Why is the maximum Rayleigh quotient equal to the maximum eigenvalue?
- Mathematics Stack Exchange&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>1-exp-family</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/1-exp-family/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/1-exp-family/</guid>
      <description>

&lt;h2 id=&#34;exponential-family-basics&#34;&gt;Exponential Family Basics&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note]&lt;/p&gt;
&lt;p&gt;Though the results below are mainly with regards to the finite
support case, they also apply to infinite support case after switching
to differential entropy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[!important]&lt;/p&gt;
&lt;p&gt;In fact, we are restricting the discussion to the natural exponential
family.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-and-definition&#34;&gt;Example and Definition&lt;/h3&gt;
&lt;p&gt;Recall the PDF of univariate Gaussian distribution: &lt;span class=&#34;math display&#34;&gt;\[
p_{\mathcal{N}(\mu, \sigma^2)}(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\]&lt;/span&gt; It can be rewritten as &lt;span class=&#34;math display&#34;&gt;\[
p_{\mathcal{N}(\mu, \sigma^2)}(x) = \exp(-\frac{1}{2 \sigma^2} x^2 +
\frac{\mu}{\sigma^2} x - \frac{\mu}{2 \sigma^2} - \frac{1}{2} \log(2 \pi
\sigma^2))
\]&lt;/span&gt; Let the mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi(x) = [x^2,
x]\)&lt;/span&gt; and the parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\theta
= [-\frac{1}{2 \sigma^2}, \frac{\mu}{\sigma^2}]\)&lt;/span&gt;. Then for a
properly-chosen function &lt;span class=&#34;math inline&#34;&gt;\(A: \R^2 \mapsto
\R\)&lt;/span&gt; we have &lt;span class=&#34;math display&#34;&gt;\[
p_{\mathcal{N}(\mu, \sigma^2)}(x) = \exp(\theta^T \phi(x) - A(\theta))
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
p_{\mathcal{N}(\mu, \sigma^2)}(x) \propto \exp(\theta^T \phi(x))
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;exponential family&lt;/strong&gt;. Given a feature
function &lt;span class=&#34;math inline&#34;&gt;\(\phi: \mathcal{X} \mapsto
\R^m\)&lt;/span&gt; and an &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional
parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \R^m\)&lt;/span&gt;,
an exponential family is defined as the set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P} \triangleq \{ p_\theta: \theta \in
\R^m \}\)&lt;/span&gt; where for a function &lt;span class=&#34;math inline&#34;&gt;\(A:
\R^m \mapsto \R\)&lt;/span&gt; the mass (or the density) function &lt;span class=&#34;math inline&#34;&gt;\(p_\theta\)&lt;/span&gt; is defined as: &lt;span class=&#34;math display&#34;&gt;\[
p_\theta(x) = \exp(\theta^T \phi(x) - A(\theta))
\]&lt;/span&gt; In the above definition,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we call &lt;span class=&#34;math inline&#34;&gt;\(\phi: \R^d \mapsto \R^m\)&lt;/span&gt;
the &lt;strong&gt;feature function&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;we call &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \R^m\)&lt;/span&gt; the
&lt;strong&gt;canonical parameters&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;we call &lt;span class=&#34;math inline&#34;&gt;\(A: \R^m \mapsto \R\)&lt;/span&gt; the
&lt;strong&gt;log-partition function&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The underlying max-entropy assumption of exponential family seems to
be the reason why we usually use softmax in the last layer in the neural
network model.&lt;/p&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;h4 id=&#34;convexity&#34;&gt;Convexity&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;derivation of log-partition function&lt;/strong&gt; in
discrete case. Given an exponential family with feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi: \mathcal{X} \mapsto \R^m\)&lt;/span&gt; over a
&lt;em&gt;finite support set&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;, canonical parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \R^m\)&lt;/span&gt;, the log partition
function &lt;span class=&#34;math inline&#34;&gt;\(A: \R^m \mapsto \R\)&lt;/span&gt; can be
determined as &lt;span class=&#34;math display&#34;&gt;\[
A(\theta) = \log \left( \sum_{x \in \mathcal{X}} e^{\theta^T \phi(x)}
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;derivatives of log-partition function&lt;/strong&gt;.
Consider an exponential family with feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi: \mathcal{X} \mapsto \R^m\)&lt;/span&gt; over a
&lt;em&gt;finite support set&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;, canonical parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \R^m\)&lt;/span&gt;, and the log partition
function &lt;span class=&#34;math inline&#34;&gt;\(A: \R^m \mapsto \R\)&lt;/span&gt; . Then,
the first- and second-order derivatives of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; satisfy:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;gradient&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; will be the mean of random vector &lt;span class=&#34;math inline&#34;&gt;\(\phi(X)\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\nabla A(\theta) = \E_{X \sim p_\theta} [\phi(x)]
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;Hessian&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; will be the covariance matrix of random
vector &lt;span class=&#34;math inline&#34;&gt;\(\phi(X)\)&lt;/span&gt;:?? &lt;span class=&#34;math display&#34;&gt;\[
\nabla^2 A(\theta) = \Cov_{X \sim p_\theta}(\phi(X))
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;convexity of log-partition function&lt;/strong&gt;. The
log-partition function of an exponential family distribution is a convex
function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;monotone gradient of convex function&lt;/strong&gt;. A
differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^d \mapsto
\R\)&lt;/span&gt; is convex if and only if its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f: \R^d \mapsto \R^d\)&lt;/span&gt; is monotone,
i.e. &lt;span class=&#34;math display&#34;&gt;\[
\forall x, y \in \R^d, (x-y)^T(\nabla f(x) - \nabla f(y)) \ge 0
\]&lt;/span&gt; Corollary: &lt;strong&gt;monotone gradient of mean vector&lt;/strong&gt;.
The mean vector &lt;span class=&#34;math inline&#34;&gt;\(\mu_\theta = \E_{X \sim
p_\theta} [\phi(X)]\)&lt;/span&gt; of an exponential family model &lt;span class=&#34;math inline&#34;&gt;\(p_\theta\)&lt;/span&gt; is a monotone function of the
canonical parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\forall \theta_1, \theta_2 \in \R^d, (\theta_1 - \theta_2)^T (\nabla
A(\theta_1) - \nabla A(\theta_2)) \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;strong-convexity&#34;&gt;Strong Convexity&lt;/h4&gt;
&lt;p&gt;Convexity is not enough for fruitful derivation. Below we introduce
the idea of strong convexity.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;strongly convex function&lt;/strong&gt;. We call a
twice-differentiable &lt;span class=&#34;math inline&#34;&gt;\(f: \R^d \mapsto
\R\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;-strongly-convex if
the eigenvalues of its Hessian are always greater than or equal to &lt;span class=&#34;math inline&#34;&gt;\(\mu &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;invertibility of the gradient of strongly convex
function&lt;/strong&gt;. The gradient of a &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;-strongly-convex function is an
invertible mapping and satisfies the following: &lt;span class=&#34;math display&#34;&gt;\[
\forall x, y \in \R^d, (\nabla f(x) - \nabla f(y))^T (x-y) \ge \mu
\|x-y\|_2^2
\]&lt;/span&gt; The implication is that, it is impossible for &lt;span class=&#34;math inline&#34;&gt;\(x \ne y\)&lt;/span&gt; to have the same gradient,
verifying that &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt; is
invertible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;estimation-of-exponential-family&#34;&gt;Estimation of Exponential
Family&lt;/h2&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood
Estimation&lt;/h3&gt;
&lt;p&gt;In this section, we assume a strong convexity on the log-partition
function. That is, we can relate the canonical parameter and the mean
vector: &lt;span class=&#34;math display&#34;&gt;\[
\mu = \nabla A(\theta), \theta = (\nabla A)^{-1}(\mu)
\]&lt;/span&gt; Suppose we observe independent and identically distributed
samples &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_n \sim
p_\theta\)&lt;/span&gt;. How do we estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;? The most natural estimation is
the maximum (log-)likelihood estimation. We try to derive it for
exponential family in the following. &lt;span class=&#34;math display&#34;&gt;\[
\theta^\text{MLE} = \arg \max_\theta l(\theta) = \log \left[
\prod_{i=1}^n p_\theta(x_i) \right]
\]&lt;/span&gt; MLE is a good fit for the exponential family because of the
&lt;span class=&#34;math inline&#34;&gt;\(\exp\)&lt;/span&gt; term in the density function.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\theta^\text{MLE} = \arg \max_\theta \sum_{i=1}^n \log
p_\theta(x_i) \\
&amp;amp;= \arg \max_\theta \sum_{i=1}^n [\theta^T \phi(x_i) - A(\theta)] \\
&amp;amp;= \arg \max_\theta \frac{1}{n} \sum_{i=1}^n [\theta^T \phi(x_i) -
A(\theta)] \\
&amp;amp;= \arg \max_\theta \theta^T \underbrace{\left[ \frac{1}{n}
\sum_{i=1}^n \phi(x_i) \right]}_{\hat \mu} - A(\theta) \\
&amp;amp;= \arg \min_\theta A(\theta) - \theta^T \hat \mu
\end{aligned}
\]&lt;/span&gt; It turns out to be a standard unconstrained convex
optimization problem. The optimal solution would be to zero out the
gradient: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla A(\theta^\text{MLE}) - \hat \mu &amp;amp;= 0 \\
\theta^\text{MLE} &amp;amp;= (\nabla A)^{-1} (\hat \mu)
\end{aligned}
\]&lt;/span&gt; As a result, the resulting &lt;span class=&#34;math inline&#34;&gt;\(\mu^\text{MLE} \triangleq \E_{\theta^\text{MLE}}
[\phi(X)] = \nabla A(\theta^\text{MLE})\)&lt;/span&gt; will be exactly the
empirical mean &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu\)&lt;/span&gt;. Suppose
&lt;span class=&#34;math inline&#34;&gt;\(\phi(X)\)&lt;/span&gt; has the covariance matrix
&lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. By the central limit
theorem, &lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n} (\hat u - \E[\phi(X)]) \stackrel{d}{\to} N(0, \Sigma)
\]&lt;/span&gt; But what we are interested in is the asymptotic performance of
the estimation of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which is
&lt;span class=&#34;math inline&#34;&gt;\(\theta^\text{MLE}\)&lt;/span&gt;. By the &lt;a href=&#34;https://www.wikiwand.com/en/Delta_method&#34;&gt;delta method&lt;/a&gt;, for
&lt;span class=&#34;math inline&#34;&gt;\(\theta^\text{MLE}\)&lt;/span&gt; which is a
function of &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n} (\theta^\text{MLE} - \theta) \stackrel{d}{\to} N(0,
\Sigma^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;method-of-moments&#34;&gt;Method of Moments&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;method of moments&lt;/strong&gt;. Given a parameterized
family of distributions &lt;span class=&#34;math inline&#34;&gt;\(\{ p_\theta: \theta
\in \R^d \}\)&lt;/span&gt;, the method of moments estimator finds the
parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; that matches
the &lt;strong&gt;population moments&lt;/strong&gt; with the &lt;strong&gt;empirical
moments&lt;/strong&gt; of i.i.d. samples &lt;span class=&#34;math inline&#34;&gt;\(x_1,
\dots, x_n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; be
the moment function. Hence, &lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt; must satisfy &lt;span class=&#34;math display&#34;&gt;\[
\E_{\hat \theta} [\phi(X)] = \frac{1}{n} \sum_{i=1}^n \phi(x_i)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;equivalence of method of moments and maximum
likelihood estimation&lt;/strong&gt;. Given an exponential family &lt;span class=&#34;math inline&#34;&gt;\(\{ p_\theta: \theta \in \R^d \}\)&lt;/span&gt; with
feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, the method
of moments estimator with &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-based moments&lt;/strong&gt; results in
the same estimator as maximum likelihood estimator &lt;span class=&#34;math inline&#34;&gt;\(\theta^\text{MLE}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof. Recall that we have shown &lt;span class=&#34;math inline&#34;&gt;\(\mu^\text{MLE} = \hat \mu\)&lt;/span&gt; in the previous
note. Additionally, due to the invertibility of &lt;span class=&#34;math inline&#34;&gt;\(\nabla A(\hat \theta) = \E_{\hat \theta}
[\phi(X)]\)&lt;/span&gt; (thanks to Yiyao :D), the solution to method of
moments is unique. As a result, the two methods are equivalent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;principle-of-maximum-entropy&#34;&gt;Principle of Maximum Entropy&lt;/h2&gt;
&lt;p&gt;The above develops the method of moments for the exponential family,
making it a model-based approach. What if the distribution is not coming
from an exponential family? In such case, we follow the principle of
maximum entropy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;principle of maximum entropy&lt;/strong&gt;. Given a
set of probability distributions &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;
(e.g. using the method of moments), conduct the inference and base the
decision on the distribution maximizing the entropy function: &lt;span class=&#34;math display&#34;&gt;\[
\arg \max_{q \in M} H_q(X) \triangleq \sum_{x \in \mathcal{X}} q(x) \log
\frac{1}{q(x)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall in the method of moments we are restricting ourselves to &lt;span class=&#34;math display&#34;&gt;\[
M_\phi \triangleq \{ q \in \mathcal{P}_\mathcal{X}: \E_q [\phi(X)] =
\frac{1}{n} \sum_{i=1}^n \phi(x_i) \}
\]&lt;/span&gt; We stick to the method of moments and the principle of maximum
entropy. Therefore, the optimization problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_q \quad &amp;amp; \sum_{x \in \mathcal{X}} q_x \log q_x \\
\text{s.t.} \quad &amp;amp; \sum_{x \in \mathcal{X}} q_x \phi(x) =
\underbrace{\frac{1}{n} \sum_{i=1}^n \phi(x_i)}_{\hat \mu} \\
&amp;amp; \sum_{x \in \mathcal{X}} q_x = 1 \\
&amp;amp; q_x \ge 0, \forall x \in \mathcal{X}
\end{aligned}
\]&lt;/span&gt; Note that we use &lt;span class=&#34;math inline&#34;&gt;\(q_x\)&lt;/span&gt; to
indicate that &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a probability
vector on a finite support. We proceed with ignoring the non-negativity
constraint and show that relaxed solution is still the optimal to the
original. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-dimensional. Denote as &lt;span class=&#34;math inline&#34;&gt;\(\gamma \in \R^k, \eta \in \R\)&lt;/span&gt; the
Lagrangian multipliers. The Lagrangian function is as follows: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}(q, \gamma, \eta) = \left[ \sum_{x \in \mathcal{X}} q_x (\log
(q_x) + \gamma^\top \phi(x) + \eta) \right] - \gamma^\top \hat \mu -
\eta
\]&lt;/span&gt; We apply the KKT conditions (verify that the regularity
condition holds) to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
1 + \log (q_x) + \gamma^\top \phi(x) + \eta = 0, \forall x \in
\mathcal{X} \\
\Downarrow \\
q_x^* = \exp[-(1 + \gamma^\top \phi(x) + \eta)] \ge 0
\end{gathered}
\]&lt;/span&gt; From above we know &lt;span class=&#34;math inline&#34;&gt;\(q_x^* \propto
\exp(-\gamma^\top \phi(x))\)&lt;/span&gt; and as a result &lt;span class=&#34;math display&#34;&gt;\[
q_x^* = \frac{\exp(-\gamma^\top \phi(x))}{\sum_{x&amp;#39; \in \mathcal{X}}
\exp(-\gamma^\top \phi(x&amp;#39;))}
\]&lt;/span&gt; Now we substitute the primal optima back to the Lagrangian
function to give &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}(q^*, \gamma, \eta) = -\log \left( \sum_{x \in \mathcal{X}}
\exp(-\gamma^\top \phi(x)) \right) - \gamma^\top \hat \mu
\]&lt;/span&gt; Hence, we can formulate the dual problem as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{\gamma \in \R^k} \quad &amp;amp; -\log \left( \sum_{x \in \mathcal{X}}
\exp(-\gamma^\top \phi(x)) \right) - \gamma^\top \hat \mu
\end{aligned}
\]&lt;/span&gt; After rewriting &lt;span class=&#34;math inline&#34;&gt;\(-\gamma\)&lt;/span&gt;
as &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, this exactly recovers
the maximum likelihood estimation problem for an exponential family with
the feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;maximum entropy as the dual to maximum
likelihood&lt;/strong&gt;. In a set of distribution &lt;span class=&#34;math inline&#34;&gt;\(M_\phi \triangleq \{ q: \E_q [\phi(X)] = \mu
\}\)&lt;/span&gt;, the distribution &lt;span class=&#34;math inline&#34;&gt;\(q^*\)&lt;/span&gt;
that maximizes the entropy will be from an exponential family with
feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. That is, for
some &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we will have &lt;span class=&#34;math display&#34;&gt;\[
q^*(x) = \exp(\theta^\top \phi(x) - A(\theta))
\]&lt;/span&gt; In addition, if &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is
the empirical mean of sample, the maximum entropy problem over &lt;span class=&#34;math inline&#34;&gt;\(M_\phi\)&lt;/span&gt; is the dual optimization problem
to the maximum likelihood estimation for the exponential family with
feature function &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We conclude the magic powers of exponential family here. With
appropriate conditions,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;exponential family + method of moments = maximum likelihood
estimation&lt;/li&gt;
&lt;li&gt;principle of max entropy + method of moments = exponential
family&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What distribution in the set &lt;span class=&#34;math inline&#34;&gt;\(M_\phi
\triangleq \{ q_x: \E_q[X] = \mu \}\)&lt;/span&gt; will maximize the entropy
of an integer-valued random variable &lt;span class=&#34;math inline&#34;&gt;\(X \in
\N\)&lt;/span&gt;​ with a fixed mean value?&lt;/p&gt;
&lt;p&gt;The problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_q \quad &amp;amp; \sum_{x \in \mathcal{X}} q_x \log q_x \\
\text{s.t.} \quad &amp;amp; \sum_{x \in \mathcal{X}} q_x x = \mu \\
&amp;amp; \sum_{x \in \mathcal{X}} q_x = 1 \\
&amp;amp; q_x \ge 0, \forall x \in \mathcal{X}
\end{aligned}
\]&lt;/span&gt; We write down the Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal{L}(\theta, \lambda) &amp;amp;= \sum_{x \in \mathcal{X}} q_x \log
q_x + \lambda_1 (\sum_{x \in \mathcal{X}} q_x - 1) + \lambda_2 (\sum_{x
\in \mathcal{X}} x q_x - \mu) \\
&amp;amp;= \sum_{x \in \mathcal{X}} [(\log q_x + \lambda_1 + \lambda_2 x)
q_x] - \lambda_1 - \lambda_2 \mu
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take its derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
0 &amp;amp;= 1 + \log q_x + \lambda_1 + \lambda_2 x \\
q_x &amp;amp;= \exp[-(1 + \lambda_1 + \lambda_2 x)]
\end{aligned}
\]&lt;/span&gt; Consider the normalization constraint to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
q_x = \frac{\exp(-\lambda_2 x)}{\sum_{x&amp;#39;=1}^\infty \exp(-\lambda_2
x&amp;#39;)} \\
\exp(-1 - \lambda_1) = \sum_{x&amp;#39;=1}^\infty \exp(-\lambda_2 x&amp;#39;)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt; to converge, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_2\)&lt;/span&gt; has to be positive. As a
result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\exp(-1 - \lambda_1) = \sum_{x&amp;#39;=1}^\infty \exp(-\lambda_2
x&amp;#39;) \\
&amp;amp;= \exp(-\lambda_2) \frac{1}{1 - \exp(-\lambda_2)} \\
&amp;amp;= \frac{1}{\exp(\lambda_2) - 1} \\
\end{aligned}
\]&lt;/span&gt; Consider the expectation constraint to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E_p[X] = \frac{\sum_{x=1}^\infty x \exp(-\lambda_2
x)}{\sum_{x&amp;#39;=1}^\infty \exp(-\lambda_2 x&amp;#39;)} \\
&amp;amp;= \frac{\exp(-\lambda_2) / [1-\exp(-\lambda_2)]^2}{\exp(-\lambda_2)
/ [1-\exp(-\lambda_2)]} \\
&amp;amp;= \frac{1}{1-\exp(-\lambda_2)} = \mu
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \lambda_2 = -\log(1-\frac{1}{\mu}) \\
&amp;amp; \lambda_1 = -\log(\mu-1) - 1 \\
&amp;amp; q_x = \frac{(1-\frac{1}{\mu})^x}{\mu-1} = \frac{1}{\mu}
(1-\frac{1}{\mu})^{x-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What distribution in the set &lt;span class=&#34;math inline&#34;&gt;\(M_\phi
\triangleq \{ q: \E_q[X] = \mu, \Cov_q[X] = \Sigma \}\)&lt;/span&gt; will
maximize the entropy of a random vector &lt;span class=&#34;math inline&#34;&gt;\(X
\in \R^d\)&lt;/span&gt;​​ with a fixed mean vector and covariance matrix??&lt;/p&gt;
&lt;p&gt;This is the first attempt to extend the discussion to a continuous
case. We know that &lt;span class=&#34;math inline&#34;&gt;\(q^*\)&lt;/span&gt; is in the
form of &lt;span class=&#34;math display&#34;&gt;\[
q^*(x) = \exp(\theta^\top \phi(x) - A(\theta))
\]&lt;/span&gt; Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov[X] &amp;amp;= \E[(X-\mu)(X-\mu)^T] \\
\Cov[X] &amp;amp;= \E[X X^T] - \E[X] \E[X]^T \\
\E[X X^T] &amp;amp;= \Cov[X] + \E[X] \E[X]^T \\
\end{aligned}
\]&lt;/span&gt; Therefore, we can choose our feature function as &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = [x_1, \dots, x_d, x_1 x_1, \dots, x_1 x_d, x_2 x_2, \dots, x_2
x_d, \dots, x_d x_d]
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a simpler way, we know &lt;span class=&#34;math inline&#34;&gt;\(q^*(x) =
\exp(\theta^T \phi(x) - A(\theta))\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \R^{d^2+d}\)&lt;/span&gt;. This form of
exponential family is exactly the Gaussian distribution: &lt;span class=&#34;math display&#34;&gt;\[
q^*(x) = \frac{\exp[-\frac{1}{2}{(x-\mu)}^\top \Sigma^{-1}
(x-\mu)]}{\sqrt{|2\pi \Sigma|}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consider the normalization constraint to give &lt;span class=&#34;math display&#34;&gt;\[
\int \exp(A(\theta)) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximizing-conditional-entropy&#34;&gt;Maximizing Conditional
Entropy&lt;/h3&gt;
&lt;p&gt;Consider the prediction problem where we want to predict the label
variable from the feature variable. Denote as &lt;span class=&#34;math inline&#34;&gt;\(P_{X,Y}\)&lt;/span&gt; the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;. The question is how to determine
the prediction rule &lt;span class=&#34;math inline&#34;&gt;\(f: \mathcal{X} \mapsto
\mathcal{Y}\)&lt;/span&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(P_{X,Y}\)&lt;/span&gt; is known…&lt;/p&gt;
&lt;p&gt;This would depend on the loss function. If the loss function is the
squared error, the MMSE estimator gives &lt;span class=&#34;math display&#34;&gt;\[
f^*(x) = \E[Y|X=x]
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(P_{X,Y}\)&lt;/span&gt; is unknown and we
know that &lt;span class=&#34;math inline&#34;&gt;\(P_{X, Y}\)&lt;/span&gt; belongs to the
family &lt;span class=&#34;math inline&#34;&gt;\(M_\phi \triangleq \{ Q_{X,Y}:
\E_Q[\phi(X, Y)] = \mu \}\)&lt;/span&gt;…&lt;/p&gt;
&lt;p&gt;In this case, we can further apply the principle of maximum
conditional entropy. But why not maximize the joint entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X, Y)\)&lt;/span&gt; as done in previous discussion?
In fact, in this case, the two approaches are equivalent due to the
following relationship: &lt;span class=&#34;math display&#34;&gt;\[
H(X,Y) = H(Y|X) + H(X)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(H(X)\)&lt;/span&gt; is
fixed/deterministic given the samples (because we can directly learn
&lt;span class=&#34;math inline&#34;&gt;\(p_X\)&lt;/span&gt; at least in a non-parametric
way from samples), we can resort to maximizing &lt;span class=&#34;math inline&#34;&gt;\(H(Y|X)\)&lt;/span&gt; to slim down the formula. Note
that even though we have been given the samples from &lt;span class=&#34;math inline&#34;&gt;\(p_{X,Y}\)&lt;/span&gt;, we can still adjust &lt;span class=&#34;math inline&#34;&gt;\(p_{Y|X}\)&lt;/span&gt; to maximize &lt;span class=&#34;math inline&#34;&gt;\(H(Y|X)\)&lt;/span&gt; (because can’t learn &lt;span class=&#34;math inline&#34;&gt;\(p_{Y|X=x}\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathcal{X}\)&lt;/span&gt; from samples). From
another perspective, in prediction task, what we are interested in is
the &lt;span class=&#34;math inline&#34;&gt;\(p_{Y|X}\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p_X\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;principle of maximum conditional
entropy&lt;/strong&gt;. Given the set of probability distributions &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;, base the prediction of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; on the distribution maximizing the
conditional entropy &lt;span class=&#34;math inline&#34;&gt;\(H(Y|X)\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\arg \max_{q \in M} H_q(Y|X) \triangleq \sum_{x \in \mathcal{X}} q(x, y)
\frac{1}{q(x|y)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;maximum conditional entropy and logistic
regression&lt;/strong&gt;. The conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(q_{Y|X}^*\)&lt;/span&gt; chosen from &lt;span class=&#34;math inline&#34;&gt;\(M_\phi \triangleq \{ Q_{X,Y}: \E_Q[Y \phi(X)] =
\mu \}\)&lt;/span&gt; (i.e., “&lt;span class=&#34;math inline&#34;&gt;\(\phi(X,Y)\)&lt;/span&gt;”
is chosen to be in the form of &lt;span class=&#34;math inline&#34;&gt;\(Y
\phi(X)\)&lt;/span&gt;) that results in the maximum conditional entropy will
follow a logistic regression model for some vector &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
q_{Y|X}^*(y|x) = \frac{\exp(y \theta^\top \phi(x))}{\sum_{y&amp;#39; \in
\mathcal{Y}} \exp(y&amp;#39; \theta^\top \phi(x))}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>Convex Optimization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</link>
      <pubDate>Fri, 07 Jan 2022 12:56:57 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</guid>
      <description>

&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;h3 id=&#34;definitions-1&#34;&gt;Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convex Set&lt;/p&gt;
&lt;p&gt;A set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt; is called a
convex set if &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, x^{(2)} \in \mathcal
X\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall \alpha \in [0,1],
\alpha x^{(1)} + (1 - \alpha)x^{(2)} \in \mathcal X\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convex Function&lt;/p&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is
convex if &lt;span class=&#34;math inline&#34;&gt;\(dom(f)\)&lt;/span&gt; is a convex set
and &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x^{(1)} + (1 - \alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1 -
\alpha)f(x^{(2)}), \forall x^{(1)}, x^{(2)} \in dom(f), \alpha \in [0,1]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is concave if &lt;span class=&#34;math inline&#34;&gt;\(-f\)&lt;/span&gt; is convex.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with a
convex domain is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + \nabla^T f(x)(y - x), \forall x, y \in dom(f)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is twice differentiable
if &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2f(x)_{ij} =
\frac{\partial^2f(x)}{\partial x_ix_j}\)&lt;/span&gt; exists at each &lt;span class=&#34;math inline&#34;&gt;\(x \in dom(f)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla^2f(x) \succeq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Any local minima of a convex function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is also a global
minima.&lt;/p&gt;
&lt;p&gt;Proof: Suppose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a local
minima. Then there exists a number &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt;
0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x \in dom(f)
\land ||x-y||_2 \le r \Rightarrow f(x) \ge f(y)\)&lt;/span&gt;. Suppose
instead there exists a global minima &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Then it must hold that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||y-z||_2 &amp;gt; r \\
0 &amp;lt; \frac{r}{||y-z||_2} &amp;lt; 1
\end{gather}
\]&lt;/span&gt; There exists some &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \epsilon
&amp;lt; r\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta = 1
- \frac{r - \epsilon}{||y-z||_2} &amp;lt; 1\)&lt;/span&gt;. Then the distance from
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to point &lt;span class=&#34;math inline&#34;&gt;\((\theta y + (1 - \theta)z)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
||y - (\theta y + (1 - \theta)z)||_2 = (1 - \theta)||y-z||_2 = \frac{r -
\epsilon}{||y-z||_2}||y-z||_2 &amp;lt; r
\]&lt;/span&gt; That is to say &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \ge f(y)
\]&lt;/span&gt; However, since &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
convex and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta &amp;lt; 1\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the global
minima, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z) &amp;lt;
\theta f(y) + (1 - \theta)f(y) = f(y)
\]&lt;/span&gt; which contradicts that &lt;span class=&#34;math inline&#34;&gt;\(f(\theta y
+ (1 - \theta)z) \ge f(y)\)&lt;/span&gt; derived. In other words, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a global minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Super-additivity&lt;/p&gt;
&lt;p&gt;For a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; such
that &lt;span class=&#34;math inline&#34;&gt;\(f(0) = 0\)&lt;/span&gt;, it is super-additive
in that &lt;span class=&#34;math display&#34;&gt;\[
f(x) + f(y) \le f(x+y)
\]&lt;/span&gt; To show it, notice that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; f(x) + f(y) = f(\frac{x}{x+y}(x+y)) + f(\frac{y}{x+y}(x+y)) \\
&amp;amp;= f(\frac{x}{x+y}(x+y) + \frac{y}{x+y} \cdot 0) +
f(\frac{y}{x+y}(x+y) + \frac{x}{x+y} \cdot 0) \\
&amp;amp;\le \frac{x}{x+y} f(x+y) + \frac{y}{x+y} f(0) + \frac{y}{x+y}
f(x+y) + \frac{x}{x+y}f(0) \\
&amp;amp;= f(x+y)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;Standard form of the optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Standard form of the convex optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;a_i^Tx = b_i (Ax = b), i = 1,\dots,s \\
&amp;amp;\text{$f_0,\dots,f_r$ are convex, and $a_0,\dots,a_s \in \R^N$}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convex-conjugate&#34;&gt;Convex Conjugate&lt;/h3&gt;
&lt;p&gt;For a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, its
&lt;strong&gt;convex conjugate&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
f^*(t) = \sup_x [x^T \cdot t - f(x)]
\]&lt;/span&gt; By definition, a Convex Conjugate pair &lt;span class=&#34;math inline&#34;&gt;\((f,f^*)\)&lt;/span&gt; has the following property: &lt;span class=&#34;math display&#34;&gt;\[
f(x) + f^*(t) \ge x^T \cdot t
\]&lt;/span&gt; As a conjugate, &lt;span class=&#34;math inline&#34;&gt;\(f^{**} =
f\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f^{**}(t) &amp;amp;= \sup_x [x^T \cdot t - f^*(x)] \\
&amp;amp;= \sup_x [x^T \cdot t - \sup_y [y^T \cdot x - f(y)]] \\
&amp;amp;= \sup_x [x^T \cdot t + \inf_y [f(y) - y^T \cdot x]] \\
&amp;amp;= \sup_x \inf_y [x^T (t-y) + f(y)] \\
&amp;amp;\Downarrow_\text{Mini-max Theorem} \\
&amp;amp;= \inf_y \sup_x [x^T (t-y) + f(y)]
\end{aligned}
\]&lt;/span&gt; The above reaches the infimum only if &lt;span class=&#34;math inline&#34;&gt;\(y=t\)&lt;/span&gt;. Otherwise, &lt;span class=&#34;math inline&#34;&gt;\(\sup_x [x^T (t-y) + f(y)]\)&lt;/span&gt; can make it to
infinity. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
f^{**}(t) = \inf_y \sup_x [f(t)] = f(t)
\]&lt;/span&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_conjugate&#34;&gt;Wiki&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/188702379&#34;&gt;凸优化-凸共轭&lt;/a&gt; || &lt;a href=&#34;https://mpra.ub.uni-muenchen.de/80502/1/MPRA_paper_80502.pdf&#34;&gt;MPRA_paper_80502.pdf
(uni-muenchen.de)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</link>
      <pubDate>Fri, 07 Jan 2022 12:42:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</guid>
      <description>
&lt;p&gt;Linear discriminant analysis is another approximation to the Bayes
optimal classifier. Instead of assuming independence between each pair
of input dimensions given certain label, LDA assumes a single common
shared covariance matrix among the input dimensions, no matter the label
is. Take the Gaussian distribution as an example: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(x|y=C_j) &amp;amp;= \mathcal{N}(x;\mu_j, \Sigma) \\
&amp;amp;=
\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}e^{-\frac{1}{2}(x-\mu_j)^T\Sigma^{-1}(x-\mu_j)}
\end{aligned}
\]&lt;/span&gt; Then the classification function will be &lt;span class=&#34;math inline&#34;&gt;\(f_{LDA} = \arg
\max\limits_{j}p(x|y=C_j)p(y=C_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;LDA’s parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\mu, \Sigma,
\varphi)\)&lt;/span&gt; is also learned with Maximum Likelihood Estimation:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\theta) &amp;amp;= \prod_{i=1}^mp(x^{(i)}, y^{(i)};\theta) \\
&amp;amp;= \prod_{i=1}^mp(x^{(i)}|y^{(i)};\theta)p(y^{(i)})
\end{aligned}
\]&lt;/span&gt; The log-likelihood function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log L(\theta) \\
&amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) + \sum_{i=1}^m\log
p(y^{(i)}, \varphi) \\
\end{aligned}
\]&lt;/span&gt; We can maximize above two summations separately.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_1(\mu,\Sigma) &amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) \\
&amp;amp;=
-\frac{1}{2}\sum_{i=1}^m(x^{(i)}-\mu_{j|y^{(i)}})^T\Sigma^{-1}(x^{(i)}-\mu_{j|y^{(i)}})
+ \log|\Sigma| + n\log2\pi\\
\end{aligned}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \mu_j} =
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) \\
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) &amp;amp;= 0
\\
\Sigma^{-1}\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) &amp;amp;= 0
\\
\end{aligned}
\]&lt;/span&gt; Since the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is real-symmetric and invertible
and thus its nullspace is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) = 0 \\
\mu_j =
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \Sigma} =
-\frac{1}{2}\sum_{i=1}^m(\Sigma^{-1}
-\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) \\
\sum_{i=1}^m(\Sigma^{-1}
-\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) = 0 \\
\Sigma^{-1}\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T)
= 0 \\
\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^mI = \Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T
\\
\sum_{i=1}^m\Sigma
=  \Sigma\Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;l_2(\varphi) = \sum_{i=1}^m\log p(y^{(i)}; \varphi) \\
&amp;amp;= \sum_{i=1}^m\log\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)}
\\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(p(y^{(i)};\varphi) =
\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} =
\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\varphi_j\)&lt;/span&gt;. We chose the
product notation because it could be easily “logged”.&lt;/p&gt;
&lt;p&gt;There is also a constraint in maximization of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;. We can form
the corresponding Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
J(\varphi, \lambda) =
\sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j +
\lambda(-1 + \sum_{j=1}^c\varphi_j)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\varphi_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\varphi_j} + \lambda = 0 \\
\varphi_j = -\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda}
\end{gather}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j =
1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-\frac{\sum_{j=1}^c\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda} = 1 \\
\lambda = -M
\end{gather}
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
\varphi_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{M}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Frobenius Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/</link>
      <pubDate>Mon, 20 Dec 2021 15:43:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/</guid>
      <description>
&lt;p&gt;Frobenius Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
||A||_F \triangleq \sqrt{\sum_{i,j}A_{ij}^2}
\]&lt;/span&gt; It can be found that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||A||_F^2 = \sum_{ij}A_{ij}^2 &amp;amp;=
\sum_{i=1}^m\sum_{j=1}^nA_{ij}A_{ij} =
\sum_{i=1}^n\sum_{j=1}^mA_{ji}A_{ji} \\
&amp;amp;= \sum_{i=1}^m(\sum_{j=1}^nA_{ij}A_{ji}^T) =
\sum_{i=1}^n(\sum_{j=1}^mA_{ij}^TA_{ji}) \\
&amp;amp;= \sum_{i=1}^m(A_{i:}A_{:i}^T) = \sum_{i=1}^n(A_{i:}^TA_{:i})\\
&amp;amp;= \sum_{i=1}^m(AA^T)_{ii} = \sum_{i=1}^n(A^TA)_{ii}\\
&amp;amp;= \tr(AA^T) = \tr(A^TA)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Chebyshev Distance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/</link>
      <pubDate>Sat, 18 Dec 2021 20:28:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/</guid>
      <description>

&lt;p&gt;Chebyshev distance is a specific form of Minkowski norm (&lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norm): &lt;span class=&#34;math display&#34;&gt;\[
d_p(x, x^\prime) = ||x - x^\prime||_p \coloneq (\sum_{i=1}^n|x_i -
x^\prime_i|^p)^{1/p}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(p \to \infty\)&lt;/span&gt;.
Chebyshev distance is in effect &lt;span class=&#34;math inline&#34;&gt;\(\max\limits_i (|x_i - x^\prime_i|)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;proof-of-discrete-form&#34;&gt;Proof of Discrete Form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = |x_i - x^\prime_i|\)&lt;/span&gt; and
without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(a_1 =
\max\limits_ia_i = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^n|x_i -
x^\prime_i|^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^na_i^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(a_1^p\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p}
\\
&amp;amp;= \lim_{p \to \infty}(a_1^p)^{1/p} \cdot \lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\
&amp;amp;= a_1 \cdot \lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\forall i, a_1 &amp;gt;
a_i\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\frac{a_i}{a_1} \le
1\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 \le &amp;amp;\sum_{i=1}^n(\frac{a_i}{a_1})^p \le n \\
1^{1/p} \le &amp;amp;(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le n^{1/p}
\text{, where $p &amp;gt; 1$} \\
\lim_{p \to \infty}1^{1/p} \le &amp;amp;\lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le \lim_{p \to
\infty}n^{1/p} \\
1 \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p}
\le 1 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(d_p(x, x^\prime) = a_1 \cdot 1
= a_1 = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;proof-of-continuous-form&#34;&gt;Proof of Continuous Form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; be continuous and
bounded on interval &lt;span class=&#34;math inline&#34;&gt;\((a, b)\)&lt;/span&gt;, then,
&lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = \sup\limits_{x
\in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sup\limits_{x \in
(a,b)}|f(x)|\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall\varepsilon &amp;gt; 0, \exists x_0 \in (a,b),
|f(x_0)| &amp;gt; S - \varepsilon\)&lt;/span&gt;. By continuity, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{x \to x_0}|f(x)| &amp;gt; S -
\varepsilon\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\exists\delta
&amp;gt; 0, \forall x \in U(x_0, \delta), ||f(x)| - \lim\limits_{x \to
x_0}|f(x)|| &amp;lt; \varepsilon \rightarrow |f(x)| &amp;gt; S -
2\varepsilon\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge
(\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 -
\delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
&amp;amp;= \lim\limits_{p \to +\infty}(2\delta(S - 2\varepsilon)^p)^{1/p} \\
&amp;amp;= S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(U(x_0, \delta)\)&lt;/span&gt; in
within the interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|f(x)|^p \ge 0\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} \\
&amp;amp;\ge S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is
arbitrarily and positively valued, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{p \to +\infty}(\int_a^b
|f(x)|^pdx)^{1/p} \ge S\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \le
\lim\limits_{p \to +\infty}(\int_a^b S^pdx)^{1/p} = \lim\limits_{p \to
+\infty}((b - a)S^p)^{1/p} = S
\]&lt;/span&gt; then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = S =
\sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/267575473&#34;&gt;p范数的极限（无穷范数）为什么是极大值范数？
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</link>
      <pubDate>Fri, 17 Jun 2022 20:32:59 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</guid>
      <description>

&lt;h2 id=&#34;two-class-logistic-regression&#34;&gt;Two-class Logistic
Regression&lt;/h2&gt;
&lt;p&gt;Logistic regression is a binary linear classifier. Suppose the
feature space is &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt;, then it
processes the feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with
a linear function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = w^Tx + b\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R\)&lt;/span&gt;. It takes a probabilistic
approach and maps &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to a
probability value between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; by applying a sigmoid function
&lt;span class=&#34;math inline&#34;&gt;\(\sigma(z) = \frac{1}{1 + e^{-z}}\)&lt;/span&gt;.
That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(y = +1|x) &amp;amp;= \sigma(f(x)) \\
p(y = -1|x) &amp;amp;= 1 - \sigma(f(x)) = \sigma(-f(x)) \\
\end{aligned}
\]&lt;/span&gt; Or equivalently, &lt;span class=&#34;math inline&#34;&gt;\(p(y|x) =
\sigma(yf(x))\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f_{LR}(x) =
\arg \max\limits_{y \in \{-1, 1\}} \sigma(y(f(x)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Logistic regression is a discriminative classifier because we are
directly modelling &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;, with no
intermediate step on &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{ (x^{(i)},
y^{(i)}): i=1, \dots, M \}\)&lt;/span&gt;, logistic regression is learning by
maximizing the &lt;strong&gt;conditional&lt;/strong&gt; log-likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
l(w, b) = \log L(w, b) = \log \prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) =
\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b)
\]&lt;/span&gt;&lt;/p&gt;
$$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp;(w^\star, b^\star) = \arg \max\limits_{w, b}l(w, b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log
\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\

&amp;amp;= \arg \min\limits_{w, b}-\sum_{i=1}^M\log
\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\
\end{aligned}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Let $J(w, b) = -\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b))$ be the
target function. There is no closed-form solution to this optimization
problem. Rather, it is to be solved by some iterative algorithm, e.g.
gradient descent. For each iteration, parameters are updated by
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{gather}
w \leftarrow w - \eta\frac{\partial J}{\partial w} \\
b \leftarrow b - \eta\frac{\partial J}{\partial b}
\end{gather}\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
Specifically,
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\frac{\partial J}{\partial w} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))}
\frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))}
\frac{\partial (y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))}
\sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)}
\\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial b} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))}
\frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))}
\frac{\partial (y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))}
\sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the posterior obtained by binary &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt; also
has the form of &lt;span class=&#34;math display&#34;&gt;\[
p(y|x) = \frac{1}{1 + e^{-(w^Tx + b)}}
\]&lt;/span&gt; This is not to say LDA and LR are the same in binary case. LDA
is a generative model which learns &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;, LR is a discriminative model which
only learns &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;. One can
discriminate with a generative model, but not reversely.&lt;/p&gt;
&lt;h3 id=&#34;logistic-loss-perspective&#34;&gt;Logistic Loss Perspective&lt;/h3&gt;
&lt;p&gt;Logistic regression is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\arg \min\limits_{w, b} \sum_{i=1}^M \log [1 + e^{-y^{(i)}(w^T
x^{(i)}+b)}] \\
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\ell(z) = \log(1 + e^{-y
z})\)&lt;/span&gt; is the logistic loss and &lt;span class=&#34;math inline&#34;&gt;\(z =
w^T x + b = \frac{P(\hat y=1|x)}{P(\hat y=0|x)}\)&lt;/span&gt; is the logit.
Square loss &lt;span class=&#34;math inline&#34;&gt;\(\ell(\hat y) = (y - \hat
y)^2\)&lt;/span&gt; is not used because this will make the problem non-convex,
forfeiting the facility of theoretical guarantee.&lt;/p&gt;
&lt;h3 id=&#34;some-history&#34;&gt;Some History&lt;/h3&gt;
&lt;p&gt;Historically, there has been efforts on adapting linear regression to
the classification task where the output is a probability value between
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, instead of between &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. Many attempts have been given to
mapping &lt;span class=&#34;math inline&#34;&gt;\((0, 1)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((-\infty, +\infty)\)&lt;/span&gt; first and then apply
linear regression on transformed values. An early work uses the quantile
function of standard normal distribution. The model was named as
“&lt;strong&gt;prob&lt;/strong&gt;abilistic un&lt;strong&gt;it&lt;/strong&gt;” (probit).
However, this model is too computationally-expensive at that time. Later
on a work that uses the quantile function of logistic distribution
followed on, naming its model as “&lt;strong&gt;log&lt;/strong&gt;istic
un&lt;strong&gt;it&lt;/strong&gt;” (logit). Essentially, the logit function is the
log of the odds: &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{logit} (p) = \ln \frac{p}{1-p}
\]&lt;/span&gt; It can be verified that in logistic regression, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{logit}(p) = w^T x + b\)&lt;/span&gt;. Therefore,
the solution of logistic regression is in essence “linear-regressing”
the logit term, explaining why it is called “logistic regression”.&lt;/p&gt;
&lt;p&gt;Also note that in machine learning, un-normalized scores for
different classes are usually called logits too. It makes some sense
since these unbounded values are to be mapped to &lt;span class=&#34;math inline&#34;&gt;\((0, 1)\)&lt;/span&gt;, which means they are “logitted”
values.&lt;/p&gt;
&lt;h2 id=&#34;multi-class-logistic-regression&#34;&gt;Multi-class Logistic
Regression&lt;/h2&gt;
&lt;p&gt;One way to train use Logistic Regression in multi-class
classification in to for each class &lt;span class=&#34;math inline&#34;&gt;\(i = {1,
\dots, C}\)&lt;/span&gt;, assign a weight vector &lt;span class=&#34;math inline&#34;&gt;\(w^{(i)}\)&lt;/span&gt;, the probability is define by the
softmax function: &lt;span class=&#34;math display&#34;&gt;\[
p(y = c|x) =\frac{\exp(w^{(c)}\cdot x + b^{(c)})}{\sum_{i=1}^C
\exp(w^{(i)} \cdot x + b^{(i)})}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(f_\text{Multiclass LR}(x) =
\arg \max\limits_{y \in \{1,\dots,C\}}p(y|x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To prevent overfitting, a prior distribution is usually added to
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose each dimension of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is independently sampled from a
Gaussian distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0,
\frac{C}{2})\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
p(w) \propto \exp(-\frac{1}{C}w^Tw)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bounding Mutual Information</title>
      <link>https://chunxy.github.io/notes/papers/bounding-mutual-information/</link>
      <pubDate>Thu, 02 Jun 2022 14:01:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/bounding-mutual-information/</guid>
      <description>

&lt;h2 id=&#34;i_textba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;The very basic bound on the Mutual Information is based on the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/overview/#KL-divergence Entropy and Conditional Entropy&#34;&gt;non-negativity&lt;/a&gt; of KL-divergence.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) = &amp;amp;\E_{p(x,y)} \log \frac{p(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} + \\
&amp;amp;\underbrace{\E_{p(x,y)} \log \frac{p(x|y)}{q(x|y)}}_{\E_{p(y)}
D_{KL}(p(x|y) || q(x|y)} \\
\ge &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log q(x|y) + H(X) \triangleq I_\text{BA}
\end{aligned}
\]&lt;/span&gt; This bound is not usually tractable since &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H(X)\)&lt;/span&gt; has no closed-form expression.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) =
p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;i_textuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; is replaced with an
unnormalized model, that is &lt;span class=&#34;math display&#34;&gt;\[
q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)}, \text{where } Z(y) = \E_{p(x)}
e^{f(x,y)}
\]&lt;/span&gt; Substituting this back to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} \log \frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \triangleq I_\text{UBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that by scaling &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; term is canceled.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) =
\frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{f(x,y)} &amp;amp;= \frac{p(x, y)}{p(x)p(y)} Z(y) \\
e^{f(x,y)} &amp;amp;= \frac{Z(y)}{p(y)} p(y|x) \\
f(x,y) &amp;amp;= \ln p(y|x) + \underbrace{\ln \frac{Z(y)}{p(y)}}_{c(y)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textdv&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By further applying Jensen’s inequality to the &lt;span class=&#34;math inline&#34;&gt;\(\E_{p(y)} Z(y)\)&lt;/span&gt; term in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log\E_{p(y)} [Z(y)] \triangleq
I_\text{DV}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The original &lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt; bound
is in effect derived from &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/#Variational Lower-bound&#34;&gt;the
variational lower bound of KL-divergence&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_\text{KL}(p_{(x,y)} || p(x) \otimes p(y)) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log [\E_{p(x) \otimes p(y)} f(x,y)] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log [\E_{p(y)} \E_{p(x)} e^{f(x,y)}] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log \E_{p(y)} Z(y) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_texttuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt; function also has the
following property: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\forall x,a&amp;gt;0,\log(x) &amp;amp;\le \frac{x}{a} + \log(a) - 1 &amp;amp;\iff \\
a + a\log(x) &amp;amp;\le x + a\log(a) &amp;amp;\iff \\
a\log(x) - x &amp;amp;\le a\log(a) - a \\
\end{aligned}
\]&lt;/span&gt; Therefore, we can insert the inequality &lt;span class=&#34;math inline&#34;&gt;\(\log Z(y) \le \frac{Z(y)}{a(y)} + \log a(y) -
1\)&lt;/span&gt; into the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; to
get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) -
1 \big)] \triangleq I_\text{TUBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This bound is tight when&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(f(x,y) = \log p(y|x) + \underbrace{\log
\frac{Z(y)}{p(y)}}_{c(y)}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(a(y) = Z(y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;i_textnwj&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By setting &lt;span class=&#34;math inline&#34;&gt;\(a(y) = e\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log
a(y) - 1 \big)] \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - e^{-1}\E_{p(y)} Z(y)\triangleq
I_\text{NWJ}
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt; is a
special case of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, its
bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(Z(y)\)&lt;/span&gt;
self-normalizes to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. In this case
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x,y) &amp;amp;= \log p(y|x) + \log \frac{e}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(x|y)}{p(x)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textnce&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate the &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; given one sample from &lt;span class=&#34;math inline&#34;&gt;\(p(x_1)p(y|x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; additional independent samples &lt;span class=&#34;math inline&#34;&gt;\(x_{2:K} \sim p^{K-1}(x_{2:K})\)&lt;/span&gt;. For any
random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is
independent from &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_1,Z;Y) &amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\
&amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\
&amp;amp;= I(X_1;Y)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y) =
I(X_1,X_{2:K};Y)\)&lt;/span&gt;. Bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; becomes bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1,X_{2:K};Y)\)&lt;/span&gt;, which can be estimated
using any of the preceding methods.&lt;/p&gt;
&lt;p&gt;Set the critic to &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 +
\overbrace{\log \frac{e^{g(x,y)}}
{a(y;x_{1:K})}}^{h(x_{1:K},y)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sample among &lt;span class=&#34;math inline&#34;&gt;\(x_{1:K}\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Usually &lt;span class=&#34;math inline&#34;&gt;\((x,y)_{1:K}\)&lt;/span&gt; are sampled from the same
marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tilde
p(x,y)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is then
uniformly drawn among &lt;span class=&#34;math inline&#34;&gt;\(y_{1:K}\)&lt;/span&gt;.
Substitute these to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\label{infonce} \begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\E_{p(y)}
Z(y) \\
&amp;amp;= \E_{p(x_{1:K},y)} [1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] -
e^{-1} \E_{p(y)} [\E_{p(x_{1:K})} e^{1 + \log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [e\E_{p(x_{1:K})}
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \E_{p(x_{1:K})p(y)}
\frac{e^{g(x,y)}}{a(y,x_{1:K})} \\
\end{aligned}
\]&lt;/span&gt; Further set &lt;span class=&#34;math inline&#34;&gt;\(a(y;x_{1:K}) =
\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}\)&lt;/span&gt;. Substitute this into
the last term in equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{infonce}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} &amp;amp;= \E_{p(y)}
\frac{\E_{p(x_{1:K})} e^{g(x,y)}}{\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}
} \\
&amp;amp;\stackrel{P}{\to} \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}}
{\E_{p(x_{1:K})} e^{g(x,y)} } \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} }] \\
&amp;amp;\approx \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log
\frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)} }]
\triangleq I_\text{NCE}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;
is tightly bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log K\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log
\frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)}} ] \\
&amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K (\log
\frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} )] + \log K \\
&amp;amp;\le \E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{\sum_{i=1}^K e^{g(x_i, y_j)}} {e^{g(x_j,y_j)}} ]} + \log K
\end{aligned}
\]&lt;/span&gt; The equality is reached when &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + {h(x_{1:K},y)} = 1 + \log
\frac{p(x|y)}{p(x)}\)&lt;/span&gt; as in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;. In this case, it can be
derived that &lt;span class=&#34;math inline&#34;&gt;\(g(x,y) = g^\star(x,y) =
\frac{p(y|x)}{p(y)}\)&lt;/span&gt;. And then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{\frac{p(y_j|x_j)}{p(y_j)} + \sum_{i=1,i \ne j}^K
\frac{p(y_j|x_i)}{p(y_j)}} {\frac{p(y_j|x_j)}{p(y_j)}} ]} + \log K \\
&amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K (1 +
\frac{p(y_j)}{p(y_j|x_j)} \sum_{i=1,i \ne j}^K
\frac{p(y_j|x_i)}{p(y_j)}) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(1 +
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \E_{p(y)} \frac{p(y|x_i)}{p(y)} \big) ]}
+ \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[1 + \frac{1}{K} \sum_{j=1}^K \big(
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{p(y_j)}{p(y_j|x_j)} ]} - \log(K-1) + \log K \\
&amp;amp;= I(X_1;Y) - \log(K-1) + \log K
\end{aligned}
\]&lt;/span&gt; This derivation is much looser than &lt;a href=&#34;https://chunxy.github.io/notes/papers/contrastive-predictive-coding/#Bounding the Mutual Information&#34;&gt;that in the InfoNCE’s original
paper&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;with-i_textuba&#34;&gt;With &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;There is another approach to the derivation of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;, that stems from an estimate
of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; (which I won’t call
empirical estimate) and that may be more intuitive: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;I(X;Y) \ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;= \E_{p^N(x,y)} f(x_1,y_1) - \E_{p^N(x,y)} \log Z(y_1) \\
&amp;amp;= \E_{p^N(x,y)} \log e^{f(x_1,y_1)} \\
&amp;amp;\quad\;- \E_{p^N(x,y)} \log \E_{p(x&amp;#39;)} e^{f(x&amp;#39;,y_1)} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}}{\E_{p(x&amp;#39;)}
e^{f(x&amp;#39;,y)}} \\
&amp;amp;\simeq \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\frac{1}{N} \sum_i
e^{f(x_i,y_1)}} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\sum_i e^{f(x_i,y_1)}}
+ \log N \\
&amp;amp;\triangleq I_\text{NCE} \le \log N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Cross Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:46 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</guid>
      <description>
&lt;p&gt;The &lt;strong&gt;cross entropy&lt;/strong&gt; between two distributions over the
same underlying set of events measures the average number of bits to
identify the event drawn from the set if a coding scheme is used for the
set is optimized for probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, instead of the true distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Cross Entropy of distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; relative to a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) = -\mathrm{E}_{x \sim p} \log q(x)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</link>
      <pubDate>Tue, 11 Jan 2022 20:26:40 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</guid>
      <description>

&lt;p&gt;If a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
differentiable and satisfies certain “regularity conditions”, we can get
a nice guarantee that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will
converge by gradient descent.&lt;/p&gt;
&lt;h3 id=&#34;l-smoothness&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness&lt;/h3&gt;
&lt;p&gt;Qualitatively, smoothness means that the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; changes in a controlled, bounded
manner. Quantitatively, smoothness assumes that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has a Lipschitz gradient. That means
there exists an &lt;span class=&#34;math inline&#34;&gt;\(L &amp;gt; 0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
||\nabla f(x) - \nabla f(y)||_2 \le L||x - y||_2, \forall x,y \in
\mathrm{dom}(f)
\]&lt;/span&gt; We will say that such function is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth or strongly smooth. &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Proof&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is
equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\frac{L}{2}||x||^2_2 -
f(x)\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With the smoothness, or the Lipschitz gradient, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from above.&lt;/p&gt;
&lt;p&gt;We will assume &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness in
all cases below. Meanwhile, the local minima (in convex case, the global
minima) is denoted as &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-convexity&#34;&gt;Convergence with Convexity&lt;/h3&gt;
&lt;p&gt;With convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with linear
approximation: &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;Now consider a &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth function
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We adopt a fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt; so that the update
in each iteration will be &lt;span class=&#34;math display&#34;&gt;\[
x_{k+1} = x_k - \frac{1}{L}\nabla f(x)
\]&lt;/span&gt; By the definition of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness, &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation} \label{diff}
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\frac{1}{L}\nabla f(x_k))^T \nabla f(x_k)
+ \frac{L}{2}||-\frac{1}{L}\nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2
\end{aligned}
\end{equation}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_k)\)&lt;/span&gt;
holds in each iteration.&lt;/p&gt;
&lt;p&gt;By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
f(x^\star) \ge f(x_k) + (x^\star - x_k)^T \nabla f(x_k) \\
\label{convexity} f(x_k) \le f(x^\star) + (x_k - x^\star)^T \nabla
f(x_k)
\end{gather}
\]&lt;/span&gt; Substituting the result back to the right-hand side of
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k) -
\frac{1}{2M}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; (x_k - x^\star)^T L(x_k - x_{k+1}) -
\frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; \frac{L}{2}(2(x_k - x^\star)^T(x_k -
x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\
\end{aligned}
\]&lt;/span&gt; By using the fact that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\
2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2
\end{aligned}
\]&lt;/span&gt; We have &lt;span class=&#34;math display&#34;&gt;\[
\label{closer} f(x_{k+1}) - f(x^\star) \le \frac{L}{2}(||x_k -
x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This indicates that &lt;span class=&#34;math inline&#34;&gt;\(x_{k+1}\)&lt;/span&gt; is
closer to the global minima &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;
than &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k [f(x_{i+1}) - f(x^\star)] &amp;amp;\le \sum_{i=0}^k
\frac{L}{2}(||x_i - x^\star||^2_2 - ||x_{i+1} - x^\star||^2_2) \\
&amp;amp;= \frac{L}{2}(||x_0 - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2) \\
&amp;amp;\le \frac{L}{2}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(f(x_{i+1}) \le f(x_i), \forall
i=0,\dots,k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_i),
\forall i=0,\dots,k\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k(f(x_{k+1}) - f(x^\star)) \le \frac{L}{2}||x_0 - x^\star||^2_2 \\
f(x_{k+1}) - f(x^\star) \le \frac{L}{2k}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;variant-step-size&#34;&gt;Variant Step Size&lt;/h5&gt;
&lt;p&gt;In real application scenario, it may be difficult to know &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; exactly. However, as long as we choose
&lt;span class=&#34;math inline&#34;&gt;\(\alpha_k \le \frac{1}{L}\)&lt;/span&gt; in each
iteration, the convergence and the rate still holds. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\alpha_k \nabla f(x_k))^T \nabla f(x_k) +
\frac{L}{2}||-\alpha_k \nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2
\\
f(x_{k+1}) &amp;amp;\le f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla
f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) - (\alpha_k -
\frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} = f(x_{k+1}) - f(x^\star)
\ge 0\)&lt;/span&gt;, the above becomes &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1}
\le \eta_{k} - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla
f(x_k)||^2_2\)&lt;/span&gt;. From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{convexity}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\eta_k = f(x_k) - f(x^\star) \le (x_k - x^\star)^T \nabla f(x_k) \le
||x_k - x^\star||_2||\nabla f(x_k)||_2 \\
0 \le \frac{\eta_k}{||x_k - x^\star||_2} \le ||\nabla f(x_k)||_2 \\
-||\nabla f(x_k)||^2_2 \le -\frac{\eta^2_k}{||x_k - x^\star||^2_2} \\
\end{gather}
\]&lt;/span&gt; Substituting it back to give &lt;span class=&#34;math display&#34;&gt;\[
\eta_{k+1} \le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)
\eta^2_k}{||x_k - x^\star||^2_2}
\]&lt;/span&gt; From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{closer}\)&lt;/span&gt; we already have &lt;span class=&#34;math display&#34;&gt;\[
||x_{k+1} - x^\star||^2_2 \le ||x_k - x^\star||^2_2 \le \dots \le ||x_0
- x^\star||^2_2
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\eta_{k+1} &amp;amp;\le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)
\eta^2_k}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_k} &amp;amp;\le \frac{1}{\eta_{k+1}} - \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot
\frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot
\frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \text{, by } \eta_k \ge
\eta_{k+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k (\frac{1}{\eta_{i+1}} - \frac{1}{\eta_i}) &amp;amp;\ge
\sum_{i=0}^k \frac{(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 -
x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_0} &amp;amp;\ge \frac{\sum_{i=0}^k
(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i -
\frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
f(x_{k+1} - f(x^\star)) &amp;amp;\le \frac{||x_0 -
x^\star||^2_2}{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i =
\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha^2_i
&amp;lt; \infty\)&lt;/span&gt;, the Variant Step Size can be shown to converge.
Particularly when &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k =
\frac{1}{k}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
f(x_{k+1} - f(x^\star)) \le \Theta(\frac{1}{\log k})||x_0 -
x^\star||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-strong-convexity&#34;&gt;Convergence with Strong
Convexity&lt;/h3&gt;
&lt;h4 id=&#34;strong-convexity&#34;&gt;Strong Convexity&lt;/h4&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly convex
if there exists a &lt;span class=&#34;math inline&#34;&gt;\(l &amp;gt; 0\)&lt;/span&gt; such
that &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2,
\forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Further, strong convexity is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(f(x) - \frac{l}{2}||x||^2_2\)&lt;/span&gt; being
convex.&lt;/p&gt;
&lt;p&gt;With strong convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with a convex quadratic
approximation.&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate-1&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size-1&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth above. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{sm} f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y -
x||^2_2, \forall x,y \in \mathrm{dom}(f)
\end{equation}
\]&lt;/span&gt; In this section we further assume that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth as well as strong convex: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{sc} f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y -
x||^2_2, \forall x,y \in \mathrm{dom}(f)
\end{equation}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(F(y) = f(x) + (y - x)^T \nabla
f(x) + \frac{l}{2}||y - x||^2_2\)&lt;/span&gt;. Take derivative of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla f(x) + l(y - x) = 0 \\
y_0 - x = -\frac{\nabla f(x)}{l}
\end{aligned}
\]&lt;/span&gt; Because the second derivative of &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(l
\succeq 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; reaches
global minimum at the local minima &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt;. Substitute &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; back to &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge F(y) \ge F(y_0) = f(x) - \frac{1}{2l}||\nabla f(x)||^2_2
\]&lt;/span&gt; This holds when &lt;span class=&#34;math inline&#34;&gt;\(y =
x^\star\)&lt;/span&gt;. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) &amp;amp;\ge f(x) - \frac{1}{2l}||\nabla f(x)||^2_2 \\
||\nabla f(x)||^2_2 &amp;amp;\ge 2l(f(x) - f(x^\star))
\end{aligned}
\]&lt;/span&gt; The above inequality is referred to as Polyak-Lojasiewicz
inequality. By combining it with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, where we implicitly assume
a fixed step-size &lt;span class=&#34;math inline&#34;&gt;\(\alpha =
\frac{1}{L}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) +
\frac{1}{2L}(-||\nabla f(x_k)||^2_2) \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) +
\frac{1}{2L}(-2l(f(x) - f(x^\star))) \\
&amp;amp;= (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\\
&amp;amp;\le (1 - \frac{l}{L})(1 - \frac{l}{L}) (f(x_{k-1}) - f(x^\star)) \\
&amp;amp;\le \dots \\
&amp;amp;\le (1 - \frac{l}{L})^{k+1} (f(x_0) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sm}\)&lt;/span&gt; and
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sc}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2 \le f&amp;amp;(y)
\le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2 \\
l &amp;amp;\le L \\
\end{aligned}
\]&lt;/span&gt; Usually it would be the case that &lt;span class=&#34;math inline&#34;&gt;\(l &amp;lt; L\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; 1 - \frac{l}{L} &amp;lt; 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;non-convex-case&#34;&gt;Non-convex Case&lt;/h3&gt;
&lt;p&gt;Without convexity condition, we can still exploit the property of
&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness. Sum up on both sides
of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(k = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k f(x_{k+1}) &amp;amp;\le \sum_{i=0}^k f(x_k) -
\frac{1}{2L}||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k f(x_{k+1}) - f(x_k) &amp;amp;\le -\frac{1}{2L} \sum_{i=0}^k
||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla
f(x_k)||^2_2
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is a
local minima, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) \le f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k
||\nabla f(x_k)||^2_2 \\
f(x^\star) - f(x_0) &amp;amp;\le - \frac{1}{2L}\sum_{i=0}^k ||\nabla
f(x_k)||^2_2 \\
\sum_{i=0}^k ||\nabla f(x_k)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
\end{aligned}
\]&lt;/span&gt; In this case, the error is measured by the first derivative.
Suppose &lt;span class=&#34;math inline&#34;&gt;\(\tilde x = \arg \min_i ||\nabla
f(x_i)||^2_2\)&lt;/span&gt;, where the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the closest to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k||\nabla f(\tilde x)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
||\nabla f(\tilde x)||^2_2 &amp;amp;\le \frac{2L(f(x_0) - f(x^\star))}{k} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Convergence%20of%20Gradient%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Convergence
of Gradient Descent.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rkganti.wordpress.com/2015/08/21/convergence-rate-of-gradient-descent-algorithm/&#34;&gt;Convergence
rate of gradient descent algorithm | bps/Hz (wordpress.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/strong-convexity&#34;&gt;Strong
convexity · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>2-uniform-convergence</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/2-uniform-convergence/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/2-uniform-convergence/</guid>
      <description>

&lt;p&gt;In a typical supervised learning, the goal is to find a function
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F} \ni f: \mathcal{X} \to
\mathcal{Y}\)&lt;/span&gt; that minimizes the loss measured by the loss
function &lt;span class=&#34;math inline&#34;&gt;\(\ell: \mathcal{Y} \times
\mathcal{Y} \to \R^+\)&lt;/span&gt; over the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_{X,Y}\)&lt;/span&gt;. However, given that the sample
size is limited, it is only possible to minimize over the given samples
&lt;span class=&#34;math inline&#34;&gt;\((x_1,y_1), \dots, (x_n,y_n)\)&lt;/span&gt;,
yielding the empirical risk minimization (ERM).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Empirical risk minimization vs. population risk minimization &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\hat f \triangleq \min_{f \in \mathcal{F}} \hat L(f) \triangleq
\frac{1}{n} \sum_{i=1}^n [\ell(f(x_i), y_i)] \tag{ERM} \\
f^* \triangleq \min_{f \in \mathcal{F}} L(f) \triangleq \E_{P_{X,Y}}
[\ell(f(X), Y)] \tag{PRM/Supervised Learning} \\
\end{gather}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In machine learning, training is exactly an optimization process. But
machine learning additionally takes into consideration the adaptation of
the trained model from training data to unknown test data. Thus, we
introduce the concept of &lt;strong&gt;generalization error&lt;/strong&gt; to
reflect the model’s generalization capability.&lt;/p&gt;
&lt;p&gt;Generalization risk alone cannot be the only index. It only measures
the performance difference between training set and test set. A model
that is the same worse on the training data and the test data
“generalizes” well. Thus, we introduce the concept of &lt;strong&gt;excess
error&lt;/strong&gt; to reflect the model’s overall capability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generalization error vs. excess error &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\epsilon_\text{gen}(\hat f) \triangleq L(\hat f) - \hat L(\hat f)
\tag{Generalization Error} \\
\epsilon_\text{excess}(\hat f) \triangleq L(\hat f) - L(f^*) \tag{Excess
Error}
\end{gather}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that generalization error may be positive, zero or negative;
excess error cannot be negative. Also note that even a small excess
error does not imply an effective learning algorithm: the EMR model may
be as bad as the PRM model. It is just that we usually assume a very low
population risk.&lt;/p&gt;
&lt;p&gt;The overall purpose is to build a bound on the excess error
&lt;strong&gt;for the ERM method&lt;/strong&gt; with respect to the sample size,
which is the main topic of &lt;strong&gt;uniform convergence
analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introductory-case-and-initial-assumptions&#34;&gt;Introductory Case and
Initial Assumptions&lt;/h2&gt;
&lt;p&gt;As a kickstart, we make the following assumptions:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;We consider the zero-one loss.&lt;/li&gt;
&lt;li&gt;We assume a &lt;strong&gt;realizable scenario&lt;/strong&gt; where &lt;span class=&#34;math inline&#34;&gt;\(L(f^*) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; belongs to the hypothesis set.&lt;/li&gt;
&lt;li&gt;We consider a finite hypothesis set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F} = \{ f_1,\dots,f_t \}\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;​ functions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that due to the realizability assumption, the excess risk is
exactly the population risk.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;excess error bound for realizable finite hypothesis
set&lt;/strong&gt;. The following population risk bound holds for the ERM
solution &lt;span class=&#34;math inline&#34;&gt;\(\hat f\)&lt;/span&gt; with probability at
least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\epsilon_\text{excess}(\hat f) = L(\hat f) \le \frac{\log t + \log
\frac{1}{\delta}}{n}
\]&lt;/span&gt; Proof. Let &lt;span class=&#34;math inline&#34;&gt;\(B \triangleq \{ f \in
\mathcal{F}, L(f) &amp;gt; \epsilon \}\)&lt;/span&gt;. Since the problem is
realizable in that &lt;span class=&#34;math inline&#34;&gt;\(L(f^*) = 0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(f^* \in \mathcal{F}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
0 \le \hat L(\hat f) \le \hat L(f^*) = L(f^*) = 0 \\
\Rightarrow \hat L(\hat f) = 0
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[t]
&amp;amp;\Pr(L(\hat f) &amp;gt; \epsilon) \\
&amp;amp;= \Pr(\hat f \in B) \\
&amp;amp;\Downarrow_{\hat L(\hat f) = 0} \\
&amp;amp;\le \Pr(\exists h \in B, \hat L(h) = 0) \\
&amp;amp;\le \sum_{h \in B} \Pr(\hat L(h) = 0) \\
\\
&amp;amp;\Downarrow \\
&amp;amp;\le \sum_{h \in B} \exp(-\epsilon n) \\
&amp;amp;= t \exp(-\epsilon n)
\end{aligned}
\quad
\begin{aligned}[t]
&amp;amp;\Pr(\hat L(h) = 0) \\
&amp;amp;= \Pr(\forall i, h(x_i) = y_i) \\
&amp;amp;= \prod_i \Pr(h(x_i) = y_i) \\
&amp;amp;\Downarrow \\
&amp;amp;= (1-L(h))^n \\
&amp;amp;\le \exp(-L(h) n) \\
&amp;amp;\Downarrow \\
\Leftarrow \quad &amp;amp; \le \exp(-\epsilon n)
\end{aligned} \quad
%
\begin{gathered}[t]
\begin{aligned}[t]
\E[\mathbb{1}[h(x_i) \ne y_i]] = L(h) \\
1 - \E[\mathbb{1}[h(x_i) = y_i]] = L(h) \\
1 - \Pr(h(x_i) = y_i) = L(h) \\
\end{aligned} \\
\Downarrow \\
\begin{gathered}
&amp;amp;\Leftarrow &amp;amp;\Pr(h(x_i) = y_i) = 1 - L(h) \\
\\ \\
&amp;amp;\Leftarrow &amp;amp;(1-z)^n \le \exp(-z n)
\end{gathered}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interestingly, &lt;span class=&#34;math inline&#34;&gt;\(\log t\)&lt;/span&gt; reflects
the capacity of the function class.&lt;/p&gt;
&lt;h2 id=&#34;uniform-convergence-analysis&#34;&gt;Uniform Convergence Analysis&lt;/h2&gt;
&lt;p&gt;The three assumptions are restrictive. In this section, we first try
to wriggle out of them and draw some general conclusions. Then, we bring
some of them back to show some more meaningful results.&lt;/p&gt;
&lt;p&gt;First note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
0 \le \epsilon_\text{excess}(\hat f) = L(\hat f) - L(f^*) \\
= \underbrace{L(\hat f) - \hat L(\hat f)}_{\epsilon_\text{gen}(\hat f)}
+ \underbrace{\hat L(\hat f) - \hat L(f^*)}_{\le 0} + \underbrace{\hat
L(f^*) - L(f^*)}_{-\epsilon_\text{gen}(f^*)}
\end{gathered}
\]&lt;/span&gt; That is (this is the &lt;strong&gt;key inequality&lt;/strong&gt; that
relates the excess error and generalization error, based on which
various inequalities are derived later on), &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\epsilon_\text{excess}(\hat f) &amp;amp;\le L(\hat f) - \hat L(\hat f) +
\hat L(f^*) - L(f^*) \\
&amp;amp;\Downarrow_{0 \le \epsilon_\text{excess}(\hat f)} \\
&amp;amp;\le |L(\hat f) - \hat L(\hat f)| + |\hat L(f^*) - L(f^*)| \\
&amp;amp;\le 2 \sup_{f \in \mathcal{F}} |L(f) - \hat L(f)|
\end{aligned}
\]&lt;/span&gt; In other words, a sufficient condition for excess risk to be
upper-bounded by &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is that
for every &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{F}\)&lt;/span&gt;, the
generalization error is &lt;strong&gt;uniformly&lt;/strong&gt; less than &lt;span class=&#34;math inline&#34;&gt;\(\epsilon/2\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\Pr(\epsilon_\text{excess}(\hat f) \le \epsilon) \ge \Pr(\sup_{f \in
\mathcal{F}} |L(f) - \hat L(f)| \le \frac{\epsilon}{2})
\label{uniconv-1} \\
\Downarrow \notag \\
\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le \Pr(\sup_{f \in
\mathcal{F}} |L(f) - \hat L(f)| \ge \frac{\epsilon}{2})
\label{uniconv-2} \\
\end{gather}
\]&lt;/span&gt; The supremum operation is still ambiguous. To demystify it, we
can instead turn to study the distribution of &lt;span class=&#34;math inline&#34;&gt;\(L(f) - \hat L(f)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{F}\)&lt;/span&gt;, which is a form of
deviation of population mean from the empirical mean. That is, we remove
&lt;span class=&#34;math inline&#34;&gt;\(\sup\)&lt;/span&gt; with following inequality:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{aligned}
&amp;amp;\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le \Pr(\sup_{f
\in \mathcal{F}} |L(f) - \hat L(f)| \ge \frac{\epsilon}{2}) \\
&amp;amp;= \Pr(\cup_{1 \le i \le t} |L(f_i) - \hat L(f_i)| \ge
\frac{\epsilon}{2}) \\
&amp;amp;\le \sum_{i=1}^t \Pr(|L(f_i) - \hat L(f_i)| \ge \frac{\epsilon}{2})
\\
\end{aligned} \label{uniconv-3}
\end{equation}
\]&lt;/span&gt; The formula on the last line is the reason why we can only
deal with &lt;strong&gt;finite hypothesis set&lt;/strong&gt; in this section. It is
also reminiscent of the law of large numbers and central limit theorem
in the probability and statistics. But remember that, both of them
provides a guarantee in an asymptotic fashion, which is not suitable due
to finite sample size. To prove a non-asymptotic generalization bound,
we can use some readily-available &lt;strong&gt;tail bounds&lt;/strong&gt; from the
probability literature.&lt;/p&gt;
&lt;h3 id=&#34;bounding-via-markovs-inequality&#34;&gt;Bounding via Markov’s
Inequality&lt;/h3&gt;
&lt;p&gt;In a verbatim way, treat every &lt;span class=&#34;math inline&#34;&gt;\(l(f(x_i),
y_i)\)&lt;/span&gt; as an i.i.d. sample &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; and assume that &lt;strong&gt;both the
expectation and the variance exist&lt;/strong&gt; for &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. That is, let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\mu \triangleq \E[z_i], \sigma^2 \triangleq \Var[z_i]
\end{gathered}
\]&lt;/span&gt; For some arbitrary &lt;span class=&#34;math inline&#34;&gt;\(f&amp;#39; \in
\mathcal{F}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat L(f&amp;#39;) = \hat \mu_n = \frac{1}{n} \sum_{i=1}^n z_i,L(f&amp;#39;) =
\mu \\
\end{gathered}
\]&lt;/span&gt; Then by Chebyshev’s inequality (derived from Markov’s
inequality), &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\left.
\begin{gathered}
\E[\frac{1}{n} \sum_{i=1}^n z_i] = \mu \\
\Var[\frac{1}{n} \sum_{i=1}^n z_i] = \frac{\sigma^2}{n} \\
\end{gathered}
\right\} \Rightarrow \Pr(|\frac{1}{n} \sum_{i=1}^n z_i - \mu | \ge
\frac{\epsilon}{2}) \le \frac{4\sigma^2}{n \epsilon^2}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Adopting &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uniconv-3}\)&lt;/span&gt;, we
have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le \sum_{i=1}^t
\Pr(|\frac{1}{n} \sum_{i=1}^n z_i - \mu | \ge \frac{\epsilon}{2}) \le
\frac{4t \sigma^2}{n \epsilon^2}
\end{aligned}
\]&lt;/span&gt; This indicates that the excess risk of &lt;span class=&#34;math inline&#34;&gt;\(\hat f\)&lt;/span&gt; will decay by a &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{O}(\frac{1}{n})\)&lt;/span&gt; rate. Is this a
good decay rate? Not yet, because even a statistical inference of &lt;a href=&#34;https://www.wikiwand.com/en/Binomial_distribution&#34;&gt;binomial
distribution&lt;/a&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; parameter can
give an exponential decay rate.&lt;/p&gt;
&lt;p&gt;Remember that sitting above the polynomial order is the exponential
order. By far, we only make very limited assumption on &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. For Gaussian and bounded random
variable, we show below that we can improve the decay rate from the
geometric one​​ to an exponential one.&lt;/p&gt;
&lt;h3 id=&#34;bounding-via-hoeffdings-inequality&#34;&gt;Bounding via Hoeffding’s
Inequality&lt;/h3&gt;
&lt;h4 id=&#34;moment-generating-function&#34;&gt;Moment Generating Function&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;moment generating function&lt;/strong&gt;. For a random
variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, the moment generating
function (MGF) &lt;span class=&#34;math inline&#34;&gt;\(M_Z: \R \to \R\)&lt;/span&gt; is
defined as &lt;span class=&#34;math display&#34;&gt;\[
M_Z(t) \triangleq \E[e^{t Z}] = \int_{-\infty}^{+\infty} p_Z(z) e^{t z}
\d z
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;MGF of sum of independent random
variables&lt;/strong&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(Z_1\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(Z_2\)&lt;/span&gt; are independent random
variables, then the MGF of their sum is the product of their MGFs: &lt;span class=&#34;math display&#34;&gt;\[
M_{Z_1 + Z_2}(t) = M_{Z_1}(t) M_{Z_2}(t)
\]&lt;/span&gt; Proof. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;M_{Z_1+Z_2}(t) = \E_{Z_1, Z_2}[e^{t(Z_1 + Z_2)}] \\
&amp;amp;= \E_{Z_1} \E_{Z_2} [e^{t(Z_1 + Z_2)}] \\
&amp;amp;= \E_{Z_1} [e^{t Z_1}] \E_{Z_2} [e^{t Z_2}] \\
&amp;amp;= M_{Z_1}(t) M_{Z_2}(t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;MGF of linear transformation of random
variables&lt;/strong&gt;. For a random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and constants &lt;span class=&#34;math inline&#34;&gt;\(\alpha \ne 0, \beta\)&lt;/span&gt;, then the MGFs of the
random variable &lt;span class=&#34;math inline&#34;&gt;\(Z&amp;#39; \triangleq \alpha
Z\)&lt;/span&gt; and the random variable &lt;span class=&#34;math inline&#34;&gt;\(Z&amp;#39;&amp;#39; \triangleq Z + \beta\)&lt;/span&gt; are
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[t]
&amp;amp;M_{Z&amp;#39;}(t) = \int_{-\infty}^{+\infty} p_{Z&amp;#39;}(z) e^{t z} \d z
\\
&amp;amp;= ??\alpha \int_{-\infty}^{+\infty} p_{Z}(z/\alpha) e^{(\alpha t) *
(z/\alpha)} \d (z/\alpha) \\
&amp;amp;= M_Z(\alpha t)
\end{aligned} \quad
\begin{aligned}[t]
&amp;amp;M_{Z&amp;#39;&amp;#39;}(t) = \int_{-\infty}^{+\infty} p_{Z&amp;#39;&amp;#39;}(z)
e^{t z} \d z \\
&amp;amp;= e^{\beta t} \int_{-\infty}^{+\infty} p_{Z}(z-\beta) e^{t
(z-\beta)} \d (z-\beta) \\
&amp;amp;= e^{\beta t} M_Z(t)
\end{aligned}
\]&lt;/span&gt; In short, &lt;span class=&#34;math display&#34;&gt;\[
M_{\alpha Z + \beta}(t) = \E[e^{(\alpha Z + \beta) t}] = e^{\beta t}
\E[e^{\alpha Z t}] = e^{\beta t} M_Z(\alpha t)
\]&lt;/span&gt; Show that the expectation of linear transformation of RV is
the linear transformation of expectation of RV??&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;chernoffs-inequality&#34;&gt;Chernoff’s Inequality&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Chernoff’s inequality&lt;/strong&gt;. Consider random
variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; with moment generating
function &lt;span class=&#34;math inline&#34;&gt;\(M_Z\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\forall t &amp;gt; 0: \Pr(Z \ge \epsilon) \le \frac{M_Z(t)}{e^{t \epsilon}}
\]&lt;/span&gt; Proof. Define the random variable &lt;span class=&#34;math inline&#34;&gt;\(V = e^{t Z}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt;. Then by the Markov’s inequality
&lt;span class=&#34;math display&#34;&gt;\[
\Pr(Z \ge \epsilon) = \Pr(V \ge e^{t \epsilon}) \le \frac{\E[V]}{e^{t
\epsilon}} = \frac{M_Z(t)}{e^{t \epsilon}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu_n - \mu\)&lt;/span&gt; can
be rewritten in a summation form &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n \frac{z_i - \mu}{n}\)&lt;/span&gt;.
Therefore, for all &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\Pr(\hat \mu_n - \mu \ge \epsilon) \le \frac{M_{\hat \mu_n -
\mu}(t)}{e^{t \epsilon}} \\
&amp;amp;= \frac{(M_{(Z - \mu)/n}(t))^n}{e^{t \epsilon}} \\
&amp;amp;= \frac{(M_{(Z - \mu)}(t/n))^n}{e^{t \epsilon}} \\
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is fixed, we have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\forall t &amp;gt; 0, \Pr(\hat \mu_n - \mu \ge \epsilon) \le \frac{(M_{(Z -
\mu)}(t))^n}{e^{t \epsilon}} \\
\Downarrow \\
\Pr(\hat \mu_n - \mu \ge \epsilon) \le \frac{\inf_{t &amp;gt; 0}\ (M_{Z -
\mu}(t))^n}{e^{t \epsilon}} \\
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If only &lt;span class=&#34;math inline&#34;&gt;\(\inf_{t &amp;gt; 0}\ M_{Z - \mu}(t)
&amp;lt; 1\)&lt;/span&gt;! We divert to the applications of Chernoff’s inequality
for some well-defined distributions. And then we show that the scenario
that &lt;span class=&#34;math inline&#34;&gt;\(\inf_{t &amp;gt; 0}\ M_{Z - \mu}(t) &amp;lt;
1\)&lt;/span&gt; is not rare.&lt;/p&gt;
&lt;h5 id=&#34;zero-mean-gaussians&#34;&gt;Zero-mean Gaussians&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;MGF of zero-mean Gaussians&lt;/strong&gt;. Suppose
&lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal{N}(0, \sigma^2)\)&lt;/span&gt; is
zero-mean Gaussian random variable. Then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;M_Z(t) = \int_{-\infty}^{+\infty} p_Z(z) e^{t z} \d z \\
&amp;amp;= \int_{-\infty}^{+\infty} \frac{e^{-\frac{z}{2 \sigma^2} + tz}}
{\sqrt{2 \pi \sigma^2}} \d z \\
&amp;amp;= \int_{-\infty}^{+\infty} \frac{e^{-\frac{(z - \sigma^2 t)^2}{2
\sigma^2} + \frac{\sigma^2 t^2}{2}}} {\sqrt{2 \pi \sigma^2}}  \d z \\
&amp;amp;= e^{\frac{\sigma^2 t^2}{2}} \int_{-\infty}^{+\infty}
\frac{e^{-\frac{(z - \sigma^2 t)^2}{2 \sigma^2}}} {\sqrt{2 \pi
\sigma^2}} \d (z-\sigma^2 t^2) \\
&amp;amp;= e^{\frac{\sigma^2 t^2}{2}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that it is the nice property of &lt;strong&gt;MGF of zero-mean
Gaussian&lt;/strong&gt; that confines our discussion to zero-mean variables. A
direct application of Chernoff’s inequality gives&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Chernoff tail bound for zero-mean
Gaussians&lt;/strong&gt;. The optimized Chernoff tail bound for &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal{N}(0, \sigma^2)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Pr(Z \ge \epsilon) &amp;amp;\le \inf_{t&amp;gt;0}\quad M_Z(t) e^{-t \epsilon}
\\
&amp;amp;= \inf_{t&amp;gt;0}\quad e^{\frac{\sigma^2 t^2}{2} - \epsilon t} \\
&amp;amp;= e^{-\frac{\epsilon^2}{2\sigma^2}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A direct application of the above theorem on the sum of i.i.d.
zero-mean Gaussians gives the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Chernoff-based concentration inequality for
zero-mean Gaussians&lt;/strong&gt;. Given i.i.d. sample &lt;span class=&#34;math inline&#34;&gt;\(z_1, \dots, z_n \sim \mathcal{N}(0,
\sigma^2)\)&lt;/span&gt;, we have the following error bound for empirical mean
&lt;span class=&#34;math inline&#34;&gt;\(\hat \mu_n = \frac{1}{n} \sum_{i=1}^{n}
z_i\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat \mu_n - \mu \ge \epsilon) \le e^{-\frac{n \epsilon^2}{2
\sigma^2}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id=&#34;sub-gaussians&#34;&gt;Sub-Gaussians&lt;/h5&gt;
&lt;p&gt;The key step in the illustration of Chernoff’s inequality for the
Gaussian case is the derivation of MGF. On the wide spectrum of
non-Gaussian distributions, we focus on those whose MGF is smaller than
that of some zero-mean Gaussians.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;sub-Gaussian random variables&lt;/strong&gt;. We call
random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; sub-Gaussian with parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; if the MGF of &lt;span class=&#34;math inline&#34;&gt;\(Z-\mu\)&lt;/span&gt; satisfies the following inequality
for all &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
M_{Z-\mu}(t) \le \exp(\frac{\sigma^2 t^2}{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the way, it is interesting to know that if a random variable is
sub-Gaussian with parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, then its variance is
upper-bounded by &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;sum of independent sub-Gaussians&lt;/strong&gt;. If
&lt;span class=&#34;math inline&#34;&gt;\(Z_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z_2\)&lt;/span&gt; are two independent sub-Gaussian
random variables with parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2^2\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(Z_1 + Z_2\)&lt;/span&gt; will be sub-Gaussian with
parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2 +
\sigma_2^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;MGF of scalar product of sub-Gaussians&lt;/strong&gt;.
If &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is a sub-Gaussian random
variable with parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;,
then &lt;span class=&#34;math inline&#34;&gt;\(c Z\)&lt;/span&gt; for scalar &lt;span class=&#34;math inline&#34;&gt;\(c \in \R\)&lt;/span&gt; will be sub-Gaussian with
parameter &lt;span class=&#34;math inline&#34;&gt;\(c^2 \sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can draw similar conclusions for sub-Gaussians.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Chernoff tail bound for sub-Gaussians&lt;/strong&gt;. The
optimized Chernoff tail bound for sub-Gaussian &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; with parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Pr(Z-\mu \ge \epsilon) \le \exp(-\frac{\epsilon^2}{2\sigma^2})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Chernoff-based concentration inequality for
sub-Gaussians&lt;/strong&gt;. Given i.i.d. sub-Gaussian samples &lt;span class=&#34;math inline&#34;&gt;\(z_1, \dots, z_n\)&lt;/span&gt; with parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, we have the following error bound
for empirical mean &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu_n = \frac{1}{n}
\sum_{i=1}^{n} z_i\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat \mu_n - \mu \ge \epsilon) \le \exp(-\frac{n \epsilon^2}{2
\sigma^2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now it remains the question that what kind of random variables are
sub-Gaussians.&lt;/p&gt;
&lt;h6 id=&#34;rademacher-distribution&#34;&gt;Rademacher Distribution&lt;/h6&gt;
&lt;p&gt;We first introduce the Rademacher random variable &lt;span class=&#34;math inline&#34;&gt;\(X_\mathsf{R}\)&lt;/span&gt; whose PMF is &lt;span class=&#34;math display&#34;&gt;\[
X_\mathsf{R} = \begin{cases}
+1 &amp;amp; \text{w.p. $1/2$} \\
-1 &amp;amp; \text{w.p. $1/2$}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can obtain its MGF as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
M_{X_\mathsf{R}}(t) &amp;amp;= \E[e^{t X}] = \frac{1}{2} (e^t + e^{-t}) \\
&amp;amp;= \frac{1}{2} \sum_{k=0}^\infty (\frac{t^k}{k!} +
\frac{(-t)^k}{k!}) \\
&amp;amp;= \sum_{s=0}^\infty \frac{t^{2s}}{(2s)!} \\
&amp;amp;\le \sum_{s=0}^\infty \frac{t^{2s}}{2^s(s)!} \\
&amp;amp;= \sum_{s=0}^\infty \frac{(t^2/2)^s}{s!} \\
&amp;amp;= e^{t^2/2}
\end{aligned}
\]&lt;/span&gt; This indicates that &lt;span class=&#34;math inline&#34;&gt;\(X_\mathsf{R}\)&lt;/span&gt; is sub-Gaussian with
parameter &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h6 id=&#34;bounded-random-variable&#34;&gt;Bounded Random Variable&lt;/h6&gt;
&lt;p&gt;Next we show that a random variable &lt;span class=&#34;math inline&#34;&gt;\(a \le
Z \le b\)&lt;/span&gt; for scalars &lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt; is
sub-Gaussian with parameter &lt;span class=&#34;math inline&#34;&gt;\((b-a)^2\)&lt;/span&gt;. To show it, we apply the
&lt;strong&gt;symmetrization trick&lt;/strong&gt;, creating another random variable
&lt;span class=&#34;math inline&#34;&gt;\(Z&amp;#39;\)&lt;/span&gt; i.i.d. as &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;M_{Z-\E[Z]}(t) = \E_Z[e^{t(Z-\E[Z])}] \\
&amp;amp;= \E_Z[e^{t(Z-\E[Z&amp;#39;])}] \\
&amp;amp;\Downarrow_{\text{Jenson&amp;#39;s inequality}} \\
&amp;amp;\le \E_Z \E_{Z&amp;#39;}[e^{t(Z-Z&amp;#39;)}] \\
&amp;amp;= \E_{Z, Z&amp;#39;} [e^{t(Z-Z&amp;#39;)}]
\end{aligned}
\]&lt;/span&gt; Directly concluding that &lt;span class=&#34;math inline&#34;&gt;\(M_{Z-\E[Z]}(t) \le e^{t(b-a)}\)&lt;/span&gt; is neither
interesting nor helpful in resulting a sub-Gaussian. We introduce
another Rademacher random variable &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma, Z, Z&amp;#39;\)&lt;/span&gt; are independent. We
have &lt;span class=&#34;math display&#34;&gt;\[
Z-Z&amp;#39; \stackrel{d}{=} \sigma(Z-Z&amp;#39;)
\]&lt;/span&gt; Also note that &lt;span class=&#34;math inline&#34;&gt;\((Z-Z&amp;#39;) \le
(b-a)^2\)&lt;/span&gt;. Hence, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;M_{Z-\E[Z]}(t) \le \E_{Z, Z&amp;#39;} [e^{t(Z-Z&amp;#39;)}] \\
&amp;amp;= \E_{Z, Z&amp;#39;, \sigma} [e^{[t(Z-Z&amp;#39;)]\sigma}] \\
&amp;amp;= \E_{Z, Z&amp;#39;} \E_{\sigma} [e^{[t(Z-Z&amp;#39;)]\sigma}] \\
&amp;amp;\le \E_{Z, Z&amp;#39;} e^{\frac{t^2(Z-Z&amp;#39;)^2}{2}} \\
&amp;amp;\le e^{\frac{t^2(b-a)^2}{2}} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This indicates that &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is
sub-Gaussian with parameter &lt;span class=&#34;math inline&#34;&gt;\((b-a)^2\)&lt;/span&gt;. In fact, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;​’s sub-Gaussian parameter can be
further tightened.&lt;/p&gt;
&lt;h4 id=&#34;hoeffdings-inequality&#34;&gt;Hoeffding’s Inequality&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Lemma: &lt;strong&gt;Hoeffding’s lemma&lt;/strong&gt;. Suppose that random
variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is bounded and satisfies
&lt;span class=&#34;math inline&#34;&gt;\(a \le Z \le b\)&lt;/span&gt; for scalars &lt;span class=&#34;math inline&#34;&gt;\(a,b \in \R\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is sub-Gaussian with parameter &lt;span class=&#34;math inline&#34;&gt;\(\frac{(b-a)^2}{4}\)&lt;/span&gt;. That is, we have &lt;span class=&#34;math display&#34;&gt;\[
M_{Z-\mu}(t) \le \exp(\frac{t^2 (b-a)^2}{8})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A direct application of Hoeffding’s lemma and Chernoff tail bound for
sub-Gaussians gives the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Hoeffding’s inequality&lt;/strong&gt;. Suppose that random
variables &lt;span class=&#34;math inline&#34;&gt;\(Z_1, \dots, Z_n\)&lt;/span&gt; are
independent and bounded as &lt;span class=&#34;math inline&#34;&gt;\(a_i \le Z_i \le
b_i\)&lt;/span&gt;. Then defining the empirical mean &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu_n \triangleq \frac{1}{n} \sum_{i=1}^n
Z_i\)&lt;/span&gt; and the underlying mean &lt;span class=&#34;math inline&#34;&gt;\(\mu
\triangleq \frac{1}{n} \sum_{i=1}^n \E[Z_i]\)&lt;/span&gt; results in the
following concentration inequality &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat \mu_n - \mu \ge \epsilon) \le \exp\left( -\frac{2n^2
\epsilon^2}{\sum_{i=1}^n (b_i-a_i)^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Hoeffding’s concentration inequality for bounded
random variables&lt;/strong&gt;. Given i.i.d. bounded samples &lt;span class=&#34;math inline&#34;&gt;\(a \le z_1, \dots, z_n \le b\)&lt;/span&gt; with mean
&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, we have the following error
bound for empirical mean &lt;span class=&#34;math inline&#34;&gt;\(\hat \mu_n =
\frac{1}{n} \sum_{i=1}^{n} z_i\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat \mu_n - \mu \ge \epsilon) \le \exp\left( -\frac{2 n
\epsilon^2}{(b-a)^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;revisiting-excess-error&#34;&gt;Revisiting Excess Error&lt;/h3&gt;
&lt;p&gt;In quite a long run of paragraphs, we have been working with
statistics. Now let’s turn to uniform convergence analysis. We
assume&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;the usage of the zero/one-loss,&lt;/li&gt;
&lt;li&gt;and a finite set of hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F} = \{ f_1,\dots,f_t \}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;excess error bound for finite hypothesis
sets&lt;/strong&gt;. Under the above assumptions, the following excess risk
bound holds for the ERM solution &lt;span class=&#34;math inline&#34;&gt;\(\hat
f\)&lt;/span&gt; with probability at least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\epsilon_{\text{excess}}(\hat f) \le \sqrt{\frac{2(\log t +
\log\frac{2}{\delta})}{n}} =
\mathcal{O}(\sqrt{\frac{\log(t/\delta)}{n}})
\]&lt;/span&gt; Proof. According to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uniconv-3}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le \Pr(\cup_{1 \le i
\le t} |L(f_i) - \hat L(f_i)| \ge \frac{\epsilon}{2})
\]&lt;/span&gt; We can apply the union bound and Hoeffding’s lemma to further
show that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\Pr(\cup_{1 \le i \le t} |L(f_i) - \hat L(f_i)| \ge
\frac{\epsilon}{2}) \le \sum_{i=1}^t \Pr(|L(f_i) - \hat L(f_i)| \ge
\frac{\epsilon}{2}) \\
&amp;amp;= \sum_{i=1}^t [\Pr(\hat L(f_i) - L(f_i) \ge \frac{\epsilon}{2}) +
\Pr(\underbrace{(-\hat L(f_i))}_{\hat \mu&amp;#39;} -
\underbrace{(-L(f_i))}_{\mu&amp;#39;}) \ge \frac{\epsilon}{2})] \\
&amp;amp;\le \sum_{i=1}^t [\exp\left( -\frac{2n (\epsilon/2)^2}{(1-0)^2}
\right) + \exp\left( -\frac{2n (\epsilon/2)^2}{(1-0)^2} \right)] \\
&amp;amp;= 2t \exp(-\frac{n\epsilon^2}{2})
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le 2t \exp(-\frac{n
\epsilon^2}{2})
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\delta = 2t \exp(-\frac{n
\epsilon^2}{2})\)&lt;/span&gt;. Then we can draw that, with probability at
least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\epsilon_\text{excess}(\hat f) \le \sqrt{\frac{2(\log (2t/\delta))}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>事件与概率</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/</link>
      <pubDate>Fri, 09 Dec 2022 17:37:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/</guid>
      <description>

&lt;h2 id=&#34;事件的运算&#34;&gt;事件的运算&lt;/h2&gt;
&lt;h3 id=&#34;事件的包含和相等&#34;&gt;事件的包含和相等&lt;/h3&gt;
&lt;p&gt;同一试验下的两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，如果&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;发生时&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;也发生，则称&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;包含&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(A
\subseteq B\)&lt;/span&gt;。如果&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;互相包含，则二者相等。&lt;/p&gt;
&lt;h3 id=&#34;事件的互斥和对立&#34;&gt;事件的互斥和对立&lt;/h3&gt;
&lt;p&gt;若两事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;不能再同一试验中同时发生，则称它们是互斥的。互斥事件的一个特例是对立事件，对于事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，其对立事件&lt;span class=&#34;math inline&#34;&gt;\(\bar A\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\{
\text{$A$不发生} \}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;事件的和并与加法定理&#34;&gt;事件的和（并）与加法定理&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$发生，或$B$发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的和，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A + B\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;概率论中的&lt;strong&gt;加法定理&lt;/strong&gt;描述的则是：若干互斥事件之和的概率，等于各事件的概率之和。&lt;/p&gt;
&lt;h4 id=&#34;全概率公式&#34;&gt;全概率公式&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(B_1, B_2,
\cdots\)&lt;/span&gt;为有限或无限个事件，它们两两互斥且在每次实验中至少发生一个（mutually
exclusive and collectively exhaustive），即 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_i B_j = \emptyset (i \ne j) \\
B_1 + B_2 + \cdots = \Omega \\
P(B_1 + B_2 + \cdots) = P(\Omega) = 1
\end{gather}
\]&lt;/span&gt; 根据事件运算的性质，&lt;span class=&#34;math inline&#34;&gt;\(A = A \Omega =
A B_1 + A B_2 + \cdots\)&lt;/span&gt;，由于&lt;span class=&#34;math inline&#34;&gt;\(B_1,
B_2, \cdots\)&lt;/span&gt;两两互斥，则显然&lt;span class=&#34;math inline&#34;&gt;\(A B_1, A
B_2, \cdots\)&lt;/span&gt;也两两互斥，故依据加法定理，有 &lt;span class=&#34;math display&#34;&gt;\[
P(A) = \sum_i P(A B_i) = P(A B_1) + P(A B_2) + \cdots
\]&lt;/span&gt; 再根据条件概率公式，有 &lt;span class=&#34;math display&#34;&gt;\[
P(A) = \sum_i P(B_i) P(A | B_i) = P(B_1) P(A|B_1) + P(B_2) P(A|B_2) +
\cdots
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这便是全概率公式。&lt;/p&gt;
&lt;h3 id=&#34;事件的积交&#34;&gt;事件的积（交）&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$，$B$都发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的积，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A B\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;事件的差&#34;&gt;事件的差&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$发生，$B$不发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的差，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A - B\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我们对事件引入了和、积、差、对立运算。显然，这些运算符在数字运算中成立的运算规律，不一定对事件运算成立，比如说&lt;span class=&#34;math inline&#34;&gt;\(A + A = A\)&lt;/span&gt;而非&lt;span class=&#34;math inline&#34;&gt;\(2 A\)&lt;/span&gt;（无意义），&lt;span class=&#34;math inline&#34;&gt;\(A A = A\)&lt;/span&gt;而非&lt;span class=&#34;math inline&#34;&gt;\(A^2\)&lt;/span&gt;（无意义），&lt;span class=&#34;math inline&#34;&gt;\((A - B) + B = A + B\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;。基于这些运算，我们还能得出一些其他性质，比如&lt;span class=&#34;math inline&#34;&gt;\(A-B=A \bar B\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(A(B-C)=AB-AC\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;基于这些基本运算，我们也可以表示出更多的事件，比如：&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 84%&#34;/&gt;
&lt;col style=&#34;width: 15%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;表达式&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(ABC\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者同时发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A B \bar C +
A B \bar C + \bar A B C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者有且仅有两件发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \bar B
\bar C + \bar A B \bar C + \bar A \bar B C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者有且仅有一件发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A + B +
C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者至少其中之一发生&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;条件概率与独立性&#34;&gt;条件概率与独立性&lt;/h2&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(P(B) \ne
0\)&lt;/span&gt;，记&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)\)&lt;/span&gt;为“在给定&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;发生的条件下&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;的条件概率”，则 &lt;span class=&#34;math display&#34;&gt;\[
P(A|B) = \frac{P(AB)}{P(B)}
\]&lt;/span&gt; 在计数测度之下，这个式子很好证明。记&lt;span class=&#34;math inline&#34;&gt;\(M_A\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M_B\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M_{AB}\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;同时发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;为所有基本事件个数，则 &lt;span class=&#34;math display&#34;&gt;\[
P(A|B) = \frac{M_{AB}}{M_B} = \frac{M_{AB}/M}{M_B/M} =
\frac{P(AB)}{P(B)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;两个事件的独立&#34;&gt;两个事件的独立&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)\)&lt;/span&gt;可能是有差异的，这个差异便反映了二者之间的关联。而如果&lt;span class=&#34;math inline&#34;&gt;\(P(A) =
P(A|B)\)&lt;/span&gt;，则称这两事件独立。根据条件概率的定义，两事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;独立时，有 &lt;span class=&#34;math display&#34;&gt;\[
P(AB) = P(A) P(B)
\]&lt;/span&gt;
我们往往是根据事件属性，而非根据以上公式，来确定事件的独立性。比如说在连续抛两次硬币的试验中，这两次试验的结果之间确实不应该有什么关联，我们自然而然地认为它们之间是相互独立的（&lt;strong&gt;相互独立同分布&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;有时我们会觉得所有的独立事件可能都来自于这样的多次相互独立同分布试验，但其实独立事件也可以来自一次试验，比如说从52张扑克牌中随机抽取一张，记事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为抽中红桃花色、事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;为抽中数字6，则可以验证，这两个事件也是相互独立的。&lt;/p&gt;
&lt;h3 id=&#34;相互独立与乘法定理&#34;&gt;相互独立与乘法定理&lt;/h3&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
\cdots\)&lt;/span&gt;为有限或无限个事件，如果从其中任意取出有限个&lt;span class=&#34;math inline&#34;&gt;\(A_{i_1}, A_{i_2}, \cdots, A_{i_m}\)&lt;/span&gt;，都有
&lt;span class=&#34;math display&#34;&gt;\[
P(A_{i_1} A_{i_2} \cdots A_{i_m}) = P(A_{i_1}) P(A_{i_2}) \cdots
P(A_{i_m})
\]&lt;/span&gt; 则称事件&lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
\cdots\)&lt;/span&gt;相互独立。概率论中的&lt;strong&gt;乘法定理&lt;/strong&gt;描述的是：多个事件相互独立时，它们同时发生的概率，等于各自概率的乘积。&lt;/p&gt;
&lt;p&gt;需要注意的是，多个事件之间两两独立并不意味着它们相互独立，比如在掷两次硬币的实验中，定义以下事件：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
A：第一次为正， P(A) = 1/2 \\
B：第二次为反， P(B) = 1/2 \\
C：两次结果相同， P(C) = 1/2
\end{gathered}
\]&lt;/span&gt; 则可以轻易验证三者两两独立，但是&lt;span class=&#34;math inline&#34;&gt;\(P(ABC) = 0 \ne 1/8 = P(A) P(B)
P(C)\)&lt;/span&gt;。这三者的关系有如下图中的三个环：两两之间本可以相互分开（独立），但三者同在，便互相捆绑住了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./borromean-rings-illusion.png&#34; style=&#34;max-width: 30%;&#34;/&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mutual Information</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</link>
      <pubDate>Thu, 19 May 2022 12:20:04 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</guid>
      <description>

&lt;p&gt;Mutual information of &lt;strong&gt;two random variables&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a measure of the mutual independence
between them. It quantifies the amount of information obtained about one
random variable by observing the other random variable. It is defined as
&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]&lt;/span&gt; By its definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I(X;Y) = I(Y;X) \\
I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \otimes p_Y) \\
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;To better illustrate the formula of mutual information between two &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian-distributed&lt;/a&gt; random
variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. We can concatenate them to form, say
an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional random variable
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, which is also
Gaussian-distributed. Then the mutual information between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be computed as: &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det
\Sigma_Z}
\]&lt;/span&gt; The key to the derivation is that mutual information is the
KL-divergence between the joint distribution and the product of the
marginal distributions.&lt;/p&gt;
&lt;p&gt;The joint can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X:Y} = N(\underbrace{\mu_X:\mu_Y}_\mu,
\underbrace{
\begin{bmatrix} \
\Sigma_{X} &amp;amp; \Cov_{XY} \\
\Cov_{YX} &amp;amp; \Sigma_{Y} \\
\end{bmatrix}
}_\Sigma
)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The product of marginals can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X} \times p_{Y} = N(\mu_x:\mu_y,
\begin{bmatrix} \
\Sigma_{xx} &amp;amp; 0 \\
0 &amp;amp; \Sigma_{yy} \\
\end{bmatrix}
)
\]&lt;/span&gt; The probability density function of an &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;-dimensional Gaussian distribution
is &lt;span class=&#34;math inline&#34;&gt;\(p(x&amp;#39;) = \frac{1}{\sqrt{|2\pi
\Sigma&amp;#39;|}}e^{-\frac{1}{2}(x&amp;#39;-\mu&amp;#39;)^T\Sigma&amp;#39;^{-1}(x&amp;#39;-\mu&amp;#39;)}\)&lt;/span&gt;.
The entropy of this Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 n&amp;#39; + \frac 1 2 \log
|2\pi\Sigma&amp;#39;|\)&lt;/span&gt;. In view of above, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \times p_Y)
= \int p_{X:Y}(\underbrace{x:y}_z) \log \frac{p_{X:Y}(x:y)} {p_X(x)
p_{Y}(y)} \d z \\
&amp;amp;= \int p_{X:Y}(\underbrace{x:y}_z) \log p_{X:Y}(x:y) \d z -
    \int p_{X:Y}(\underbrace{x:y}_z) \log p_X(x) \d z \\
&amp;amp;\quad\quad\quad -\int p_{X:Y}(\underbrace{x:y}_z) \log p_Y(y) \d z
\\
&amp;amp;= \int p_{Z}(z) \log p_{Z}(z) \d z -
    \int p_{X}(x) \log p_X(x) \d x -
    \int p_{Y}(y) \log p_Y(y) \d y \\
&amp;amp;= -(\log \sqrt{\det(2\pi \Sigma)} + \frac n 2) +
    (\log \sqrt{\det(2\pi \Sigma_{X}}) + \frac {n_X} 2) \\
&amp;amp;\quad\quad\quad +(\log \sqrt{\det(2\pi \Sigma_{Y}}) + \frac {n_Y}
2) \\    
&amp;amp;= \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;kronecker-gaussian&#34;&gt;Kronecker Gaussian&lt;/h4&gt;
&lt;p&gt;Consider the multivariate Gaussian distribution random vector &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; of the same length &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. Suppose they are both independent
internally and they have the component-wise correlation coefficient
&lt;span class=&#34;math inline&#34;&gt;\(\text{corr\_coef}(X_i, Y_j) = \delta_{ij}
\rho\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\rho \in (-1,
1)\)&lt;/span&gt; (open to ensure the covariance matrix is invertible), &lt;span class=&#34;math inline&#34;&gt;\(1 \le i, j \le k\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\delta_{ij}\)&lt;/span&gt; is the Kronecker’s delta:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{ij} =
\begin{cases}
0, &amp;amp; i \ne j \\
1, &amp;amp; i = j
\end{cases}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z_k\)&lt;/span&gt; be the vector
concatenated by &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;. It is easy to draw its covariance
matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{Z_k}\)&lt;/span&gt; like &lt;span class=&#34;math display&#34;&gt;\[
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{pmatrix}
\]&lt;/span&gt; The mutual information between the &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_{X_k} \det \Sigma_{Y_k}}{\det
\Sigma_{Z_k}} = -\frac 1 2 \log \det \Sigma_{Z_k}
\]&lt;/span&gt; The problem remains as how to compute &lt;span class=&#34;math inline&#34;&gt;\(\det \Sigma_{Z_{k}}\)&lt;/span&gt;. After applying the
Laplacian expansion along the first column, it remains to deal with the
determinants of following two matrices (dashed lines rule out the
row/column to be deleted): &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
A_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right)
}_\text{$2k$ columns},
B_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right)
}_\text{$2k$ columns}, \\
\det Z_k =
\left.
\begin{cases}
\det A_k - \rho \det B_k, &amp;amp; \text{$k$ is odd} \\
\det A_k + \rho \det B_k, &amp;amp; \text{$k$ is even} \\
\end{cases}
\right\}
\Rightarrow \det Z_k = \det A_k + (-1)^k \rho \det B_k
\end{gather}
\]&lt;/span&gt; Applying the Laplacian expansion along the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th row of &lt;span class=&#34;math inline&#34;&gt;\(A_k\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\det A_k =
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right|
}_\text{$2k$ columns}
= \det Z_{k-1}
\]&lt;/span&gt; Applying the Laplacian expansion along the first row of &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\det B_k = (-1)^k \rho
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right|
}_\text{$2k$ columns}
= (-1)^k \rho \det Z_{k-1}
\]&lt;/span&gt; In all, &lt;span class=&#34;math inline&#34;&gt;\(\det Z_k = \det Z_{k-1} -
(-1)^{2k} \rho^2 \det Z_{k-1} = (1 - \rho^2) Z_{k-1}\)&lt;/span&gt;. Because
&lt;span class=&#34;math inline&#34;&gt;\(\det Z_1 = 1 - \rho^2\)&lt;/span&gt;, we finally
have &lt;span class=&#34;math display&#34;&gt;\[
\det Z_k = (1 - \rho^2)^k
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = -\frac 1 2 \log \det \Sigma_{Z_k} = -\frac k 2 \log (1 -
\rho^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/438607/mutual-information-between-subsets-of-variables-in-the-multivariate-normal-distr&#34;&gt;Mutual
information between subsets of variables in the multivariate normal
distribution - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.nyu.edu/~kleeman/infolect7.pdf&#34;&gt;Information
Theory and Predictability Lecture 7: Gaussian Case&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>3-rademacher-complexity</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/3-rademacher-complexity/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/3-rademacher-complexity/</guid>
      <description>

&lt;h2 id=&#34;rademacher-complexity&#34;&gt;Rademacher Complexity&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note]&lt;/p&gt;
&lt;p&gt;Sometimes people use hypothesis set and function set interchangeably.
But to differentiate, a hypothesis &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is typically a feature function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; composed with a loss function &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this section, we deal with the &lt;strong&gt;infinite hypothesis
set&lt;/strong&gt;, but still adopt the zero-one loss.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;Rademacher complexity&lt;/strong&gt;. For a function
set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, given an i.i.d.
sample of points &lt;span class=&#34;math inline&#34;&gt;\(\{X_1, \dots, X_n\} \sim
X^n\)&lt;/span&gt; from a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt;, and a
statistically-independent Rademacher variables &lt;span class=&#34;math inline&#34;&gt;\(\sigma = [\sigma_1, \dots, \sigma_n]\)&lt;/span&gt;
(with i.i.d. components), &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;’s Rademacher complexity is
defined as &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\rade}{\mathsf{R}} \rade_{\mathcal{P},n}(\mathcal{H})
\triangleq \E_{X^n, \sigma} \left[ \sup_{h \in \mathcal{H}} \frac{1}{n}
\sum_{i=1}^n \sigma_i h(X_i) \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rademacher complexity reflects the generalization capability of the
function class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; w.r.t.
the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; given sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. A large Rademacher complexity
indicates that the model is able to transform the random variable of
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; to match a
statistically-independent variable such that the correlation can be
maximized. Or simply, a large Rademacher complexity indicates better
(over)fitting capability.&lt;/p&gt;
&lt;p&gt;In learning theory, given a function class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;, we usually define &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} \triangleq \{ h(x,y) \triangleq \ell(f(x), y): f \in
\mathcal{F} \}
\]&lt;/span&gt; In Rademacher complexity discussion, &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is still chosen to be
&lt;strong&gt;zero/one-loss&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bounding-via-mcdiarmids-inequality&#34;&gt;Bounding via McDiarmid’s
Inequality&lt;/h3&gt;
&lt;p&gt;Recall in the uniform convergence theorem, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation} \label{uniconv}
\Pr(\epsilon_\text{excess}(\hat f) \ge \epsilon) \le \Pr(\sup_{f \in
\mathcal{F}} |L(f) - \hat L(f)| \ge \frac{\epsilon}{2})
\end{equation}
\]&lt;/span&gt; The supremum on the right-hand side is not easy to deal with.
McDiarmid’s inequality comes handy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;McDiarmid’s inequality&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(f: \mathcal{X}^n \mapsto \R\)&lt;/span&gt; be a function
such that for every &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n,x_1&amp;#39;,\dots,x_n&amp;#39;\)&lt;/span&gt; the
following holds &lt;span class=&#34;math display&#34;&gt;\[
\forall 1 \le i \le n, |f(x_1,\dots,x_i,\dots,x_n) -
f(x_1,\dots,x_i&amp;#39;,\dots,x_n)| \le c_i
\]&lt;/span&gt; Then, assuming &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt; are the realizations of
independent random variables &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; respectively, we have &lt;span class=&#34;math display&#34;&gt;\[
\Pr(f(x_1,\dots,x_n) - \E[f(X_1,\dots,X_n)]   \ge \epsilon) \le
\exp(\frac{-2 \epsilon^2}{\sum_{i=1}^n c_i^2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note]&lt;/p&gt;
&lt;p&gt;McDiarmid’s inequality is a generalization of Hoeffding’s inequality.
Simply choosing &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;​ in the
McDiarmid’s inequality to be the empirical mean would recover the
Hoeffding’s inequality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now back to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uniconv}\)&lt;/span&gt;. For
convenience, we define the &lt;strong&gt;worst-case generalization
error&lt;/strong&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
G_n &amp;amp;\triangleq \sup_{f \in \mathcal{F}} [L(f) - \hat L(f)] \\
&amp;amp;= \sup_{f \in \mathcal{F}} [\E[\ell(f(X), Y)] - \frac{1}{n}
\sum_{i=1}^n \ell(f(X_i), Y_i)]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Treating &lt;span class=&#34;math inline&#34;&gt;\(G_{n}\)&lt;/span&gt; as the supremum
over &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; of function &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; defined on &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{F}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-sized sample, we have &lt;span class=&#34;math inline&#34;&gt;\(G_{n}(x_1,\dots,x_n) = \sup_{f \in \mathcal{F}}
g_f(x_1, \dots, x_n)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g_f(x_1,\dots,x_n) = L(f) - \hat L(f)\)&lt;/span&gt;.
Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\underbrace{g_f(x_1,\dots,x_i,\dots,x_n)}_{A(f)} -
\underbrace{g_f(x_1,\dots,x_i&amp;#39;,\dots,x_n)}_{B(f)} \\
&amp;amp;= \frac{1}{n} [\ell(f(x_i&amp;#39;), y_i&amp;#39;) - \ell(f(x_i), y_i)] \\
&amp;amp;\Downarrow_\text{$\ell$ is a zero/one-loss} \\
&amp;amp;\le \frac{1}{n}
\end{aligned}
\]&lt;/span&gt; &amp;gt; Lemma. If for every &lt;span class=&#34;math inline&#34;&gt;\(f \in
\mathcal{F}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(|A(f) - B(f)| \le
\epsilon\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(|\sup_{f} A(f) -
\sup_{f} B(f)| \le \epsilon\)&lt;/span&gt;. &amp;gt; &amp;gt; Proof. &amp;gt; &lt;span class=&#34;math display&#34;&gt;\[
&amp;gt; \begin{aligned}
&amp;gt; \sup_{f} A(f) - \sup_{f} B(f) \le \sup_f [A(f) - B(f)] \le \epsilon
\\
&amp;gt; \sup_{f} B(f) - \sup_{f} A(f) \le \sup_f [B(f) - A(f)] \le \epsilon
&amp;gt; \end{aligned}
&amp;gt; \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a result, &lt;span class=&#34;math display&#34;&gt;\[
\left| \sup_{f \in \mathcal{F}} g_f(x_1,\dots,x_i,\dots,x_n) - \sup_{f
\in \mathcal{F}} g_f(x_1,\dots,x_i&amp;#39;,\dots,x_n) \right| \le
\frac{1}{n}
\]&lt;/span&gt; Apply the McDiarmid’s inequality to give &lt;span class=&#34;math display&#34;&gt;\[
\Pr[G_n - \E[G_n] \ge \epsilon] \le \exp(-2n \epsilon^2)
\]&lt;/span&gt; The remaining step is to bound &lt;span class=&#34;math inline&#34;&gt;\(\E[G_n]\)&lt;/span&gt; (it has to be small for the above
to be meaningful). &lt;span class=&#34;math inline&#34;&gt;\(\E[G_n]\)&lt;/span&gt; is
actually an expectation over a sample &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;​. &lt;span class=&#34;math display&#34;&gt;\[
\E[G_n] = \E_\mathrm{X} \left[ \sup_{f \in \mathcal{F}} [L(f) - \hat
L_\mathrm{X}(f)] \right]
\]&lt;/span&gt; We again apply the symmetrization trick. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}&amp;#39;\)&lt;/span&gt; be a virtual sample
independent from &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}\)&lt;/span&gt; but
converge in distribution to &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(L(f) = \E[\hat L_{\mathrm{X}&amp;#39;}(f)]\)&lt;/span&gt;.
As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E[G_n] &amp;amp;= \E_\mathrm{X} \left[ \sup_{f \in \mathcal{F}}
[\E_{\mathrm{X}&amp;#39;}[\hat L_{\mathrm{X}&amp;#39;}(f)] - \hat
L_\mathrm{X}(f)] \right] \\
&amp;amp;= \E_\mathrm{X} \left[ \sup_{f \in \mathcal{F}}
[\E_{\mathrm{X}&amp;#39;}[\hat L_{\mathrm{X}&amp;#39;}(f) - \hat
L_\mathrm{X}(f)]] \right] \\
&amp;amp;\le \E_\mathrm{X} \left[ \E_{\mathrm{X}&amp;#39;} [\sup_{f \in
\mathcal{F}}[\hat L_{\mathrm{X}&amp;#39;}(f) - \hat L_\mathrm{X}(f)]]
\right] \\
&amp;amp;= \E_{\mathrm{X}, \mathrm{X}&amp;#39;} \sup_{f \in \mathcal{F}} [\hat
L_{\mathrm{X}&amp;#39;}(f) - \hat L_\mathrm{X}(f)]
\end{aligned}
\]&lt;/span&gt; Again we introduce another Rademacher random vector of length
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma, \mathrm{X}, \mathrm{X}&amp;#39;\)&lt;/span&gt; are
independent. Let &lt;span class=&#34;math inline&#34;&gt;\(Z_i \triangleq \ell(f(X_i),
Y_i), Z_i&amp;#39; \triangleq \ell(f(X_i&amp;#39;), Y_i&amp;#39;)\)&lt;/span&gt;. We have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
Z_i&amp;#39; - Z_i &amp;amp;\stackrel{d}{=}&amp;amp; \sigma_i [Z_i&amp;#39; - Z_i] \\
\hat L_\mathrm{X&amp;#39;}(f) - \hat L_\mathrm{X}(f)
&amp;amp;\stackrel{d}{=}&amp;amp; \frac{1}{n} \sum_{i=1}^n \sigma_i [Z_i&amp;#39; -
Z_i]
\end{gathered}
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[G_n] \le \E_{\mathrm{X}, \mathrm{X}&amp;#39;} \sup_{f \in
\mathcal{F}} [\hat L_{\mathrm{X}&amp;#39;}(f) - \hat L_\mathrm{X}(f)] \\
&amp;amp;= \E_{\mathrm{X}, \mathrm{X}&amp;#39;, \sigma} \sup_{f \in \mathcal{F}}
[\frac{1}{n} \sum_{i=1}^n \sigma_i [Z_i&amp;#39; - Z_i]] \\
&amp;amp;= \E_{\mathrm{X}&amp;#39;, \sigma} \sup_{f \in \mathcal{F}}
[\frac{1}{n} \sum_{i=1}^n \sigma_i Z_i&amp;#39;] \\
&amp;amp;\quad + \E_{\mathrm{X}, \sigma} \sup_{f \in \mathcal{F}}
[\frac{1}{n} \sum_{i=1}^n -\sigma_i Z_i] \\
&amp;amp;\le 2 \E_{\mathrm{X}, \sigma} \sup_{f \in \mathcal{F}} [\frac{1}{n}
\sum_{i=1}^n \sigma_i Z_i] \\
&amp;amp;= 2 \rade_{n}(\mathcal{\mathcal{H}})
\end{aligned}
\]&lt;/span&gt; Next note that excess error is bounded by twice the worst-case
generalization error: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\Pr[L(\hat f) - L(f^*) \ge \epsilon] \le \Pr[\sup_{f \in
\mathcal{F}} |L(f) - \hat L(f)| \ge \frac{\epsilon}{2}] \\
&amp;amp;= \Pr[\underbrace{\sup_{f \in \mathcal{F}} [L(f) - \hat
L(f)]}_{G_n} \ge \frac{\epsilon}{2}] + \Pr[\underbrace{\sup_{f \in
\mathcal{F}} [(-L(f)) - (-\hat L(f))]}_{G_n&amp;#39;} \ge
\frac{\epsilon}{2}] \\
&amp;amp;= \Pr[G_n - \E[G_n] \ge \frac{\epsilon}{2} - \E[G_n]] +
\Pr[G_n&amp;#39; - \E[G_n&amp;#39;] \ge \frac{\epsilon}{2} - \E[G_n&amp;#39;]] \\
&amp;amp;\le \exp[-2n(\frac{\epsilon}{2} - \E[G_n])^2] +
\exp[-2n(\frac{\epsilon}{2} - \E[G_n&amp;#39;])^2] \\
&amp;amp;\Downarrow_{-\mathcal{H} \triangleq \{ h(x,y) \triangleq
-\ell(f(x), y): f \in \mathcal{F} \}} \\
&amp;amp;\le \exp[-2n(\frac{\epsilon}{2} - 2\rade_n(\mathcal{H}))^2] +
\exp[-2n(\frac{\epsilon}{2} - 2\rade_n(\mathcal{-H}))^2] \\
&amp;amp;= 2 \exp[-2n(\frac{\epsilon}{2} - 2\rade_n(\mathcal{H}))^2]
\end{aligned}
\]&lt;/span&gt; Putting things together, we have&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;excess error bound via Rademacher
complexity&lt;/strong&gt;. For a hypothesis set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;, define &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} = \{ h(x,y) \triangleq \mathbb{1}[f(x) \ne y]: f \in
\mathcal{F} \}
\]&lt;/span&gt; Then, with probability at least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
L(\hat f) - L(f^*) \le 4 \rade_n(\mathcal{H}) + \sqrt{\frac{2
\log(2/\delta)}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here we emphasize that, a small Rademacher complexity implies a small
gap between ERM model and PRM model. But they may as well be equally
bad. It is just that we usually assume a very low population risk.&lt;/p&gt;
&lt;h3 id=&#34;comparison-of-convergence-bounds&#34;&gt;Comparison of Convergence
Bounds&lt;/h3&gt;
&lt;p&gt;Realizability case corresponds to the noiseless setting;
non-realizability case corresponds to the noisy setting.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 16%&#34;/&gt;
&lt;col style=&#34;width: 5%&#34;/&gt;
&lt;col style=&#34;width: 8%&#34;/&gt;
&lt;col style=&#34;width: 10%&#34;/&gt;
&lt;col style=&#34;width: 20%&#34;/&gt;
&lt;col style=&#34;width: 38%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;0/1-loss&lt;/th&gt;
&lt;th&gt;Realizability&lt;/th&gt;
&lt;th&gt;Finite Hypothesis&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_i-\mu\)&lt;/span&gt; Assumption&lt;/th&gt;
&lt;th&gt;Excess Risk Bound&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Introductory case&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\log t/\delta}{n}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Via Markov’s inequality&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;Expectation and variance exists.&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(2 \sigma\sqrt{\frac{t}{n
\delta}}\)&lt;/span&gt; (derived on my own)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Via Chernoff’s inequality&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;Sub-Gaussian&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8\log
(2t/\delta)}{n}}\)&lt;/span&gt; (derived on my own)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Via Hoeffding’s inequality&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;Bounded&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{2\log
(2t/\delta)}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Via McDiarmid’s Inequality&lt;/td&gt;
&lt;td&gt;✔&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(4 \mathsf{R}_n(\mathcal{H}) +
\sqrt{\frac{2 \log(2/\delta)}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;empirical-rademacher-complexity&#34;&gt;Empirical Rademacher
Complexity&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;empirical Rademacher complexity&lt;/strong&gt;. For a
hypothesis set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; and
fixed dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} = \{ x_1,\dots,x_n
\}\)&lt;/span&gt;, we define &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;’s empirical Rademacher
complexity as &lt;span class=&#34;math display&#34;&gt;\[
\hat \rade_{\mathrm{X}}(\mathcal{H}) \triangleq \E_{\sigma} \left[
\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \sigma_i h(x_i)
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Can we approximate Rademacher complexity via sampling? That is, given
a fixed dataset, can we use stochastic optimization (gradient ascent) to
obtain the max of the model’s capability. The answer is yes. Refer to
the Question 6 of Homework 1.&lt;/p&gt;
&lt;h2 id=&#34;rademacher-complexity-algebra&#34;&gt;Rademacher Complexity
Algebra&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;basic properties of Rademacher
complexity&lt;/strong&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Monotonicity&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1 \subseteq \mathcal{H}_2\)&lt;/span&gt;,
then &lt;span class=&#34;math inline&#34;&gt;\(\rade(\mathcal{H}_1) \le
\rade(\mathcal{H}_2)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Singleton set&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H} = \{ h \}\)&lt;/span&gt; contains only one
function, then &lt;span class=&#34;math inline&#34;&gt;\(\rade(\mathcal{H}) =
0\)&lt;/span&gt;​​​.&lt;/p&gt;
&lt;p&gt;Proof. See below: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\rade_{\mathcal{P},n}(\mathcal{H}) &amp;amp;= \E_{X^n, \sigma} \left[
\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \sigma_i h(X_i)
\right] \\
&amp;amp;= \E_{X^n, \sigma} \left[\frac{1}{n} \sum_{i=1}^n \sigma_i h(X_i)
\right] \\
&amp;amp;= \frac{1}{n} \sum_{i=1}^n \E[\sigma_i] \E[h(X_i)] = 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non-negativity&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\rade_n(\mathcal{H}) \ge 0\)&lt;/span&gt; for any
non-empty &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;​.&lt;/p&gt;
&lt;p&gt;Proof. This is immediate after the property monotonicity and property
singleton set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scalar product&lt;/strong&gt;: For constant &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c
\mathcal{H} \triangleq \{ c h: h \in \mathcal{H} \}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rade_n(c \mathcal{H}) = |c|
\rade_n(\mathcal{H})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lipschitz composition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(g: \R \mapsto \R\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-Lipschitz function, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\forall z, z&amp;#39; \in \R, |g(z) - g(z&amp;#39;)| \le \rho |z - z&amp;#39;|
\]&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\rade_n(g \circ \mathcal{H})
\le \rho \rade_n(\mathcal{H})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Convex hull&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For a hypothesis set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H} = \{
h_1,\dots,h_t \}\)&lt;/span&gt;, we define its convex hull as &lt;span class=&#34;math display&#34;&gt;\[
\textrm{convex-hull}(\mathcal{H}) \triangleq \{ \sum_{i=1}^t \alpha_i
h_i: \alpha_1,\dots,\alpha_t \ge 0, \sum_{i=1}^t \alpha_i = 1 \}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(\rade_n(\textrm{convex-hull}(\mathcal{H})) =
\rade_n(\mathcal{H})\)&lt;/span&gt;​.&lt;/p&gt;
&lt;p&gt;Interestingly, a convex hull contains an infinite number of
hypothesis. But its Rademacher complexity does not increase with its
infinite cardinality.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;applications-of-rademacher-complexity&#34;&gt;Applications of
Rademacher Complexity&lt;/h2&gt;
&lt;p&gt;In this section, we study how to apply the Rademacher complexity to
various machine learning models.&lt;/p&gt;
&lt;h3 id=&#34;l_2-norm-bounded-linear-functions&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;-norm-bounded Linear Functions&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;empirical Rademacher complexity of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;-norm-bounded linear
functions&lt;/strong&gt;. Consider the following set of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;-norm bounded linear functions: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} = \{ h_w(x) \triangleq w^T x: \|w\|_2 \le M  \}
\]&lt;/span&gt; Then for a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} = \{
x_1,\dots,x_n \}\)&lt;/span&gt;, we have the following bound on the empirical
Rademacher complexity: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\rade}{\mathsf{R}} \hat \rade_{\mathrm{X}}(\mathcal{H}) \le
\frac{M \max_i \|x_i\|_2}{\sqrt{n}}
\]&lt;/span&gt; Proof. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[t]
&amp;amp;\hat \rade_{\mathrm{X}}(\mathcal{H}) = \E_{\sigma} \left[
\sup_{\|w\|_2 \le M} \frac{1}{n} \sum_{i=1}^n \sigma_i w^T x_i \right]
\\
&amp;amp;= \frac{1}{n} \E_{\sigma} \left[ \sup_{\|w\|_2 \le M} w^T
(\sum_{i=1}^n \sigma_i x_i) \right] \\
&amp;amp;\Downarrow_\text{optimization over a compact set} \\
&amp;amp;= \frac{1}{n} \E_{\sigma} \left[ \max_{\|w\|_2 \le M} w^T
(\sum_{i=1}^n \sigma_i x_i) \right] \\
&amp;amp;\Downarrow_\text{Cauchy-Schwartz inequality} \\
&amp;amp;= \frac{1}{n} \E_{\sigma} \left[ M \|\sum_{i=1}^n \sigma_i x_i\|_2
\right] \\
&amp;amp;= \frac{M}{n} \E_{\sigma} \left[ \|\sum_{i=1}^n \sigma_i x_i\|_2
\right] \\
\end{aligned} \quad \text{cont&amp;#39;d}
\begin{aligned}[t]
&amp;amp;\le \frac{M}{n} \sqrt{\E_{\sigma} \left[ \|\sum_{i=1}^n \sigma_i
x_i\|_2^2 \right]} \\
&amp;amp;= \frac{M}{n} \sqrt{\E_{\sigma} \left[ \sum_{i=1}^n \sum_{j=1}^n
\sigma_i \sigma_j x_i^T x_j \right]} \\
&amp;amp;= \frac{M}{n} \sqrt{\left[ \sum_{i=1}^n \sum_{j=1}^n \E[\sigma_i
\sigma_j] x_i^T x_j \right]} \\
&amp;amp;= \frac{M}{n} \sqrt{\sum_{i=1}^n \|x_i\|_2^2} \\
&amp;amp;\le \frac{M}{n} \sqrt{n \max_i \|x_i\|_2^2} \\
&amp;amp;= \frac{M \max_i \|x_i\|_2}{\sqrt{n}} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The implication of the above theorem is that excess risk is of order
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{O}(\frac{1}{\sqrt{n}})\)&lt;/span&gt;​.&lt;/p&gt;
&lt;h3 id=&#34;neural-network&#34;&gt;Neural Network&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;empirical Rademacher complexity of ReLU-based and
Frobenius-norm-bounded feedforward neural nets&lt;/strong&gt;. Consider the
following set of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-layer neural
nets with ReLU activation function: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} = \{ h(x) = W_L \psi_\text{ReLU}(W_{L-1} \dots
\psi_\text{ReLU}(W_1 x)): \forall i, \|W_i\|_F \le M \}
\]&lt;/span&gt; Then for a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} = \{
x_1,\dots,x_n \}\)&lt;/span&gt;, we have the following bound based on the
empirical Rademacher complexity: &lt;span class=&#34;math display&#34;&gt;\[
\hat \rade_{\mathrm{X}}(\mathcal{H}) \le \frac{(\sqrt{2L} + 1) M \max_i
\|x_i\|_2}{\sqrt{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[!note]&lt;/p&gt;
&lt;p&gt;Similar result can be derived for ReLU-like activation functions in
the above theorem. When we say ReLU-like, we mean that the activation
function needs to be Lipschitz and homogeneous for positive scalars
(e.g. &lt;span class=&#34;math inline&#34;&gt;\(\forall a &amp;gt; 0,
\mathop{\mathrm{ReLU}}(az) = a\mathop{\mathrm{ReLU}}(z)\)&lt;/span&gt;). These
two properties are critical in the proof.&lt;/p&gt;
&lt;p&gt;As an aside, linearity is homogeneity plus additivity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;massart-lemma&#34;&gt;Massart Lemma&lt;/h2&gt;
&lt;p&gt;Deriving bounds for different models can be tedious. It would be
immensely helpful to have a general rule or framework that allows us to
plug in different models and obtain the bounds more easily. Massart
lemma is one of such rule.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Massart lemma&lt;/strong&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H} = \{ h_1,\dots,h_t \}\)&lt;/span&gt; is a
finite set of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; functions. Also,
suppose that for every &lt;span class=&#34;math inline&#34;&gt;\(h \in
\mathcal{H}\)&lt;/span&gt; and dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}
= \{ x_1,\dots,x_n \}\)&lt;/span&gt; the following holds: &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{n} \sum_{i=1}^n h(x_i)^2 \le M
\]&lt;/span&gt; Then, the following bound on the empirical Rademacher
complexity holds: &lt;span class=&#34;math display&#34;&gt;\[
\hat \rade_{\mathrm{X}}(\mathcal{H}) \le \sqrt{\frac{2M \log t}{n}}
\]&lt;/span&gt; Proof. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[t]
&amp;amp;\hat \rade_\mathrm{X}(\mathcal{H}) = \frac{1}{n} \E_\sigma \left[
\sup_{h \in \mathcal{H}} \sum_{i=1}^n \sigma_i h(x_i) \right] \\
&amp;amp;\Downarrow_{\mathrm{h} = {[h(x_1),\dots,h(x_n)]}^\intercal} \\
&amp;amp;= \frac{1}{n} \E_\sigma \left[ \max_{\mathrm{h} \in \mathrm{H}}
\sigma^T \mathrm{h} \right] \\
&amp;amp;= \frac{1}{n} \E_\sigma \left[ \frac{1}{\lambda} \log
\max_{\mathrm{h} \in \mathrm{H}} \exp(\lambda \sigma^T \mathrm{h})
\right] \\
&amp;amp;\le \frac{1}{n} \E_\sigma \left[ \frac{1}{\lambda} \log
\sum_{j=1}^t \exp(\lambda \sigma^T \mathrm{h}_j) \right] \\
&amp;amp;\le \frac{1}{n \lambda} \log \E_\sigma \left[ \sum_{j=1}^t
\exp(\lambda \sigma^T \mathrm{h_j}) \right] \\
\end{aligned} \quad \text{cont&amp;#39;d}
\begin{aligned}[t]
&amp;amp;= \frac{1}{n \lambda} \log \sum_{j=1}^t \E_\sigma \left[
\exp(\lambda \sigma^T \mathrm{h}_j) \right] \\
&amp;amp;= \frac{1}{n \lambda} \log \sum_{j=1}^t \prod_{i=1}^n
\underbrace{\E \left[ \exp(\lambda \sigma_i^T h_j(x_i))
\right]}_{M_\sigma(\lambda h_j(x_i))} \\
&amp;amp;\le \frac{1}{n \lambda} \log \sum_{j=1}^t \prod_{i=1}^n
\exp(\frac{1}{2} h_j^2(x_i) \lambda^2)  \\
&amp;amp;\le \frac{1}{n \lambda} \log \sum_{j=1}^t \exp(\frac{nM
\lambda^2}{2} ) \\
&amp;amp;= \frac{\log t}{n \lambda} + \frac{M \lambda}{2} \\
&amp;amp;\le \sqrt{\frac{2M \lambda \log t}{n}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Massart lemma is essentially helpful in the derivation of Rademacher
complexity of various &lt;strong&gt;norm-bounded linear
functions&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;l_1-norm-bounded-linear-functions&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm-bounded Linear Functions&lt;/h3&gt;
&lt;p&gt;Massart lemma can be applied to &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm-bounded functions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;empirical Rademacher complexity of &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm-bounded linear
functions&lt;/strong&gt;. Consider the following set of &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm bounded linear functions: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} = \{ \R^d \to \R \ni h_w(x) \triangleq w^T x: \|w\|_1 \le
M  \}
\]&lt;/span&gt; Then for a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} = \{
x_1,\dots,x_n \}\)&lt;/span&gt;, we have the following bound on the empirical
Rademacher complexity: &lt;span class=&#34;math display&#34;&gt;\[
\hat \rade_{\mathrm{X}}(\mathcal{H}) \le M \max_i \|x_i\|_\infty
\sqrt{\frac{2 \log (2d)}{{n}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Proof. We highlight that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; is the convex hull of the
following set of functions: &lt;span class=&#34;math display&#34;&gt;\[
\tilde{\mathcal{H}} = \{ h_1(x),
h_2(x),\dots,h_d(x),-h_1(x),\dots,-h_d(x) \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h_i(x) = M x_i\)&lt;/span&gt;.
This is because, for any &lt;span class=&#34;math inline&#34;&gt;\(h_{w} \in
\mathcal{H}\)&lt;/span&gt;, we can rewrite it as &lt;span class=&#34;math display&#34;&gt;\[
h_w(x) = w^T x = \sum_{i=1}^n w_i x_i = \underbrace{\sum_{i=1}^n
\frac{|w_i|}{\|w\|_1}}_{\text{sum to 1}} \underbrace{\sign(w_i) \|w\|_1
x_i}_{\in \mathrm{convex-hull}(\tilde{\mathcal{H}})}
\]&lt;/span&gt; By Rademacher complexity’s property, we know that &lt;span class=&#34;math inline&#34;&gt;\(\hat \rade_\mathrm{X}(\tilde{\mathcal{H}}) = \hat
\rade_\mathrm{X}(\mathcal{H})\)&lt;/span&gt;. Hence, we may use Massart lemma
on &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{H}}\)&lt;/span&gt;. To bound on
&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n} \sum_{i=1}^n
h_w^2(x_i)\)&lt;/span&gt;, note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{n} \sum_{i=1}^n h_w^2(x_i) = \frac{1}{n} \sum_{i=1}^n (w^T
x_i)^2 \\
&amp;amp;\Downarrow_\text{Holder&amp;#39;s inequality} \\
&amp;amp;\le \frac{1}{n} \sum_{i=1}^n \|w\|_1^2 \|x_i\|_\infty^2 \\
&amp;amp;\le M^2 \max_i \|x_i\|_\infty^2
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{aligned}
\hat \rade_\mathrm{X}(\mathcal{H}) &amp;amp;\le \sqrt{\frac{2(M^2 \max_i
\|x\|_\infty^2) \log (2d)}{n}} \\
&amp;amp;= M \max_i \|x_i\|_\infty \sqrt{\frac{2 \log (2d)}{{n}}}
\end{aligned} \tag{By Holder&amp;#39;s Inequality}
\end{equation}
\]&lt;/span&gt; The derivation via Cauchy-Schwartz inequality is not as tight
as Holder’s inequality, which is to be shown below. Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{n} \sum_{i=1}^n h_w^2(x_i) = \frac{1}{n} \sum_{i=1}^n (w^T
x_i)^2 \\
&amp;amp;\Downarrow_\text{Cauchy-Schwartz inequality} \\
&amp;amp;\le \frac{1}{n} \sum_{i=1}^n \|w\|_2^2 \|x_i\|_2^2 \\
&amp;amp;\le \frac{1}{n} \sum_{i=1}^n \|w\|_1^2 \|x_i\|_2^2 \\
&amp;amp;\le M^2 \max_i \|x_i\|_2^2
\end{aligned}
\]&lt;/span&gt; From another perspective, since &lt;span class=&#34;math inline&#34;&gt;\(\|w\|_2 \le \|w\|_1 \le \sqrt{d} \|w\|_2\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\|w\|_1\)&lt;/span&gt; cannot be greater than
&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\|w\|_2\)&lt;/span&gt; is no greater than &lt;span class=&#34;math inline&#34;&gt;\(M \sqrt{d}\)&lt;/span&gt;. Thus, &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} \subseteq \mathcal{H}&amp;#39; \triangleq \{ h_w(x) = w^T x:
\|w\|_2 \le M\sqrt{d} \}
\]&lt;/span&gt; By Rademacher complexity’s property and from previous
conclusion on &lt;u&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;-norm-bounded
Linear Functions&lt;/u&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\hat \rade_\mathrm{X}(\mathcal{H}) \le \hat
\rade_{\mathrm{X}}(\mathcal{H&amp;#39;}) = M \max_i \|x_i\|_2
\sqrt{\frac{d}{n}} \tag{By Cauchy-Schwartz Inequality}
\end{equation}
\]&lt;/span&gt; Obviously, the bound derived with Massart lemma is tighter for
&lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm bounded linear functions,
because of the order of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and
because &lt;span class=&#34;math inline&#34;&gt;\(\|x\|_\infty \le
\|x\|_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;l_infty-norm-bounded-linear-functions&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_\infty\)&lt;/span&gt;-norm-bounded Linear
Functions&lt;/h3&gt;
&lt;p&gt;A similar bound can be derived for &lt;span class=&#34;math inline&#34;&gt;\(l_\infty\)&lt;/span&gt;-norm-bounded functions, which is
a convex hull of &lt;span class=&#34;math inline&#34;&gt;\(2^d\)&lt;/span&gt; points.&lt;/p&gt;
&lt;p&gt;Another application of Massart lemma is to be shown in
&lt;strong&gt;connecting the VC dimension and Rademacher
complexity&lt;/strong&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</link>
      <pubDate>Fri, 07 Jan 2022 12:52:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</guid>
      <description>

&lt;p&gt;Support vector machine is used in binary classification task. It aims
to find a &lt;strong&gt;linear hyperplane&lt;/strong&gt; that separates the data
with different labels with the maximum margin. By symmetry, there should
be at least one margin point on both side of the decision boundary.
These points are called &lt;strong&gt;support vectors&lt;/strong&gt;. Further, not
all points on the margin can be called support vectors. If we move
support vectors away from the margin, the decision hyperplane should
change.&lt;/p&gt;
&lt;h3 id=&#34;hard-margin-svm&#34;&gt;Hard-margin SVM&lt;/h3&gt;
&lt;p&gt;Suppose the data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)},
y^{(i)}), i=1, \dots, M\}\)&lt;/span&gt;, and is linearly separable. The
separation hyperplane will be in the form of &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is above the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;gt; 0\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = -1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is below the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;lt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The distance (margin) from a data point to the separation hyperplane
will be &lt;span class=&#34;math display&#34;&gt;\[
d^{(i)} = \frac{|w^Tx^{(i)} + b|}{||w||_2} = \frac{y^{(i)}(w^Tx^{(i)} +
b)}{||w||_2}\\
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d^{(i)}\)&lt;/span&gt; is called the
&lt;strong&gt;geometric distance&lt;/strong&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(|w^Tx^{(i)} + b|\)&lt;/span&gt; is called the
&lt;strong&gt;functional distance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;margin&lt;/strong&gt; of the hyperplane &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[
d = \min\limits_{i \in \{1,\dots,M\}} d^{(i)} = \min\limits_{i \in
\{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; The maximum margin solution is found by solving &lt;span class=&#34;math display&#34;&gt;\[
\max\limits_{w,b} \min\limits_{i \in \{1,\dots,M\}}
\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; Suppose &lt;span class=&#34;math inline&#34;&gt;\((w^\prime,
b^\prime)\)&lt;/span&gt; is one solution to the above. Then &lt;span class=&#34;math inline&#34;&gt;\((\lambda w^\prime, \lambda b^\prime)\)&lt;/span&gt; is
also a solution, because &lt;span class=&#34;math display&#34;&gt;\[
\frac{y^{(i)}((\lambda w^{\prime})^Tx^{(i)} + \lambda
b^\prime)}{||\lambda w^\prime||_2} = \frac{\lambda
y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{\lambda ||w^\prime||_2} =
\frac{y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{||w^\prime||_2}
\]&lt;/span&gt; There is an extra degree of freedom in this problem.
Therefore, we may impose &lt;span class=&#34;math display&#34;&gt;\[
\min\limits_{i \in \{1,\dots,M\}} y^{(i)}(w^Tx^{(i)}+b) = 1
\]&lt;/span&gt; to consume this freedom. Consequently, the problem becomes
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max \frac{1}{||w||_2} \iff \min \frac{1}{2}||w||^2_2 \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1, i = 1,\dots,M
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-margin-svm&#34;&gt;Soft-margin SVM&lt;/h3&gt;
&lt;p&gt;It may not happen that the real data are linearly-separable, or there
may exist noisy samples that disrupt this linear separability. In such
case, we may allow some samples to violate the margin. We define some
slack variables &lt;span class=&#34;math inline&#34;&gt;\(\xi_i \ge 0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;gt; 0\)&lt;/span&gt; means that the sample is
inside the margin (&lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;lt; 1\)&lt;/span&gt;),
on the boundary (&lt;span class=&#34;math inline&#34;&gt;\(\xi_i = 1\)&lt;/span&gt;), or
even worse, misclassified (&lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;gt;
1\)&lt;/span&gt;). &lt;span class=&#34;math inline&#34;&gt;\(\xi_i = 0\)&lt;/span&gt; means that
the sample is outside the margin.&lt;/p&gt;
&lt;p&gt;Of course, these slack variables should be as small as possible. Then
the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\min \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, \xi_i \ge 0,
i=1,\dots,M
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is a penalty factor on
violating the margin. To some extent, it avoid overfitting.&lt;/p&gt;
&lt;p&gt;To solve this, first convert the problem into the standard form:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \frac{1}{2}||w||^2_2 &amp;amp;+ C\sum_{i=1}^M\xi_i \\
s.t.\quad 1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b) &amp;amp;\le 0, i=1,\dots,M \\
-\xi_i &amp;amp;\le 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; This problem gives a strong duality. The solution will satisfy
the KKT conditions. We first write down the Lagrangian: &lt;span class=&#34;math display&#34;&gt;\[
L(w,b,\xi,\lambda,\mu) = \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i +
\sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b)) -
\sum_{i=1}^M\mu_i\xi_i
\]&lt;/span&gt; Then we form the dual problem: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\lambda,\mu;\lambda_i,\mu_i \ge 0}
\min_{w,b,\xi}L(w,b,\xi,\lambda,\mu)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial L}{\partial w} = w - \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)},
\frac{\partial^2 L}{\partial w\partial w} = I \succeq 0 \\
\frac{\partial L}{\partial b} = -\sum_{i=1}^M\lambda_iy^{(i)},
\frac{\partial^2 L}{\partial b\partial b} = 0 \succeq 0 \\
\frac{\partial L}{\partial \xi} = C - \lambda - \mu, \frac{\partial^2
L}{\partial \xi\partial \xi} = 0 \succeq 0
\end{gather}
\]&lt;/span&gt; The Hessian matrices of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; are positive semi-definite.
Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min\limits_{w,b,\xi}L(w,\xi,\lambda,\mu)\)&lt;/span&gt;
is obtained at its local minimum, i.e. where its first-order derivative
meets &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
w = \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)} \\
\sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda + \mu = C
\]&lt;/span&gt; Substitute above back to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; to transform the dual problem into
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}
\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + C\sum_{i=1}^M\xi_i -
\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}b) +
\sum_{i=1}^M(\lambda_i - C)\xi_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda_i,\mu_i \ge 0, i=1,\dots,M \\
\Downarrow \\
\max_{\lambda,\mu}D(\lambda,\mu) =
-\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; Since we know the optimal solution exists, instead of taking
derivative of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to solve it, we assume the
solution to above problem is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} \\
\mu^\star = C - \lambda^\star
\end{gathered}
\]&lt;/span&gt; Complementary constraints will give &lt;span class=&#34;math display&#34;&gt;\[
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} +
b^\star)) = 0 \\
\mu_i\xi_i = 0
\]&lt;/span&gt; There is at least one &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j &amp;gt; 0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(w^\star =
\sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} = 0\)&lt;/span&gt;, which means the
primal constraints &lt;span class=&#34;math display&#34;&gt;\[
1 - \xi_i - y^{(i)}((w^\star)^Tx^{(i)} + b^\star) =  1 - \xi_i -
y^{(i)}b^\star\le 0
\]&lt;/span&gt; won’t hold no matter what value &lt;span class=&#34;math inline&#34;&gt;\(b^\star\)&lt;/span&gt; takes. This specific &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j\)&lt;/span&gt;’s slackness complementary
will give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
0 &amp;amp;= (1 - \xi^\star_j - y^{(j)}((w^\star)^Tx^{(j)} + b^\star)) \\
b^\star &amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - (w^\star)^Tx^{(j)} \\
&amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} -
\sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)}\cdot x^{(j)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
According to the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt; (not known yet), added
with primal constraints and the complementary constraints, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0 \\
\xi_i \ge 0 \\
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} +
b^\star)) = 0 \\
\mu^\star_i\xi^\star_i = 0 \\
\lambda^\star_i + \mu^\star_i = C
\end{gather}
\]&lt;/span&gt; we can make interpretations on the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt;: $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\lambda^\star_i = 0 &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i = C \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \ge 1 \\
0 &amp;lt; \lambda^\star_i &amp;lt; C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i &amp;gt; 0 \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 1 \\
\lambda^\star_i = C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i = 0 \Rightarrow \xi^\star_i \ge 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 1 \\

\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ They corresponds to cases where sample &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is beyond the margin, is on the
margin, within the margin, respectively. The key to the above derivation
is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;, which is to be
solved in &lt;a href=&#34;#Solving%20SVM&#34;&gt;Solving SVM&lt;/a&gt; section.&lt;/p&gt;
&lt;h4 id=&#34;hinge-loss-perspective&#34;&gt;Hinge Loss Perspective&lt;/h4&gt;
&lt;p&gt;As an aside, soft-margin SVM is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\min \frac{1}{C}||w||^2_2 + \sum_{i=1}^M\max(0, 1 - y^{(i)}(w^Tx^{(i)} +
b))
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l_h(\hat y) = \max(0, 1 - y
\hat y)\)&lt;/span&gt; is called the &lt;strong&gt;hinge loss&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{C} \|w\|_2^2\)&lt;/span&gt; plays as the
regularization term.&lt;/p&gt;
&lt;h3 id=&#34;kernel-svm&#34;&gt;Kernel SVM&lt;/h3&gt;
&lt;p&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \cdot x^{(j)}\)&lt;/span&gt;
in &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda,\mu)\)&lt;/span&gt; is the inner
product between two samples. We can make a table storing these inner
products. This naturally introduces the kernel trick, which means we can
manipulate the entry in this table by remapping the &lt;span class=&#34;math inline&#34;&gt;\((x^{(i)}, x^{(j)})\)&lt;/span&gt; so that the entry
represents the inner product of some higher-dimensional features. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{(i)}) \cdot \phi(x^{(j)})
\]&lt;/span&gt; This saves us from explicitly defining the mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a higher-dimensional feature. This
also saves the time of the computation of the inner products of these
higher-dimensional features, than that of transform-then-inner-product
method.&lt;/p&gt;
&lt;p&gt;SVM is a linear classifier, which means its decision boundary is a
linear hyperplane in input feature space. By applying kernel trick, we
implicitly map the input feature to a higher dimensional one. Therefore
the decision boundary would become a linear hyperplane in this
higher-dimensional space: &lt;span class=&#34;math display&#34;&gt;\[
w_\phi\phi(x) + b_\phi = 0
\]&lt;/span&gt; And thus this hyperplane is not linear in original feature
space.&lt;/p&gt;
&lt;h4 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input feature &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1,\dots,x_N] \in
\R^N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order polynomial: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x, x^\prime) = (x^Tx^\prime)^p =
(\sum_{i=1}^Nx_ix^\prime_i)^p
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
K(x,x^\prime) = (\sum_{i=1}^Nx_ix^\prime_i)^2 =
(\sum_{i=1}^N\sum_{j=1}^Nx_ix_jx^\prime_ix^\prime_j)^2 =
\phi(x)\phi(x^\prime)
\]&lt;/span&gt; The transformation applied on original input feature will be
&lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = [x_1x_1,x_1x_2,\dots,x_1x_N,x_2x_1,\dots
x_2x_N,\dots,x_Nx_1,\dots,x_Nx_N] \in \R^{N^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is similar to a Gaussian &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = e^{-\gamma||x - x^\prime||^2_2}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a
hyper-parameter to be determined.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This kernel is the case where it is hard to define the
transformation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;
explicitly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-svm&#34;&gt;Solving SVM&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; is the key to the
final solution and by far it remains unsolved: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}D(\lambda,\mu) =
-\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; We can attack it by sequential minimal optimization.&lt;/p&gt;
&lt;h3 id=&#34;multi-class-svm&#34;&gt;Multi-class SVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 v.s. 1&lt;/p&gt;
&lt;p&gt;Train 1-vs-1 SVM for all pairs of classes. Then decide by majority
voting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1 v.s. rest&lt;/p&gt;
&lt;p&gt;Train 1-vs-rest SVM for all classes (&lt;span class=&#34;math inline&#34;&gt;\(y =
1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; belongs to the
“1”). Choose the class with the largest geometric margin.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</link>
      <pubDate>Mon, 03 Jan 2022 14:50:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</guid>
      <description>

&lt;h3 id=&#34;convex-and-differentiable-function&#34;&gt;Convex and Differentiable
Function&lt;/h3&gt;
&lt;p&gt;Given a convex, differentiable &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n
\mapsto \R\)&lt;/span&gt;, if we are at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is minimized along each coordinate
axis, have we found a global minima? That is, does &lt;span class=&#34;math display&#34;&gt;\[
\forall d, i,f(x + d \cdot e_i) \ge f(x) \Rightarrow f(x) = \min_zf(z),
\text{ where $e_i$ is the $i$-th standard basis ?}
\]&lt;/span&gt; The answer is yes. This is because &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(x) = \left[
\begin{array} \\
\frac{\partial f}{\partial x_1},
\cdots,
\frac{\partial f}{\partial x_n}
\end{array}
\right]
= 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convex-but-non-differentiable-function&#34;&gt;Convex but
Non-differentiable Function&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is only convex but not
differentiable, the above will not necessarily hold. However, if &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; can be decomposed such that &lt;span class=&#34;math display&#34;&gt;\[
f(x) = g(x) + \sum_{i=1}^nh_i(x_i)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; is convex and
differentiable, and each &lt;span class=&#34;math inline&#34;&gt;\(h_i(x_i)\)&lt;/span&gt;
is convex but possibly non-differentiable, the global minima still
holds. For any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;= g(y) - g(x) + \sum_{i=1}^nh_i(y_i) -
\sum_{i=1}^nh_i(x_i) \\
&amp;amp;\ge \nabla g(x)^T(y - x) + \sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\
&amp;amp;= \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)]
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; obtains
minimum along each axes, for any &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x + d \cdot e_k) &amp;amp;\ge f(x) \\
g(x + d \cdot e_k) + \sum_{i=1,i\ne k}^nh_i(x_i) + h_k(x_k + d) &amp;amp;\ge
g(x) + \sum_{i=1}^nh_i(x_i) \\
g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) &amp;amp;\ge 0 \\
\end{aligned}
\]&lt;/span&gt; By &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;the linearity of
subgradient&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial(g_k + h_k) =
\nabla_k g + \partial h_k \Rightarrow -\nabla_k g \in \partial
h_k\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
h_k(y_k) - h_k(x_k) \ge -\nabla_k g(x)(y_k - x_k)
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
f(y) - f(x) = \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) -
h_i(x_i)] \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD
&lt;a href=&#34;https://chunxy.github.io/uploads/Coordinate%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Coordinate
Descent.pdf&lt;/a&gt; ======= ## External Materials
&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; notes&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;Coordinate%20Descent.pdf&#34;&gt;Coordinate Descent.pdf&lt;/a&gt; || &lt;a href=&#34;https://zeyuan.wordpress.com/2016/06/13/coordinate-descent/&#34;&gt;Coordinate
Descent in One Line, or Three if Accelerated | A Butterfly Valley
(wordpress.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>FlatNCE</title>
      <link>https://chunxy.github.io/notes/papers/flatnce/</link>
      <pubDate>Sat, 27 Aug 2022 21:42:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/flatnce/</guid>
      <description>

&lt;p&gt;FlatNCE provides a way to compute the gradient of InfoNCE without
introducing the rounding error when subtracting between two similar
numbers.&lt;/p&gt;
&lt;p&gt;Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(g^\ominus_{ij}\)&lt;/span&gt;
is the affinity score between reference sample &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and negative (noise) sample &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; is the affinity score
between positive sample and itself/its transformation &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the batch index. Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{InfoNCE}\)&lt;/span&gt; the batch estimate
of the loss from InfoNCE: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{detach}{\mathop{\text{detach}}}
\newcommand{logsumexp}{\mathop{\text{logsumexp}}}
\begin{gather}
\hat l_\text{InfoNCE} = \logsumexp_j g^\ominus_{ij} - g^\oplus_{ii} =
\log (\sum_{j \ne i} \exp g^\ominus_{ij} ) -g^\oplus_{ii} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually, the above is calculated as &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} }
\exp(g^\ominus_{ij} - \max_k g^\ominus_{ik}) ] + \max_k g^\ominus_{ik} -
g^\oplus_{ii}
\]&lt;/span&gt; When the learning saturates, &lt;span class=&#34;math inline&#34;&gt;\(\hat
l_\text{InfoNCE}\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which means &lt;span class=&#34;math inline&#34;&gt;\(\max_k g^\ominus_{ik}\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; and thus &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} }
\exp(g^\ominus_{ij} - g^\oplus_{ii}) ] + \underbrace{g^\oplus_{ii} -
g^\oplus_{ii} }_{\text{error-prone}}
\]&lt;/span&gt; A rounding error will very likely happen when &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_of_significance&#34;&gt;subtracting
two near numbers&lt;/a&gt;. Such error will accumulate and fail the InfoNCE.
As said, FlatNCE provides a way to circumvent this rounding error.&lt;/p&gt;
&lt;h2 id=&#34;gradient-perspective&#34;&gt;Gradient Perspective&lt;/h2&gt;
&lt;p&gt;Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{FlatNCE}\)&lt;/span&gt;
the batch estimate of the negative loss from FlatNCE: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat l_\text{FlatNCE} &amp;amp;= \exp [ \logsumexp_{j \ne i} (
g^\ominus_{ij} - g^\oplus_{ii} ) - \detach \logsumexp_{j \ne i} (
g^\ominus_{ij} - g^\oplus_{ii} ) ] \\
&amp;amp;= \frac{\exp \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii}
) } {\detach [ \exp \logsumexp_{j \ne i} ( g^\ominus_{ij} -
g^\oplus_{ii} ) ] } \\
&amp;amp;= \frac{\exp \log \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) -
g_\theta (x_i, y_i) ]} {\detach \{\exp \log \sum_{j \ne i} \exp [
g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta
(x_i, y_i) ]} {\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j)
- g_\theta (x_i, y_i) ] \} }
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By putting the positive sample into the contrasting samples, &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{FlatNCE}^\oplus =  \frac{1 + \sum_j \exp \big(
g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) \big)} {1 +
\text{detach}[\sum_j \exp \big( g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i,
y_i) \big)]}
\]&lt;/span&gt; where the &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; comes from
adding the positive sample &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; to
the set of negative samples (let’s denote this “negative” sample by
&lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_0\)&lt;/span&gt;). It can be easily found
that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_\theta \hat l_\text{FlatNCE}^\oplus (g_\theta) = \nabla_\theta
\hat l_\text{InfoNCE} (g_\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may further find that the gradient of FlatNCE is an
importance-weighted estimator of the form &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla_\theta \hat l^\oplus_\text{FlatNCE} &amp;amp;= \frac{\sum_{j \ne i}
\{ \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] [\nabla_\theta
g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i, y_i)] \} }
{\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta
(x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \{ \exp [ g_\theta(x_i, y&amp;#39;_j) ]
[\nabla_\theta g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i,
y_i)] \} } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] } \\
&amp;amp;= \sum_{k \ne i} \left\{ \underbrace {\frac{ \exp [ g_\theta(x_i,
y&amp;#39;_k) ] } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] }
}_{w_k} \nabla_\theta g_\theta(x_i, y&amp;#39;_k) \right\} - \nabla_\theta
g_\theta(x_i, y_i) \\
\end{aligned}
\]&lt;/span&gt; As the learning progresses, &lt;span class=&#34;math inline&#34;&gt;\(w_k\)&lt;/span&gt;’s other than &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, which will cause the gradient to
vanish.&lt;/p&gt;
&lt;h2 id=&#34;lower-bound-perspective&#34;&gt;Lower-bound Perspective&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{InfoNCE}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{FlatNCE}\)&lt;/span&gt; are part of the
lower bounds to the mutual information in two methods. Given &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; the positive sample and &lt;span class=&#34;math inline&#34;&gt;\(y_{j&amp;gt;0}\)&lt;/span&gt; are the negative samples,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\label{lemma3.3} \begin{aligned}
-\hat l^{K, \theta}_\text{InfoNCE} &amp;amp;= -\log \{ \frac 1 K \sum_{j
&amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)] \} \\
&amp;amp;= \sup_v (v \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)] - (-1 - \log (-v)) \\
&amp;amp;\Downarrow_{v = -e^{-u}} \\
&amp;amp;\ge  -e^{-u} \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)] - (-1 - \log (e^{-u}) \\
&amp;amp;= 1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0) - u]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; as the primal
critic and &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; as the dual critic.
Since arbitrary choice of &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; lower-bounds the mutual
information, we can either jointly optimize &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; or more preferably, train in an
iterative fashion. Given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;,
set &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat u(g_\theta) = \log ({\frac 1 K \sum_j \exp[g_\theta(x,y_j) -
g_\theta(x, y)]})
\]&lt;/span&gt; Then we fix &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and only
update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is fixed, the only gradient comes from
&lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt;. Plugin &lt;span class=&#34;math inline&#34;&gt;\(\hat u\)&lt;/span&gt; to the right-hand side of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0) - u] \\
&amp;amp;= 1 - \log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0, y_0)]}) \notag \\
&amp;amp;\quad - \frac 1 K \frac{\sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)}{{\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0, y_0)]}} \\
&amp;amp;= -\log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0,
y_0)]}) \label{obj} \\
&amp;amp;= -\hat l^K_\text{InfoNCE}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which tightly lower-bounds the &lt;span class=&#34;math inline&#34;&gt;\(-\hat
l^K_\text{InfoNCE}\)&lt;/span&gt;. However we update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; will always hold. The
objective of the whole FlatNCE is to enlarge &lt;span class=&#34;math inline&#34;&gt;\(\eqref{obj}\)&lt;/span&gt; after substituting &lt;span class=&#34;math inline&#34;&gt;\(u = \hat u(g_\theta&amp;#39;)\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(-\hat l^{K, \theta&amp;#39;}_\text{InfoNCE}\)&lt;/span&gt;
can float up.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kexue.fm/archives/8586&#34;&gt;FlatNCE：小批次对比学习效果差的原因竟是浮点误差？
- 科学空间|Scientific Spaces (kexue.fm)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gaussian Distribution</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/</link>
      <pubDate>Sun, 08 May 2022 19:09:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/</guid>
      <description>

&lt;h2 id=&#34;gaussian-distribution&#34;&gt;Gaussian Distribution&lt;/h2&gt;
&lt;h3 id=&#34;one-dimensional&#34;&gt;One-dimensional&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;-d random variable
&lt;span class=&#34;math inline&#34;&gt;\(x \sim N(\mu, \sigma^2)\)&lt;/span&gt;, then its
density function is &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x -
\mu}{\sigma})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To verify that it integrates to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} p(x) \d x &amp;amp;=
\sqrt{(\int_{-\infty}^{+\infty} p(x) \d x) \cdot
(\int_{-\infty}^{+\infty} p(y) \d y)} \\
&amp;amp;= \sqrt {\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x)p(y)
\d x \d y} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I^2 =
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} p(x)p(y) \d x \d
y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-\infty}^{+\infty} p(x) \d x = \sqrt{I^2} \\
I^2 = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x - \mu}{\sigma})^2}
\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{y - \mu}{\sigma})^2}
\d x \d y
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = \frac{x-\mu}{\sigma}, v =
\frac{y-\mu}{\sigma}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
\frac{1}{\sigma\sqrt{2\pi}}
e^{-\frac{1}{2}u^2}\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}v^2} \d u
\d v \\
&amp;amp;= \frac{1}{2\pi} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
e^{-\frac{1}{2}(u^2 + v^2)} \d u \d v
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = r \sin \theta, v = r \cos
\theta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
e^{-\frac{1}{2}(u^2+v^2)} \d u \d v &amp;amp;=
\int_{0}^{2\pi} \int_{0}^{+\infty} e^{-\frac{1}{2} (r^2 \sin^2\theta +
r^2 \cos^2\theta)} r \d r \d\theta \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} -e^{-\frac{1}{2} r^2}
\d(-\frac{1}{2} r^2) \d\theta \\
&amp;amp;= \int_{0}^{2\pi}-e^{t}\Big|_{t=0}^{t=-\infty}d\theta \\
&amp;amp;= \int_{0}^{2\pi}d\theta \\
&amp;amp;= 2\pi
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I^2 =
\frac{1}{2\pi}2\pi = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\int_{-\infty}^{+\infty}p(x)dx = \sqrt{I^2} =
1\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independent-standard-n-dimensional&#34;&gt;Independent standard &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;,
suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne
j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i
(i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution, we can
derive the joint distribution density function for random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; to be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\newcommand{z}{\mathrm{z}}
p(\z) &amp;amp;= p(z_1, z_2, ..., z_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}
e^{-\frac{1}{2} z_i^2} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2}\z^T\z} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;first-order-correlated-n-dimensional&#34;&gt;First-order correlated
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;We have given the joint distribution function of independent &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional standard Gaussian
distribution. What if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; dimensions
of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; are not standard, are not
independent with each other, but are correlated only in first order?&lt;/p&gt;
&lt;p&gt;We may begin with standard Gaussian random variables &lt;span class=&#34;math inline&#34;&gt;\(X = [X_1, \dots, X_n]\)&lt;/span&gt;. Then we can shift
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and linearly transform it with an
invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(B^{-1}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; mentioned above will just be such a
matrix as &lt;span class=&#34;math inline&#34;&gt;\(Z = B^{-1} (X - \mu)\)&lt;/span&gt; and
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
X = B^{-1}(Z - \mu) \sim \mathcal{N}(0, I) \\
p_X(\x) = \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \x^T\x}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is to take on values
in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Z}\)&lt;/span&gt;, which is a subset
of &lt;span class=&#34;math inline&#34;&gt;\(\R^{n}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{z}{\mathrm{z}} P_Z(Z \in \mathcal{Z}) = \int_\mathcal{Z}
p_Z(\z) \d \z
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(Z = f(X) = BX + \mu\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is invertible, the mapping
&lt;span class=&#34;math inline&#34;&gt;\(X \to Z\)&lt;/span&gt; is one-to-one, therefore
the multivariate &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian
transformation&lt;/a&gt; is &lt;span class=&#34;math display&#34;&gt;\[
J(X \to Z) = B^{-1} \\
\]&lt;/span&gt; with its determinant &lt;span class=&#34;math inline&#34;&gt;\(J = |J(X \to
Z)| = |B^{-1}| = |B|^{-1}\)&lt;/span&gt;. Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J| &amp;amp;= \sqrt{|B|^{-1}|B|^{-1}} \\
&amp;amp;= \sqrt{|B|^{-1}|B^T|^{-1}} \\
&amp;amp;= \sqrt{|BB^T|^{-1}} \\
&amp;amp;= |BB^T|^{-\frac{1}{2}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Z (Z \in \mathcal{Z}) &amp;amp;= P_X (X \in f^{-1}(\mathcal{Z})) \\
P_Z (Z \in \mathcal{Z}) &amp;amp;= \int_{f^{-1}(\mathcal{Z})} p_X(\x) \d \x
\\
&amp;amp;\Downarrow_{\x = f^{-1}(\z)} \\
\int_\mathcal{Z} p_Z (\z) \d \z &amp;amp;= \int_\mathcal{Z} p_X (f^{-1}(\z))
|J| \d \z \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p_Z (\z) = p_X (f^{-1}(\z))|J| = p_X (B^{-1} (\z - \mu) |J| \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2}
(\z-\mu)^T(B^{-1})^T B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T
(B^T)^{-1} B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T
(BB^T)^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}|BB^T|^\frac{1}{2}} e^{-\frac{1}{2}
(\z-\mu)^T (BB^T)^{-1} (\z-\mu)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= E[(\z-\mu) (\z-\mu)^T] \\
&amp;amp;= E[B X X^T B^T] \\
&amp;amp;= B E[X X^T] B^T \\
&amp;amp;= B I B^T \\
&amp;amp;= B B^T
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, &lt;span class=&#34;math display&#34;&gt;\[
p_Z(\z) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_Z|}} e^{-\frac{1}{2} (\z-\mu)^T
\Sigma_Z^{-1} (\z-\mu)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gestalt.ink/gaussians&#34;&gt;Gaussians |
gestalt.ink&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/40225646&#34;&gt;为什么高斯分布概率密度函数的积分等于1
- 知乎 (zhihu.com)&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/58987388&#34;&gt;多元高斯分布完全解析 - 知乎
(zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>KL-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</guid>
      <description>

&lt;h2 id=&#34;kl-divergence&#34;&gt;KL-divergence&lt;/h2&gt;
&lt;p&gt;KL-divergence, denoted as &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|q)\)&lt;/span&gt;, is statistical distance,
measuring how the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is different from the reference
probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, both
defined on &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathcal{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In information theory, it measures the &lt;strong&gt;relative
entropy&lt;/strong&gt; from &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which is the average number of extra
bits required to represent a message with &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In discrete form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\sum_{x \in \mathcal{X}} p(x)\log
\frac{q(x)}{p(x)} = \sum_{x \in \mathcal{X}} p(x)\log \frac{p(x)}{q(x)}
\]&lt;/span&gt; In continuous form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\int_{x \in \mathcal{X}} p(x) \log
\frac{q(x)}{p(x)}dx = \int_{x \in \mathcal{X}} p(x) \log
\frac{p(x)}{q(x)}dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;variational-lower-bound&#34;&gt;Variational Lower-bound&lt;/h3&gt;
&lt;p&gt;One property of KL-divergence is &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p || q) = \sup_{T: \Omega \to \R} \E_{p} [T] - \log
(\E_q[e^T])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proof is as follows. Given a distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; and a function &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, construct the Gibbs distribution &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(g(x) = \frac{q(x)e^{T(x)}}{Z}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z = \E_{q(x)} e^{T(x)}\)&lt;/span&gt;. Then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E&amp;amp;_{p(x)} T(x) - \log Z = \E_{p(x)} [T(x) - \log Z] \\
&amp;amp;= \E_{p(x)} [\log e^{T(x)} - \log \E_{q(x)} e^{T(x)}] \\
&amp;amp;= \E_{p(x)} \log \frac{e^{T(x)}} {\E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{q(x) e^{T(x)}} {q(x) \E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{g(x)} {q(x)} \\
\end{aligned}
\]&lt;/span&gt; Finally KL-divergence minus above gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;D_\text{KL}(p || q) - (\E_{p(x)} T(x) - \log Z) \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {q(x)} - \E_{p(x)} \log \frac{g(x)}
{q(x)} \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {g(x)} \triangleq D_\text{KL}(p || g)
\ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are random variables, both of some
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt;. Then the
KL-divergence between them can be formulated as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_\text{KL}(p_X || p_Y) &amp;amp;= \int p_X(x) \log \frac{p_X(x)} {p_Y(x)}
\d x = \int p_X(x) \log [
    \sqrt \frac{|\Sigma_X|}{|\Sigma_Y|}
    \frac {
        e^{-\frac 1 2 (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X)}
    } {
        e^{-\frac 1 2 (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)}
    }
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} -
\frac 1 2 \int p_X(x) [
    (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) +
    (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} -
\frac 1 2 \int p_X(x) (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) \d x \\
&amp;amp;\quad -\frac 1 2 \int p_X(x) (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y) \d
x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \int p_X(x)
x^T \Sigma_X^{-1} x \d x + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \int p_X(x) x^T \Sigma_Y^{-1} x \d x + \frac 1 2
\mu_Y^T \Sigma_Y^{-1} \mu_Y \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2
\tr(\Sigma_X^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_X^{-1} \mu_X +
\frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \tr(\Sigma_Y^{-1} \Sigma_X) - \frac 1 2 \mu_X
\Sigma_Y^{-1} \mu_X + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y     \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y) + \mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu_X \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y)] \\
&amp;amp;\quad + \frac 1 2 [\mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu^T_X
\Sigma_Y^{-1} \mu_X + \mu_X^T \Sigma_Y^{-1} \mu_Y - \mu_Y^T
\Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y) + (\mu_Y^T - \mu_X^T) \Sigma_Y^{-1} (\mu_Y - \mu_X)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>4-vc-dimension</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/4-vc-dimension/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/4-vc-dimension/</guid>
      <description>

&lt;h2 id=&#34;vc-dimension&#34;&gt;VC Dimension&lt;/h2&gt;
&lt;p&gt;Recall that in Massart lemma, we transfer our focus from the
hypothesis set to &lt;span class=&#34;math inline&#34;&gt;\(\{ [h(x_1),\dots,h(x_n)]:
h \in \mathcal{H} \}\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. Another aspect is this set’s
cardinality &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, especially its
relation with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. In classification task, &lt;span class=&#34;math inline&#34;&gt;\(t\textendash n\)&lt;/span&gt;​ relation corresponds to
label assignment which easily goes to exponential. But we particularly
hope that this is not the case.&lt;/p&gt;
&lt;p&gt;We restrict ourselves to &lt;strong&gt;binary classification task&lt;/strong&gt;
and &lt;strong&gt;zero/one-loss&lt;/strong&gt; in this section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;shattering coefficient&lt;/strong&gt;. Given a function
set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; whose members map a
feature vector &lt;span class=&#34;math inline&#34;&gt;\(x \mapsto
\mathcal{X}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y} = \{ 0,1
\}\)&lt;/span&gt;, we define shattering coefficient &lt;span class=&#34;math inline&#34;&gt;\(s(\mathcal{F}, n)\)&lt;/span&gt; as the maximum number
of different label assignment over datasets of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\{
x_1,\dots,x_n \} \to \mathcal{X}^n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
s_\mathcal{F}(n) \triangleq \max_{x_1,\dots,x_n \in \mathcal{X}} \left|
\{ [f(x_1),\dots,f(x_n)]: f \in \mathcal{F} \} \right|
\]&lt;/span&gt; A trivial upper bound for shattering coefficient is &lt;span class=&#34;math inline&#34;&gt;\(2^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Massart lemma applied to shattering
coefficient&lt;/strong&gt;. Given function set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; whose members map the input
to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y} = \{ 0,1 \}\)&lt;/span&gt;, a
direct application of Massart lemma (&lt;span class=&#34;math inline&#34;&gt;\(M=1, t
= s_\mathcal{F}(n)\)&lt;/span&gt;) gives the following bound on its empirical
Rademacher complexity over every dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X}\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\r}{\mathsf{R}} \hat \r_\mathrm{X}(\mathcal{F}) \le
\sqrt{\frac{2 \log s_\mathcal{F}(n)}{n}}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\r_n(\mathcal{F}) \le \sqrt{\frac{2 \log s_\mathcal{F}(n)}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;VC dimension&lt;/strong&gt;. Given a function set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; with Boolean output in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y}=\{ 0,1 \}\)&lt;/span&gt;, we define its VC
dimension &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{VC}{\mathrm{VC}}
\VC(\mathcal{F})\)&lt;/span&gt; as the size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the largest dataset &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that can be shattered by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\VC(\mathcal{F}) = \sup\{ n: S_\mathcal{F}(n) = 2^n \}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
S_\mathcal{F}(n) = \left| \{ [f(x_1),\dots,f(x_n)]: f \in \mathcal{F} \}
\right|,\ x_1\dots,x_n \in S
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;VC dimension kind of reflects the richness of the function set in
classification task.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Here are some examples of function set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interval functions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}_\text{int} = \{ f_{a,b}(x) = \one[a
\le x \le b]: a,b \in \R \}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The VC dimension is &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;. Actually,
the shattering coefficient &lt;span class=&#34;math inline&#34;&gt;\(s_{\mathcal{F}_\text{int}}(n)\)&lt;/span&gt; for the set
of interval functions is &lt;span class=&#34;math inline&#34;&gt;\(\binom{n+1}{2} +
1\)&lt;/span&gt;​.&lt;/p&gt;
&lt;p&gt;The implication of this example is that the VC dimension is usually
the dimension of parameter that characterizes the function set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Binarized sinus functions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\{ f(x) = \one[\sin(\omega x) \le 0]: \omega \in
\R \}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The VC dimension is &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. We
show this by construction. Consider the data points &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x_i = 2^i\)&lt;/span&gt;. Denote as &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_n\)&lt;/span&gt; the output. Then it is
necessary that &lt;span class=&#34;math inline&#34;&gt;\(\omega x_i\)&lt;/span&gt; is
between &lt;span class=&#34;math inline&#34;&gt;\((2k_i+1)\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((2k_i + 2y_i)\pi\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(k_i\)&lt;/span&gt;. We show that setting &lt;span class=&#34;math inline&#34;&gt;\(\omega = (0.y_1 \cdots y_n)_2 \pi\)&lt;/span&gt;
suffices. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sin(\omega x_i) &amp;amp;= \sin((y_1 \cdots y_i.y_{i+1} \cdots y_n)_2 \pi)
\\
&amp;amp;= \sin((y_i.y_{i+1} \cdots y_n)_2 \pi) \\
&amp;amp;= y_i
\end{aligned}
\]&lt;/span&gt; This holds for any &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_n\)&lt;/span&gt;. As a result, the
shattering coefficient for the set of binarized sinus functions is &lt;span class=&#34;math inline&#34;&gt;\(2^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The implication of this example is that even when there is only one
parameter, the VC dimension can go to infinity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Binarized convex functions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}_\text{convex} = \{ f(x) = \one[g(x)
\le 0]: \text{$g$ is convex} \}\)&lt;/span&gt;​&lt;/p&gt;
&lt;p&gt;The VC dimension is &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. We
show this by construction. Consider the data points &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n \in \R^d\)&lt;/span&gt; on the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional sphere. Denote as &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_n\)&lt;/span&gt; the arbitrary output. Let
&lt;span class=&#34;math inline&#34;&gt;\(I = \{ i: y_i = 1 \}\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\textrm{convex-hull}(\{ x_i: i \in I \})\)&lt;/span&gt;
cannot contain &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;’s whose label
are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; (due to the geometric
property of sphere). Let &lt;span class=&#34;math display&#34;&gt;\[
g(x) = \one[x \in \textrm{convex-hull}(\{ x_i: i \in I \})]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then we have &lt;span class=&#34;math display&#34;&gt;\[
g(x_j) = \one[x_j \in \textrm{convex-hull}(\{ x_i: i \in I \})] = y_j
\]&lt;/span&gt; As a result, the shattering coefficient for the set of
binarized convex functions is &lt;span class=&#34;math inline&#34;&gt;\(2^n\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;VC dimension of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional hyperplanes&lt;/strong&gt;.
Consider the set of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;​-dimensional
linear (what about affine??) classification rules: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{F} = \{ \one[w^\intercal x \ge 0]: w \in \R^d \}
\]&lt;/span&gt; Then, the VC dimension &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F})\)&lt;/span&gt; of this function set
will be &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof. We first show that &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) \le d\)&lt;/span&gt;. Suppose on the
contrary that &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) &amp;gt;
d\)&lt;/span&gt;. Then, there exists &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_{d+1}\)&lt;/span&gt; that can be shattered
by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;. From linear algebra
knowledge, we know that there exists &lt;span class=&#34;math inline&#34;&gt;\(c_1,
\dots, c_{d+1} \in \R\)&lt;/span&gt; which are not all zeros such that &lt;span class=&#34;math display&#34;&gt;\[
c_1 x_1 + \dots + c_{d+1} x_{d+1} = 0
\]&lt;/span&gt; Without loss of generality, suppose &lt;span class=&#34;math inline&#34;&gt;\(c_1 &amp;lt; 0\)&lt;/span&gt;. There exists &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\forall i, y_i = \one[w^\intercal x_i \ge 0] = \one[c_i \ge 0]
\]&lt;/span&gt; Then &lt;span class=&#34;math display&#34;&gt;\[
c_1 w^\intercal x_1 + \dots + c_{d+1} w^\intercal x_{d+1} =
w^\intercal(c_1 x_1 + \dots + c_{d+1} x_{d+1}) = 0
\]&lt;/span&gt; For any &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, if &lt;span class=&#34;math inline&#34;&gt;\(c_i \ge 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(w^\intercal x_i \ge 0\)&lt;/span&gt;; if &lt;span class=&#34;math inline&#34;&gt;\(c_i &amp;lt; 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(w^\intercal x_i &amp;lt; 0\)&lt;/span&gt;. Hence, &lt;span class=&#34;math inline&#34;&gt;\(c_i w^\intercal x_i \ge 0\)&lt;/span&gt;. Specifically,
&lt;span class=&#34;math inline&#34;&gt;\(c_1 w^\intercal x_i &amp;gt; 0\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
c_1 w^\intercal x_1 + \dots + c_{d+1} w^\intercal x_{d+1} &amp;gt; 0
\]&lt;/span&gt; which is a contradiction.&lt;/p&gt;
&lt;p&gt;Then it suffices to show that &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) = d\)&lt;/span&gt; if there exists
&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_d\)&lt;/span&gt; that can be
shattered by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;. We choose
&lt;span class=&#34;math inline&#34;&gt;\(x_i = e_i\)&lt;/span&gt; which is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th standard basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^d\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(w
= [y_1,\dots,y_d]^T\)&lt;/span&gt; would suffice. Q.E.D.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;connection-with-rademacher-complexity&#34;&gt;Connection with
Rademacher Complexity&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Lemma: &lt;strong&gt;Sauer’s lemma&lt;/strong&gt;. Consider a function set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; with VC dimension &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) = d\)&lt;/span&gt;. Then, for every
integer &lt;span class=&#34;math inline&#34;&gt;\(n \in \N\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
s_\mathcal{F}(n) \le \sum_{i=0}^d {n \choose i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sauer’s lemma trivially holds if &lt;span class=&#34;math inline&#34;&gt;\(n \le
d\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^d \binom{n}{i}
= \sum_{i=1}^n \binom{n}{i} = 2^n\)&lt;/span&gt;. We care about the case when
&lt;span class=&#34;math inline&#34;&gt;\(d &amp;lt; n\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(s_\mathcal{F}(n) \le \sum_{i=1}^d \binom{n}{i} \le
(\frac{e n}{d})^d\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Rademacher complexity bound with VC
dimension&lt;/strong&gt;. Consider function set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; with Boolean output in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y} = \{ 0,1 \}\)&lt;/span&gt;, whose VC
dimension is &lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) = d\)&lt;/span&gt;.
Then, we have the following bound on the Rademacher complexity over
every dataset &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\hat \r(\mathcal{F}) \le \sqrt{ \frac{2d (\log (n/d) + 1)}{n} }
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\r_n(\mathcal{F}) \le \sqrt{\frac{2d (\log(n/d) + 1)}{n}}
\]&lt;/span&gt; Proof. A direct application of Sauer’s lemma and &lt;u&gt;Massart
lemma applied to shattering coefficient&lt;/u&gt; gives the above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;excess risk bound via VC dimension&lt;/strong&gt;.
Consider a hypothesis set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; with Boolean output which has
&lt;span class=&#34;math inline&#34;&gt;\(\VC(\mathcal{F}) = d\)&lt;/span&gt;. Suppose the
loss function is &lt;strong&gt;zero/one loss&lt;/strong&gt;. Then, with probability
at least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
L(\hat f) - L(f^*) \le \sqrt{\frac{32 d((\log n/d) + 1)}{n}} +
\sqrt{\frac{2 \log(2/\delta)}{n}}
\]&lt;/span&gt; Proof. A direction application of excess risk bound via
Rademacher complexity gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;L(\hat f) - L(f^*) \le 4 \r_n(\mathcal{H}) + \sqrt{\frac{2
\log(2/\delta)}{n}} \\
&amp;amp;\le 4 \sqrt{\frac{2d(\log(n/d) + 1)}{n}} + \sqrt{\frac{2
\log(2/\delta)}{n}} \\
&amp;amp;= \sqrt{\frac{32d(\log(n/d) + 1)}{n}} + \sqrt{\frac{2
\log(2/\delta)}{n}} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</link>
      <pubDate>Fri, 07 Jan 2022 12:58:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</guid>
      <description>
&lt;p&gt;If a probabilistic model contains only observable variables, maximum
likelihood estimation or Bayesian methods can be adopted to derive the
model parameters. However it is also possible that a probabilistic model
contains unobservable variables (called &lt;strong&gt;latent
variables&lt;/strong&gt;). Latent variables are those that you cannot observe
but you know its existence and its influence in a random trial. In such
case, &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange multipliers&lt;/a&gt; may
be hard to apply because of the existence of the “log of sum” term.&lt;/p&gt;
&lt;p&gt;Given observed samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} =
\{\x^{(1)}, \x^{(2)}, ..., \x^{(m)}\}\)&lt;/span&gt; (with unobservable latent
variable samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Z}\)&lt;/span&gt;), MLE
tries to the find best parameters &lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = \arg\max_{\theta}\log(p(\mathrm{X};\theta))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The log-likelihood function is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log p(\mathrm{X};\theta) = \sum_{\x \in \mathrm{X}}
\log p(\x; \theta) \\
&amp;amp;= \sum_{\x \in \mathrm{X}} \log \E_{\z \sim q} \frac{p(\x, \z;
\theta)}{q(\z)} \\
&amp;amp;\Downarrow_\text{by Jensen&amp;#39;s Inequality} \\
&amp;amp;\ge \sum_{\x \in \mathrm{X}} \E_{\z \sim q} \log \frac{p(\x, \z;
\theta)}{q(\z)} \triangleq B(q, \theta) \\
&amp;amp;\text{where $q$ is an arbitrary reference probability measure}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we enlarge the &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt;,
the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; can be
lifted, which gives us a gentle guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; may increase.&lt;/p&gt;
&lt;p&gt;Expectation maximization lifts this bound iteratively. To begin with,
we choose a random initial estimation for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(\theta^0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\((q, \theta)\)&lt;/span&gt;. We can specifically set
&lt;span class=&#34;math inline&#34;&gt;\(q(\z) = p(\z | \x; \theta^{t})\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt; is the estimation in
current iteration. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(B(q,
\theta)\)&lt;/span&gt; becomes &lt;span class=&#34;math display&#34;&gt;\[
B(q, \theta) = \sum_{\x \in \mathrm{X}} \E_{\z \sim p(\cdot | \x;
\theta^{t})} \log \frac{p(\x, \z; \theta)}{p(\z | \x; \theta^{t})}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\log \frac{1}{p(\z | \x;
\theta^{t})}\)&lt;/span&gt; is irrelevant to the optimization of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can simplify the problem as
maximizing &lt;span class=&#34;math display&#34;&gt;\[
Q(\theta^t, \theta) \triangleq \sum_{\x \in \mathrm{X}} \E_{\z \sim
p(\cdot | \x; \theta^{t})} \log p(\x, \z; \theta) \label{q-function}
\]&lt;/span&gt; The above step is called the &lt;strong&gt;expectation&lt;/strong&gt; step
because we are choosing a probability measure for the expectation term
in &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt;. The next step is
the &lt;strong&gt;maximization&lt;/strong&gt; step where we fix &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt; and maximize &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;-function&lt;/strong&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{q-function}\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This accounts for the estimation
in the next iteration: &lt;span class=&#34;math display&#34;&gt;\[
\theta^{t+1} = \arg\max_{\theta} Q(\theta^t, \theta)
\]&lt;/span&gt; We do these two steps back and forth, comprising the whole
expectation maximization algorithm.&lt;/p&gt;
&lt;p&gt;The reason we choose &lt;span class=&#34;math inline&#34;&gt;\(q(\z) = p(\z | \x;
\theta^{t})\)&lt;/span&gt;, which is conditioned on &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;, is that we want to inject some
dynamics into the algorithm. Say if we choose &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to be some static measure not relative
to &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;, the algorithm will end
at the very first iteration. On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(p(\z | \x; \theta^t)\)&lt;/span&gt; is the most
approachable probability measure w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\z\)&lt;/span&gt; and relative to &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;. After all, in latent models,
&lt;span class=&#34;math inline&#34;&gt;\(p(\z | \x; \theta^t)\)&lt;/span&gt; is much easier
to compute than &lt;span class=&#34;math inline&#34;&gt;\(p(\z;
\theta^t)\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eigenvectors and Eigenvalues</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</guid>
      <description>

&lt;h2 id=&#34;eigenvectors-and-eigenvalues&#34;&gt;Eigenvectors and Eigenvalues&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;eigenvector&lt;/strong&gt; of a &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a
&lt;strong&gt;nonzero&lt;/strong&gt; vector &lt;span class=&#34;math inline&#34;&gt;\(\rm
x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\rm x = \lambda \rm
x\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.
This scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the
corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt;. Note that by definition,
&lt;span class=&#34;math inline&#34;&gt;\((\lambda, \vec 0)\)&lt;/span&gt; is a pair of
eigenvalue and eigenvector of any square matrix. However, &lt;span class=&#34;math inline&#34;&gt;\(\vec 0\)&lt;/span&gt; is just too trivial an eigenvector
that people exclude it from the eigen discussion.&lt;/p&gt;
&lt;p&gt;Note, though, &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; can be an
eigenvalue. Also note that if &lt;span class=&#34;math inline&#34;&gt;\((\lambda,
v)\)&lt;/span&gt; is a pair of eigen of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\((\lambda, kv)\)&lt;/span&gt; is also a pair of
eigen.&lt;/p&gt;
&lt;h3 id=&#34;similarity&#34;&gt;Similarity&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrices, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
is similar to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; if there is an
invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(PAP^{-1} = B\)&lt;/span&gt;, or equivalently &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}BP = A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are similar, then they have the same
characteristic polynomial and hence then the same eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
B - \lambda I = PAP^{-1} - \lambda PP^{-1} = P(A - \lambda I)P^{-1} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(B - \lambda I) = \det(P(A - \lambda I)P^{-1}) \\
&amp;amp;= \det(P) \cdot \det(A - \lambda I) \cdot \det(P^{-1}) \\
&amp;amp;= \det(A - \lambda I)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independence-between-eigenvectors&#34;&gt;Independence between
Eigenvectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are
the eigenvectors corresponding to the distinct eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, ..., \lambda_r\)&lt;/span&gt; of
an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2,
... , v_r\)&lt;/span&gt; are also called eigenvectors from different
&lt;strong&gt;eigenspaces&lt;/strong&gt;), then &lt;span class=&#34;math inline&#34;&gt;\(v_1,
v_2, ..., v_r\)&lt;/span&gt; are linearly independent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;Suppose instead these vectors are dependent. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the least index such that &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is a linear combination of
previous vectors. Then there exists scalars &lt;span class=&#34;math inline&#34;&gt;\(c_1, c_2, ..., c_p\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1v_1 + c_2v_2 + \cdots + c_pv_p = v_{p+1} \label{lincom}
  \end{equation}
  \]&lt;/span&gt; Multiply both sides with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to obtain &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1\lambda_1v_1 + c_2\lambda_2v_2 + \cdots + c_p\lambda_pv_p =
\lambda_{p+1}v_{p+1} \label{eq1}
  \end{equation}
  \]&lt;/span&gt; Multiply both sides of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{p+1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1\lambda_{p+1}v_1 + c_2\lambda_{p+1}v_2 + \cdots +
c_p\lambda_{p+1}v_p = \lambda_{p+1}v_{p+1} \label{eq2}
  \end{equation}
  \]&lt;/span&gt; Subtract &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt;
from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1(\lambda_1 - \lambda_{p+1})v_1 + c_2(\lambda_2 - \lambda_{p+1})v_2
+ \cdots + c_p(\lambda_p - \lambda_{p+1})v_p = 0 \label{diff}
  \end{equation}
  \]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ...,
v_p\)&lt;/span&gt; are independent, the weights in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; are all zeros. None of &lt;span class=&#34;math inline&#34;&gt;\((\lambda_i - \lambda_{p+1})\)&lt;/span&gt; is zero, so
&lt;span class=&#34;math inline&#34;&gt;\(c_i = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; says &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which is impossible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, as the eigenvector, is
nonzero so that the conclusion can hold even for &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagonalization&#34;&gt;Diagonalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; linearly independent
eigenvectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(AP = PD\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\p}{\mathrm{p}} P = [\p_1, \p_2, ...,
\p_n]\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a
diagonal matrix, &lt;span class=&#34;math display&#34;&gt;\[
AP = [A\p_1, A\p_2, ..., A\p_n] = [D_{11} \p_1, D_{22} \p_2, ..., D_{nn}
\p_n]
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible,
&lt;span class=&#34;math inline&#34;&gt;\(\p_i\)&lt;/span&gt;’s are linearly independent,
which indicates that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;’s
are &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors.&lt;/p&gt;
&lt;p&gt;From this, we could also see that if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, then &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; must be the matrix concatenated with
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; must be the diagonal matrix filled with
corresponding eigenvalues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Easy to show.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all matrices are diagonalizable. For example, the matrix &lt;span class=&#34;math inline&#34;&gt;\(\begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1
\end{bmatrix}\)&lt;/span&gt; is not diagonalizable.&lt;/p&gt;
&lt;p&gt;The diagonalization of a square matrix is also referred to as
&lt;strong&gt;eigen decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-rank-trace-and-determinant&#34;&gt;Eigenvalues: Rank, Trace
and Determinant&lt;/h3&gt;
&lt;p&gt;Since the characteristic equation of an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix is a polynomial of degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the equation always has exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; roots, counting multiplicities,
including complex roots. There are some relations between eigenvalues
and matrix’s rank, trace and determinant: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\rank = \text{the number of nonzero real eigenvalues, including
multiplicities} \\
\tr = \text{sum of the eigenvalues} \\
\det = \text{product of the eigenvalues}
\end{gather}
\]&lt;/span&gt; ## Power Iteration&lt;/p&gt;
&lt;p&gt;We may obtain the eigenvalues of an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; diagonalizable matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s by solving &lt;span class=&#34;math inline&#34;&gt;\(\det (A - \lambda I) = 0\)&lt;/span&gt;. Then the
corresponding eigenvectors can be solved. The order of complexity of
this method is cubic.&lt;/p&gt;
&lt;p&gt;But chances are that we don’t want all the eigenpairs, but instead
only those with largest eigenvalues, like when we find the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/#Eckart-Young-Mirsky Theorem&#34;&gt;best rank-&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; approximation using SVD&lt;/a&gt;. It is an
overkill to solve all eigenpairs. Luckily there is another lightweight
iterative method that can help.&lt;/p&gt;
&lt;p&gt;Begin with an arbitrary vector &lt;span class=&#34;math inline&#34;&gt;\(x_0 = x
\in \R^n\)&lt;/span&gt;. The iteration rule is &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1} = \frac{A x_t}{||A x_t||}
\]&lt;/span&gt; Unroll &lt;span class=&#34;math inline&#34;&gt;\(x_{t+1}\)&lt;/span&gt; to get
&lt;span class=&#34;math inline&#34;&gt;\(x_t = \frac{A^t x}{||A^t x||}\)&lt;/span&gt;.
Because &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; can be written as a linear combination
of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; normalized independent eigenvectors:
&lt;span class=&#34;math display&#34;&gt;\[
x = \sum_{i=1}^n \alpha_i v_i
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_n\)&lt;/span&gt; be
arranged such that corresponding eigenvalues go from large to small.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;A^t x = A^t \sum_{i=1}^N \alpha_i v_i \\
&amp;amp;= \sum_{i=1}^N \alpha_i \lambda_i^t v_i \\
&amp;amp;= \alpha_1 \lambda_1^t \sum_{i=1}^N \frac{\alpha_i}{\alpha_1}
(\frac{\lambda_i}{\lambda_1})^t v_i
\end{aligned}
\]&lt;/span&gt; Under the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt; is strictly larger than other
eigenvalues, &lt;span class=&#34;math inline&#34;&gt;\((\frac{\lambda_i}{\lambda_1})^t
\to 0, A^t x \to \alpha_1 \lambda_1^t v_1\)&lt;/span&gt;. Thus, &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1} = \frac{A^t x}{||A^t x||} \to v_1
\]&lt;/span&gt; To find the corresponding eigenvalue, observe that &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1}^T A x_{t+1} \to \lambda_1
\]&lt;/span&gt; This gives the eigenpair with the largest eigenvalue. To find
the next one, repeat the above with &lt;span class=&#34;math display&#34;&gt;\[
A&amp;#39; = A - \lambda_1 v_1 v_1^T
\]&lt;/span&gt; The process above is called the deflation for the power
method. Refer to &lt;a href=&#34;Finding%20Eigenvalues.pdf&#34;&gt;Finding
Eigenvalues.pdf&lt;/a&gt; for the proof of correctness. This might also be
related to &lt;a href=&#34;https://www.wikiwand.com/en/Min-max_theorem&#34;&gt;Courant–Fischer
theorem&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD
&lt;a href=&#34;https://chunxy.github.io/uploads/Eigen%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Eigen
Decomposition.pdf&lt;/a&gt; ======= &lt;a href=&#34;Eigen%20Decomposition.pdf&#34;&gt;Eigen
Decomposition.pdf&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; notes&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Orthogonality and Projection</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</link>
      <pubDate>Mon, 20 Dec 2021 10:19:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</guid>
      <description>

&lt;h2 id=&#34;orthogonality-and-independence&#34;&gt;Orthogonality and
Independence&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; are
orthogonal to each other, then they are independent with each other.&lt;/p&gt;
&lt;h2 id=&#34;orthonormality&#34;&gt;Orthonormality&lt;/h2&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; has orthonormal columns if and only if
&lt;span class=&#34;math inline&#34;&gt;\(U^TU = I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;orthogonal matrix&lt;/strong&gt; is a square invertible matrix
&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(U^{-1} = U^T\)&lt;/span&gt;. By its definition, it has
orthonormal columns and orthonormal rows.&lt;/p&gt;
&lt;h2 id=&#34;projection&#34;&gt;Projection&lt;/h2&gt;
&lt;h3 id=&#34;projection-onto-orthogonal-basis&#34;&gt;Projection onto Orthogonal
Basis&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; be an
orthogonal basis for a subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;
of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, the weights in the linear combination
&lt;span class=&#34;math display&#34;&gt;\[
y = c_1u_1 + c_2u_2 + ... + c_ku_k
\]&lt;/span&gt; are given by &lt;span class=&#34;math display&#34;&gt;\[
c_i = \frac{y \cdot u_i}{u_i \cdot u_i} \label{coef}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-vector&#34;&gt;Projection onto Vector&lt;/h3&gt;
&lt;p&gt;Given a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; in
&lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; and another vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, we wish to decompose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \alpha u\)&lt;/span&gt;
for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is some vector orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
z &amp;amp;= y - \hat y \\
z \cdot u &amp;amp;= (y - \alpha u) \cdot u \\
y \cdot u - \alpha u \cdot u &amp;amp;= 0 \\
\alpha &amp;amp;= \frac{y \cdot u}{u \cdot u}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \frac{y \cdot u}{u \cdot
u}u\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z = y -
\hat y\)&lt;/span&gt; is called the component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(cu\)&lt;/span&gt; for any scalar &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is the projection onto the
subspace &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; spanned by &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. In this sense, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also denoted as &lt;span class=&#34;math inline&#34;&gt;\(\Pi_L(y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-subspace&#34;&gt;Projection onto Subspace&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; be a linear subspace of
&lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, then each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; can be uniquely written in the form:
&lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W^\perp\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\hat
y\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\Pi_W(y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also called the best
approximation to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, in the sense that: &lt;span class=&#34;math display&#34;&gt;\[
||y - \hat y||_2 \le ||y - v||_2, \forall v \in W
\]&lt;/span&gt; It can be shown by &lt;span class=&#34;math display&#34;&gt;\[
y - v = (y - \hat y) + (\hat y - v)
\]&lt;/span&gt; which gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||y - v||_2^2 = ||(y - \hat y) + (\hat y - v)||_2^2 \\
&amp;amp;= ||y - \hat y||_2^2 + ||\hat y - v||_2^2 + 2(y - \hat y)^T (\hat y
- v) \\
&amp;amp;\Downarrow_{y - \hat y \in W^\perp, \hat y - v \in W} \\
&amp;amp;= ||y - \hat y||_2^2 + ||\hat y - v||_2^2 \\
&amp;amp;&amp;gt; ||y - \hat y||_2^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-column-space&#34;&gt;Projection onto Column Space&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(A \in \R^{n \times m}\)&lt;/span&gt; is a
matrix, for any &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; we may
still want to find its projection onto the column space (also a linear
subspace) spanned by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. As
mentioned above, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be
decomposed into &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\hat y \in \Col A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z \in (\Col A)^\perp = \Nul A^T\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
y = \hat y + z \label{decomp} \\
A^T z = 0 \label{perp} \\
\exists x \in \R^n, A x = \hat y \label{proj}
\end{gather}
\]&lt;/span&gt; Substitute &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; in
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{proj}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{decomp}\)&lt;/span&gt;, and then substitute &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{decomp}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{perp}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^T (y - A x) &amp;amp;= 0 \\
A^T A x &amp;amp;= A^T y \\
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\rank {A^T A} = \rank
A\)&lt;/span&gt;. If either &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s columns
are independent or &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is of full
rank, we can solve the above equation as &lt;span class=&#34;math display&#34;&gt;\[
x = (A^T A)^{-1} A^T y
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\hat y = A x = A (A^T A)^{-1} A^T y
\]&lt;/span&gt; For any &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt;, its
projection onto &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; can be found
by left-multiplying &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s
&lt;strong&gt;projection matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(P
\triangleq A (A^T A)^{-1} A^T\)&lt;/span&gt;. And we can verify that &lt;span class=&#34;math inline&#34;&gt;\(z \in \Nul A^T\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A^T z = A^T (y - \hat y) = A^T (y - A (A^T A)^{-1} A^T y) = A^T y - A^T
y = 0
\]&lt;/span&gt; There are some interesting properties with this projection
matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is symmetric;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is
&lt;strong&gt;idempotent&lt;/strong&gt; in that &lt;span class=&#34;math inline&#34;&gt;\(P^2 =
P\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Col P = \Col A\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For every vector &lt;span class=&#34;math inline&#34;&gt;\(x \in \Col A\)&lt;/span&gt;,
by the definition of projection matrix, we have &lt;span class=&#34;math display&#34;&gt;\[
P x = x
\]&lt;/span&gt; which means &lt;span class=&#34;math inline&#34;&gt;\(x \in \Col P\)&lt;/span&gt;
and thus &lt;span class=&#34;math inline&#34;&gt;\(\Col A \subseteq \Col
P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For every vector &lt;span class=&#34;math inline&#34;&gt;\(y \in \Col P\)&lt;/span&gt;,
there exists a vector &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
P z = y
\]&lt;/span&gt; By the definition of projection matrix, we know that &lt;span class=&#34;math inline&#34;&gt;\((P z) \in \Col A\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\Col P \subseteq \Col A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\Col P = \Col A\)&lt;/span&gt;.
Interestingly and as a direct result, we have &lt;span class=&#34;math display&#34;&gt;\[
A^T P = A^T
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-projection-matrix&#34;&gt;General Projection Matrix&lt;/h3&gt;
&lt;p&gt;The projection matrix derived above for a column space is actually an
&lt;strong&gt;orthogonal projection matrix&lt;/strong&gt;. The residual &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; of the original vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; after projection is perpendicular to
&lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; and thus to &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a more general case, the residual is not necessarily perpendicular
to &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also the closest point in &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. This instead is the definitive
property of a &lt;strong&gt;projection&lt;/strong&gt;, suitable for any set of
vectors like linear subspace or non-convex set.&lt;/p&gt;
&lt;p&gt;We can similarly develop the notion of a general projection matrix.
If an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; satisfies that &lt;span class=&#34;math inline&#34;&gt;\(P^2 = P\)&lt;/span&gt;, then it is a &lt;strong&gt;projection
matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Note that we develop the concept of orthogonal projection matrix from
the orthogonal projection onto a linear subspace. But for a general
projection matrix, we only require that &lt;span class=&#34;math inline&#34;&gt;\(P^2
= P\)&lt;/span&gt;. We don’t associate general projection matrix with general
projection, because for a vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(P y\)&lt;/span&gt; may not be the projection of
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto the projection space &lt;span class=&#34;math inline&#34;&gt;\(\{ P x: x \in \R^n \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math display&#34;&gt;\[
P = \begin{pmatrix}
0 &amp;amp; 1 \\
0 &amp;amp; 1
\end{pmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is a projection matrix
and it transforms all 2-D vectors &lt;span class=&#34;math inline&#34;&gt;\(y = [y_1,
y_2]^T\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\hat y = [y_2,
y_2]^T\)&lt;/span&gt;. The projection space formed by &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P} = \{ x \in \R^2: x_1 = x_2
\}\)&lt;/span&gt;. Clearly, neither &lt;span class=&#34;math inline&#34;&gt;\(\hat
y\)&lt;/span&gt; is the closest point to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt;, nor the residual &lt;span class=&#34;math inline&#34;&gt;\(y - \hat y\)&lt;/span&gt; is perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are some properties with a general projection matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is an orthogonal
projection matrix if and only if it is symmetric;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is an orthogonal
projection matrix if and only if its singular values are either &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;Refer to &lt;a href=&#34;https://math.stackexchange.com/questions/1109755/a-projection-p-is-orthogonal-if-and-only-if-its-spectral-norm-is-1&#34;&gt;this&lt;/a&gt;
or &lt;a href=&#34;https://math.stackexchange.com/questions/4407294/singular-values-of-projection-matrix&#34;&gt;this&lt;/a&gt;
for proof.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible only if it
is the identity matrix.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>$f$-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence can be treated as
the generalization of the KL-divergence. For continuous random variable,
it is defined as &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has to satisfy
that &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a convex function. The reason for
these two constraints is that we hope &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
D_f (p||q) = 0 \text{ when $p=q$} \\
\forall p, q, D_f (p||q) \ge 0
\end{gather}
\]&lt;/span&gt; To show it, &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \ge f(\int
\frac{p(x)}{q(x)} q(x) \d x) = f(1) = 0 \\
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x\log x\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence becomes KL-divergence.
The log sum inequality property can be derived using the formulation of
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^n a_i \log \frac{a_i}{b_i} &amp;amp;= \sum_{i=1}^n b_i
\underbrace{\frac{a_i}{b_i} \log \frac{a_i}{b_i}}_{f(\frac{a_i}{b_i})}
\\
&amp;amp;= b \sum_{i=1}^n \frac{b_i}{b} f(\frac{a_i}{b_i}) \\
&amp;amp;\ge b f(\sum_{i=1}^n \frac{b_i}{b} \frac{a_i}{b_i}) \\
&amp;amp;= b f(\frac{a}{b}) \\
&amp;amp;= a \log \frac{a}{b}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;variational-f-divergence&#34;&gt;Variational &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; have no closed-form expression, it is
difficult to compute the &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence. Therefore in practice &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence is computed with a
variational expression: &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ
T(x)] \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the convex
conjugate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. The derivation is
as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int f^{**}(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int \sup_t [\frac{p(x)}{q(x)} t - f^*(t)] q(x)\ \d x \\
&amp;amp;= \int \sup_t[p(x) t - f^*(t) q(x)]\ \d x \\
&amp;amp;\Downarrow_{T(x) = \arg \sup_t[p(x) t - f^*(t) q(x)]} \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \int [p(x) T(x) - f^*(T(x)) q(x)]\ \d
x \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)]
\}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</guid>
      <description>

&lt;p&gt;Given a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)},
y^{(i)}),i=1,\dots,M\}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \in \R^N\)&lt;/span&gt; is the feature vector
and &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} \in R\)&lt;/span&gt; is the output,
learn a linear function &lt;span class=&#34;math inline&#34;&gt;\(f = w^Tx + b: \R^N
\mapsto \R\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N, b \in
\R\)&lt;/span&gt;, that best predicts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for any feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The “best” is usually measured by the
mean square error: &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{MSE}(f,\mathcal D) = \frac{1}{M}\sum_{i=1}^M(y^{(i)} -
f(x^{(i)}))^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As an aside, it is very convenient to standardize the feature (so
that the solution won’t depend to unit used in the measurement) and
pre-center the label (so that the intercept term, or the bias term &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be omitted).&lt;/p&gt;
&lt;h3 id=&#34;ordinary-least-squares&#34;&gt;Ordinary Least Squares&lt;/h3&gt;
&lt;p&gt;OLS selects the linear regression parameters &lt;span class=&#34;math inline&#34;&gt;\(w, b\)&lt;/span&gt; that minimizes the mean squared
error: &lt;span class=&#34;math display&#34;&gt;\[
w^\star, b^\star = \arg \min_{w,b}\frac{1}{M}\sum_{i=1}^M(y^{(i)} -
w^Tx^{(i)} - b)^2
\]&lt;/span&gt; This is equivalent to the least squares problem. Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
X =
\left [
\begin{array}{c|c}
(x^{(1)})^T &amp;amp; 1 \\
(x^{(2)})^T &amp;amp; 1 \\
\vdots &amp;amp; \vdots \\
(x^{(M)})^T &amp;amp; 1
\end{array}
\right ],
W = \begin{bmatrix}
w \\
b \\
\end{bmatrix},
Y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(M)}
\end{bmatrix}
\end{gathered}
\]&lt;/span&gt; Least squares problem aims to find the best approximation to
&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;’s column space, which is &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;’s projection in &lt;span class=&#34;math inline&#34;&gt;\(\Col(X)\)&lt;/span&gt;. Therefore we solve &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X\hat W = \Pi_{\Col(X)}Y \Rightarrow Y &amp;amp;- X\hat W \in \Nul(X^T) \\
\Downarrow\\
X^T(Y - X\hat W) &amp;amp;= 0 \\
X^TX\hat W &amp;amp;= X^TY
\end{aligned}
\]&lt;/span&gt; Columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are
independent &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible because they share the
same nullspace. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is not
invertible, there are infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt;. We can get one specific &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; by enforcing regularization on
&lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; or adding more samples when
&lt;span class=&#34;math inline&#34;&gt;\(M &amp;lt; N + 1\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible, there is a unique
solution that &lt;span class=&#34;math inline&#34;&gt;\(\hat W =
(X^TX)^{-1}X^TY\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This gives the same result with minimizing the MSE: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
W^\star = \arg \min_{W} \mathrm{MSE}(W) = \frac{1}{M}(Y - XW)^T(Y - XW)
\label{target} \\
\Downarrow \notag \\ \notag \\
\begin{aligned}
\frac{\partial \mathrm{MSE}}{\partial W} &amp;amp;= \frac{\partial(Y^TY -
Y^TXW - W^TX^TY + W^TX^TXW)}{\partial W} \\
&amp;amp;= \frac{\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\partial W} \\
&amp;amp;= -2X^TY + 2X^TXW \\
\end{aligned} \notag \\
\Downarrow_{\text{making it zero}} \notag \\ \notag \\
0 = -2X^TY + 2X^TXW^\star \notag \\
X^TXW^\star = X^TY \notag
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are chances that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is too
large, making equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{target}\)&lt;/span&gt; much computationally
expensive. In this case, we can use gradient descent. The update rule
will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
W^{(t+1)} &amp;amp;= W^{(t)} - \frac{\eta}{2} \nabla \mathrm{MSE}(W^{(t)})
\\
&amp;amp;= W^{(t)} - \eta(X^TXW^{(t)} -X^TY)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-probabilistic-view&#34;&gt;A Probabilistic View&lt;/h3&gt;
&lt;h4 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood
Estimation&lt;/h4&gt;
&lt;p&gt;In classification task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the
feature, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the label; in
regression task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the “label”,
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the “feature”. From a
probabilistic point of view, we would like estimate &lt;span class=&#34;math inline&#34;&gt;\(p(\text{feature}|\text{label})\)&lt;/span&gt;. In this
case, we treat &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as the “feature”
composed of a deterministic function and a noise sampled from an
identical and independent Gaussian distribution, i.e., for random
variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X, \mathcal Y\)&lt;/span&gt; (to
distinguish from matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), &lt;span class=&#34;math display&#34;&gt;\[
\mathcal Y = \mathcal XW + \epsilon, \text{ where }\epsilon \sim
\mathcal N(0, \sigma^2)
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
p(Y|X;W,\sigma^2) = p(\epsilon=Y - XW;\sigma^2) = \mathcal N(Y -
XW;0,\sigma^2I)
\]&lt;/span&gt; Then OLS can be attacked by maximum likelihood estimation. The
log-likelihood function will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(W,\sigma^2) &amp;amp;= \log(L(W, \sigma^2)) = \log(p(Y|X;W,\sigma^2)) =
\log \mathcal N(Y - XW;0,\sigma^2I) \\
&amp;amp;=
\log(\frac{1}{\sqrt{(2\pi)^M|\sigma^2I|}}e^{-\frac{1}{2\sigma^2}(Y -
XW)^T(Y - XW)}) \\
&amp;amp;= -\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2
- \frac{M}{2}\log 2\pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\arg \max_{W,\sigma^2}l(W, \sigma^2) &amp;amp;= \arg
\max_{W,\sigma^2}-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) -
\frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi \\
&amp;amp;= \arg \min_{W,\sigma^2}\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) +
\frac{M}{2}\log \sigma^2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and
make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\frac{1}{\sigma^2}(X^TXW^\star - X^TY) = 0 \\
X^TXW^\star = X^TY
\end{gathered}
\]&lt;/span&gt; Substitute &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; back,
take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and
make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{\star2}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\frac{M}{2\sigma^{\star2}} - \frac{(Y - XW^\star)^T(Y -
XW^\star)}{2(\sigma^{\star2})^2} = 0 \\
\sigma^{\star2} = \frac{1}{M}(Y - XW^\star)^T(Y - XW^\star)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may further analyze the efficacy of &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; as a point estimator. Suppose
&lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt; is invertible. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;W^\star = (X^T X)^{-1} X^T Y \\
&amp;amp;= (X^T X)^{-1} X^T (X W_\text{real} + Z) \\
&amp;amp;= W_\text{real} + (X^T X)^{-1} X^T Z
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; contains the
noise term for each sample. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;
is unbiased because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[W^\star] = \E[W_\text{real} + (X^T X)^{-1} X^T Z] \\
&amp;amp;= W_\text{real} + (X^T X)^{-1} X^T \E[Z] = W_\text{real}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;
will be: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\Cov[W^\star] = \Cov[W_\text{real} + (X^T X)^{-1} X^T Z] \\
&amp;amp;= 0 + (X^T X)^{-1} X^T \Cov[Z] X (X^T X)^{-1} \\
&amp;amp;= (X^T X)^{-1} X^T {\sigma^\star}^2 I X (X^T X)^{-1} \\
&amp;amp;= {\sigma^\star}^2 (X^T X)^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;maximum-a-posteriori&#34;&gt;Maximum a Posteriori&lt;/h4&gt;
&lt;p&gt;If we take the Bayesian view and add a prior to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(W
\sim \mathcal N(0,\frac{C}{2}I)\)&lt;/span&gt;, or rather &lt;span class=&#34;math inline&#34;&gt;\(p(W) \propto \exp(-\frac{1}{C}W^TW)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p(W|X, Y) = \frac{p(W, X, Y)}{p(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(X, W)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)P(X)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)}{P(Y|X)} \\
&amp;amp;\propto p(Y|X,W)p(W)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \max_W p(W|X,Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;regularized-variants&#34;&gt;Regularized Variants&lt;/h2&gt;
&lt;p&gt;Linear regression tends to have high variance, due to the sum of
random variables in its inference formula. This is why we tend to
restrict the magnitude of coefficients.&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;By adding a regularization term to OLS objective we obtain the ridge
regression: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \frac{\alpha}{2}||W||^2_2
\]&lt;/span&gt; By regularization, we are “shrink” the amount of &lt;span class=&#34;math inline&#34;&gt;\(||W||_2\)&lt;/span&gt;, whose magnitude is controlled by
the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We prefer smaller
weights because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smaller weights are more robust to the perturbations of input;&lt;/li&gt;
&lt;li&gt;there are better chances to zero out some dimensions of the input
feature, which may be uninformative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The constant term before the sum of square errors that before &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not of significance. We choose
them to be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; because this
gives a nicer closed-form solution to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Take derivative of the objective
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
-X^TY + X^TXW^\star + \alpha W^\star = 0 \\
(X^TX + \alpha I)W^\star = X^TY
\end{gathered}
\]&lt;/span&gt; The “ridge” comes from the &lt;span class=&#34;math inline&#34;&gt;\(\alpha
I\)&lt;/span&gt; term, which is added to the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h3&gt;
&lt;p&gt;In ridge regression, there is still chance that some weights are
small but not zero, because the regularization term is small so far as
the weights are fairly small. Think in this way: &lt;span class=&#34;math inline&#34;&gt;\(0.01^2 = 0.0001 \ll 0.01\)&lt;/span&gt;, though it is
not rigid.&lt;/p&gt;
&lt;p&gt;To get better regularization, we can change the regularization term
to &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \alpha||W||_1
\]&lt;/span&gt; The reason to choose &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;
norm is that, &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is the least number
that preserves convexity among the &lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norms. Interestingly, when the
penalty term on the &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm is
large enough, the resulting estimation will be zero (refer to &lt;a href=&#34;https://stats.stackexchange.com/questions/280823/what-is-the-mathematical-rigorous-proof-that-l1-regularization-will-give-sparse&#34;&gt;this&lt;/a&gt;).
The solution to the lasso regression can be efficiently approached using
&lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/coordinate-descent/&#34;&gt;coordinate descent&lt;/a&gt; or &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/least-angle-regression/&#34;&gt;least angle regression&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6018889.html&#34;&gt;Lasso回归算法：
坐标轴下降法与最小角回归法小结 - 刘建平Pinard - 博客园
(cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/76055830&#34;&gt;LASSO回归求解 - 知乎
(zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hastie.su.domains/StatLearnSparsity/&#34;&gt;Statistical
Learning with Sparsity: the Lasso and Generalizations
(su.domains)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Subgradient</title>
      <link>https://chunxy.github.io/notes/articles/optimization/subgradient/</link>
      <pubDate>Tue, 11 Jan 2022 16:09:23 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/subgradient/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Subgradient of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
\partial f(x) = \{g|f(y) \ge f(x) + g^T(y - x), \forall y \in dom(f) \}
\]&lt;/span&gt; Or put it another way, the subgradient defines a hyper-plane
passing through &lt;span class=&#34;math inline&#34;&gt;\((x,f(x))\)&lt;/span&gt;: $$ ^T ( -
) = 0,&lt;/p&gt;
y ^n, t \ &lt;span class=&#34;math display&#34;&gt;\[
And
\]&lt;/span&gt; ^T ( - ) , (y, t) epi(f) &lt;span class=&#34;math display&#34;&gt;\[
This is because by definition,
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
g^T(y - x) - (f(y) - f(x)) &amp;amp;\le 0, \forall y &amp;amp;\Rightarrow \\
g^T(y - x) - (t - f(x)) &amp;amp;\le 0, \forall y,\forall t \ge f(y)
&amp;amp;\Rightarrow \\
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
&amp;amp;\le 0, \forall (y, t) \in epi(f)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\left[\begin{array}\\ g \\ -1
\\\end{array}\right]\)&lt;/span&gt; is the normal of the tangent plane of
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;The subgradient &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is
a set, instead of a single number like the gradient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Subgradient is a monotonic operator: &lt;span class=&#34;math display&#34;&gt;\[
(u - v)^T(y - x) \ge 0, \forall u \in \partial f(y), \forall v \in
\partial f(x)
\]&lt;/span&gt; This can be shown by: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left.
\begin{array} \\
f(y) \ge f(x) + v^T(y - x) \\
f(x) \ge f(y) + u^T(x - y) \\
\end{array}
\right\}
&amp;amp;\Rightarrow
f(y) + f(x) \ge f(x) + f(y) - (u - v)^T(y - x) \\
&amp;amp;\Rightarrow (u - v)^T(y - x) \ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x^\star = \arg \min_x f(x) \iff 0 \in
\partial f(x^\star)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \{\nabla f(x)\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x) \and p \in
\partial f(x)\)&lt;/span&gt;, then for &lt;span class=&#34;math inline&#34;&gt;\(y = x + r(p
- \nabla f(x))\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;\ge p^T(y - x) \\
f(x + r(p - \nabla f(x))) - f(x) &amp;amp;\ge rp^T(p - \nabla f(x)) \\
\frac{f(x + r(p - \nabla f(x))) - f(x)}{r} &amp;amp;\ge p^T(p - \nabla f(x))
\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(p - \nabla f(x))^T(p - \nabla f(x)) &amp;amp;\le \frac{f(x + r(p - \nabla
f(x))) - f(x)}{r} - \nabla f(x)^T(p - \nabla f(x)) \\
||p - \nabla f(x)||^2_2 &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)-
\nabla f(x)^Tr(p - \nabla f(x))}{r} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take limit on &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; on both sides to
give &lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}||p - \nabla f(x)||^2_2 \le \lim_{r \to 0}\Big( \frac{f(x
+ r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big)
\\
\]&lt;/span&gt; By the definition of gradient, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f&amp;amp;(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))
\\
&amp;amp;= \mathcal{O}(||r(p - \nabla f(x))||^2_2) \\
&amp;amp;= ||p - \nabla f(x)||^2\mathcal{O}(r^2) \\
&amp;amp;= \mathcal{O}(r^2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla
f(x)^Tr(p - \nabla f(x))}{r} \Big) = \lim_{r \to
0}\frac{\mathcal{O}(r^2)}{r} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||p - \nabla f(x)||^2_2 \le 0 \\
0 \le ||p - \nabla f(x)||^2_2 \le 0 \\
||p - \nabla f(x)||^2_2 = 0
\end{gather}
\]&lt;/span&gt; contradicting the assumption that &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x)\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(p = \nabla f(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \alpha_1 f_1(x) + \alpha_2
f_2(x)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) =
\alpha_1 \partial f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/139083837&#34;&gt;凸优化笔记16：次梯度
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>5-kernel-methods</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/5-kernel-methods/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/5-kernel-methods/</guid>
      <description>

&lt;h2 id=&#34;kernel-methods&#34;&gt;Kernel Methods&lt;/h2&gt;
&lt;h3 id=&#34;approximation-error&#34;&gt;Approximation Error&lt;/h3&gt;
&lt;p&gt;While a bounded excess risk is necessary for satisfactory learning
performance, satisfactory learning results also require a small value
for &lt;span class=&#34;math inline&#34;&gt;\(L(f^*)\)&lt;/span&gt;, commonly called the
approximation error.&lt;/p&gt;
&lt;p&gt;We can decompose the loss of a supervised learning model as the sum
of &lt;strong&gt;excess risk&lt;/strong&gt; (also called estimation error or
variance &amp;lt;not necessarily the statistical one&amp;gt;) and
&lt;strong&gt;approximation error&lt;/strong&gt; (also called bias): &lt;span class=&#34;math display&#34;&gt;\[
L(\hat f) = \underbrace{L(\hat f) - L(f^*)}_{\text{excess risk}} +
\underbrace{\color{red}{L(f^*)}}_\text{approximation eror}
\]&lt;/span&gt; Another key question in machine learning is how to reduce the
approximation error.&lt;/p&gt;
&lt;h3 id=&#34;revisiting-linear-regression&#34;&gt;Revisiting Linear Regression&lt;/h3&gt;
&lt;p&gt;Recall that in &lt;strong&gt;linear regression&lt;/strong&gt; we want to learn a
linear model &lt;span class=&#34;math inline&#34;&gt;\(f_w(x) = w^T x\)&lt;/span&gt; to
predict a continuous label &lt;span class=&#34;math inline&#34;&gt;\(Y \in
\R\)&lt;/span&gt;. When using squared-error loss, we obtain the following ERM
task: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\min_{w}\ \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2
\tag{Linear Regression} \label{linreg}
\end{equation}
\]&lt;/span&gt; One way to empower the above formulation is to extend linear
function set to some richer function set. This is reminiscent of
powerful models like neural network. However, another powerful and
well-established approach is the kernel method which substitutes &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^d\)&lt;/span&gt; with a &lt;strong&gt;fixed&lt;/strong&gt;
feature map &lt;span class=&#34;math inline&#34;&gt;\(\phi(x): \R^d \to \R^m\)&lt;/span&gt;
transferring the input to a potentially high-dimensional space &lt;span class=&#34;math inline&#34;&gt;\(m \gg d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\min_w\ \frac{1}{n} \sum_{i=1}^n (w^\top \phi(x_i) - y_i)^2 \tag{Kernel
Method} \label{kernel-method}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The typical way to solve the above problem would be the gradient
descent: &lt;span class=&#34;math display&#34;&gt;\[
w^{t+1} = w^{t} - \gamma \frac{2}{n} \sum_{i=1}^n ((w^t)^\top \phi(x_i)
- y_i) \phi(x_i)
\]&lt;/span&gt; One observation (&lt;strong&gt;representer theorem&lt;/strong&gt;) is
that, the update to &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is a linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_i)\)&lt;/span&gt;’s. If
&lt;span class=&#34;math inline&#34;&gt;\(w^0 = 0\)&lt;/span&gt;, we have that &lt;span class=&#34;math inline&#34;&gt;\(w^t \in \Col(\phi(x_1),\dots,\phi(x_n))\)&lt;/span&gt;.
In this sense, the update of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is controlled by &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_1),\dots,\phi(x_n)\)&lt;/span&gt;, which is only
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional. If &lt;span class=&#34;math inline&#34;&gt;\(m &amp;lt; n\)&lt;/span&gt;, the above is not interesting
because &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional vectors already cover &lt;span class=&#34;math inline&#34;&gt;\(\R^m\)&lt;/span&gt;. On the other hand if &lt;span class=&#34;math inline&#34;&gt;\(m &amp;gt; n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(w^t\)&lt;/span&gt; will reside in a &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional subspace. &lt;strong&gt;We reduce
an &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional optimization
problem to an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional
one.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s rewrite &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; as the linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_1),\dots,\phi(x_n)\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(w = \sum_{j=1}^n \alpha_j \phi(x_j)\)&lt;/span&gt;. The
problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\min_\alpha \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^n \alpha_j
\underbrace{\langle \phi(x_j), \phi(x_i) \rangle}_{\mathcal{K}(x_j,
x_i)} - y_i \right)^2
\]&lt;/span&gt; It turns out that the denotation of the problem can be
simplified as &lt;span class=&#34;math display&#34;&gt;\[
\min_\alpha \frac{1}{n} \| y - K \alpha \|_2^2
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
y = [y_1,\dots,y_n]^\top, \alpha = [\alpha_1, \dots, \alpha_n]^\top \\
K =
\begin{pmatrix}
\mathcal{K}(x_1, x_1) &amp;amp; \cdots &amp;amp; \mathcal{K}(x_1, x_n) \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal{K}(x_n, x_1) &amp;amp; \cdots &amp;amp; \mathcal{K}(x_n, x_n)
\end{pmatrix}
\]&lt;/span&gt; The question remains for what function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}: \R^m \times \R^m \to \R\)&lt;/span&gt;,
there exists a &lt;span class=&#34;math inline&#34;&gt;\(\phi: \R^d \to \R\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}(x,x&amp;#39;) = \langle
\phi(x), \phi(x&amp;#39;) \rangle\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-function&#34;&gt;Kernel Function&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;kernel function&lt;/strong&gt;. We call &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}: \mathcal{X} \times \mathcal{X} \to
\R\)&lt;/span&gt; a kernel function, if for every integer &lt;span class=&#34;math inline&#34;&gt;\(t \in \N\)&lt;/span&gt; and vectors &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_t \in \mathcal{X}\)&lt;/span&gt;, the
matrix &lt;span class=&#34;math inline&#34;&gt;\(K \in \R^{t \times t}\)&lt;/span&gt; with
the &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt;-th entry &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}(x_i, x_j)\)&lt;/span&gt; will be symmetric
and positive semi-definite (why better PSD??): &lt;span class=&#34;math display&#34;&gt;\[
K =
\begin{pmatrix}
\mathcal{K}(x_1, x_1) &amp;amp; \cdots &amp;amp; \mathcal{K}(x_1, x_n) \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal{K}(x_n, x_1) &amp;amp; \cdots &amp;amp; \mathcal{K}(x_n, x_n)
\end{pmatrix} \succeq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}: \R^d \times \R^d
\to \R\)&lt;/span&gt; is a kernel function if and only if there exists a
feature map &lt;span class=&#34;math inline&#34;&gt;\(\phi: \R^d \to \R^m\)&lt;/span&gt;
such that for every &lt;span class=&#34;math inline&#34;&gt;\(x, x&amp;#39;\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{K}(x, x&amp;#39;) = \langle \phi(x), \phi(x&amp;#39;) \rangle
\]&lt;/span&gt; Note that there is a subtlety that &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; can be &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear kernel&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_\text{linear}(x, x&amp;#39;) = \langle x,
x&amp;#39; \rangle\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Degree-&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; &lt;strong&gt;polynomial
kernel&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_\text{poly-$r$}(x, x&amp;#39;) = (1 +
\langle x, x&amp;#39; \rangle)^r\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The interesting thing about the polynomial kernel is that, if we
directly use the degree-&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; feature
map &lt;span class=&#34;math inline&#34;&gt;\(\phi_r\)&lt;/span&gt;, the number of variables
will be &lt;span class=&#34;math inline&#34;&gt;\(m = O(d^r)\)&lt;/span&gt;; but in this
case, the number of variables is just &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;​.&lt;/p&gt;
&lt;p&gt;As for the proof, it simply follows from the fact that sum (between
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x^\top x\)&lt;/span&gt;) and product (among &lt;span class=&#34;math inline&#34;&gt;\((1+x^\top x)\)&lt;/span&gt;’s) of kernel functions are
kernel functions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gaussian kernel&lt;/strong&gt; with bandwidth &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_\text{Gaussian($\sigma^2$)}(x, x’) =
\exp(-\frac{\|x-x&amp;#39;\|_2^2}{2\sigma^2})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The reason why we don’t use &lt;span class=&#34;math inline&#34;&gt;\(f(x, x&amp;#39;)
= -\|x-x&amp;#39;\|_2^2\)&lt;/span&gt;​ is that the resulting matrix is not
PSD.&lt;/p&gt;
&lt;p&gt;As for the proof, we can separate the terms: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\exp(-\frac{\|x-x&amp;#39;\|_2^2}{2\sigma^2}) =
\exp(-\frac{\|x\|_2^2}{2\sigma^2} - \frac{\|x&amp;#39;\|_2^2}{2\sigma^2} +
\frac{x^T x&amp;#39;}{\sigma^2}) \\
&amp;amp;= \underbrace{\exp(-\frac{\|x\|_2^2}{2\sigma^2})}_{\varphi(x)}
\underbrace{\exp(-\frac{\|x&amp;#39;\|_2^2}{2\sigma^2})}_{\varphi(x&amp;#39;)}
\exp(\frac{x^T x&amp;#39;}{\sigma^2}) \\
\end{aligned}
\]&lt;/span&gt; Now it remains to show that &lt;span class=&#34;math inline&#34;&gt;\(\exp(\frac{x^T x&amp;#39;}{\sigma^2})\)&lt;/span&gt; is a
kernel function. To show it, recall the Taylor series: &lt;span class=&#34;math display&#34;&gt;\[
\exp(\frac{x^T x&amp;#39;}{\sigma^2}) = \sum_{i=0}^\infty \frac{(x^T
x&amp;#39;)^m}{\sigma^{2m} i!}
\]&lt;/span&gt; which is the sum of infinite-many kernel functions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;properties&#34;&gt;Properties&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;sum of kernel functions&lt;/strong&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_2\)&lt;/span&gt; are kernel functions, then
their sum, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{K}(x,x&amp;#39;) = \mathcal{K}_1(x,x&amp;#39;) +
\mathcal{K}_2(x,x&amp;#39;)
\]&lt;/span&gt; will also be a kernel function.&lt;/p&gt;
&lt;p&gt;Proof. Consider concatenating &lt;span class=&#34;math inline&#34;&gt;\(\phi(x) =
[\phi_1(x), \phi_2(x)]\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;product of kernel functions&lt;/strong&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}_2\)&lt;/span&gt; are kernel functions, then
their product, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{K}(x,x&amp;#39;) = \mathcal{K}_1(x,x&amp;#39;) \times
\mathcal{K}_2(x,x&amp;#39;)
\]&lt;/span&gt; will also be a kernel function.&lt;/p&gt;
&lt;p&gt;Proof. Let &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; be two symmetric
matrices whose spectral decompositions are &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A \odot B &amp;amp;= \left( \sum_{i=1}^t \lambda_i u_i u_i^T \right) \odot
\left( \sum_{j=1}^{t&amp;#39;} \gamma_j v_j v_j^T \right) \\
&amp;amp;= \sum_{i=1}^t \sum_{j=1}^{t&amp;#39;} \lambda_i \gamma_j (u_i u_i^T)
\odot (v_j v_j^T) \\
&amp;amp;= \sum_{i=1}^t \sum_{j=1}^{t&amp;#39;} \lambda_i \gamma_j (u_i \odot
v_j) (u_i \odot v_j)^T \\
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math inline&#34;&gt;\(A \odot B\)&lt;/span&gt; is
PSD.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;defining-function-space-via-kernel&#34;&gt;Defining Function Space via
Kernel&lt;/h4&gt;
&lt;p&gt;The motivation of RKHS is that, we need to define a function space
(which is a linear transformation of the feature function associated
with the kernel function) based on the kernel function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt;. The reason why we need such
function space is that, we can never resort to optimizing over &lt;span class=&#34;math inline&#34;&gt;\(\set{w^T \phi: w \in \R^m}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is not guaranteed to have a
closed-form formula in kernel method. We need to find a surrogate
function space to do the optimization.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;reproducing kernel Hilbert space (RKHS)&lt;/strong&gt;.
For kernel function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt;, we
define the reproducing kernel Hilbert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; as the following set of
functions: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H} = \{ f(x) \triangleq \sum_{i=1}^t \alpha_i \mathcal{K}(x_i,
x): t \in \N, \alpha_1,\dots,\alpha_t \in \R, x_1,\dots,x_t \in
\mathcal{X} \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This looks frustrating because it seems that we convert an &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional optimization problem to an
infinite-dimensional one. But later the &lt;em&gt;representer theorem&lt;/em&gt;
will show us interesting conclusion.&lt;/p&gt;
&lt;h5 id=&#34;example-rkhs-of-linear-kernel&#34;&gt;Example: RKHS of linear
kernel&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal{H} &amp;amp;= \{ \sum_{i=1}^t \alpha_i x_i^T x: t \in \N,
\alpha_1,\dots,\alpha_t \in \R, x_1,\dots,x_t \in \mathcal{X} \} \\
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^t \alpha_i x_i^T x = (\sum_{i=1}^t \alpha_i x_i^T) x
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((\sum_{i=1}^t \alpha_i
x_i^T)\)&lt;/span&gt; can span all the &lt;span class=&#34;math inline&#34;&gt;\(w \in
\R^d\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; is the set of all the linear
functions.&lt;/p&gt;
&lt;h5 id=&#34;example-rkhs-of-polynomial-kernel&#34;&gt;Example: RKHS of polynomial
kernel&lt;/h5&gt;
&lt;p&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(r=2\)&lt;/span&gt;​ as an example. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal{H} &amp;amp;= \{ \sum_{i=1}^t \alpha_i (1 + x_i^T x)^2: t \in \N,
\alpha_1,\dots,\alpha_t \in \R, x_1,\dots,x_t \in \mathcal{X} \} \\
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\sum_{i=1}^t \alpha_i (1 + x_i^T x)^2 = \sum_{i=1}^t \alpha_i (1 +
2 x_i^T x + (x_i^T x)^2) \\
&amp;amp;= \sum_{i=1}^t \alpha_i + (\sum_{i=1}^t 2\alpha_i x_i^T) x +
\sum_{i=1}^t \alpha_i (x^T x_i x_i^T x) \\
&amp;amp;= \sum_{i=1}^t \alpha_i + (\sum_{i=1}^t 2\alpha_i x_i^T) x + x^T
(\sum_{i=1}^t \alpha_i x_i x_i^T) x \\
\end{aligned}
\]&lt;/span&gt; Now with the quadratic form &lt;span class=&#34;math inline&#34;&gt;\(x^T A
x + bx + c\)&lt;/span&gt; for arbitrary symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with rank &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and constant &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, we first apply spectral decomposition
to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to get &lt;span class=&#34;math display&#34;&gt;\[
A = \sum_{i=1}^r \lambda_i v_i v_i^T
\]&lt;/span&gt; For &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,r\)&lt;/span&gt;, we
choose &lt;span class=&#34;math display&#34;&gt;\[
x_i = v_i, x_{r+i} = -v_{i}, \\
b = \sum_{i=1}^r \gamma_i v_i ??, \\
\alpha_i + \alpha_{r+i} = \lambda_{i}, \\
\alpha_i - \alpha_{r+i} = \gamma_{i}/2 \\
\]&lt;/span&gt; Further, we can choose &lt;span class=&#34;math inline&#34;&gt;\(x_{2r+1},\dots,x_{2r+k} = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{2r+1},\dots,\alpha_{2r+k}\)&lt;/span&gt; such
that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_{2r+k} =
c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this way, we can rewrite any quadratic function into the form of
element of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;. As a
result, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;​ is the set of
all the quadratic functions.&lt;/p&gt;
&lt;h4 id=&#34;hilbert-space&#34;&gt;Hilbert Space&lt;/h4&gt;
&lt;p&gt;Note that in the above we go to RKHS via the kernel instead of the
feature function. This is due to the same reason that the computation of
feature function is in a higher dimension, which is more costly.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;inner product in an RKHS&lt;/strong&gt;. Given two
functions &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \sum_{i=1}^t \alpha_i
\mathcal{K}(x_i, x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(x) =
\sum_{j=1}^{t&amp;#39;} = \alpha_j&amp;#39; \mathcal{K}(x_j&amp;#39;, x)\)&lt;/span&gt; in
the RKHS &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; of kernel
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt;, we define their inner
produce as &lt;span class=&#34;math display&#34;&gt;\[
\langle f,g \rangle = \sum_{i=1}^t \sum_{j=1}^{t&amp;#39;} \alpha_i \beta_j
\mathcal{K}(x_i, x_j&amp;#39;)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;RKHS with the defined inner product is a Hilbert
space&lt;/strong&gt;. The RKHS defined for a kernel &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt; coupled with the above inner
product will result in a Hilbert space, where the following holds for
every &lt;span class=&#34;math inline&#34;&gt;\(f, f_1, f_2, g \in
\mathcal{H}\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\langle f, g
\rangle = \langle g, f \rangle\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linearity and homogenity&lt;/strong&gt;: for all &lt;span class=&#34;math inline&#34;&gt;\(\gamma \in \R\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\langle f_1 + \gamma f_2, g \rangle = \langle f_1,
g \rangle + \gamma \langle f_2, g \rangle\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positive definiteness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\langle f, f \rangle \ge 0\)&lt;/span&gt; where the
equality holds only for &lt;span class=&#34;math inline&#34;&gt;\(f = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;With the Hilbert space, we can determine the norm resident in it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;norm in an RKHS&lt;/strong&gt;. For a function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \sum_{i=1}^t \alpha_i \mathcal{K}(x_i,
x)\)&lt;/span&gt; in the RKHS &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;
for kernel &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt;, we define
its norm as &lt;span class=&#34;math display&#34;&gt;\[
\|f\|_\mathcal{H}^2 \triangleq \langle f,f \rangle = \sum_{i=1}^t
\sum_{j=1}^t \alpha_i \alpha_j \mathcal{K}(x_i, x_j) = \alpha^\top K
\alpha
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(K \in \R^{t \times
t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K_{ij} = \mathcal{K}(x_i,
x_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;learning-with-kernel-functions&#34;&gt;Learning with Kernel
Functions&lt;/h3&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(f(x) = w^T x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(x) = (w&amp;#39;)^T x\)&lt;/span&gt; that belong to the
&lt;u&gt;RKHS of linear kernel&lt;/u&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_\text{linear} = \{ f_w(x) = w^\top x:
w \in \R^d \}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\langle f,g \rangle &amp;amp;= \sum_i \sum_j \alpha_i \beta_j x_i^T x_j \\
&amp;amp;= \sum_i \alpha_i x_i^T \sum_j \beta_j x_j \\
&amp;amp;= \sum_i \alpha_i x_i^T w&amp;#39; \\
&amp;amp;= w^T w&amp;#39; \\
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math inline&#34;&gt;\(\|f\|_{\mathcal{H}_\text{linear}} =
\|w\|\)&lt;/span&gt;. We generalize from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{linreg}\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\min_{f \in \mathcal{H}}\ \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i) +
Q(\|f\|_\mathcal{H}) \tag{Regulaized Kernel Method}
\label{regularized-kernel-method}
\end{equation}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; is a
general RKHS &lt;span class=&#34;math inline&#34;&gt;\(Q: \R^+ \to \R\)&lt;/span&gt; is an
increasing function acting as a regularization term.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;representer theorem&lt;/strong&gt;. Given samples &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt;, consider the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{regularized-kernel-method}\)&lt;/span&gt;
problem. Every optimal solution &lt;span class=&#34;math inline&#34;&gt;\(f^* \in
\mathcal{H}\)&lt;/span&gt; satisfies the following for some real coefficients
&lt;span class=&#34;math inline&#34;&gt;\(\alpha_1,\dots,\alpha_n \in \R\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
f^*(x) = \sum_{j=1}^n \alpha_j \mathcal{K}(x_j, x)
\]&lt;/span&gt; Proof. To show it, first note that &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; comes from &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; be the feature function associated
with the kernel function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(f^*(x)\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\((w^*)^\top \phi(x)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(w^* = \sum_{j=1}^n \alpha_j \phi(x_j) + v\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is the component that is
perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(\Col(\phi(x_1), \dots,
\phi(x_n))\)&lt;/span&gt;​.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde w = \sum_{j=1}^n \alpha_j
\phi(x_j)\)&lt;/span&gt;. We claim that &lt;span class=&#34;math inline&#34;&gt;\(v =
0\)&lt;/span&gt;; otherwise &lt;span class=&#34;math inline&#34;&gt;\(\tilde f(x) = \tilde
w^\top x\)&lt;/span&gt; incurs the same loss but smaller norm, contradicting
the fact that &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the
optimal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Representer theorem implies that the kernel method solves the
following optimization problem: &lt;span class=&#34;math display&#34;&gt;\[
\min_{\alpha \in \R^n} \frac{1}{n} \sum_{i=1}^n \ell \left( \sum_{j=1}^n
\alpha_j \mathcal{K}(x_j, x_i), y_i \right) + Q(\sqrt{\alpha^\top K
\alpha})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note how we drop the intractable &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{kernel-method}\)&lt;/span&gt; in the above
formulation.&lt;/p&gt;
&lt;h4 id=&#34;example-kernel-based-ridge-regression&#34;&gt;Example: Kernel-based
Ridge Regression&lt;/h4&gt;
&lt;p&gt;We choose &lt;span class=&#34;math inline&#34;&gt;\(\ell(\hat y, y) = (\hat
y-y)^2\)&lt;/span&gt;. We choose &lt;span class=&#34;math inline&#34;&gt;\(Q(w) = \lambda
\|w\|_2^2\)&lt;/span&gt;. The kernel function is the linear kernel. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\min_{w \in \R^d} \sum_{i=1}^n (w^\top x_i -y_i)^2 + \lambda \|w\|_2^2
\\
\Downarrow \\
\min_{\alpha \in \R^n} \|y - K \alpha\|_2^2 + \lambda \alpha^\top K
\alpha
\end{gathered}
\]&lt;/span&gt; The above is a convex optimization problem. As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
2 K (K \alpha^* - y) + 2\lambda K \alpha^* &amp;amp;= 0 \\
K (K + \lambda) \alpha^* &amp;amp;= K y \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha^* = (K + \lambda I)^{-1} K
y\)&lt;/span&gt; will be the solution.&lt;/p&gt;
&lt;h4 id=&#34;example-kernel-based-svm&#34;&gt;Example: Kernel-based SVM&lt;/h4&gt;
&lt;p&gt;We choose &lt;span class=&#34;math inline&#34;&gt;\(\ell(\hat y, y) = \max(0, 1 -
\hat y y)\)&lt;/span&gt; which is the &lt;strong&gt;hinge loss&lt;/strong&gt;. We choose
&lt;span class=&#34;math inline&#34;&gt;\(Q(w) = \lambda \|w\|_2^2\)&lt;/span&gt;. The
kernel function is the linear kernel. &lt;span class=&#34;math display&#34;&gt;\[
\min_{w \in \R^d} \sum_{i=1}^n \ell(w^T x_i, y_i) + \lambda \|w\|_2^2 \\
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\max(0, 1-z) =
\max_{\alpha \in [0,1]} \alpha(1-z)\)&lt;/span&gt;. The problem can be
reformulated as &lt;span class=&#34;math display&#34;&gt;\[
\min_{w \in \R^d} \max_{\alpha \in [0,1]^n} \sum_{i=1}^n \alpha_i (1 -
y_i w^T x_i) + \lambda \|w\|_2^2 \\
\]&lt;/span&gt; The &lt;u&gt;minimax theorem&lt;/u&gt; implies that (the reason for
choosing &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in [0,1]^n\)&lt;/span&gt; instead
of &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in \{ 0,1 \}^n\)&lt;/span&gt; is that
we want to satisfy the condition of minimax theorem) we can swap the
&lt;span class=&#34;math inline&#34;&gt;\(\min\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\max\)&lt;/span&gt;​ in the above formula to obtain
&lt;span class=&#34;math display&#34;&gt;\[
\max_{\alpha \in [0,1]^n} \min_{w \in \R^d} \sum_{i=1}^n \alpha_i (1 -
y_i w^T x_i) + \lambda \|w\|_2^2 \\
\]&lt;/span&gt; The minimization term inside gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min_{w \in \R^d} \sum_{i=1}^n \alpha_i (1 - y_i w^\top x_i) +
\lambda \|w\|_2^2 \\
=\ &amp;amp;\sum_{i=1}^n \alpha_i + \min_{w \in \R^d} - \sum_{i=1}^n
\alpha_i y_i x_i^\top w + \lambda \|w\|_2^2 \\
=\ &amp;amp;\sum_{i=1}^n \alpha_i - \frac{2}{\lambda} \big\| \sum_{i=1}^n
\alpha_i y_i x_i^\top \big\|_2^2 \\
=\ &amp;amp;\sum_{i=1}^n \alpha_i - \frac{2}{\lambda} \sum_{i=1}^n
\sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangle \\
\end{aligned}
\]&lt;/span&gt; Therefore, the dual optimization problem to SVM is &lt;span class=&#34;math display&#34;&gt;\[
\max_{\alpha \in [0,1]^n} \sum_{i=1}^n \alpha_i - \frac{2}{\lambda}
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \underbrace{\langle
x_i, x_j \rangle}_{\mathcal{K}(x_i, x_j)}
\]&lt;/span&gt; If we define &lt;span class=&#34;math inline&#34;&gt;\(\tilde K = [y_i y_j
\mathcal{K}(x_i, x_j)]\)&lt;/span&gt;, the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\max_{\alpha \in [0,1]^n} 1^\top \alpha - \frac{2}{\lambda} \alpha^\top
\tilde K \alpha
\]&lt;/span&gt; The method for solving such constrained optimization would be
the coordinate descent.&lt;/p&gt;
&lt;h3 id=&#34;shift-invariant-kernel-functions&#34;&gt;Shift-invariant Kernel
Functions&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;shift-invariant kernel&lt;/strong&gt;. We call a kernel
function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}: \mathcal{X} \times
\mathcal{X} \to \R\)&lt;/span&gt; shift-invariant if there exists function
&lt;span class=&#34;math inline&#34;&gt;\(\kappa: \mathcal{X} \to \R\)&lt;/span&gt; such
that for every &lt;span class=&#34;math inline&#34;&gt;\(x,x&amp;#39; \in
\mathcal{X}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{K}(x,x&amp;#39;) = \kappa(x-x&amp;#39;)
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;
&lt;strong&gt;must be even&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One example would be the Gaussian kernel. On the other hand, linear
kernel and polynomial kernel are not shift-invariant (easy to verify).
An important question to ask, &lt;strong&gt;how to determine whether a kernel
function is shift-invariant or not&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;Fourier series&lt;/strong&gt;. Given a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^d \to \R\)&lt;/span&gt;, we define its
&lt;strong&gt;Fourier (transform) series&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat f: \R \to \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
\hat f(\omega) \triangleq \int f(x) \exp(-i\omega^\top x) \d x
\]&lt;/span&gt; The &lt;strong&gt;inverse Fourier transform&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; would be (related to spherical
coordinate??) &lt;span class=&#34;math display&#34;&gt;\[
f(x) = (\frac{1}{2\pi})^d \int \hat f(\omega) \exp(i \omega^\top x) \d
\omega \\
\Rightarrow (\frac{1}{2\pi})^d \int \hat f(\omega) \d \omega = f(0)
\]&lt;/span&gt; The Fouries (transform) series of &lt;span class=&#34;math inline&#34;&gt;\(\hat f\)&lt;/span&gt; would be &lt;span class=&#34;math display&#34;&gt;\[
\hat{\hat f}(x) = \int \hat f(\omega) \exp(-i\omega^\top x) \d x =
(\frac{1}{2 \pi})^d f(-x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Bochner’s theorem&lt;/strong&gt;. A function &lt;span class=&#34;math inline&#34;&gt;\(\kappa: \R^d \to \R\)&lt;/span&gt; results in a valid
shift-invariant kernel &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}(x,x&amp;#39;)
= \kappa(x-x&amp;#39;)\)&lt;/span&gt; if and only if its Fourier transform &lt;span class=&#34;math inline&#34;&gt;\(\hat \kappa\)&lt;/span&gt; is real and non-negative
everywhere, i.e., &lt;span class=&#34;math display&#34;&gt;\[
\forall \omega \in \R^d, \hat \kappa(\omega) \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;example-gaussian-kernel&#34;&gt;Example: Gaussian Kernel&lt;/h4&gt;
&lt;p&gt;The Gaussian kernel &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}(x,x&amp;#39;)
= \kappa_\text{Gaussian}(z \triangleq x-x&amp;#39;) =
\exp(-\frac{\|z\|_2^2}{2\sigma^2})\)&lt;/span&gt; is a valid and
shift-invariant kernel function because &lt;span class=&#34;math display&#34;&gt;\[
\hat \kappa_\text{Gaussian}(\omega) = ({\sqrt{2\pi}}{\sigma})^d
\exp(-\frac{\sigma^2 \|\omega\|_2^2}{2}) \ge 0
\]&lt;/span&gt; There are two interesting observations:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Gaussian-shaped function is &lt;strong&gt;fixed point&lt;/strong&gt; w.r.t.
Fourier transform in that its Fourier series is still
Gaussian-shaped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{2 \pi})^d \hat
\kappa_\text{Gaussian}\)&lt;/span&gt; is in essence the PDF of normal
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,
\frac{1}{\sigma^2} I)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is small, &lt;span class=&#34;math inline&#34;&gt;\(\hat \kappa_\text{Gaussian}(\omega)\)&lt;/span&gt; will
spread out; if &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is large,
&lt;span class=&#34;math inline&#34;&gt;\(\hat \kappa_\text{Gaussian}(\omega)\)&lt;/span&gt;
will concentrate.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;non-example-box-similarity-score&#34;&gt;Non-Example: Box Similarity
Score&lt;/h4&gt;
&lt;p&gt;A non-example of shift-invariant kernel is the &lt;strong&gt;box similarity
score&lt;/strong&gt; where &lt;span class=&#34;math display&#34;&gt;\[
s(x,x&amp;#39;) = \begin{cases}
1 &amp;amp; \text{if $\|x-x&amp;#39;\|_2 \le \epsilon$} \\
0 &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt; In this example, we can easily tell that &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is shift-invariant. But we still have
to check if box similarity score is a valid kernel function. We form the
following function &lt;span class=&#34;math display&#34;&gt;\[
\Pi_\epsilon(z) = \begin{cases}
1 &amp;amp; \text{if $\|z\| \le \epsilon$} \\
0 &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt; We attempt to check both the validity and the shift-invariance
in one gut via Bochner’s theorem. Let’s try with &lt;span class=&#34;math inline&#34;&gt;\(x \in \R\)&lt;/span&gt; first: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\hat \Pi_\epsilon(\omega) = \int_{-\infty}^{\infty} \Pi_\epsilon(x)
\exp(-i \omega x) \d x \\
&amp;amp;= \int_{-\epsilon}^{+\epsilon} [\cos (\omega x) + i \sin(\omega x)]
\d x \\
&amp;amp;= \int_{-\epsilon}^{+\epsilon} \cos (\omega x) \d x \\
&amp;amp;= \frac{2 \sin (\omega \epsilon)}{\omega}
\end{aligned}
\]&lt;/span&gt; The Fourier series &lt;span class=&#34;math inline&#34;&gt;\(\hat
\Pi_\epsilon\)&lt;/span&gt; is not non-negative everywhere. As a result, box
similarity score is not valid or not shift-invariant. But box similarity
score is shift-invariant. Thus, box similarity score is not valid.&lt;/p&gt;
&lt;h4 id=&#34;example-sinc-kernel&#34;&gt;Example: Sinc Kernel&lt;/h4&gt;
&lt;p&gt;The kernel function associated with the &lt;span class=&#34;math inline&#34;&gt;\(\DeclareMathOperator{\sinc}{sinc}
\kappa_\text{sinc}(z) = \sinc z \triangleq \frac{\sin(\pi z)}{\pi
z}\)&lt;/span&gt; function (note that here &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is univariate) is a valid and
shift-invariant kernel function. This holds because &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat \Pi_\epsilon(\omega) = \frac{2 \sin (\omega \epsilon)}{\omega} = 2
\epsilon \sinc(\frac{\omega \epsilon}{\pi}) \\
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}
\frac{1}{2\pi} \Pi_\epsilon(-z) &amp;amp;= \hat{\hat \Pi}_\epsilon(z) \\
&amp;amp;\Downarrow_\text{evenness of $\Pi_\epsilon$} \\
\frac{1}{2\pi} \Pi_\epsilon(z) &amp;amp;=\int 2 \epsilon \sinc(\frac{\omega
\epsilon}{\pi}) \exp(-i\omega z)\d \omega \\
\frac{1}{2\pi} \Pi_\epsilon(z) &amp;amp;= C_1 \cdot \widehat{\sinc}(C_2 z)
\ge 0
\end{aligned} \\
\end{gathered}
\]&lt;/span&gt; In fact, &lt;span class=&#34;math display&#34;&gt;\[
\widehat{\sinc}(\omega) = \mathbb{1}[-1 \le \omega \le 1] \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above inspires us of &lt;strong&gt;how to construct a shift-invariant
kernel&lt;/strong&gt;: begin from a non-negative even function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and take &lt;span class=&#34;math inline&#34;&gt;\(\hat f\)&lt;/span&gt;’s Fourier series as the kernel
function. &lt;span class=&#34;math inline&#34;&gt;\(\hat f\)&lt;/span&gt; in this case is
also called the &lt;strong&gt;synthesis function&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;random-fourier-features&#34;&gt;Random Fourier Features&lt;/h4&gt;
&lt;p&gt;Recall that in the synthesizing process, &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; is the carefully-chosen
non-negative even function to form a valid shift-invariant kernel.
However, Fourier transform of &lt;span class=&#34;math inline&#34;&gt;\(\hat
\kappa\)&lt;/span&gt; is not always easy to derive. To tackle it, note that
because &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; is non-negative and
even, &lt;span class=&#34;math inline&#34;&gt;\(\hat \kappa\)&lt;/span&gt; is also
non-negative and even. Recall the inverse Fourier transform: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
(\frac{1}{2\pi})^d \int \hat \kappa(\omega) \d \omega = \kappa(0) \\ \\
\begin{aligned}
\kappa(\underbrace{x-x&amp;#39;}_z) &amp;amp;= (\frac{1}{2 \pi})^d \int \hat
\kappa(\omega) \exp(i \omega z) \d \omega \ge 0 \\
% &amp;amp;\Downarrow_{\text{connecting to synthesizing due to the evenness
of $\hat \kappa$ ??}} \\
% &amp;amp;= (\frac{1}{2 \pi})^d \int \hat \kappa(\omega) \exp(-i \omega z)
\d \omega \\
&amp;amp;= (\frac{1}{2 \pi})^d \int \hat \kappa(\omega) \exp(i \omega x - i
\omega x&amp;#39;) \d \omega \\
\end{aligned}
\end{gathered}
\]&lt;/span&gt; We can intentionally rescale &lt;span class=&#34;math inline&#34;&gt;\(\kappa(0)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{2\pi})^d \hat \kappa\)&lt;/span&gt; is a PDF
and &lt;span class=&#34;math inline&#34;&gt;\(\kappa(z)\)&lt;/span&gt; is in a form of
expectation: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\kappa(z) = \E_{\omega \sim (\frac{1}{2 \pi})^d \hat \kappa}[e^{i
\omega^\top (x-x&amp;#39;)}] = \E[\underbrace{\exp(iw^\top x)}_{\phi(x)}
\underbrace{\exp(-iw^\top x&amp;#39;)}_{\overline{\phi(x&amp;#39;)}}] \\
&amp;amp;= \E_{\omega \sim (\frac{1}{2 \pi})^d \hat \kappa}[\cos\omega
(x-x&amp;#39;) - i\sin\omega (x-x&amp;#39;)] \\
&amp;amp;\Downarrow_\text{evenness of $\hat \kappa$} \\
&amp;amp;= \E_{\omega \sim (\frac{1}{2 \pi})^d \hat \kappa}[\cos\omega
(x-x&amp;#39;)] \\
&amp;amp;= \E_{\omega \sim (\frac{1}{2 \pi})^d \hat \kappa}[\cos \omega x
\cos \omega x&amp;#39; + \sin \omega x \sin \omega x&amp;#39;] \\
&amp;amp;\Downarrow_\text{using $r$ samples} \\
&amp;amp;\approx \frac{1}{r} \sum_{i=1}^r (\cos \omega_i x \cos \omega_i
x&amp;#39; + \sin \omega_i x \sin \omega_i x&amp;#39;)
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde \phi: \R^d \to
\R^{2r}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde \phi(x) =
\frac{1}{\sqrt{r}}[\cos(\omega_1 x), \dots, \cos(\omega_r x),
\sin(\omega_1 x), \dots, \sin(\omega_r x)]\)&lt;/span&gt;. We have &lt;span class=&#34;math display&#34;&gt;\[
\kappa(z) \approx \tilde \phi(x)^\top \tilde \phi(x&amp;#39;)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tilde \phi\)&lt;/span&gt; is called the
&lt;strong&gt;random Fourier features&lt;/strong&gt;. With this approximation, we
obtain the proxy problem to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{kernel-method}\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\min_{w \in \R^m}\ \frac{1}{n} \sum_{i=1}^n (w^\top \phi(x) - y_i)^2 \\
\approx \\
\min_{\tilde w \in \R^{2r}}\ \frac{1}{n} \sum_{i=1}^n (\tilde w^\top
\tilde\phi(x) - y_i)^2 \\
\end{gather*}
\]&lt;/span&gt; This trick is especially helpful in Gaussian kernel case where
&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. Using the kernel function
(instead of feature function) notation, we have the following
theorem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;approximation error of random Fourier
features&lt;/strong&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt; is a shift-invariant kernel
function. We consider a subset of the resulting RKHS &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; where the Fourier
coefficients are bounded by &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{H}_C \triangleq \{ (\frac{1}{2\pi})^d \int \alpha(\omega) \hat
\kappa(\omega) \underbrace{\exp(i\omega z)}_{\phi_{\omega}(z)} \d
\omega: |\alpha(\omega)| \le C \}
\]&lt;/span&gt; Consider the norm &lt;span class=&#34;math inline&#34;&gt;\(\| \cdot
\|\)&lt;/span&gt; inducted by a distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;-based inner product &lt;span class=&#34;math inline&#34;&gt;\(\langle f,g \rangle = \E_q[f(x) g(x)]\)&lt;/span&gt;.
Then for &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; and sample &lt;span class=&#34;math inline&#34;&gt;\(\omega_1,\dots,\omega_m\)&lt;/span&gt; i.i.d. drawn from
&lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{2\pi})^d \hat \kappa\)&lt;/span&gt;,
with probability at least &lt;span class=&#34;math inline&#34;&gt;\(1-\delta\)&lt;/span&gt;,
there exists coefficients &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1,\dots,\alpha_m\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\| \frac{1}{m} \sum_{i=1}^m \alpha_i \phi_{\omega_i} - f^* \| \le
\sqrt{\frac{C^2}{m}} + \sqrt{\frac{2 \log(1/\delta)}{m}}
\]&lt;/span&gt; Proof. See Homework 2.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>Non-linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</link>
      <pubDate>Fri, 07 Jan 2022 12:46:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</guid>
      <description>

&lt;p&gt;The basic idea about non-linear regression is to perform the feature
mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, followed by linear
regression in the new feature space.&lt;/p&gt;
&lt;h3 id=&#34;polynomial-regression&#34;&gt;Polynomial Regression&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a scalar, the feature
mapping in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order Polynomial
Regression is &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) =
\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
\]&lt;/span&gt; The regression function and the parameters are then like those
in linear regression: &lt;span class=&#34;math display&#34;&gt;\[
f(x) = [w_0,w_1,w_2,\dots,w^p]\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
= w^T\phi(x)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; captures all
features in each degree from &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; up
to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. In higher-dimensional input
feature space, there are many more entries for feature mapping in each
degree. Take a 2-D input for example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;degree 1: &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;degree 2: &lt;span class=&#34;math inline&#34;&gt;\([x_1^2, x_1x_2,
x_2^2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;degree 3: &lt;span class=&#34;math inline&#34;&gt;\([x_1^3, x_1^2x_2, x_1x_2^2,
x_2^3]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-trick-in-non-linear-regression&#34;&gt;Kernel Trick in
Non-linear Regression&lt;/h3&gt;
&lt;p&gt;Many Linear Regression algorithms learning algorithms depend on the
calculation of inner products between feature vectors, either during
training or in prediction, without directly depending on the feature
vector. We can transform the feature vector by applying a feature
mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; to do the Non-linear
Regression task.&lt;/p&gt;
&lt;p&gt;However, it is not necessary to explicitly define the feature mapping
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; when only the inner products
are needed. Instead, we can define a function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K: \R^N \times \R^N \mapsto \R\)&lt;/span&gt;
that directly calculate the inner products of pseudo-transformed
features: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = \phi_{pseudo}(x)^T\phi_{pseudo}(x^\prime)
\]&lt;/span&gt; This will be more flexible and computationally-efficient.&lt;/p&gt;
&lt;h4 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h4&gt;
&lt;p&gt;Recall Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \min_{W}\frac{1}{2}||Y - X^TW||^2_2 +
\frac{\alpha}{2}||W||^2_2 \iff (XX^T + \alpha I_{N+1})W^\star = XY
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\((XX^T + \alpha
I_{N+1})\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(W^\star =
(XX^T + \alpha I_{N+1})^{-1}XY\)&lt;/span&gt;. By &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/&#34;&gt;matrix identity&lt;/a&gt; &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; we can obtain that &lt;span class=&#34;math inline&#34;&gt;\(W^\star =
X(X^TX + \alpha I_M)^{-1}Y\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(X \in
\R^{(N+1)\times M}, (X^TX + \alpha I_M)^{-1}Y \in \R^{M}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; can be rewritten as a linear
combination of columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;,
i.e. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; lies in the span of
input vectors: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \sum_{i=1}^M\lambda_ix^{(i)}
\]&lt;/span&gt; For a new data &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we
make prediction by &lt;span class=&#34;math display&#34;&gt;\[
y^* = (x^*)^TW = (x^*)^TX(X^TX + \alpha I_M)^{-1}Y
\]&lt;/span&gt; which totally depends on the inner products.&lt;/p&gt;
&lt;h4 id=&#34;support-vector-regression&#34;&gt;Support Vector Regression&lt;/h4&gt;
&lt;p&gt;Support Vector Regression learns a hyper-plane that incorporates
within its margin as many points as possible. As a comparison, &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/&#34;&gt;Support Vector Machine&lt;/a&gt; learns a
hyper-plane that excludes outside its margin as many points as possible.
Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i)
\\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b \le \epsilon + \xi_i, i=1,\dots,M \\
y^{(i)} - w^Tx^{(i)} - b \ge \epsilon + \xi^*_i, i=1,\dots,M \\
\xi_i, \xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a
hyper-parameter to be determined. Transform the problem into the
standard optimization form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i)
\\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i \le 0, i=1,\dots,M
\\
w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i \le 0, i=1,\dots,M \\
-\xi_i, -\xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(w,\xi,\xi^*,\lambda,\mu,\nu,\nu^*) = &amp;amp;\frac{1}{2}||w||^2_2 +
C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
&amp;amp;+ \sum_{i=1}^M\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \epsilon -
\xi_i) \\
&amp;amp;+ \sum_{i=1}^M\mu_i(w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i)
\\
&amp;amp;- \sum_{i=1}^M\nu_i\xi_i -\sum_{i=1}^M\nu^*_i\xi^*_i
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Support%20Vector%20Regression.pdf&#34; target=&#34;_blank&#34;&gt;Support
Vector Regression.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>常见分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</link>
      <pubDate>Thu, 11 Aug 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;h2 id=&#34;离散型&#34;&gt;离散型&lt;/h2&gt;
&lt;h3 id=&#34;二项分布binomial-distribution&#34;&gt;二项分布（binomial
distribution）&lt;/h3&gt;
&lt;p&gt;如果离散型随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从二项分布，一般记作&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n, p)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(x;n,p) = {n \choose x} p^x (1-p)^{n-x}, x = 0,1,\dots \\
\E[X] = np \\
\Var[X] = np(1-p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;二项分布可以帮助纠正一个生活中很常见的谬误，比如说身高高于两米的人占人类总体的&lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;，那么是否说明随机选取的100个人中一定至少有1个人高于两米呢？记&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为100个人中身高高于两米的人数，显然&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(100, 0.01)\)&lt;/span&gt;，经计算可得&lt;span class=&#34;math inline&#34;&gt;\(P(X=0) \approx
0.366\)&lt;/span&gt;。其实也就意味着，100个人中，能至少看到1个身高高于两米的人的概率其实大约是&lt;span class=&#34;math inline&#34;&gt;\(1-0.366 = 63.4\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;泊松分布poisson-distribution&#34;&gt;泊松分布（Poisson
distribution）&lt;/h3&gt;
&lt;p&gt;泊松分布产生于表示在一定时间或空间内出现的事件个数的场景。泊松分布有一些基本假设，设观察的这一单位时间或空间为&lt;span class=&#34;math inline&#34;&gt;\([0, 1)\)&lt;/span&gt;，取一个很大的自然数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，将&lt;span class=&#34;math inline&#34;&gt;\([0,1)\)&lt;/span&gt;平分为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;段窗口：&lt;span class=&#34;math inline&#34;&gt;\(l_1
= [0, \frac{1}{n}), l_2 = [\frac{1}{n}, \frac{2}{n}), \dots, l_n =
[\frac{n-1}{n}, 1)\)&lt;/span&gt;，则：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;在每段&lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt;内，恰发生一个事件的概率正比于这段的长度&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;，即可取为&lt;span class=&#34;math inline&#34;&gt;\(\frac{\lambda}{n}\)&lt;/span&gt;；又假定&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;很大故&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;很小时，不可能发生两次以上事件；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_1, l_2, \dots,
l_n\)&lt;/span&gt;中是否发生事件是相互独立的；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样的基本假设下，单位窗口内发生事件的总数记为随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;。此时&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;应当服从二项分布，而当&lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;则服从泊松分布，故泊松分布也可以看作是某种形式的二项分布取极限而得到：
&lt;span class=&#34;math display&#34;&gt;\[
P(X = i; \lambda) = \lim_{n \to \infty} {n \choose i}
(\frac{\lambda}{n})^i (1 - \frac{\lambda}{n})^{n-i}
\]&lt;/span&gt; 将&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} {n \choose
i} / n^i = 1 / i!\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to
\infty} (1 - \frac{\lambda}{n})^{n-i} =
e^{-\lambda}\)&lt;/span&gt;代入即可得到泊松分布的分布律。&lt;/p&gt;
&lt;p&gt;一般如果&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n,p)\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;较大、&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;较小、&lt;span class=&#34;math inline&#34;&gt;\(np =
\lambda\)&lt;/span&gt;不太大时，&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布接近于泊松分布&lt;span class=&#34;math inline&#34;&gt;\(P(\lambda)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(x;\lambda) = e^{-\lambda} \frac{\lambda^x}{x!}, x = 0,1,\dots \\
\E[X] = \lambda \\
\Var[X] = \lambda
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;伯努利分布bernoulli-distribution&#34;&gt;伯努利分布（Bernoulli
distribution）&lt;/h3&gt;
&lt;p&gt;伯努利分布&lt;span class=&#34;math inline&#34;&gt;\(B(1,
p)\)&lt;/span&gt;实际上是二项分布中&lt;span class=&#34;math inline&#34;&gt;\(n =
1\)&lt;/span&gt;的一个特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(1;1,p) = p, B(0;1,p) = 1 - p \\
\E[X] = p \\
\Var[X] = p(1 - p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;多项分布multinomial-distribution&#34;&gt;多项分布（multinomial
distribution）&lt;/h3&gt;
&lt;p&gt;多项分布其实就是二项分布的推广，不像二项分布，多项分布的取值的是多值的而不是二值的（binary）。假设有&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果，且这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果互相对立、完备穷举（mutually
exclusive and collectively exhaustive），此时它们的概率之和为&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，即&lt;span class=&#34;math inline&#34;&gt;\(p_1 +
\dots + p_k = 1\)&lt;/span&gt;，多项分布计算的则是这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果分别发生&lt;span class=&#34;math inline&#34;&gt;\(n_1, \dots, n_k\)&lt;/span&gt;次时的概率。令&lt;span class=&#34;math inline&#34;&gt;\(N = n_1 + \dots + n_k, \vec p = [p_1, \dots, p_k],
\vec n = [n_1, \dots, n_k]\)&lt;/span&gt;，则： &lt;span class=&#34;math display&#34;&gt;\[
P(\vec n; \vec p) = \frac{N!}{n_1! \dots n_k!} p_1^{n_1} \dots p_k^{n_k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;多项分布可以拓展到连续情况，此时&lt;span class=&#34;math inline&#34;&gt;\(n_1,
\dots, n_k \in \R_+\)&lt;/span&gt;，而概率质量函数变为 &lt;span class=&#34;math display&#34;&gt;\[
p(\vec n; \vec p) = \frac{\Gamma(N + 1)}{\Gamma(n_1 + 1) \dots
\Gamma(n_k + 1)} p_1^{n_1} \dots p_k^{n_k}
\]&lt;/span&gt;
连续情况下的多项分布也是&lt;code&gt;sklearn&lt;/code&gt;中能将TFIDF特征应用到&lt;code&gt;MultinomialNB&lt;/code&gt;的基本原理。&lt;/p&gt;
&lt;h3 id=&#34;分类分布categorical-distribution&#34;&gt;分类分布（categorical
distribution）&lt;/h3&gt;
&lt;p&gt;类似伯努利分布是二项分布&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时的特例，分类分布则是多项分布&lt;span class=&#34;math inline&#34;&gt;\(N=1\)&lt;/span&gt;时的特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(\vec n; \vec p, 1) = \prod_{i=1}^k p_i ^{n_i} \\
\E[X] = \vec p \\
\Var[X] = \vec p (1 - \vec p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;连续型&#34;&gt;连续型&lt;/h2&gt;
&lt;h3 id=&#34;指数分布exponential-distribution&#34;&gt;指数分布（exponential
distribution）&lt;/h3&gt;
&lt;p&gt;指数分布最常见的一个场景是寿命估计。设想一种大批生产的电器元件，其元件寿命&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;是随机变量，在“无老化”的假定下——即“若元件在时刻&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;尚正常工作，则其失效率总为某个与&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;无关的常数&lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;”，那么&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从参数为&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的指数分布。&lt;/p&gt;
&lt;p&gt;上述假设用概率语言描述则是 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{h \to 0} P(x \le X \le x+h | X &amp;gt; x) / h = \lambda
\]&lt;/span&gt; 注意到 &lt;span class=&#34;math display&#34;&gt;\[
P(x \le X \le x+h | X &amp;gt; x) = \frac{P(\{ x \le X \le x+h \} \cap \{ X
&amp;gt; x \})}{P(X &amp;gt; x)} = \frac{P(x &amp;lt; X \le x+h)}{P(X &amp;gt; x)}
\]&lt;/span&gt; 所以 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lim_{h \to 0} \frac{P(x &amp;lt; X \le x+h)}{h P(x &amp;lt; X))} &amp;amp;= \lambda
\\
\lim_{h \to 0} \frac{F(x + h) - F(x)}{h(1 - F(x))} &amp;amp;= \lambda \\
\frac{F&amp;#39;(x)}{1 - F(x)} &amp;amp;= \lambda
\end{aligned}
\]&lt;/span&gt; 上述微分方程的通解为&lt;span class=&#34;math inline&#34;&gt;\(F(x) = 1 -
Ce^{-\lambda x}\)&lt;/span&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(F(0) =
0\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(C = 1\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
p(x;\lambda) = \begin{cases}
\lambda e^{-\lambda x}, &amp;amp; x &amp;gt; 0 \\
0, &amp;amp; x \le 0
\end{cases} \\
\E[X] = \lambda^{-1} \\
\Var[X] = \lambda^{-2}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;正态分布normal-distribution&#34;&gt;正态分布（normal
distribution）&lt;/h3&gt;
&lt;p&gt;正态分布也叫作高斯分布（Gaussian distribution），一维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x -
\mu)^2}{2 \sigma^2}}
\]&lt;/span&gt; 二维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p \Big( (x,y); \mu_X, \mu_Y, \sigma_X, \sigma_Y, \sigma_{XY} \Big) =
\frac{1}{2\pi \sqrt{(\sigma_X^2 \sigma_Y^2 - \sigma_{XY}^2)}}
e^{-\frac 1 {2(1 - \sigma_{XY}^2)} \left(\frac{(x-\mu_X)^2} {\sigma_X^2}
- \frac{2\sigma_{XY}(x - \mu_X)(y - \mu_Y)} {\sigma_X \sigma_Y} +
\frac{(y-\mu_Y)^2} {\sigma_Y^2} \right)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(\x; \mu, \Sigma) = \frac{1}{\sqrt{|2\pi \Sigma|}} e^{-\frac{1}{2}
(\x-\mu)^T \Sigma^{-1} (\x-\mu)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/clustering/</link>
      <pubDate>Sat, 09 Apr 2022 14:32:03 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/clustering/</guid>
      <description>

&lt;h2 id=&#34;big-picture&#34;&gt;Big Picture&lt;/h2&gt;
&lt;p&gt;The clustering algorithms can be broadly split into two categories
depending on whether the number of clusters is given or to be determined
by user. &lt;strong&gt;Partitional&lt;/strong&gt; ones pre-set the number of
clusters; while &lt;strong&gt;hierarchical&lt;/strong&gt; ones output a dendrogram
that illustrates how clusters are built level by level. Users are free
to choose the level of clustering in this hierarchical clustering.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hierarchical algorithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bottom-up&lt;/strong&gt; agglomerative clustering&lt;/p&gt;
&lt;p&gt;This approach starts with each object in a separate cluster, and
repeatedly&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;joins the most similar pair of clusters,&lt;/li&gt;
&lt;li&gt;update the similarity of the new cluster to others until there is
only one cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are five further methods in this approach. The difference among
them lies in the way to measure inter-cluster similarity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single-linkage&lt;/strong&gt; measures the similarity between two
clusters as the distance between their closest members.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-linkage&lt;/strong&gt; measures the similarity between
two clusters as the distance between their furthest members.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average-linkage&lt;/strong&gt; measures the similarity between two
clusters as the average of distances of all the cross-cluster
pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt; measures the similarity between two
clusters as the distance between their centers.&lt;/li&gt;
&lt;li&gt;DBSCAN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Top-down&lt;/strong&gt; divisive clustering&lt;/p&gt;
&lt;p&gt;This approach starts with all the data in a single cluster, and
repeatedly split each cluster into two using a partition algorithm until
each object is in a separate cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partitional algorithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K-means&lt;/li&gt;
&lt;li&gt;Gaussian mixture model&lt;/li&gt;
&lt;li&gt;Soft K-means&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h2&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{x^{(i)},
i=1,\dots,M\}\)&lt;/span&gt;, a clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; points into &lt;span class=&#34;math inline&#34;&gt;\(K (K \le M)\)&lt;/span&gt; clusters is a partition of
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; disjoint groups &lt;span class=&#34;math inline&#34;&gt;\(\{C_1,\dots,C_K\}\)&lt;/span&gt;. Suppose we have a
function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that evaluates the
clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; and returns
lower score with better clustering. The best clustering will be &lt;span class=&#34;math display&#34;&gt;\[
\arg \min_{\mathcal C} f(\mathcal C)
\]&lt;/span&gt; The number of all possibilities of clustering with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements is called the &lt;strong&gt;Bell
number&lt;/strong&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(B_M\)&lt;/span&gt;.
The calculation of the Bell number is based on dynamic programming. The
number of ways to cluster &lt;span class=&#34;math inline&#34;&gt;\(M+1\)&lt;/span&gt;
elements is the sum of number of ways to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; element and cluster
it, with the rest belong to one single cluster&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; elements and cluster
them, with the rest belong to one single cluster&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements and cluster
them, with the rest belong to one single cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_{M+1} = \sum_{i=1}^M\binom{M}{0}B_i \\
B_0 = 1
\end{gather}
\]&lt;/span&gt; It grows exponentially with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and thus the exhaustive method will be
computationally intractable. We need either an approximation algorithm
or a scoring function with special properties.&lt;/p&gt;
&lt;h3 id=&#34;k-means&#34;&gt;K-means&lt;/h3&gt;
&lt;p&gt;K-means assumes there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;
clusters. This greatly eliminates many possibilities described above.
Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\min_{c_1,\dots,c_K}\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \text{
where }z^{(i)} = \arg \min_{j \in \{1,\dots,K\}}||x^{(i)}-c_j||^2_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}\)&lt;/span&gt; is the cluster
index to which &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is assigned.
K-means’ objective is to assign each point to its closest cluster center
and minimize the total within-cluster square errors. For cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(C_j =
\{x^{(i)}|z^{(i)} = j\}\)&lt;/span&gt; be the set of points assigned to it,
then the cluster center of cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
c_j = \frac{1}{|C_j|}\sum_{x^{(i)} \in C_j}x^{(i)}
\]&lt;/span&gt; However, both the cluster center &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\c}{\mathrm{c}} \c\)&lt;/span&gt; and the
assignment &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\z}{\mathrm{z}}
\z\)&lt;/span&gt; is initially unknown. K-means solves this by randomly pick
up initial cluster centers and enter the
assign-data-to-clusters/update-cluster-centers loop, until the cluster
centers converge or become satisfactory. Rewrite the objective of
K-means as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\z,\c}(l(\z,\c) \coloneq \sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2)
\]&lt;/span&gt; K-means is in essence a coordinate descent of the loss &lt;span class=&#34;math inline&#34;&gt;\(l(\z,\c)\)&lt;/span&gt;. The main loop of K-means is
to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assign data points to its nearest cluster center, i.e. minimizing
over the assignment &lt;span class=&#34;math inline&#34;&gt;\(\z\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;update cluster centers according to the points assigned to,
i.e. minimizing over the centroids &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is monotonically decreasing
after each step in the above loop. Also, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is lower-bounded by &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and thus K-means will converge
finally.&lt;/p&gt;
&lt;p&gt;K-means clustering produces a &lt;a href=&#34;http://en.wikipedia.org/wiki/Voronoi_diagram&#34;&gt;Voronoi diagram&lt;/a&gt;
which consists of linear decision boundaries.&lt;/p&gt;
&lt;p&gt;For more discussion and an interesting image compression method with
K-means, please refer &lt;a href=&#34;https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-mixture-model&#34;&gt;Gaussian Mixture Model&lt;/h3&gt;
&lt;p&gt;A cluster can also be modelled by a multi-variate Gaussian with
elliptical shape: the elliptical shape is controlled by the covariance
matrix; the location is controlled by the mean. Gaussian mixture model
is a weighted sum of, say &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;,
Gaussian distributions: &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^K\pi_j\mathcal N(x;\mu_j, \Sigma_j)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; is the prior that a
sample is generated from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th
Gaussian. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x;\mu_j,
\Sigma_j)\)&lt;/span&gt; is the probability to generate the sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. Putting it together, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; is the total probability of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; over its latent &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(z =
j\)&lt;/span&gt; representing the prior condition that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is sampled from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^Kp(x, z = j) = \sum_{j=1}^K p(z = j) p(x|z = j)
\]&lt;/span&gt; GMM is learned by &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(L(\pi,\mu,\Sigma) \triangleq
\prod_{i=1}^Mp(x^{(i)})\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt; The log-likelihood function form will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(l(\pi,\mu,\Sigma) \triangleq \log
L(\pi,\mu,\Sigma)\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-lagrange-multiplier&#34;&gt;Attempt with &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange Multiplier&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)}) =
\prod_{i=1}^M\sum_{j=1}^Kp(z_j)p(x|z = z_j) =
\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal
N(x^{(i)};\mu_j, \Sigma_j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\end{gather}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
J(\pi,\mu,\Sigma,\lambda) = -\sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal
N(x^{(i)};\mu_j, \Sigma_j) + \lambda(\sum_{j=1}^K\pi_j - 1)
\]&lt;/span&gt; This expression is just too hard to zero the derivatives. For
example, writing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x^{(i)};\mu_j,
\Sigma_j)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}_j\)&lt;/span&gt;,
then take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial J}{\partial \pi_j} &amp;amp;= -\sum_{i=1}^M\frac{\mathcal
N(x^{(i)};\mu_j, \Sigma_j)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_j,
\Sigma_j)} + \lambda \\
&amp;amp;\Downarrow \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_1,
\Sigma_1)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_1, \Sigma_1)} \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_2,
\Sigma_2)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_2, \Sigma_2)} \\
&amp;amp;\dots \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_K,
\Sigma_K)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_K, \Sigma_K)}\\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By far, the whole expression is far too complicated for us to
continue…&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-expectation-maximization&#34;&gt;Attempt with Expectation
Maximization&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^M p(x^{(i)}) = \prod_{i=1}^M
\sum_{j=1}^K p(x^{(i)}, z^{(i)} = j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^Kp(x^{(i)}, z^{(i)}
= j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)
\end{gather}
\]&lt;/span&gt; By Jensen’s inequality, if &lt;span class=&#34;math inline&#34;&gt;\(\forall
i \in \{ 1,\dots,M \}, j \in \{1,\dots,K\}, \alpha_j^{(i)} \ge
0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K\alpha_j^{(i)} =
1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\pi,\mu,\Sigma) &amp;amp;= \sum_{i=1}^M \log \sum_{j=1}^Kp(x^{(i)},
z^{(i)} = j) = \sum_{i=1}^M \log \sum_{j=1}^K \alpha^{(i)}_j
\frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\alpha^{(i)}_j} \\
&amp;amp;\ge B(\alpha,\pi,\mu,\Sigma) := \sum_{i=1}^M \sum_{j=1}^K
\alpha^{(i)}_j \log \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\pi_j}{\alpha^{(i)}_j} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the lower bound of
&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. If we can enlarge &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, we can gently guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is increasing in this process. As
illustrated in &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/expectation-maximization/&#34;&gt;expectation
maximization&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; is
chosen to be &lt;span class=&#34;math inline&#34;&gt;\(p(z^{(i)}=j | x^{(i)}; \pi^t,
\mu^t,\Sigma^t)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\pi^t,
\mu^t,\Sigma^t\)&lt;/span&gt; are estimations in current iteration. With &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; being fixed, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B(\alpha,\pi,\mu,\Sigma) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log
\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j - \sum_{i=1}^M \sum_{j=1}^K
\alpha^{(i)}_j \log \alpha^{(i)}_j \\
\end{aligned}
\]&lt;/span&gt; Grouping terms in &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; that
are relevant to &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;, we are to
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_\pi \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j \\
s.t.\quad \sum_{i=1}^K \pi_j = 1
\end{gather}
\]&lt;/span&gt; We do this by Lagrangian multipliers: &lt;span class=&#34;math display&#34;&gt;\[
J(\pi, \lambda) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j +
\lambda(1 - \sum_{i=1}^K\pi_i )
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial J}{\partial \pi_k} =\sum_{i=1}^M
\frac{\alpha^{(i)}_k}{\pi_k} - \lambda \\
\Downarrow \\
\pi_k^{t+1} = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{\lambda}
\]&lt;/span&gt; With the knowledge that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K \pi_j = 1, \sum_{i=1}^M\alpha^{(i)}_j
= 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\pi_k^{t+1} = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{M}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \mu_k} &amp;amp;= \frac{\partial \sum_{i=1}^M
\sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j,
\Sigma_j)}{\partial \mu_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k (x^{(i)}
- \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k)}{\partial \mu_k} \\
&amp;amp;= \sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) &amp;amp;= 0, \text{ by the
invertibility of $\Sigma_j^{-1}$} \\
\Downarrow \\
\mu_k^{t+1} &amp;amp;= \frac{\sum_{i=1}^M \alpha^{(i)}_k x^{(i)}}{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k\)&lt;/span&gt;
and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \Sigma_k} &amp;amp;= \frac{\partial \sum_{i=1}^M
\sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j,
\Sigma_j)}{\partial \Sigma_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k ((x^{(i)}
- \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k) + \log|\Sigma_k|)}{\partial
\Sigma_k} \\
&amp;amp;= -\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)}
- \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
-\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} -
\mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} -
\mu_k)^T + I) &amp;amp;= 0 \\
\end{aligned} \\
\Downarrow \\
\Sigma_k^{t+1} = \frac{\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k)
(x^{(i)} - \mu_k)^T}{\sum_{i=1}^M \alpha^{(i)}_k} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-k-means&#34;&gt;Soft K-means&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; in GMM, which
measures how likely the sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is
generated from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian, can
be viewed as a contribution of sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to the cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. That is, a sample can contribute
different portions to different clusters and these portions add up to
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the case where &lt;span class=&#34;math inline&#34;&gt;\(\pi_j = \frac{1}{K},
\Sigma_k = I\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p(x^{(i)}|z^{(i)} = j) = \mathcal N(x^{(i)};\mu_j, I) =
\frac{1}{\sqrt{(2\pi)^N}}\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2) \\
\alpha^{(i)}_j = p(z^{(i)} = j | x^{(i)}) = \frac{p(x^{(i)}|z^{(i)} =
j)p(z^{(i)} = j)}{p(x^{(i)})} = \frac{\mathcal N(x^{(i)};\mu_j, I)
\frac{1}{K}}{\frac{1}{K} \sum_{k=1}^K \mathcal N(x;\mu_k, I)} =
\frac{\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2)}{\sum_{k=1}^K
\exp(-\frac{1}{2}||x^{(k)}-\mu_j||^2_2)}
\end{gather}
\]&lt;/span&gt; The “soft K-means” comes from the softmax of the Euclidean
distance.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/gaussian-mixture-models-vs-k-means-which-one-to-choose-62f2736025f0&#34;&gt;Gaussian
Mixture Models vs K-Means. | by K.Kubara | Towards Data Science&lt;/a&gt; &lt;a href=&#34;https://neptune.ai/blog/clustering-algorithms&#34;&gt;Exploring
Clustering Algorithms: Explanation and Use Cases (neptune.ai)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>6-online-learning</title>
      <link>https://chunxy.github.io/courses/machine-learning-theory/6-online-learning/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/machine-learning-theory/6-online-learning/</guid>
      <description>

&lt;p&gt;In standard supervised learning, we consider the training data are
given in a &lt;strong&gt;batch&lt;/strong&gt; randomly sampled from a
&lt;strong&gt;fixed&lt;/strong&gt; distribution. However, in real-world
applications, data may come in an one-by-one fashion, while the
underlying distribution evolves as the time goes.&lt;/p&gt;
&lt;p&gt;In online learning, we suppose the learning task is formed as a game
between the &lt;strong&gt;learner&lt;/strong&gt; and &lt;strong&gt;nature&lt;/strong&gt;
players:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;at every iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, nature
reveals the input &lt;span class=&#34;math inline&#34;&gt;\(x_t \in
\mathcal{X}\)&lt;/span&gt; to the learner;&lt;/li&gt;
&lt;li&gt;learner outputs a prediction &lt;span class=&#34;math inline&#34;&gt;\(p_t \in
\mathcal{Y}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;nature reveals the true label &lt;span class=&#34;math inline&#34;&gt;\(y_t \in
\mathcal{Y}\)&lt;/span&gt;, and Learner will suffer a loss &lt;span class=&#34;math inline&#34;&gt;\(l(y_t, p_t)\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;learner updates its prediction model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the online learning framework, we no longer have two different
phases of training and testing. An implicit goal in online learning is
to distribute the computations over all the iterations as uniformly as
possible.&lt;/p&gt;
&lt;h2 id=&#34;online-learning&#34;&gt;Online Learning&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;regret of an online learner&lt;/strong&gt;. Given an
&lt;strong&gt;expert&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(h: \mathcal{X} \to
\mathcal{Y}\)&lt;/span&gt;, the regret of the online learner is defined as the
&lt;strong&gt;extra&lt;/strong&gt; cumulative loss of the learner with respect to
expert &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\reg}{{\mathrm{Regret}}} \reg(h) \triangleq
\sum_{t=1}^T[\ell(y_t, p_t)] - \sum_{t=1}^T[\ell(y_t, h(x_t))]
\]&lt;/span&gt; Next, for a set of experts &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, we define the learner’s
regret as the worst-case regret for any expert &lt;span class=&#34;math inline&#34;&gt;\(h \in \mathcal{H}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\reg(\mathcal{H}) \triangleq \max_{h \in \mathcal{H}} \reg(h) =
\sum_{i=1}^T[\ell(y_t, p_t)] - \min_{h \in \mathcal{H}}
\sum_{i=1}^T[\ell(y_t, h(x_t))]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The overall objective is to have a regret that is &lt;strong&gt;sublinear
in &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-learning-strategies&#34;&gt;Examples of Learning
Strategies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Consider a binary classification task with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y} = \{ 0,1 \}\)&lt;/span&gt;, zero/one loss
and an &lt;strong&gt;adversary nature&lt;/strong&gt; that always generates the
opposite label to the learner’s prediction.&lt;/p&gt;
&lt;p&gt;Consider the expert system &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H} =
\{ h_0, h_1 \}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h_i(x) =
i\)&lt;/span&gt;. We claim that the regret w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; will be at least &lt;span class=&#34;math inline&#34;&gt;\(\frac{T}{2}\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\reg(h_i) &amp;amp;= \sum_{t=1}^T[\ell(y_t, p_t)] - \sum_{t=1}^T[\ell(y_t,
h_i(x_t))] \\
&amp;amp;= T - \sum_{t=1}^T[\ell(y_t, h_i(x_t))] \\
\end{aligned}
\]&lt;/span&gt; But &lt;span class=&#34;math inline&#34;&gt;\(\reg(h_0) + \reg(h_1) =
T\)&lt;/span&gt;. As a result, &lt;span class=&#34;math inline&#34;&gt;\(\reg(\mathcal{H}) =
\max_{h \in \{ h_0, h_1 \}} \reg(h) \ge \frac{T}{2}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider a binary classification task with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Y} = \{ 0,1 \}\)&lt;/span&gt;, zero/one loss
and a &lt;strong&gt;realizable scenario&lt;/strong&gt; where nature generates the
label according to an expert &lt;span class=&#34;math inline&#34;&gt;\(h^* \in
\mathcal{H}\)&lt;/span&gt;. In this scenario, the cumulative loss of a
learning algorithm is equal to its regret w.r.t &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider the &lt;strong&gt;follow-the-best algorithm&lt;/strong&gt; where at each
iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; we arbitrarily choose
among the experts with the best score up to now for prediction. The
regret w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; can be as
large as &lt;span class=&#34;math inline&#34;&gt;\(|\mathcal{H}|-1\)&lt;/span&gt;. This is
because every time an expert makes a mistake, it is excluded from
candidates.&lt;/p&gt;
&lt;p&gt;Consider the &lt;strong&gt;majority algorithm&lt;/strong&gt; where we vote for
the label with the majority vote among experts in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;. The regret w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt; is upper-bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log_2|\mathcal{H}|\)&lt;/span&gt;. This is because
every time there is a mistake, half of considered experts are excluded
from consideration.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;online-convex-optimization&#34;&gt;Online Convex Optimization&lt;/h3&gt;
&lt;p&gt;Adversary nature and realizable scenario represents the two extreme
wings of situations. To analyze the general non-realizable situations,
we introduce a framework called &lt;strong&gt;online convex
optimization&lt;/strong&gt; where&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;learner chooses model parameters &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; from a convex set &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nature chooses a convex loss function &lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the regret w.r.t. model parameter &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[
\reg(u) = \sum_{t=1}^T f_t(w_t) - \sum_{t=1}^T f_t(u) \\
\reg(S) = \sum_{t=1}^T f_t(w_t) - \min_{u \in S} \sum_{t=1}^T f_t(u)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;example-online-linear-regression&#34;&gt;Example: Online Linear
Regression&lt;/h4&gt;
&lt;p&gt;The setting is as follows:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;nature reveals input vector &lt;span class=&#34;math inline&#34;&gt;\(x_t \in
\R^d\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;learner chooses model parameters &lt;span class=&#34;math inline&#34;&gt;\(w_t
\in \R^d\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nature reveals output &lt;span class=&#34;math inline&#34;&gt;\(y_t \in
\R\)&lt;/span&gt; and the loss value at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
f_t(w_t) = (w_t^\top x_t - y_t)^2
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;convexification&#34;&gt;Convexification&lt;/h4&gt;
&lt;p&gt;Consider a &lt;strong&gt;general loss&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; (which doesn’t have to be convex)
and a &lt;strong&gt;finite set&lt;/strong&gt; of experts &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H} = \{ h_1,\dots,h_m \}\)&lt;/span&gt;. To
convexify the problem, the learner searches for a probability
distribution over the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; experts
which means &lt;span class=&#34;math inline&#34;&gt;\(w_t \in \Delta_m\)&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(\Delta_m\)&lt;/span&gt; is the set of all
categorical distributions over &lt;span class=&#34;math inline&#34;&gt;\(\{ 1,\dots,m
\}\)&lt;/span&gt;. The online learning algorithm will be as follows:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;nature reveals input vector &lt;span class=&#34;math inline&#34;&gt;\(x_t \in
\R^d\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;learner chooses model parameters &lt;span class=&#34;math inline&#34;&gt;\(w_t
\in \R^m\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nature reveals output &lt;span class=&#34;math inline&#34;&gt;\(y_t \in
\R\)&lt;/span&gt; and the loss value at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
f_t(w_t) = w_t^\top \underbrace{[\ell(h_1(x_t), y_t), \dots,
\ell(h_m(x_t), y_t)]}_{L_t} \\
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will assume a online convex optimization setting from now on.&lt;/p&gt;
&lt;h3 id=&#34;follow-the-leader-strategy&#34;&gt;Follow-the-leader Strategy&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;follow-the-leader (FTL)&lt;/strong&gt; strategy seems a natural
choice to the online learning: &lt;span class=&#34;math display&#34;&gt;\[
w_t = \arg \min_{w \in S} \sum_{i=1}^{t-1} f_i(w)
\]&lt;/span&gt; Note that now it is a convex optimization problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lemma: &lt;strong&gt;regret bound for FTL&lt;/strong&gt;. Given that &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; is chosen according to a FTL
strategy, we have the following upper bound on the FTL learner’s regret
at iteration &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\reg(S) \le \sum_{i=1}^T [f_t(w_t) - f_{t}(w_{t+1})]
\]&lt;/span&gt; Proof. Recall that &lt;span class=&#34;math inline&#34;&gt;\(\reg(S) =
\max_{u \in S} \reg(u)\)&lt;/span&gt;. We need to show for every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\reg(u) = \sum_{t=1}^T [f_t(w_t) - f_t(u)] \le \sum_{t=1}^T [f_t(w_t) -
f_t(w_{t+1})] \\
\iff \\
\sum_{t=1}^T f_t(w_{t+1}) \le \sum_{t=1}^T f_t(u)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1}\)&lt;/span&gt; is the
&lt;strong&gt;one-step-ahead&lt;/strong&gt; expert (relative to &lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt;); so it should perform better than
any other choice. We use induction to show it. When &lt;span class=&#34;math inline&#34;&gt;\(T=1\)&lt;/span&gt;, we trivially have &lt;span class=&#34;math inline&#34;&gt;\(\forall u, f_1(u) \ge f_1(w_2)\)&lt;/span&gt;. Suppose
when &lt;span class=&#34;math inline&#34;&gt;\(T=k\)&lt;/span&gt;, the above holds. Then,
for every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^k f_t(w_{t+1}) \le \sum_{i=1}^k f_t(u)
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^{k+1} f_t(w_{t+1}) &amp;amp;\le f_{k+1}(w_{k+2}) + \sum_{i=1}^k
f_t(u) \\
&amp;amp;\Downarrow_{u=w_{k+2}} \\
\sum_{i=1}^{k+1} f_t(w_{t+1}) &amp;amp;\le \sum_{i=1}^{k+1} f_t(w_{k+2}) \\
&amp;amp;= \min_{w \in S} \sum_{i=1}^{k+1} f_t(w) \\
&amp;amp;\Downarrow \\
\sum_{i=1}^{k+1} f_t(w_{t+1}) &amp;amp;\le \sum_{i=1}^{k+1} f_t(u)
\end{aligned}
\]&lt;/span&gt; which holds for every &lt;span class=&#34;math inline&#34;&gt;\(u \in
S\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;example-i&#34;&gt;Example I&lt;/h4&gt;
&lt;p&gt;Consider a quadratic loss function where nature chooses the input
&lt;span class=&#34;math inline&#34;&gt;\(z_t\)&lt;/span&gt; satisfying the norm bound &lt;span class=&#34;math inline&#34;&gt;\(\|z_t\|_2 \le M\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f_t(w) = \frac{1}{2} \|w-z_t\|_2^2
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(S = \{ x:\|x\|_2 \le M
\}\)&lt;/span&gt;. Then the FTL strategy will choose &lt;span class=&#34;math display&#34;&gt;\[
w_{t+1} = \arg \min_{w \in S} \sum_{i=1}^t \frac{1}{2} \|w-z_i\|_2^2 =
\frac{1}{t} \sum_{i=1}^t z_i
\]&lt;/span&gt; Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
w_{t+1} = \frac{t-1}{t} w_t + \frac{1}{t} z_t \\
w_{t+1} - z_t = \frac{t-1}{t}(w_t - z_t)
\end{gathered}
\]&lt;/span&gt; The regret bound will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\reg(S) \le \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})] \\
&amp;amp;= \sum_{t=1}^T \frac{1}{2} [\|w_t-z_t\|_2^2 - \|w_{t+1}-z_t\|_2^2]
\\
&amp;amp;= \sum_{t=1}^T \frac{1}{2} [\|w_t-z_t\|_2^2 - (1-\frac{1}{t})^2
\|w_t-z_t\|_2^2] \\
&amp;amp;= \sum_{t=1}^T \frac{1}{2} [(\frac{2}{t} -
\frac{1}{t^2})\|w_t-z_t\|_2^2] \\
&amp;amp;\le \sum_{t=1}^T \frac{1}{t} \|w_t-z_t\|_2^2 \\
&amp;amp;\le \sum_{t=1}^T \frac{1}{t} 4M^2 \\
&amp;amp;\le 4M^2 (\log T + 1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;example-ii&#34;&gt;Example II&lt;/h4&gt;
&lt;p&gt;There are cases where FTL could also fail. We give an example here.
Consider a linear loss function where nature chooses the input &lt;span class=&#34;math inline&#34;&gt;\(z_t\)&lt;/span&gt; satisfying the norm bound &lt;span class=&#34;math inline&#34;&gt;\(\|z_t\|_2 \le M\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_t(w) = w^\top z_t
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(S = \{ x:\|x\|_2 \le M
\}\)&lt;/span&gt;. Then the FTL strategy will choose &lt;span class=&#34;math display&#34;&gt;\[
w_{t+1} = \arg \min_{w \in S} \sum_{i=1}^t w^\top z_t = \arg \min_{w \in
S} w^\top \sum_{i=1}^t z_t = \frac{M}{\|\sum_{i=1}^t z_t\|_2}
\sum_{i=1}^t z_t
\]&lt;/span&gt; But consider the one-dimensional input sequence &lt;span class=&#34;math inline&#34;&gt;\(-0.5, 1, -1, 1, -1, \dots\)&lt;/span&gt; The parameters
learner gives would be &lt;span class=&#34;math inline&#34;&gt;\(M, -M, M, -M,
\dots\)&lt;/span&gt; The loss incurred would be &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^T w_i z_i = (T-1)M = M \cdot
O(T)\)&lt;/span&gt;. But the strategy that sets &lt;span class=&#34;math inline&#34;&gt;\(w=0\)&lt;/span&gt; will give a loss of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The crux of the problem is the sudden change of the nature.&lt;/p&gt;
&lt;h3 id=&#34;follow-the-regularized-leader-strategy&#34;&gt;Follow-the-regularized-leader
Strategy&lt;/h3&gt;
&lt;p&gt;Continuing the discussion of &lt;u&gt;Example II&lt;/u&gt;, one makeup for it is
the &lt;strong&gt;follow-the-regularized-leader (FTRL)&lt;/strong&gt; strategy. That
is, the learner returns &lt;span class=&#34;math display&#34;&gt;\[
w_{t+1} = \arg \min_{w \in S} \psi(w) + \sum_{i=1}^{t} f_i(w)
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\psi(x)\)&lt;/span&gt; be the common
choice &lt;span class=&#34;math inline&#34;&gt;\(\frac{\lambda}{2}\|x\|_2^2\)&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(f_i\)&lt;/span&gt; still be the linear
loss. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
w_{t+1} &amp;amp;= \arg \min_{w \in S} \frac{\lambda}{2} \|w\|_2^2 + w^\top
\sum_{i=1}^{t} z_i \\
&amp;amp;\Downarrow_{v_t = -\frac{1}{\lambda} \sum_{i=1}^t z_i}  \\
&amp;amp;= \arg \min_{w \in S} \frac{\lambda}{2} \|w - v_t\|_2^2 -
\frac{\lambda}{2} \|v_t\|_2^2 \\
&amp;amp;= \arg \min_{w \in S} \frac{\lambda}{2} \|w - v_t\|_2^2 \\
&amp;amp;= \Pi_S (v_t) \\
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(S=\R^d\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
w_{t+1} = -\frac{1}{\lambda} v_t \\
w_{t+1} - w_t = -\frac{1}{\lambda} z_t = -\frac{1}{\lambda} \nabla
f_t(w_t) \\
\end{gathered}
\]&lt;/span&gt; The update rule resembles the formulation of gradient descent.
If &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is too large, the update
of learner will be too stable and learner is not able to adapt to the
environment. If &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;​ is too
small, learner can easily overfit to the noise.&lt;/p&gt;
&lt;p&gt;But actually, we don’t need to update at every step. By the
closed-form formula of &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1}\)&lt;/span&gt;, we
can accumulate via &lt;span class=&#34;math inline&#34;&gt;\(v_t\)&lt;/span&gt; and do the
projection at the last step &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lemma: &lt;strong&gt;regret bound for FTRL&lt;/strong&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(f_t(w) = w^\top z_t\)&lt;/span&gt; is &lt;strong&gt;linear
loss&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a convex set
and &lt;span class=&#34;math inline&#34;&gt;\(\psi(w) = \frac{\lambda}{2}
\|w\|_2^2\)&lt;/span&gt;. We have the following upper bound on the FTRL
learner’s regret at iteration &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\reg(u) \le \frac{\lambda}{2} \|u\|_2^2 + \frac{1}{\lambda} \sum_{i=1}^T
\|z_t\|_2^2
\]&lt;/span&gt; Proof. A natural idea is to reuse the conclusion from FTL. We
may synthesize an iteration &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; for
FTL such that &lt;span class=&#34;math inline&#34;&gt;\(f_0(w) = \psi(w)\)&lt;/span&gt;. By
doing so, &lt;span class=&#34;math display&#34;&gt;\[
w_{t+1}^\text{FTL} = \arg \min_{w \in S} \sum_{i=0}^T f_t(w) = \arg
\min_{w \in S} [\psi(w) + \sum_{i=1}^T f_t(w)] = w_{t+1}^\text{FTRL}
\]&lt;/span&gt; Thus, for every &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f_t(w_t^\text{FTRL}) - f(u) = f_t(w_t^\text{FTL})
- f(u)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t=1,\dots,T\)&lt;/span&gt; and
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\reg_{0:T}^\text{FTL}(u) &amp;amp;= \psi(w_0) - \psi(u) +
\reg_{1:T}^\text{FTL}(u) \\
&amp;amp;= \psi(w_0) - \psi(u) + \reg_{1:T}^\text{FTRL}(u)
\end{aligned}
\]&lt;/span&gt; Note that by FTL’s bound, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\reg_{0:T}^\text{FTL}(u) = \sum_{t=0}^T [f_t(w_t) - f_t(u)] \\
&amp;amp;= \psi(w_0) - \psi(u) + \sum_{t=1}^T [f_t(w_t) - f_t(u)] \\
&amp;amp;\Downarrow_\text{one step ahead} \\
&amp;amp;\le \psi(w_0) - \psi(w_1) + \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})]
\\
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\reg_{1:T}^\text{FTRL}(u) \le \psi(u) - \psi(w_1) + \sum_{t=1}^T
[f_t(w_t) - f_t(w_{t+1})] + \psi(w_1) - \psi(w_0) \\
&amp;amp;= \psi(u) - \psi(w_1) + \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})] \\
&amp;amp;= \psi(u) - \psi(w_1) + \sum_{t=1}^T z_t^\top (w_t - w_{t+1}) \\
&amp;amp;\le \psi(u) - \psi(w_1) + \sum_{t=1}^T \|z_t\|_2 \|w_t -
w_{t+1}\|_2 \\
&amp;amp;= \frac{\lambda}{2} \|u\|_2^2 - \frac{\lambda}{2} \|w_1\|_2^2 +
\sum_{t=1}^T \|z_t\|_2 \|\Pi_S (-\frac{1}{\lambda} \sum_{i=1}^{t-1} z_i)
- \Pi_S (-\frac{1}{\lambda} \sum_{i=1}^t z_i)\|_2 \\
&amp;amp;\Downarrow_\text{by the shrinking property of projection of Hilbert
norm, which is $\ell_2$ here ??} \\
&amp;amp;\le \frac{\lambda}{2} \|u\|_2^2 - \frac{\lambda}{2} \|w_1\|_2^2 +
\sum_{t=1}^T \|z_t\|_2 \frac{1}{\lambda} \|z_t\|_2 \\
&amp;amp;\le \frac{\lambda}{2} \|u\|_2^2 + \frac{1}{\lambda} \sum_{t=1}^T
\|z_t\|_2^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;best choice of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for FTRL&lt;/strong&gt;. Given above
inequality, suppose that &lt;span class=&#34;math inline&#34;&gt;\(\|u\|_2 \le
B\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\|z_t\| \le M\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\reg(S) \le \frac{\lambda B^2}{2} + \frac{TM^2}{\lambda}
\]&lt;/span&gt; Minimizing the upper bound over &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; results in &lt;span class=&#34;math inline&#34;&gt;\(\lambda^* = \frac{M}{B} \sqrt{2T}\)&lt;/span&gt;, under
which &lt;span class=&#34;math display&#34;&gt;\[
\reg(S) \le MB \sqrt{2T}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above is the conclusion drawn for &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt;-norm regularizer and linear loss.
We would like to extend them to general case.&lt;/p&gt;
&lt;h4 id=&#34;online-gradient-descent-general-loss&#34;&gt;Online Gradient Descent
(General Loss)&lt;/h4&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is not a linear function,
we can “linearize” it using the Taylor expansion: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(w) &amp;amp;\approx f(w_t) + \nabla f(w_t)^\top (w-w_t) = [f(w_t) - \nabla
f(w_t)^\top w_t] + \underbrace{\nabla f(w_t)^\top}_{z_t} w
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
w_{t+1} &amp;amp;= \arg \min_{w \in S} \psi(w) + \sum_{i=1}^{t} f_i(w) \\
&amp;amp;\approx \arg \min_{w \in S} \psi(w) + \sum_{i=1}^{t} \nabla
f_{i}(w_{i})^\top w \\
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(f_{t+1}\)&lt;/span&gt; first
appears in the derivation of &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1}\)&lt;/span&gt;. The latest model learner output
is &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt;​ so we just expand there.
This gives rise to the online gradient descent algorithm. The update
rule will be&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Algorithm: &lt;strong&gt;online gradient descent&lt;/strong&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Pick an &lt;strong&gt;initial point&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_1 \in S\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; do:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Output &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; (in fact, this &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; is computed in last timestep and
formally should have been &lt;span class=&#34;math inline&#34;&gt;\(w_{t-1}\)&lt;/span&gt;
in the previous context) and receive &lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(z_t = \nabla
f_t(w_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Pick a stepsize &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t &amp;gt; 0\)&lt;/span&gt;
(which is usually fixed w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(w_{t+\frac{1}{2}} \leftarrow w_t
- \alpha_t z_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Project &lt;span class=&#34;math inline&#34;&gt;\(w_{t+\frac{1}{2}}\)&lt;/span&gt; onto
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1} \leftarrow
\Pi_S(w_{t+\frac{1}{2}})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;regret bound for OGD learner&lt;/strong&gt;. In the online
convex optimization setting, we have the following regret bound for the
OGD learner initialized at &lt;span class=&#34;math inline&#34;&gt;\(w_1 = 0\)&lt;/span&gt;
with respect to every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\reg(u) \le \frac{\|u\|_2^2}{2\alpha} + \frac{\alpha}{2} \sum_{i=1}^T
\|z_t\|_2^2
\]&lt;/span&gt; If we assume that every &lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-Lipschitz (and thus &lt;span class=&#34;math inline&#34;&gt;\(\|z_t\|_2^2 = \|\nabla f_t(w_t)\|_2^2 \le
\rho^2\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\|u\|_2 \le B\)&lt;/span&gt;
for every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt;, we will further
have the following for &lt;span class=&#34;math inline&#34;&gt;\(\alpha^* =
\frac{B}{\rho \sqrt{T}}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\reg(S) \le B \rho \sqrt{T}
\]&lt;/span&gt; Proof. Because &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1}\)&lt;/span&gt; is
the projection of &lt;span class=&#34;math inline&#34;&gt;\(w_{t+\frac{1}{2}}\)&lt;/span&gt;
onto &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, for every &lt;span class=&#34;math inline&#34;&gt;\(u \in S\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\|w_{t+\frac{1}{2}} - u\|_2^2 = \|w_{t+\frac{1}{2}} - w_{t+1} +
w_{t+1} - u\|_2^2 \\
&amp;amp;= \underbrace{\|w_{t+\frac{1}{2}} - w_{t+1}\|_2^2}_{\ge 0} +
\|w_{t+1} - u\|_2^2 \\
&amp;amp;\quad\quad + 2\underbrace{(w_{t+\frac{1}{2}} - w_{t+1})^\top
(w_{t+1} - u)}_{\ge 0 \text{ due to the property of projection}} \\
&amp;amp;\ge \|w_{t+1} - u\|_2^2
\end{aligned}
\]&lt;/span&gt; As a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{2} \|w_{t+1} - u\|_2^2 - \frac{1}{2} \|w_t - u\|_2^2 \\
\le &amp;amp;\frac{1}{2} \|w_{t+\frac{1}{2}} - u\|_2^2 - \frac{1}{2} \|w_t -
u\|_2^2 \\
= &amp;amp;\frac{1}{2} \|w_t - \alpha z_t - u\|_2^2 - \frac{1}{2} \|w_t -
u\|_2^2 \\
= &amp;amp;\frac{1}{2} \|\alpha z_t\|_2^2 - \alpha z_t^\top (w_t - u) \\
= &amp;amp;\frac{1}{2} \alpha^2 \|z_t\|_2^2 - \alpha \nabla f_t(w_t)^\top
(w_t - u) \\
\le &amp;amp;\frac{1}{2} \alpha^2 \|z_t\|_2^2 - \alpha [f_t(w) - f_t(u)] \\
\end{aligned}
\]&lt;/span&gt; Adding up the above inequality for &lt;span class=&#34;math inline&#34;&gt;\(t=1,\dots,T\)&lt;/span&gt; gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\alpha \sum_{t=1}^T (f_t(w_t) - f_t(u)) &amp;amp;\le \frac{1}{2}
\sum_{t=1}^T \alpha^2 \|z_t\|_2^2 + \frac{1}{2} \|w_1 - u\|_2^2 -
\frac{1}{2} \|w_{T+1} - u\|_2^2 \\
\sum_{t=1}^T (f_t(w_t) - f_t(u)) &amp;amp;\le \frac{\alpha}{2} \sum_{t=1}^T
\|z_t\|_2^2 + \frac{1}{2\alpha} \|w_1 - u\|_2^2 - \frac{1}{2\alpha}
\|w_{T+1} - u\|_2^2 \\
&amp;amp;\le \frac{\alpha}{2} \sum_{t=1}^T \|z_t\|_2^2 + \frac{1}{2\alpha}
\|w_1 - u\|_2^2 \\
&amp;amp;\Downarrow_{w_1=0} \\
&amp;amp;= \frac{\alpha}{2} \sum_{t=1}^T \|z_t\|_2^2 + \frac{1}{2\alpha}
\|u\|_2^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;online-mirror-descent-general-regularizer&#34;&gt;Online Mirror Descent
(General Regularizer)&lt;/h4&gt;
&lt;h5 id=&#34;fenchel-conjugate&#34;&gt;Fenchel Conjugate&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;Fenchel (convex) conjugate&lt;/strong&gt;. For a
function &lt;span class=&#34;math inline&#34;&gt;\(\psi: \R^d \to \R\)&lt;/span&gt;, we
define its Fenchel conjugate as &lt;span class=&#34;math display&#34;&gt;\[
\psi^*(\theta) = \sup_{\omega \in \R^d} \omega^\top \theta -
\psi(\omega)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;convexity of Fenchel conjugate&lt;/strong&gt;. For
every function &lt;span class=&#34;math inline&#34;&gt;\(\psi: \R^d \to \R\)&lt;/span&gt;,
its Fenchel conjugate &lt;span class=&#34;math inline&#34;&gt;\(\psi^*\)&lt;/span&gt; is a
convex function.&lt;/p&gt;
&lt;p&gt;Proof. &lt;span class=&#34;math inline&#34;&gt;\(\psi^*\)&lt;/span&gt; is the supremum
over affine functions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;gradient of Fenchel conjugate&lt;/strong&gt;. Consider
the Fenchel conjugate of a differentiable function &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. Then by &lt;u&gt;Danskin’s theorem&lt;/u&gt;,
the gradient of the conjugate function will be &lt;span class=&#34;math display&#34;&gt;\[
\nabla \psi^*(\theta) = \arg \max_{\omega \in \R^d}\ \omega^\top \theta
- \psi(\omega)
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is convex, then
&lt;span class=&#34;math display&#34;&gt;\[
\nabla \psi^*(\theta) = \arg \max_{\omega \in \R^d}\ \omega^\top \theta
- \psi(\omega) = (\nabla \psi)^{-1}(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;Fenchel conjugate of convex functions&lt;/strong&gt;.
Consider a convex function &lt;span class=&#34;math inline&#34;&gt;\(\psi: \R^d \to
\R\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;’s double
conjugate &lt;span class=&#34;math inline&#34;&gt;\(\psi^{**} = \psi\)&lt;/span&gt;. On the
other hand, if &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is not convex,
&lt;span class=&#34;math inline&#34;&gt;\(\psi^{**}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;’s &lt;strong&gt;convex
envelope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;Fenchel-Young inequality&lt;/strong&gt;. Consider
&lt;span class=&#34;math inline&#34;&gt;\(\psi: \R^d \to \R\)&lt;/span&gt; and its Fenchel
conjugate &lt;span class=&#34;math inline&#34;&gt;\(\psi^*\)&lt;/span&gt;. Then for every
&lt;span class=&#34;math inline&#34;&gt;\(\omega, \theta \in \R^d\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\omega^\top \theta \le \psi(\omega) + \psi^*(\theta)
\]&lt;/span&gt; Proof. The proof is simply an interpretation of the
definition: &lt;span class=&#34;math display&#34;&gt;\[
\psi^*(\theta) = \max_{\omega} [\omega^\top \theta - \psi(\omega)] \ge
\omega^\top \theta - \psi(\omega)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some examples of Fenchel conjugate: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\psi(\omega) = \frac{\lambda}{2} \|\omega\|_2^2 \to \psi^*(\theta) =
\frac{1}{2\lambda} \|\theta\|_2^2 \\
\psi(\omega) = \frac{1}{2} \omega^\top A \omega \to \psi^*(\theta) =
\frac{1}{2} \theta^\top A^{-1} \theta \\
\psi(\omega) = \sum_{i=1}^d \omega_i \log \omega_i \to \psi^*(\theta) =
\sum_{i=1}^d \exp(\theta_i - 1)
\end{gathered}
\]&lt;/span&gt; Now refer to the optimization problem &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;w_{t+1} = \arg \min_{w \in S} \psi(w) + \sum_{i=1}^{t} f_i(w) \\
&amp;amp;\approx \arg \min_{w \in S} \psi(w) + \sum_{i=1}^{t} \nabla
f_{i}(w_{i})^\top w \\
&amp;amp;= \arg \max_{w \in S} \underbrace{\left[-\sum_{i=1}^{t} \nabla
f_{i}(w_{i})\right]^\top}_{v_{t+1}^\top} w - \psi(w) \\
&amp;amp;= \nabla \psi^*(v_t)
\end{aligned}
\]&lt;/span&gt; This motivates the online mirror descent.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Algorithm: &lt;strong&gt;online mirror descent&lt;/strong&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Pick an &lt;strong&gt;initial point&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_1 \in S\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; do:
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Output &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; (in fact, this &lt;span class=&#34;math inline&#34;&gt;\(w_t\)&lt;/span&gt; is computed in last timestep and
formally should have been &lt;span class=&#34;math inline&#34;&gt;\(w_{t-1}\)&lt;/span&gt;
in the previous context) and receive &lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(z_t = -\nabla
f_t(w_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(v_t \leftarrow v_{t-1} +
z_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(w_{t+1} \leftarrow \arg \min_{w}
\psi(w) - w^\top \theta = \nabla \psi^*(v_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;strong&gt;strongly-convex and smooth function&lt;/strong&gt;.
Given a convex differentiable function &lt;span class=&#34;math inline&#34;&gt;\(\psi
: \R^d \to \R\)&lt;/span&gt;, consider its Bregman divergence &lt;span class=&#34;math inline&#34;&gt;\(D_\psi\)&lt;/span&gt;. Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We call &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;-strongly-convex with respect to norm
function &lt;span class=&#34;math inline&#34;&gt;\(\| \cdot \|\)&lt;/span&gt; if for every
&lt;span class=&#34;math inline&#34;&gt;\(w, u\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(D_\psi(w \| u) \ge \mu \|w − u\|_2^2\)&lt;/span&gt;,
which is equivalent to that for every &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H_\psi(x)
\succeq \mu I\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We call &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;-smooth with respect to norm
function &lt;span class=&#34;math inline&#34;&gt;\(\| \cdot \|\)&lt;/span&gt; if for every
&lt;span class=&#34;math inline&#34;&gt;\(w, u\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(D_\psi(w \| u) \le \frac{\lambda}{2} \|w −
u\|_2^2\)&lt;/span&gt;, which is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\|
\nabla \psi(w) - \nabla \psi(u)\|_2 \le \lambda \|w-u\|\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong convexity and smoothness&lt;/strong&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;-strongly-convex w.r.t to norm &lt;span class=&#34;math inline&#34;&gt;\(\| \cdot \|\)&lt;/span&gt; if and only if its Fenchel
conjugate &lt;span class=&#34;math inline&#34;&gt;\(\psi^*\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\mu}\)&lt;/span&gt;-smooth w.r.t. dual norm
&lt;span class=&#34;math inline&#34;&gt;\(\| \cdot \|_*\)&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\| w \|_* = \max_{\|u\| \le 1} w^\top u
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>Least Angle Regression</title>
      <link>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</link>
      <pubDate>Sun, 19 Dec 2021 15:09:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</guid>
      <description>

&lt;h2 id=&#34;forward-selection&#34;&gt;Forward Selection&lt;/h2&gt;
&lt;p&gt;In cases where we are to solve &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;
so that &lt;span class=&#34;math inline&#34;&gt;\(Y = XW\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Y \in \R^M, X \in \R^{M \times N}, W \in
\R^N\)&lt;/span&gt;, we can do it in an iterative and greedy way.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right,
do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \frac{Y \cdot \mathrm x^{(k)}}{||\mathrm x ^{(k)}||_2} \mathrm
x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from
&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or we have used all columns in &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently this algorithm is efficient. However, it does not
guarantee an exact solution, even when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; lies in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;forward-stagewise&#34;&gt;Forward Stagewise&lt;/h2&gt;
&lt;p&gt;Like forward selection, forward stagewise selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Unlike Forward
Selection, Forward Selection does not subtract the whole projection from
&lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Instead, it proceeds along
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; with a small
step.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\},\eta\text{ is a small
constant}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right,
do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \eta \frac{Y \cdot \mathrm x^{(k)}}{||\mathrm x ^{(k)}||_2}
\mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;del&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt;
from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/del&gt; Go to step 2 until
&lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently forward stagewise gives an exact solution when &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is small enough. But it is more
time-consuming.&lt;/p&gt;
&lt;h2 id=&#34;least-angle-regression&#34;&gt;Least Angle Regression&lt;/h2&gt;
&lt;p&gt;LARS a is a compromise between forward selection and forward
stagewise. Likewise, LARS selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm
x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm
x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. LARS proceeds stagewise like
forward stagewise, however with its own methodology to determine the
step size.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}, d = \arg\max_{\mathrm x \in
C}\frac{Y \cdot \mathrm x }{||\mathrm x||_2}\)&lt;/span&gt;
initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then do the following: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \eta \frac{Y \cdot d}{||d||_2} d \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is determined such
that there is some &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)} \in
C\)&lt;/span&gt; onto which the projection of &lt;span class=&#34;math inline&#34;&gt;\((Y -
\hat Y)\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Or in other words, &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; lies on the bisector
hyper-plane between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; be along this bisector.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;
is sufficiently small.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


</description>
    </item>
    
    <item>
      <title>Dimension Reduction</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/</link>
      <pubDate>Fri, 07 Jan 2022 12:39:56 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/</guid>
      <description>

&lt;h2 id=&#34;unsupervised-dimension-reduction&#34;&gt;Unsupervised Dimension
Reduction&lt;/h2&gt;
&lt;p&gt;Dimensionality reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduces computational cost;&lt;/li&gt;
&lt;li&gt;de-noises by projecting onto lower-dimensional space and back to
original space;&lt;/li&gt;
&lt;li&gt;makes results easier to understand by reducing the
collinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compared to feature selection,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the goal of feature selection is to remove features that are not
informative with respect to the class label. This obviously reduces the
dimensionality of the feature space;&lt;/li&gt;
&lt;li&gt;dimensionality reduction can be used to find a meaningful lower-dim
feature space even when there is information in each feature dimension
so that none can be discarded;&lt;/li&gt;
&lt;li&gt;dimensionality reduction is unsupervised while feature selection is
supervised.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compared to data compression,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dimensionality reduction can be seen as a simplistic form of data
compression. But they are not equivalent, as the goal of data
compression is to reduce the entropy of the representation, which is not
limited to the dimensionality reduction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-dimensionality-reduction&#34;&gt;Linear Dimensionality
Reduction&lt;/h3&gt;
&lt;p&gt;Linear dimensionality reduction projects data onto lower-dimensional
space by representing the data with a new basis consisting of some major
components. Mathematically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x^{(i)} = \sum_{k=1}^Kz^{(i)}_kb^{(k)}, \text{ where $z^{(i)} \in \R^K$
is the weight, $b^{(k)} \in \R^N$ is the basis vector.} \\
X = BZ, \text{ where $X = [x^{(i)}, \dots, x^{(M)}], B = [b^{(1)},
\dots, b^{(K)}], Z = [z^{(1)}, \dots, z^{(M)}]$}
\end{gather}
\]&lt;/span&gt; The objective can be set to minimize the error when recovering
from &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;alternating-least-squares&#34;&gt;Alternating Least Squares&lt;/h4&gt;
&lt;p&gt;By leveraging the idea of coordinate descent and OLS solution to
linear regression, we can optimize &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; alternatively, until convergence.&lt;/p&gt;
&lt;p&gt;Fix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, take derivative w.r.t.
&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
2(X - BZ)(-Z^T) = 0 \\
BZZ^T = XZ^T \\
B = XZ^T(ZZ^T)^{-1}
\end{gather}
\]&lt;/span&gt; Fix &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, take derivative
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(Z^T\)&lt;/span&gt; and make it zero to give
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial||X - BZ||^2_F}{\partial Z^T} = \frac{\partial||X^T -
Z^TB^T||^2_F}{\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\
2(X^T - Z^TB^T)(-B) = 0 \\
Z^TB^TB = X^TB \\
Z^T = X^TB(B^TB)^{-1} \\
Z = (B^TB)^{-1}B^TX
\end{gather}
\]&lt;/span&gt; Assume we have run the ALS to convergence and obtain the
global optimal parameters &lt;span class=&#34;math display&#34;&gt;\[
B^\star, Z^\star = \arg \min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X -
BZ)^2_{ij}
\]&lt;/span&gt; Let any invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(R \in
\R^{K \times K}\)&lt;/span&gt;. We can construct a pair of &lt;span class=&#34;math inline&#34;&gt;\(\tilde B, \tilde Z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde B = B^\star R, \tilde Z =
R^{-1}Z^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||X - \tilde B \tilde Z||^2_F = ||X - B^\star RR^{-1} Z^\star||^2_F =
||X - B^\star Z^\star||^2_F
\]&lt;/span&gt; Thus the global optima is not unique. We may force
regularization on &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; and obtain
the unique optima.&lt;/p&gt;
&lt;h4 id=&#34;principal-component-analysis&#34;&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;Principal Component Analysis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Suppose the SVD for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $N \times N$, $\Sigma$ is diagonal, $V$ is $M \times M$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{N \times M}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = \min\{N,M\}
\end{gather}
\]&lt;/span&gt; By only preserving the &lt;span class=&#34;math inline&#34;&gt;\(K \le
\rank(\Sigma)\)&lt;/span&gt; most prominent singular values, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be approximated as &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U_K \in \R^{N \times K}$ is first $K$ columns of $U$} \\
\text{$V_K \in \R^{M \times K}$ is first $K$ columns of $V$} \\
\Sigma_K \in \R^{K \times K} = diag(\sigma_1, \sigma_2, ..., \sigma_K)
\\
\end{gather}
\]&lt;/span&gt; &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/#Eckart-Young-Mirsky Theorem&#34;&gt;Eckart-Young-Mirsky
theorem&lt;/a&gt; will tell that &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt; is the best
approximation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; among &lt;span class=&#34;math inline&#34;&gt;\(N \times M\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in terms of Frobenius norm.&lt;/p&gt;
&lt;p&gt;We may obtain the &lt;span class=&#34;math inline&#34;&gt;\(B^\star,
Z^\star\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
B^\star = U_K, Z^\star = \Sigma_K V^T_K
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the data is centered in advance, &lt;span class=&#34;math inline&#34;&gt;\(B^\star\)&lt;/span&gt; is essentially the principal
component in PCA and &lt;span class=&#34;math inline&#34;&gt;\(Z^\star\)&lt;/span&gt; is the
transformed &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in the basis formed
by &lt;span class=&#34;math inline&#34;&gt;\(B^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;random-projection&#34;&gt;Random Projection&lt;/h4&gt;
&lt;p&gt;The above methods all minimize the Frobenius norm during
reconstruction. This objective may become time-consuming when input
dimension becomes large. We may use some randomly-generated vectors as
basis to do the projection. This greatly saves time, at the expense of
losing accuracy. We can measure such projection by checking whether the
structure of the data can be preserved, e.g. the distance between
points.&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss Lemma tells that a random function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \frac{1}{\sqrt{K}}Ax\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K
\times N\)&lt;/span&gt; matrix with i.i.d. entries sampled from a standard
Gaussian, can preserve the distance between any two points within error
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
(1 - \epsilon)||x^{(i)} - x^{(j)}||^2_2 \le ||f(x^{(i)}) -
f(x^{j})||^2_2 \le (1 + \epsilon)||x^{(i)} - x^{(j)}||^2_2
\]&lt;/span&gt; with the probability at least &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt; as long as &lt;span class=&#34;math display&#34;&gt;\[
K \ge \frac{8\log M}{\epsilon^2}
\]&lt;/span&gt; And usually this requirement is quite conservative.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-dimensionality-reduction&#34;&gt;Non-linear Dimensionality
Reduction&lt;/h3&gt;
&lt;p&gt;We can introduce the feature mapping to handle the non-linearity
problem in Dimensionality Reduction. Similar to non-linear regression,
we can introduce the kernel trick in this case, yielding the &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/#Kernel PCA&#34;&gt;Kernel PCA&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;supervised-dimensionality-reduction&#34;&gt;Supervised Dimensionality
Reduction&lt;/h2&gt;
&lt;p&gt;There is no guarantee that the unsupervised Dimension Reduction can
throw insight into the classification problem. It may or may not help.
Otherwise supervised Dimension Reduction such as &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/&#34;&gt;Fisher’s Linear Discriminant&lt;/a&gt; finds
a lower-dimensional space so as to minimize the class overlap.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</link>
      <pubDate>Tue, 11 Jan 2022 16:10:51 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</guid>
      <description>

&lt;p&gt;Given a data matrix &lt;span class=&#34;math inline&#34;&gt;\(X = [x^{(1)}, \dots,
x^{(M)}] \in \mathbb R^{N \times M}\)&lt;/span&gt;, the goal of PCA is to
identify the directions of maximum variance contained in the data
(compared with directional derivative, this is directional variance) and
project data onto those directions.&lt;/p&gt;
&lt;h3 id=&#34;linear-pca&#34;&gt;Linear PCA&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(v \in \mathbb R^{N \times 1}\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt;, the variance in
the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2, \text{where }\mu =
\frac{1}{M}\sum_{i=1}^Mv^Tx^{(i)} = v^T\bar x
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{M}\sum_{i=1}^Mx^{(i)} =
0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2 =
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - v^T\bar x)^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(x^{(i)} - \bar x)(x^{(i)} - \bar x)^Tv
\\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(x^{(i)} - \bar x)(x^{(i)} - \bar
x)^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^TXX^Tv \\
\end{aligned}
\]&lt;/span&gt; Suppose we want to identify the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; of the maximum variance, we can
formulate the problem as &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v \quad \frac{1}{M}v^TXX^Tv \\
s.t.\quad v^Tv = 1
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\frac{1}{M}XX^T\)&lt;/span&gt;, which is real symmetric, we can form the
Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
L(v, \lambda) = v^T\Sigma v + \lambda (1 - v^Tv)
\]&lt;/span&gt; We represent the constraint as &lt;span class=&#34;math inline&#34;&gt;\(1 -
v^Tv = 0\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(v^Tv - 1 =
0\)&lt;/span&gt;. The reason is if we take derivative w.r.t &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial v} = 2\Sigma v - 2\lambda v
\]&lt;/span&gt; which is more intuitive than &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial v} = 2\Sigma v +
2\lambda v\)&lt;/span&gt;. Let the derivative be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(v^T v = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v
\ne 0\)&lt;/span&gt;, this means the optimal solution &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star, v^\star)\)&lt;/span&gt; must be a pair
of eigenvalue and eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v^\star = v_i, \lambda^\star = \lambda_i
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; is rescaled
such that &lt;span class=&#34;math inline&#34;&gt;\(v_i^Tv_i = 1\)&lt;/span&gt;. Substitute
the result back to the objective to give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}v^TXX^Tv &amp;amp;= (v_i)^T\Sigma v_i \\
&amp;amp;= (v_i)^T\lambda_iv_i \\
&amp;amp;= \lambda_i(v_i)^Tv_i \\
&amp;amp;= \lambda_i
\end{aligned}
\]&lt;/span&gt; So the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; largest
directions of variance will be the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_N\)&lt;/span&gt;, corresponding to the
eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 &amp;gt; \lambda_2 &amp;gt;
... &amp;gt; \lambda_N\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;compared-with-svd&#34;&gt;Compared with SVD&lt;/h4&gt;
&lt;p&gt;Intuitively, principal components can be obtained by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Diagonalizable&#34;&gt;spectral decomposition&lt;/a&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;’s covariance matrix. But in practice,
principal components are usually solved with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;, which is more efficient in
computation and can handle sparse representation. One thing worth notice
is that, before applying SVD, PCA centers the data firstly. That said,
we cannot equalize PCA and SVD. For a more detailed discussion on the
relation between PCA and SVD, please refer to &lt;a href=&#34;https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8&#34;&gt;this
blog&lt;/a&gt; and &lt;a href=&#34;https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca&#34;&gt;this
post&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;another-view-on-pca&#34;&gt;Another view on PCA&lt;/h4&gt;
&lt;p&gt;Finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\max_{v} \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)})^2 \\
\]&lt;/span&gt; is equivalent to finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\min_{v} \frac{1}{M}\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2
\]&lt;/span&gt; both subject to &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 =
1\)&lt;/span&gt; and inter-orthogonality. This second notation is essentially
the projection of &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(||v|| = 1\)&lt;/span&gt; and thus the objective
indicates minimizing the overall approximation error.&lt;/p&gt;
&lt;h3 id=&#34;kernel-pca&#34;&gt;Kernel PCA&lt;/h3&gt;
&lt;p&gt;We can map the data to a higher-dimension space: &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \to \phi(x^{(i)})\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X} = [\phi(x^{(1)}), ...,
\phi(x^{(M)})]\)&lt;/span&gt;. Kernel tricks allow us not to explicitly define
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, but to only focus on the
inner product of mapped data: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal
K(x^{(i)}, x^{(j)}) = \phi(x^{i})^T\phi(x^{j})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, the variance along direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(v^Tv=1\)&lt;/span&gt;, is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2, \text{where }\mu =
\frac{1}{M}\sum_{i=1}^Mv^T\phi (x^{(i)}) = v^T\bar\phi(x)
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x) = \frac{1}{M}\sum_{i=1}^M\phi(x^{(i)})
= 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2 =
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - v^T\bar\phi(x))^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(\phi(x^{(i)}) - \bar{\phi(x)})(\phi
(x^{(i)}) - \bar\phi(x))^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(\phi(x^{(i)}) - \bar\phi(x))(\phi
(x^{(i)}) - \bar\phi(x))^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M\phi(x^{(i)})(\phi(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
\end{aligned}
\]&lt;/span&gt; Then comes the standard form of linear PCA. Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\frac{1}{M}\mathcal{X}\mathcal{X}^T\)&lt;/span&gt;, we would like to solve
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v\quad \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
s.t.\quad v^T v=1
\end{gather}
\]&lt;/span&gt; This is equivalent to solving &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors. Unfortunately,
this cannot be directly solved like in linear PCA since we don’t know
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. However note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
\Sigma v &amp;amp;= \lambda v \\
v &amp;amp;= \frac{1}{\lambda}\Sigma v
\end{aligned} \\
\begin{aligned}
v &amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M\phi(x^{(i)})[(\phi(x^{(i)})^Tv]
\\
&amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M[(\phi(x^{(i)})^Tv]\phi(x^{(i)})
\end{aligned}
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt;’s and
can be written as &lt;span class=&#34;math display&#34;&gt;\[
v = \mathcal{X}\alpha, \alpha \in \R^N
\]&lt;/span&gt; Substitute this back to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v =
\lambda v\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
\lambda\mathcal{X}\alpha \\
\frac{1}{M}\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
M\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(K =
\mathcal{X}^T\mathcal{X}\)&lt;/span&gt; which is invertible. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
KK\alpha = M\lambda K\alpha \\
K\alpha = M\lambda\alpha
\end{gathered}
\]&lt;/span&gt; We can solve &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; as
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvector. Kernel trick can
be applied since &lt;span class=&#34;math inline&#34;&gt;\(K_{ij} =
(\mathcal{X}^T\mathcal{X})_{ij} =
\phi(x^{(i)})^T\phi(x^{(j)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\lambda\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvalue, which is proportional to
&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvalue and thus the
objective. So &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s largest &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvalues correspond to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; largest objective one-by-one. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, ..., \alpha_L\)&lt;/span&gt; has to be solved
with constraint &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i^T\alpha_i =
\frac{1}{M\lambda}\)&lt;/span&gt; so that for &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;’s eigenvector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^Tv &amp;amp;= (\mathcal{X}\alpha)^T\mathcal{X}\alpha \\
&amp;amp;= \alpha^T(\mathcal{X}^T\mathcal{X})\alpha \\
&amp;amp;= \alpha^TK\alpha \\
&amp;amp;= M\lambda\alpha^T\alpha \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a sample &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we compute
its dot-products with &lt;span class=&#34;math inline&#34;&gt;\(v_1,\dots,v_L\)&lt;/span&gt;
in the kernel space to get the projected coordinates. To get &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th coordinate of transformed sample,
&lt;span class=&#34;math display&#34;&gt;\[
\phi(x^*)^T \phi(v_j) = \phi(x^*)^T \mathcal X \alpha_j = [\mathcal
K(x^*, x^{(1)}), \mathcal K(x^*, x^{(2)}), \dots, \mathcal K(x^*,
x^{(M)})]\alpha_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;centralizing-in-kernel-trick&#34;&gt;Centralizing in Kernel Trick&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde\phi(x^{(i)}) = \phi(x^{(i)}) -
\bar\phi(x), \tilde{\mathcal{X}} = [\tilde\phi(x^{(1)}),
\tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\)&lt;/span&gt;. We have assumed
that &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; is pre-centered
by subtracting &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.
However, as said, we didn’t explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, so there is no way to calculate
&lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt;, let alone &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From another perspective, the objective of pre-centering is to get
&lt;span class=&#34;math inline&#34;&gt;\(\tilde K =
\tilde{\mathcal{X}}\tilde{\mathcal{X}}^T =
\sum_{i=1}^M\tilde\phi(x^{(i)})\tilde\phi(x^{(i)})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{M \times 1}\)&lt;/span&gt;
represent a &lt;span class=&#34;math inline&#34;&gt;\(M \times 1\)&lt;/span&gt; column
vector with all elements being &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\bar\phi(x) = \frac{1}{M}(\phi(x^{(1)}) + \phi(x^{(2)}) + ... +
\phi(x^{(M)})) = \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_M\)&lt;/span&gt;
represents a &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt; matrix with
all elements being &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt;.
Pre-center all data to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde{\mathcal{X}} &amp;amp;= [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}),
..., \tilde\phi(x^{(M)})]\\
&amp;amp;= [\phi(x_1) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1},
\phi(x^{(2)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, ...,
\phi(x^{(M)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}] \\
&amp;amp;= [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})] -
\frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T
\\
&amp;amp;= \mathcal{X} - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times
1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \mathcal{X}\mathbb{1}_M \\
\tilde K &amp;amp;= \tilde{\mathcal{X}}^T\tilde{\mathcal{X}} \\
&amp;amp;= (\mathcal{X} - \mathcal{X}\mathbb{1}_M)^T(\mathcal{X} -
\mathcal{X}\mathbb{1}_M) \\
&amp;amp;= (\mathcal{X}(I - \mathbb{1}_M))^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)^T\mathcal{X}^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)K(I - \mathbb{1}_M) \\
\tilde K_{ij} &amp;amp;= (\tilde{\mathcal{X}}^T\tilde{\mathcal{X}})_{ij} \\
&amp;amp;= \tilde\phi(x^{(i)})^T\tilde\phi(x^{(j)})\\
&amp;amp;= (\phi(x^{(i)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times
1})^T(\phi(x^{(j)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;explained-variance&#34;&gt;Explained Variance&lt;/h3&gt;
&lt;p&gt;It remains a question that in real application, how many principal
components to choose to represent the original data. Explained variance
can be a good measure on this. We can choose a number of principal
components such that they “explains” a certain percentage &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; of the original data.&lt;/p&gt;
&lt;p&gt;Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}v_i^TXX^Tv_i\)&lt;/span&gt; is the
directional variance explained along &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;, or the variance of the first
dimension of transformed data samples. We may choose the first &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; principal components such that &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^k v_i^TXX^Tv_i \ge \tau \sum_{i=1}^N \sigma_i^2
\]&lt;/span&gt; ## External Materials&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/59775730&#34;&gt;数据降维:
核主成分分析(Kernel PCA)原理解析&lt;/a&gt; || &lt;a href=&#34;https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained&#34;&gt;Explained
variance&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Source Coding Theorem</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/</guid>
      <description>

&lt;h2 id=&#34;notations-and-concepts&#34;&gt;Notations and Concepts&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt;
(we specifically use the random variable symbol to denote the ensemble)
is a triplet &lt;span class=&#34;math inline&#34;&gt;\((X, \newcommand{A}{\mathcal A}
\newcommand{P}{\mathcal P} \A_X, \P_X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; denotes the random variable, which
takes on values from &lt;span class=&#34;math inline&#34;&gt;\(\A_X = \{a_1, a_1,
\dots \}\)&lt;/span&gt;, that has probability &lt;span class=&#34;math inline&#34;&gt;\(\P_X
= \{p_1, p_2, \dots \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Shannon information content of an outcome &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/strong&gt; is: &lt;span class=&#34;math display&#34;&gt;\[
h(x) = \log_2 \frac{1}{P(x)}
\]&lt;/span&gt; The &lt;strong&gt;raw bit content of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_0(X) = \log |\mathcal A_X|
\]&lt;/span&gt; The &lt;strong&gt;smallest &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset &lt;span class=&#34;math inline&#34;&gt;\(S_\delta\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal A_X\)&lt;/span&gt;&lt;/strong&gt; is the smallest
subset satisfying &lt;span class=&#34;math display&#34;&gt;\[
P(X \in S_\delta) \ge 1 - \delta
\]&lt;/span&gt; The &lt;strong&gt;essential bit content of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X) = \log |S_\delta|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;shannons-source-coding-theorem&#34;&gt;Shannon’s Source Coding
Theorem&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be an ensemble with
entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; bits, and &lt;span class=&#34;math inline&#34;&gt;\(X^N\)&lt;/span&gt; be the ensemble composed of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such i.i.d. random variables. Given
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \delta &amp;lt; 1\)&lt;/span&gt;, there exists a
positive integer &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; such that for
&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
|\frac{1}{N} H_\delta (X^N) - H| &amp;lt; \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or put it in a verbal way,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. random variables each
with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; can be
compressed into more than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits
with negligible information loss as &lt;span class=&#34;math inline&#34;&gt;\(N \to
\infty\)&lt;/span&gt;; conversely if they are compressed fewer than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits, it is virtually certain that
there is information loss.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let the random variable &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; be defined
for the ensemble &lt;span class=&#34;math inline&#34;&gt;\(Y = X^N\)&lt;/span&gt;, which
composes of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. ensembles
&lt;span class=&#34;math inline&#34;&gt;\(X_1 \dots X_N\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; can be
re-written as the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;
information contents &lt;span class=&#34;math inline&#34;&gt;\(h_i = \log \frac 1
{P(X_i)}, i \in 1,2,\dots,N\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots
P(X_N)} = \frac 1 N (\log \frac{1}{P(X_1)} + \dots + \log
\frac{1}{P(X_N)})
\]&lt;/span&gt; Each of these information contents is in turn a random
variable with mean &lt;span class=&#34;math inline&#34;&gt;\(\bar h_i = H(X) =
H\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{h_i}^2 =
\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A long string of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; symbols would
usually contain roughly &lt;span class=&#34;math inline&#34;&gt;\(p_1 N\)&lt;/span&gt;
occurrences of symbol &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_2 N\)&lt;/span&gt; occurrences of symbol &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt;… The probability of those elements is
roughly &lt;span class=&#34;math inline&#34;&gt;\((p_1)^{p_1 N} (p_2)^{p_2 N}
\dots\)&lt;/span&gt; The information content of each such element is thus
roughly &lt;span class=&#34;math inline&#34;&gt;\(N \sum_i p_i \log \frac{1}{p_i} = N
H\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define the typical elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{A}_X^N\)&lt;/span&gt; to be just those element
that have probability close to &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;. Note that interestingly, the
most probable string is not usually typical because its probability is
far away from &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We introduce another parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to control how close the
probability has to be to &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;
for an element to be typical. The set of typical elements is called
&lt;strong&gt;typical set&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\beta}\)&lt;/span&gt; and is defined as &lt;span class=&#34;math display&#34;&gt;\[
T_{N \beta} = \left\{y \in \mathcal A_X^N: [\frac 1 N \log
\frac{1}{P(y)} - H]^2 &amp;lt; \beta^2 \right\}
\]&lt;/span&gt; By the &lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;Weak Law of Large
Numbers&lt;/a&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P \left( \left( \frac 1 N \log \frac{1}{P(Y)} - H \right)^2 \ge \beta^2
\right) \le \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; and thus &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; As &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, the
probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; falls in &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; draws near to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. We need to relate this to the theorem
that for any given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta\)&lt;/span&gt;, there is a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \simeq NH\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;lt; N(H +
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; is not the
best sufficient subset for compression (because it doesn’t include those
most probable). Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\log |T_{N
\beta}|\)&lt;/span&gt; upper-bounds the &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N)\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;. On the other hand, for all &lt;span class=&#34;math inline&#34;&gt;\(y \in T_{N \beta}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P(y) &amp;gt; 2^{-N(H + \beta)}\)&lt;/span&gt;, thus &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;amp;&amp;lt; \sum_{y \in T_{N \beta}} P(y)
&amp;lt; 1 \\
&amp;amp;\Downarrow \\
|T_{N \beta}| &amp;amp;&amp;lt; 2^{N(H + \beta)} \\
\end{align*}
\]&lt;/span&gt; If we set &lt;span class=&#34;math inline&#34;&gt;\(\beta =
\epsilon\)&lt;/span&gt; and set &lt;span class=&#34;math inline&#34;&gt;\(N \ge N_0\)&lt;/span&gt;
in a way such that &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{\epsilon^2 N_0} \le
\delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in T_{N
\epsilon}) \ge 1 - \delta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\epsilon}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset. Then, &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \le \log |T_{N \epsilon}| \le N(H +
\epsilon)\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(N \ge
N_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H -
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This part is reached by contradiction. Suppose instead there exists a
&lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;#39;\)&lt;/span&gt; such that there exists
a sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; which
results in &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^{N}) \le N(H -
\epsilon)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\epsilon
&amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon/2\)&lt;/span&gt;, we now have
&lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]&lt;/span&gt; Denote the associated subset by &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;. We are to disprove &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; can
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. We break this probability into two parts: &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39;) = P(Y \in S&amp;#39; \cap T_{N \beta}) + P(Y \in S&amp;#39; \cap
\overline{T_{N \beta}})
\]&lt;/span&gt; For the first part, we have &lt;span class=&#34;math display&#34;&gt;\[
|S&amp;#39; \cap T_{N \beta}| \le |S&amp;#39;| \le 2^{N(H - 2\beta)}
\]&lt;/span&gt; Thus, the maximum value of the first part is obtained when
&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap T_{N \beta}\)&lt;/span&gt; contains
&lt;span class=&#34;math inline&#34;&gt;\(2^{N(H - 2\beta)}\)&lt;/span&gt; outcomes all with
probability &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)}\)&lt;/span&gt;
(property of elements in &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\beta}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;As for the second part, since &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap
\overline{T_{N \beta}} \subseteq \overline{T_{N \beta}}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39; \cap \overline{T_{N \beta}}) \le P(Y \in \overline{T_{N
\beta}}) &amp;lt; \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} +
\frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; For arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and
a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we can have
&lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \le 1 - \delta\)&lt;/span&gt;
instead of &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. Then we shall conclude that &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; cannot
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt;
N(H - \epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind Shannon’s source coding theorem
is that, as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; grows, the deviation
of the random variable may grow in a lower order than that of the number
of values the random variable can take on. Therefore, the resulting
outcomes will fall in a narrower range, making the typical set smaller.
Encoding the elements inside the typical set is &lt;em&gt;almost&lt;/em&gt; enough
for the purpose of communication.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/functional-analysis/fourier-transform/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/functional-analysis/fourier-transform/</guid>
      <description>

&lt;h2 id=&#34;continuous-time-fourier-transform&#34;&gt;Continuous-time Fourier
Transform&lt;/h2&gt;
&lt;h3 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h3&gt;
&lt;p&gt;In Euclidean space, we usually represent a vector by a set of
independent and orthogonal base vectors (basis). Orthogonality means the
inner product between two basis is zero. Inner product can also be
defined on some common interval between two functions, and thus the
orthogonality.&lt;/p&gt;
&lt;h4 id=&#34;frequency-domain&#34;&gt;Frequency Domain&lt;/h4&gt;
&lt;p&gt;It is intuitive to model after the inner product between vectors.
Function (signal) on its domain can be viewed as an “infinite-dimension”
vector. We represent this infinity in the definition of function inner
product by integration. In particular, given two functions &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, an interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[
\int\limits_{x \in I}s(x)g(x)dx
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are orthogonal on interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; if their inner product &lt;span class=&#34;math inline&#34;&gt;\(\int_{x \in I}s(x)g(x)dx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set of basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt; Euclidean
space contains at most &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;
independent orthogonal basis. For an “infinite-dimension” function
space, there are an infinite number of basis, among which a group of
sine and cosine functions satisfy. For integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and positive integers &lt;span class=&#34;math inline&#34;&gt;\(m, n\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx = 0, m, n \ge 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m = n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \cos^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 - \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 + \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin(nx) \cos(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin(2nx)}{2}dx \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m \ne n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) + \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) - \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} - \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\sin((n+m)x) + \sin((n-m)x)}{2}dx \\
&amp;amp;= -\frac{\frac{\cos((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\cos((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, we can use the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(x), \sin(x), \cos(2x), \sin(2x),
\dots\)&lt;/span&gt; to fit any &lt;strong&gt;continuous function&lt;/strong&gt; on
interval &lt;span class=&#34;math inline&#34;&gt;\([-\pi, \pi]\)&lt;/span&gt;. Or use &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(2\pi fx), \sin(2\pi fx), \cos(2\pi f2x),
\sin(2\pi f2x), \dots\)&lt;/span&gt; to fit any function on interval &lt;span class=&#34;math inline&#34;&gt;\([\frac{-1}{2f} + \frac{k}{f}, \frac{1}{2f} +
\frac{k}{f}]\)&lt;/span&gt;, which can be any interval by properly choosing
the value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (frequency) and the
integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;
approximated with such series up to level &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; can be written as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{split}
s_N(x) &amp;amp;= a_0 + \sum_{n=1}^N \big(
\underbrace{a_n}_{A_n\sin(\varphi_n)} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\cos(\varphi_n)} \sin(2\pi fnx) \big) \\
&amp;amp;= a_0 + \sum_{n=1}^N \bigg( A_n\sin(2\pi fnx + \varphi_n) \bigg)
\text{, where} \\
\end{split} \\
\notag &amp;amp;A_n = \sqrt{a_n^2 + b_n^2}, \sin(\varphi_n) =
\frac{a_n}{\sqrt{a_n^2 + b_n^2}}, \cos(\varphi_n) =
\frac{b_n}{\sqrt{a_n^2 + b_n^2}}
\end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A_n\)&lt;/span&gt; can be interpreted as
the amplitude, &lt;span class=&#34;math inline&#34;&gt;\(\varphi_n\)&lt;/span&gt; as the
phase, &lt;span class=&#34;math inline&#34;&gt;\(nf\)&lt;/span&gt; as the frequency.&lt;/p&gt;
&lt;h4 id=&#34;complex-domain&#34;&gt;Complex Domain&lt;/h4&gt;
&lt;p&gt;By Euler’s Formula we have &lt;span class=&#34;math display&#34;&gt;\[
e^{ix} = \cos x + i\sin x
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(s_N(x)\)&lt;/span&gt; can be
re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
s_N(x) &amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg(
\underbrace{a_n}_{A_n\cos \phi_n} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\sin \phi_n} \sin(2\pi fnx) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( A_n\cos(2\pi fnx - \phi_n)
\bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) + i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) - i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;\Downarrow_\text{by multiplication rule between complex
numbers in polar form}  \\
&amp;amp;= &amp;amp;&amp;amp; \sum_{n=-N}^N c_ne^{i 2\pi fnx} \\
\end{alignat}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
c_n &amp;amp;=
\begin{cases}
\frac{A_n}{2}(\cos \phi_n - i\sin \phi_n) = \frac{1}{2}(a_n - ib_n),
&amp;amp;n &amp;gt; 0 \\
\overline{c_{|n|}} =\frac{A_n}{2}(\cos \phi_n + i\sin \phi_n), &amp;amp;n
&amp;lt; 0 \\
a_0, &amp;amp;n = 0
\end{cases}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(N \to +\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; can be reconstructed as the Fourier
Series: &lt;span class=&#34;math display&#34;&gt;\[
s(x) = \lim_{N \to +\infty} s_N(x) = a_0 + \sum_{n=1}^{+\infty} \bigg(
a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg) \\
\]&lt;/span&gt; The problem comes how &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt;
can be computed. By the orthogonality mentioned before, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0
+ \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx)
\bigg)] \d x = \frac{1}{f} a_0 \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \cos(2\pi fkx) \d x =
\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \cos (2\pi fkx) \d x =
\frac{1}{2f} a_k \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \sin(2\pi fkx) \d x
=\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \sin (2\pi fkx) \d x =
\frac{1}{2f} b_k \\
\end{gather}
\]&lt;/span&gt; The computation of &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; can be combined together by
the Euler’s Formula: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) e^{-i 2\pi fkx} \d x &amp;amp; =
\int_{-1/2f+f/k}^{1/2f+f/k} (a_k \cos(2\pi fkx) + b_k \sin(2\pi fkx))
(\cos(2\pi fkx) - i \sin(2\pi fkx)) \d x \\
&amp;amp;= \frac{1}{2f} (a_k - i b_k) = \frac{1}{f} c_k
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;fourier-transform&#34;&gt;Fourier Transform&lt;/h3&gt;
&lt;p&gt;We have been through representing a continuous function on a certain
interval using the Fourier Series. This can be quite useful for periodic
functions. As long as we figure out the representation on its repeating
interval, we obtain the representation on its whole domain. The problem
is more of computing the factor for each sine and cosine function.&lt;/p&gt;
&lt;p&gt;The process of finding out factors for an arbitrary continuous
function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is called Fourier
Transform. It transforms the function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; from the time domain to the frequency
domain. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is periodic, it can
be easily represented by the Fourier Series as discussed in previous
section. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is not periodic, we
can treat the periodic interval as &lt;span class=&#34;math inline&#34;&gt;\([-\infty,
+\infty]\)&lt;/span&gt;. Its Fourier Transform and the inverse will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\hat s \stackrel{\mathcal F}\Longleftrightarrow s \\
\hat s(f) = \int_{-\infty}^{+\infty} s(t) e^{-i 2\pi f x} \d t \\
s(x) = \int_{-\infty}^{+\infty} \hat s(f) e^{i 2\pi f x} \d f
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;discrete-time-fourier-transform&#34;&gt;Discrete-time Fourier
Transform&lt;/h2&gt;
&lt;p&gt;The domain (time axis) of the function (signal) is continuous in our
discussion by now. When the time axis is discrete (and usually takes on
a series of integers), we are facing the discrete-time Fourier
transform. We will be using the term &lt;strong&gt;signal&lt;/strong&gt; instead of
the function from now on.&lt;/p&gt;
&lt;p&gt;For a discrete signal &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, its
Fourier transform is &lt;span class=&#34;math display&#34;&gt;\[
\hat s(\omega) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i\omega k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the
angular speed. &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is in the
unit of radian/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat
s(\omega)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; between two signal samples. &lt;span class=&#34;math display&#34;&gt;\[
\hat s(f) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i 2\pi f k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the
“frequency”. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the unit of
cycles/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(f)\)&lt;/span&gt; is the
weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; cycles between two signal samples.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jezzamon.com/fourier/zh-cn.html&#34;&gt;傅里叶变换交互式入门
(jezzamon.com)&lt;/a&gt; || &lt;a href=&#34;https://charlesliuyx.github.io/2018/02/18/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E8%AE%A9%E4%BD%A0%E6%B0%B8%E8%BF%9C%E5%BF%98%E4%B8%8D%E4%BA%86%E7%9A%84%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E8%A7%A3%E6%9E%90/&#34;&gt;傅立叶变换与群&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/1119070975&#34;&gt;如何理解傅里叶变换公式？
- 苗华栋的回答 - 知乎&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/334686351&#34;&gt;如何理解傅里叶变换公式？
- 马同学的回答 - 知乎&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Stirling&#39;s Approximation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/</guid>
      <description>

&lt;h2 id=&#34;stirlings-approximation&#34;&gt;Stirling’s Approximation&lt;/h2&gt;
&lt;p&gt;Stirling’s approximation, which states that &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(n+1) \sim \sqrt{2 \pi n} \left( \frac{n}{e}
\right)^n\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;,
is useful when estimating the order of &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt;. Notably, it is quite accurate even
when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is small.&lt;/p&gt;
&lt;p&gt;The authentic proof entails Gamma function and Laplace’s method.
However in integer case, Stirling’s approximation can be approached with
Poisson distribution. Start from a Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
P(r ; \lambda) = e^{-\lambda} \frac{\lambda^r}{r!}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda_1)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(Y \sim P(\lambda_2)\)&lt;/span&gt;, and suppose
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(X + Y \sim P(\lambda_1 + \lambda_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \Pr(X + Y = r) = \sum_{k=0}^r \Pr(X = k) \Pr(Y = r-k) \\
&amp;amp;= \sum_{k=0}^r e^{-\lambda_1} \frac{\lambda_1^k}{k!} e^{-\lambda_2}
\frac{\lambda_2^{r-k}}{(r-k)!} \\
&amp;amp;= \frac{e^{-(\lambda_1 + \lambda_2)}}{r!} \sum_{k=0}^r \frac{r!}{k!
(r-k)!} \lambda_1^k \lambda_2^{r-k} \\
&amp;amp;= e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^r}{r!}
\\
&amp;amp;= P(r; \lambda_1 + \lambda_2)
\end{aligned}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, a random variable &lt;span class=&#34;math inline&#34;&gt;\(X \sim
P(\lambda)\)&lt;/span&gt; (with integer &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) can be treated as the addition
of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; independent &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim P(1)\)&lt;/span&gt;. By the central limit
theorem, for a large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\mathbb \Pr(\frac{\underbrace{\sum_i Y_i}_X - \lambda}{\sqrt{\lambda}}
\le x) \simeq \Phi(x)
\]&lt;/span&gt; Or put it another way, the mass of the Poisson distribution
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; follows is well approximated by
the density of the Gaussian distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(x;\lambda) &amp;amp;\simeq N(x; \lambda, \lambda) \\
e^{-r} \frac{\lambda^r}{r!} &amp;amp;\approx \frac{1}{\sqrt{2\pi \lambda}}
e^{-\frac{(r - \lambda)^2}{2\lambda}}
\end{aligned}
\]&lt;/span&gt; Plug &lt;span class=&#34;math inline&#34;&gt;\(r = \lambda\)&lt;/span&gt; into
this formula and rearrange it to have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{-\lambda} \frac{\lambda^\lambda}{\lambda!} &amp;amp;\approx
\frac{1}{\sqrt{2\pi \lambda}} \\
\lambda! &amp;amp;\approx \sqrt{2\pi \lambda} \left( \frac{\lambda}{e}
\right)^\lambda
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jenson-Shannon Divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/</guid>
      <description>

&lt;h2 id=&#34;jenson-shannon-divergence&#34;&gt;Jenson-Shannon Divergence&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, &lt;strong&gt;Jenson-Shannon
divergence&lt;/strong&gt; is another method of measuring the distance between
two distributions. It is based on &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; with some notable
differences. KL-divergence does not make a good measure of distance
between distributions, since in the first place it is &lt;em&gt;not
symmetric&lt;/em&gt;. Another disadvantage of KL-divergence is that it is
&lt;em&gt;not bounded from above&lt;/em&gt;. Jenson-Shannon divergence, on the other
hand, overcomes these two problems of KL-divergence.&lt;/p&gt;
&lt;p&gt;Given two distributions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; defined on the same sample space,
Jenson-Shannon divergence is defined as &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\JSD}{\mathop{\text{JSD}}}
\JSD(p;q) \coloneq \frac{1}{2} (D_\text{KL}(p || m) + D_\text{KL}(q ||
m)), \text{where $m = \frac{p+q}{2}$}
\]&lt;/span&gt; Since the JSD is the addition of two KL-divergences, it is
non-negative by the non-negativity of KL-divergence and it reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(p=q\)&lt;/span&gt;. On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\JSD(p;q) &amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log
\frac{p(x)}{(p(x)+q(x))/2} + \E_{x \sim q} \log
\frac{q(x)}{(p(x)+q(x))/2} \big) \\
&amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log \frac{2}{( 1 + e^{\ln
\frac{q(x)}{p(x)}} )} + \E_{x \sim q} \log \frac{2}{( 1 + e^{\ln
\frac{p(x)}{q(x)}} )} \big) \\
\end{aligned}
\]&lt;/span&gt; Due to the concavity of &lt;span class=&#34;math inline&#34;&gt;\(f(x) =
\log \frac{2}{1 + e^x}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\JSD(p;q) &amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log \frac{2}{( 1 +
e^{\ln \frac{q(x)}{p(x)}} )} + \E_{x \sim q} \log \frac{2}{( 1 + e^{\ln
\frac{p(x)}{q(x)}} )} \big) \\
&amp;amp;\le \frac{1}{2} \big( \log \frac{2}{( 1 + e^{\E_{x \sim p} \ln
\frac{q(x)}{p(x)}} )} + \log \frac{2}{( 1 + e^{\E_{x \sim q} \ln
\frac{p(x)}{q(x)}} )} \big) \\
&amp;amp;\le \frac{1}{2} \big( 2 \log \frac{2}{( 1 + e^{( \E_{x \sim p} \ln
\frac{q(x)}{p(x)} + \E_{x \sim q} \ln \frac{p(x)}{q(x)} ) / 2} )} \big)
\\
&amp;amp;= \log \frac{2}{( 1 + e^{-\frac{1}{2} ( D_\text{KL}(p||q) +
D_\text{KL}(q||p) )} )}
\end{aligned}
\]&lt;/span&gt; This upper bound is attributed to Crooks. Since the
KL-divergence can go to positive infinity, we can conclude that &lt;span class=&#34;math inline&#34;&gt;\(\JSD(p;q)\)&lt;/span&gt; is upper-bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log 2\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\J}{\mathop{\text{J}}} \J(p;q)
\coloneq \frac{1}{2} ( D_\text{KL}(p||q) + D_\text{KL}(q||p) )\)&lt;/span&gt;
term is also known as &lt;strong&gt;Jeffreys divergence&lt;/strong&gt; (the
coefficient &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; may be
ignored in some other place). Even more accurately, the upper bound can
be rewritten as
(&lt;a href=&#34;https://chunxy.github.io/uploads/Inequalities%20between%20the%20Jenson-Shannon%20and%20Jeffreys%20divergences.pdf&#34; target=&#34;_blank&#34;&gt;in
this note&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\JSD(p;q) \le \min (\frac{1}{4} \J(p;q), \log \frac{2}{1 +
e^{-\J(p;q)}})
\]&lt;/span&gt; A lower bound in terms of Jeffreys divergence can be derived
as
(&lt;a href=&#34;https://chunxy.github.io/uploads/A%20Note%20on%20Bound%20for%20Jensen-Shannon%20Divergence%20by%20Jeffreys.pdf&#34; target=&#34;_blank&#34;&gt;in
this note&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\JSD(p;q) \ge \frac{1}{4} \ln(1 + 2\J(p;q))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remarkably, the square root of JSD between two distributions
satisfies the &lt;a href=&#34;https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition&#34;&gt;metric
axioms&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;relation-with-entropy&#34;&gt;Relation with Entropy&lt;/h3&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\JSD(p;q) = \frac{1}{2} (D_\text{KL}(p || \frac{p+q}{2}) +
D_\text{KL}(q || \frac{p+q}{2})) \\
&amp;amp;= \frac{1}{2} [H(p||\frac{p+q}{2}) - H(p) + H(q||\frac{p+q}{2}) -
H(q)] \\
&amp;amp;=
\begin{aligned}[t]
&amp;amp;-\frac{1}{2} [\E_{x \sim p} \log \frac{p+q}{2}(x) + \E_{x \sim q}
\log \frac{p+q}{2}(x)] \\
&amp;amp;- \frac{1}{2} [H(p) + H(q)] \\
\end{aligned} \\
&amp;amp;= -\E_{x \sim \frac{p+q}{2}} \frac{p+q}{2}(x) - \frac{1}{2} [H(p) +
H(q)] \\
&amp;amp;= H(\frac{p+q}{2}) - \frac{1}{2} [H(p) + H(q)]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Unconscious Statistics</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/</link>
      <pubDate>Tue, 03 May 2022 10:50:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/</guid>
      <description>

&lt;h2 id=&#34;law-of-the-unconscious-statistician&#34;&gt;Law of the Unconscious
Statistician&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, the &lt;strong&gt;law of the
unconscious statistician&lt;/strong&gt; (LOTUS), is a theorem used to
calculate the expected value of a function &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt; of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; when one knows the probability
distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; but one does not
know the distribution of &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the probability mass function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \sum_x g(x) p(x)
\]&lt;/span&gt; If the probability density function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)p(x)\ \d x
\]&lt;/span&gt; If the cumulative distribution function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)\ \d F(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;marginal-expectation&#34;&gt;Marginal Expectation&lt;/h2&gt;
&lt;p&gt;If the joint distribution of two random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is known, then the expectation of one
component can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\E[X] = \int_{-\infty}^{+\infty} x p_X(x)\; \d x =
\int_{-\infty}^{+\infty} x \int_{-\infty}^{+\infty} p(x,y)\; \d y\; \d x
= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xp(x,y)\ \d y\ \d x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\E [X] = \E_{y \sim p_Y} [\E_{x \sim p(X|Y=y)}]  =
\int_{-\infty}^{+\infty} p(y) \bigg( \int_{-\infty}^{+\infty} x p(x|y)\
\d x \bigg) \d y
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/questions/185729/expected-value-of-a-marginal-distribution-when-the-joint-distribution-is-given&#34;&gt;StackExchange
Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-non-negative-random-variables&#34;&gt;Expectation of
Non-negative Random Variables&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable whose
value is non-negative, and &lt;strong&gt;its expectation exists&lt;/strong&gt;,
and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is continuous, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[b]
\E (X) &amp;amp;= \int_{0}^{+\infty} x p(x)\ \d x = \int_{0}^{+\infty} x\ \d
\big( P(x) - 1 \big) \\
&amp;amp;= [x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty} - \int_0^{+\infty}
\big( P(x) - 1 \big)\ \d x
\end{aligned}
\]&lt;/span&gt; Because the expectation exists, the above expression and
especially the &lt;span class=&#34;math inline&#34;&gt;\([x \big( P(x) - 1
\big)]\bigg|_{x=0}^{+\infty}\)&lt;/span&gt; term must converge: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
[x \big( P(x) - 1 \big)]\bigg|_{x=0} = 0 \\
[P(x) - 1]\bigg|_{x \to +\infty} = 0 \Rightarrow [x \big( P(x) - 1
\big)]\bigg|_{x \to +\infty} = 0
\end{gather}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \int_{0}^{+\infty} \big (1 - P(x) \big)\ \d x
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is discrete and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; only takes on integer values, supposing
the max value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E(X) = \sum_{k=0}^{N} [k P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [(\sum_{j=0}^{k-1} 1) P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [\sum_{j=0}^{k-1} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} [\sum_{k=j+1}^{N} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} P(X &amp;gt; j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/305031/expectation-when-cumulative-distribution-function-is-given&#34;&gt;StackExchange
Discussion&lt;/a&gt; || &lt;a href=&#34;https://en.wikipedia.org/wiki/Summation_by_parts&#34;&gt;Summation by
Parts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-and-quantile-function&#34;&gt;Expectation and Quantile
Function&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be the PDF and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; be the CDF of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Q =
F^{-1}\)&lt;/span&gt; be the inverse of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;
is called the &lt;strong&gt;quantile function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\int_0^1 Q(p)\ \d p \stackrel{p=F(x)}{\Longrightarrow} =
\int_{-\infty}^{+\infty} x f(x)\ \d x = \E(X)
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/a/18439&#34;&gt;StackExchange
Answer&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>协方差与相关系数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Sun, 01 May 2022 10:41:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description>

&lt;p&gt;以下以二维随机变量为例，展示协方差以及相关系数的概念。虽然协方差和相关系数相对期望、方差来说显得复杂，但是他们依旧是随机变量的数字特征。&lt;/p&gt;
&lt;h2 id=&#34;协方差&#34;&gt;协方差&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\([X,
Y]\)&lt;/span&gt;为一组二维随机变量，如果&lt;span class=&#34;math inline&#34;&gt;\(\mathrm
E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}\)&lt;/span&gt;存在，则称 &lt;span class=&#34;math display&#34;&gt;\[
\notag \mathrm {Cov}(X, Y) \triangleq \mathrm E\{[X - \mathrm E(X)][Y -
\mathrm E(Y)]\}
\]&lt;/span&gt; 为随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的协方差。在实际中计算协方差时，更多的是使用以下公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\mathrm {Cov}(X, Y) = \mathrm E\{[X - \mathrm E(X)][Y - \mathrm
E(Y)]\} \\
&amp;amp;= \mathrm E[XY - X\mathrm E(Y) - \mathrm E(X)Y + \mathrm E(X)
\mathrm E(Y)] \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X)\mathrm E(Y) - \mathrm E(X)\mathrm
E(Y) + \mathrm E(X) \mathrm E(Y) \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X) \mathrm E(Y)
\end{aligned}
\]&lt;/span&gt; 而二维随机变量&lt;span class=&#34;math inline&#34;&gt;\([X,
Y]\)&lt;/span&gt;对应的协方差矩阵即为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma = \begin{bmatrix} \Cov(X,X) &amp;amp; \Cov(X,Y) \\ \Cov(Y,X) &amp;amp;
\Cov(Y,Y) \\ \end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关系数&#34;&gt;相关系数&lt;/h2&gt;
&lt;p&gt;协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们可以首先将随机变量标准化：
&lt;span class=&#34;math display&#34;&gt;\[
X^\star = \frac{X - \E(X)}{\sqrt{\Var(X)}}，Y^\star = \frac{Y -
\E(Y)}{\sqrt{\Var(Y)}}
\]&lt;/span&gt; 再求协方差&lt;span class=&#34;math inline&#34;&gt;\(\Cov(X^\star,
Y^\star)\)&lt;/span&gt;，这便是随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的相关系数： &lt;span class=&#34;math display&#34;&gt;\[
\rho(X, Y) = \mathrm{Cov}(X^\star, Y^\star) = \frac{\Cov(X,
Y)}{\sqrt{\Var(X) \Var(Y)}}
\]&lt;/span&gt; 实际上对于任意常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，都有
&lt;span class=&#34;math display&#34;&gt;\[
\rho(cX, cY) = \rho(X, Y)
\]&lt;/span&gt; 相关系数绝对值小于等于&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;。该性质可以由Cauchy-Schwartz不等式在期望中的应用直接证明，也可以&lt;a href=&#34;https://statproofbook.github.io/P/corr-range.html&#34;&gt;通过期望、方差性质证明&lt;/a&gt;。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>2-convex-set</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</guid>
      <description>

&lt;h2 id=&#34;convex-set&#34;&gt;Convex Set&lt;/h2&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k \in \R^n\)&lt;/span&gt;,
we say that &lt;span class=&#34;math inline&#34;&gt;\(y = \sum_{i=1}^k \alpha_i
x^{i}\)&lt;/span&gt; is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;linear combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots, \alpha_k \in \R\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;affine combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = 1\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;convex combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 \le \alpha_1, \dots, \alpha_k\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S \in \R^n\)&lt;/span&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;linear
subspace&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S,
\alpha,\beta \in \R, \alpha x + \beta y \in S\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is an &lt;strong&gt;affine
subspace&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S,
\alpha \in \R, \alpha x + (1-\alpha) y \in S\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;convex
set&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S, 0 \le
\alpha \le 1, \alpha x + (1 - \alpha) y \in S\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is affine.&lt;/li&gt;
&lt;li&gt;Any affine combination of a finite number of points in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; belongs to &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\(S = {x} + V \triangleq \{ x + v: v \in V
\}\)&lt;/span&gt;. Note that though &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is
unique, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is convex.&lt;/li&gt;
&lt;li&gt;Any convex combination of a finite number of points in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; belongs to &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;examples-of-convex-set&#34;&gt;Examples of Convex Set&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Nonnegative orthant (in 2-D, an orthant is called a quadrant):
&lt;span class=&#34;math inline&#34;&gt;\(\R^n_+ \triangleq \{ x \in \R^n: \forall i,
x_i \ge 0 \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hyperplane&lt;/p&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^n, b \in R\)&lt;/span&gt;, the
hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s, c)\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: s^T x = c \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Half-space&lt;/p&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^n, b \in R\)&lt;/span&gt;, the
upper half-space &lt;span class=&#34;math inline&#34;&gt;\(H^+(s, c)\)&lt;/span&gt; is the
set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: s^T x \ge c \}\)&lt;/span&gt;;
the lower half-space &lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)\)&lt;/span&gt; is
the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n | s^T x \le c
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s, c)\)&lt;/span&gt; is
the intersection of &lt;span class=&#34;math inline&#34;&gt;\(H^+(s, c)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Euclidean ball&lt;/p&gt;
&lt;p&gt;Given the center &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \R^n\)&lt;/span&gt;
and the radius &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt; 0\)&lt;/span&gt;, the
Euclidean ball &lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r)\)&lt;/span&gt; is the
set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: ||x - \bar x||_2 \le r
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A generalization of Euclidean ball would be to extend the norm to
other numbers that are larger than 1. For &lt;span class=&#34;math inline&#34;&gt;\(q
\ge 1\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
B_q(\bar x, r) = \{ x \in \R^n: ||x - \bar x||_q \le r \}
\]&lt;/span&gt; is also a convex set. Note that &lt;span class=&#34;math inline&#34;&gt;\(||z||_\infty = \lim_{q \to \infty} (\sum_i
z_i^q)^{1/q} = \max_i z_i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convex cone&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A set &lt;span class=&#34;math inline&#34;&gt;\(K \in \R^n\)&lt;/span&gt; is
called a &lt;strong&gt;cone&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x
\in K, \alpha &amp;gt; 0, \alpha x \in K\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Linear subspace is a cone. Affine subspace is not necessarily so.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;convex cone&lt;/strong&gt; is a cone that is
convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some examples of convex cone include &lt;span class=&#34;math inline&#34;&gt;\(\R^n_+\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^n\)&lt;/span&gt;, which is the set of
&lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; PSD matrices.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving
Operations&lt;/h3&gt;
&lt;p&gt;For any two convex sets &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;, there are some binary
operators that will preserve the convexity after being applied.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Set operations&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_1 \cup S_2\)&lt;/span&gt; is not necessarily
convex. &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2\)&lt;/span&gt; is always
convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Affine transformations&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto
\R^m\)&lt;/span&gt; is &lt;strong&gt;affine&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in \R^n, \alpha \in \R\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
A(\alpha x + (1 - \alpha) y) = \alpha A(x) + (1 - \alpha) A(y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto \R^m\)&lt;/span&gt; be
affine, &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be convex.
Then &lt;span class=&#34;math inline&#34;&gt;\(A(S) \triangleq \{ A(x): x \in S
\}\)&lt;/span&gt; is convex.&lt;/p&gt;
&lt;p&gt;There are two types of affine transformation worth noting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(A(x) = U
x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(U \in \R^{n \times
n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U U^T = U^T U=
I\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Projection&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(A(x) = P
x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(P \in \R^{n \times
n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^2 = P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;orthogonal projection&lt;/strong&gt;, its projection matrix
further satisfies &lt;span class=&#34;math inline&#34;&gt;\(P^T = P\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an example of affine transformation, given center &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; and axes &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; which is positive definite, the
&lt;strong&gt;ellipsoid&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(E(\bar x,
Q)\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n | (x -
\bar x)^T Q (x - \bar x) \le 1 \}\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r) = E(\bar x, I/r^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: When &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is positive
semi-definite, it might occur that &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in
\R^n | (x - \bar x)^T Q (x - \bar x) \le 1 \}\)&lt;/span&gt; will degenerate
into two parallel lines. Just consider the case when &lt;span class=&#34;math inline&#34;&gt;\(Q = [1,2] [1,2]^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: There always exists an affine transformation &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A(B(0, 1)) = E(\bar x, Q)\)&lt;/span&gt;. Note that
&lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r) = E(\bar x, I/r^2)\)&lt;/span&gt;.
See &lt;a href=&#34;../2-cvxanal.pdf&#34;&gt;this handout&lt;/a&gt; for the construction of
such transformation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, ellipsoid is also a convex set (Problem 2 of Homework 1
proves this from the first principle).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;topological-preparation&#34;&gt;Topological Preparation&lt;/h3&gt;
&lt;h4 id=&#34;basic-topology&#34;&gt;Basic Topology&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Given a set &lt;span class=&#34;math inline&#34;&gt;\(S \in \R^n, S \ne
\emptyset\)&lt;/span&gt; and a point &lt;span class=&#34;math inline&#34;&gt;\(x \notin
S\)&lt;/span&gt;, we want to find a point in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that is closest (in terms of Euclidean
distance) to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Formally, &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \arg\min_{z \in S} ||z - x||_2\)&lt;/span&gt;
is called the &lt;strong&gt;projection&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that this projection does not necessarily exist. Neither the
projection is unique. Under what conditions can we guarantee the
existence and uniqueness of projection? Before that, some concepts are
needed. Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be a
set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is an
&lt;strong&gt;interior point&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\exists
\epsilon &amp;gt; 0, B(x, \epsilon) \subseteq S\)&lt;/span&gt;. The collection of
all interior points of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is called
the &lt;strong&gt;interior&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;,
denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathrm{int}}
S\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We say that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is
&lt;strong&gt;open&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(S =
\mathop{\mathrm{int}} S\)&lt;/span&gt;. We say that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is &lt;strong&gt;closed&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\R^n \setminus S\)&lt;/span&gt; is open. Note that it
can be the case that a set is neither open nor closed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The intersection of any family of closed sets is
closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be continuous and &lt;span class=&#34;math inline&#34;&gt;\(c \in
\R\)&lt;/span&gt; be a constant number. Then &lt;span class=&#34;math inline&#34;&gt;\(S =
\{ x \in \R^n: f(x) \le c \}\)&lt;/span&gt; is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is closed if and
only if for every convergent sequence &lt;span class=&#34;math inline&#34;&gt;\(\{ x_n
\}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, its limit is in
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
\R^n\)&lt;/span&gt; is &lt;strong&gt;compact&lt;/strong&gt; if it is closed and bounded
(&lt;span class=&#34;math inline&#34;&gt;\(\exists M &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq B(0, M)\)&lt;/span&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;projection&#34;&gt;Projection&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Weierstrass theorem&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be continuous and
&lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be compact. Then
&lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \in S} f(x)\)&lt;/span&gt; always has a
solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be
non-empty, closed, and convex. Then for every &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;, there exists a unique &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Existence&lt;/p&gt;
&lt;p&gt;We may assume that &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;.
Consider any &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \in S\)&lt;/span&gt; and
define &lt;span class=&#34;math inline&#34;&gt;\(T \triangleq S \cap B(x, ||x -
x&amp;#39;||_2)\)&lt;/span&gt;. Observe that&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\arg \min_{z \in S} ||x - z||_2 = \arg
\min_{z \in T} ||x - z||_2\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is closed and bounded.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then by &lt;em&gt;Weierstrass’ theorem&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; that solves &lt;span class=&#34;math inline&#34;&gt;\(\min_{z \in T} ||x - z||_2\)&lt;/span&gt; exists. And by
Point 1 above, &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.
Note that above argument does not need convexity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uniqueness&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 =
\Pi_S(x), \hat x_2 = \Pi_S(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 \ne \hat x_2\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(z = (\hat x_1 + \hat x_2) / 2\)&lt;/span&gt;. By
convexity, &lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s distance to &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|| x - (\hat x_1 + \hat x_2)/2 ||_2 &amp;amp;= || (x - \hat x_1)/2 + (x -
\hat x_2)/2 ||_2 \\
&amp;amp;\le || (x - \hat x_1)/2 ||_2 + || (x - \hat x_2)/2 ||_2 \\
&amp;amp;= \min_{z \in S} ||x - z||_2
\end{aligned}
\]&lt;/span&gt; If the &lt;span class=&#34;math inline&#34;&gt;\(\le\)&lt;/span&gt; is strict, the
fact that &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat x_2\)&lt;/span&gt; are the projection will be
contradicted; else &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_1)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_2)\)&lt;/span&gt; are collinear,
which means &lt;span class=&#34;math inline&#34;&gt;\(x, \hat x_1, \hat x_2\)&lt;/span&gt;
are collinear, in which case the only possible way for &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_2)\)&lt;/span&gt; to be equal is &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 = \hat x_2\)&lt;/span&gt;, contradicting the
assumption &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 \ne \hat
x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As a result, &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 = \hat
x_2\)&lt;/span&gt;, which means the projection of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is unique.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be
non-empty, closed, and convex. Then for any &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\hat x = \Pi_{S}(x) \iff \forall z \in S, (z - \hat x)^T(x - \hat x) \le
0
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 \le \alpha \le 1\)&lt;/span&gt;, define &lt;span class=&#34;math inline&#34;&gt;\(z(\alpha) = \alpha z + (1 - \alpha) \hat
x\)&lt;/span&gt;. By convexity, &lt;span class=&#34;math inline&#34;&gt;\(z(\alpha) \in
S\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||\hat x - x||_2^2 \le ||z(\alpha) - x||_2^2
\]&lt;/span&gt; Note on the other hand that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\quad ||z(\alpha) - x||_2^2 = [z(\alpha) - x]^T [z(\alpha) - x] \\
&amp;amp;= [\hat x + \alpha (z - \hat x) - x]^T [\hat x + \alpha (z - \hat
x) - x] \\
&amp;amp;= [(\hat x - x) + \alpha (z - \hat x)]^T [(\hat x - x) + \alpha (z
- \hat x)] \\
&amp;amp;= ||\hat x - x||_2^2 + 2\alpha (z - \hat x)^T (\hat x - x) +
\alpha^2 ||z - \hat x||_2^2
\end{aligned}
\]&lt;/span&gt; To make the above hold for any &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in [0,1]\)&lt;/span&gt;, it must be the case
that &lt;span class=&#34;math inline&#34;&gt;\((z - \hat x)^T (\hat x - x) \ge
0\)&lt;/span&gt; or equivalently &lt;span class=&#34;math display&#34;&gt;\[
(z - \hat x)^T (x - \hat x) \le 0
\]&lt;/span&gt; Convexity is crucial in this property. Just consider the
following case:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./convexity-is-important.png&#34; style=&#34;zoom: 50%;&#34;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that there exists some &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \ne \Pi_S(x)\)&lt;/span&gt; such that for every
&lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
(z - x&amp;#39;)^T (x - x&amp;#39;) \le 0
\]&lt;/span&gt; Then for &lt;span class=&#34;math inline&#34;&gt;\(\Pi_S(x) \in S\)&lt;/span&gt;,
we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
(\Pi_S(x) - x&amp;#39;)^T (x - x&amp;#39;) \le 0 \label{eq1}
\end{equation}
\]&lt;/span&gt; From the proof of sufficiency we know that for this specific
&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
(x&amp;#39; - \Pi_S(x))^T (x - \Pi_S(x)) \le 0 \label{eq2}
\end{equation}
\]&lt;/span&gt; Add up together &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
(\Pi_S(x) - x&amp;#39;)^T (\Pi_S(x) - x&amp;#39;) \le 0
\]&lt;/span&gt; This indicates that &lt;span class=&#34;math inline&#34;&gt;\(\Pi_S(x) =
x&amp;#39;\)&lt;/span&gt;, which concludes the proof.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;separation&#34;&gt;Separation&lt;/h4&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt;,
it is easy to certify that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 \ne
\emptyset\)&lt;/span&gt; so long as there is a &lt;span class=&#34;math inline&#34;&gt;\(x
\in S_1\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(x \in
S_2\)&lt;/span&gt;. But how can one certify that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 = \emptyset\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;One geometric intuition is to find a hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s,c)\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \subseteq H^+(s,c) \setminus H(s,c)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(S_2 \subseteq H^-(s,c) \setminus
H(s,c)\)&lt;/span&gt;. Obviously a hyperplane separation won’t always work.
But it helps in the discussion of convex set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;point-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be non-empty, closed and
convex. Let &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Then there
exists &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\left( \max_{z \in S} y^T z \right) &amp;lt; y^T x
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;p&gt;By the projection theorem, &lt;span class=&#34;math inline&#34;&gt;\(\hat x
\triangleq \Pi_S(x)\)&lt;/span&gt; exists and is unique. Take &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\((x -
\hat x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y \ne \vec 0\)&lt;/span&gt;
because &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Then according
the projection’s property, for every &lt;span class=&#34;math inline&#34;&gt;\(z \in
S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
(z - \hat x)^T \underbrace{(x - \hat x)}_{y} &amp;amp;\le 0 \\
z^T y - \hat x^T y &amp;amp;\le 0 \\
\end{aligned} \quad \Rightarrow \quad
\begin{aligned}
y^T z &amp;amp;\le y^T (x  - y) \\
&amp;amp;\le y^T x - ||y||_2^2 \\
&amp;amp;&amp;lt; y^T x
\end{aligned}
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in S} y^T z\)&lt;/span&gt; is
obtained at &lt;span class=&#34;math inline&#34;&gt;\(z = \hat x\)&lt;/span&gt;, which
concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The theorem above is an algebraic description of point-set separation
and in essence reveals a hyperplane separation. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is exactly the normal vector of this
hyperplane. &lt;span class=&#34;math inline&#34;&gt;\((\max_{z \in S} y^T z) &amp;lt; y^T
x\)&lt;/span&gt; actually says that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is
more distant in the direction of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;
than every point &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A direct result of the theorem above is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A closed convex set &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
\R^n\)&lt;/span&gt; is the intersection of all half-spaces containing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{\substack{\text{$S \subseteq H$ and} \\ \text{$H$ is a
halfspace}}} H
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;p&gt;Without loss of generality, due to that &lt;span class=&#34;math inline&#34;&gt;\(H^+(s,c) = H^-(-s,-c)\)&lt;/span&gt;, we claim that
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the intersection of all the
lower half-spaces containing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{H^-(s, c) \supseteq S} H^-(s,c)
\]&lt;/span&gt; We begin with two special cases: &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. We show that &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; satisfies the above because &lt;span class=&#34;math display&#34;&gt;\[
\emptyset = H^-(0, -1) \cap \text{any lower halfspace} \\
\R^n = \bigcap_{c \ge 0} H^-(0, c)
\]&lt;/span&gt; Now consider any set &lt;span class=&#34;math inline&#34;&gt;\(\emptyset
\subsetneq S \subsetneq \R^n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x
\notin S\)&lt;/span&gt;. Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, there
exists &lt;span class=&#34;math inline&#34;&gt;\(y_x \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
c_x \triangleq \max_{z \in S} y_x^T z &amp;lt; y_x^T x
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq H^-_x
\triangleq H^-(y_x, c_x)\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(x \notin H^-_x\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq H^-_x\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S \subseteq \bigcap_{x \notin S} H^-_x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\bigcap_{x
\notin S} H^-_x \subseteq S\)&lt;/span&gt;. Suppose on the contrary there
exists &lt;span class=&#34;math inline&#34;&gt;\(z \in \bigcap_{x \notin S}
H^-_x\)&lt;/span&gt; but &lt;span class=&#34;math inline&#34;&gt;\(z \notin S\)&lt;/span&gt;.
However, &lt;span class=&#34;math display&#34;&gt;\[
z \in \bigcap_{x \notin S} H^-_x = \bigcap_{\{ x \notin S: x \ne z \}
\cup \{ z \} } H^-_x \subseteq H^-_z
\]&lt;/span&gt; which contradicts the fact that &lt;span class=&#34;math inline&#34;&gt;\(z
\notin H^-_z\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\bigcap_{x
\notin S} H^-_x \subseteq S\)&lt;/span&gt; and consequently &lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{x \notin S} H^-_x
\]&lt;/span&gt; Notice that &lt;span class=&#34;math inline&#34;&gt;\(\{ H^-_x: x \notin S
\}\)&lt;/span&gt; contains all the lower half-spaces that superset &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Because for every lower half-space
&lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c) \supseteq S\)&lt;/span&gt;, there exists
&lt;span class=&#34;math inline&#34;&gt;\(x \in H^+(s, c) \setminus H(s, c)\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. For this
specific &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, there exists &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S} s^T z \le c &amp;lt; s^T x
\]&lt;/span&gt; Therefore, every &lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)
\supseteq S\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\(H^-_x\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;, which concludes the
proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By far, the separation between a point (or a set of a single point)
and a set is settled. Now we consider the set-set separation. It is easy
to conjecture the geometric intuition derived above as the
following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conjecture: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt; be two non-empty,
closed and convex sets with &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt;. Then there exists &lt;span class=&#34;math inline&#34;&gt;\(y \in
\R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
\]&lt;/span&gt; Disproof:&lt;/p&gt;
&lt;p&gt;Just consider &lt;span class=&#34;math inline&#34;&gt;\(S_1 = \{ (u, v): u \ge 1/v,
v \ge 1 \}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_2 = \{ (u, 0): u
\ge 1 \}\)&lt;/span&gt;. The only possible separation hyperplane is the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis, with the corresponding normal
vector &lt;span class=&#34;math inline&#34;&gt;\(y = [0, -1]^T\)&lt;/span&gt;. However in
this case, &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in S_1} y^T z\)&lt;/span&gt;
does not exist at all and thus &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in
S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z\)&lt;/span&gt; does not hold.&lt;/p&gt;
&lt;p&gt;Trivially changing &lt;span class=&#34;math inline&#34;&gt;\(\max\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sup\)&lt;/span&gt; won’t help, since &lt;span class=&#34;math inline&#34;&gt;\(\sup_{z \in S_1} [0, -1] z = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lack of boundedness forbids us to take the advantage of
&lt;em&gt;Weierstrass’ theorem&lt;/em&gt; to show the existence of maxima. It would
be cheering if the predicate of the conjecture can be relaxed so that
boundedness and thus compactness applies, yielding the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt; be two non-empty,
closed and convex sets, with &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt; and either &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt;
or &lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt; being bounded. Then there
exists &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
\]&lt;/span&gt; Hint of proof:&lt;/p&gt;
&lt;p&gt;We only show the proof when &lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;
is bounded. The case when &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt; is
bounded can be handled similarly.&lt;/p&gt;
&lt;p&gt;Consider the set &lt;span class=&#34;math inline&#34;&gt;\(S \triangleq \{ x - y: x
\in S_1, y \in S_2 \}\)&lt;/span&gt; (which will be &lt;span class=&#34;math inline&#34;&gt;\(\{ x - y: x \in S_2, y \in S_1 \}\)&lt;/span&gt; in the
other case). Since &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \notin S\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is non-empty obviously. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; can be further verified to be closed
and convex (??). Then the property of point-set separation can be
applied to proceed with the proof.&lt;/p&gt;
&lt;p&gt;That is, there exists &lt;span class=&#34;math inline&#34;&gt;\(u \in \R^n\)&lt;/span&gt;
such that &lt;span class=&#34;math display&#34;&gt;\[
\left( \max_{z \in S} u^T z \right) &amp;lt; u^T \cdot 0 = 0 \\
\Downarrow \\
\max_{x \in S_1, y \in S_2} u^T (x - y) &amp;lt; 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(y^* = \min_{y \in S_2} u^T y\)&lt;/span&gt; (such &lt;span class=&#34;math inline&#34;&gt;\(y^*\)&lt;/span&gt; exists because the compactness of
&lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;) to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{x \in S_1} u^T (x - y^*) &amp;amp;&amp;lt; 0 \\
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; u^T y^* \\
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; \min_{y \in S_2} u^T y \\
\end{aligned}
\]&lt;/span&gt; which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a richer discussion of convex set separation, please refer &lt;a href=&#34;https://www.wikiwand.com/en/Hyperplane_separation_theorem&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 12:55:32 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</guid>
      <description>

&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be decomposed into &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\text{$U$ is $m \times m$, $V$ is $n \times n$} \\
U U^T = I \\
V V^T = I \\
\Sigma = \diag_{m \times n} (\sigma_1, \sigma_2, ..., \sigma_r) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_r \ge 0 \\
r = \rank A
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;This is shown by construction.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Construction of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real symmetric matrix, so it
can be diagonalized by an orthogonal matrix. Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be this matrix and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; be the corresponding diagonal
matrix consisting of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s
eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
V^{-1} (A^T A) V = \Lambda
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be arranged such that corresponding
eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are decreasing.
Because &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; consists of orthonormal
basis, we have &lt;span class=&#34;math inline&#34;&gt;\(V^T V = I\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
V^T (A^T A) V = \Lambda
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is positive semi-definite, it has
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; non-negative real eigenvalues,
&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; of which are positive. Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = \sqrt{\lambda_i}, i = 1, 2, ...,
n\)&lt;/span&gt; (which are defined as &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s the &lt;strong&gt;singular
values&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\lambda_1, \lambda_2, ..., \lambda_r &amp;gt; 0, \lambda_r, \lambda_{r+1},
..., \lambda_{n} = 0 \\
\sigma_1, \sigma_2, ..., \sigma_r &amp;gt; 0, \sigma_r, \sigma_{r+1}, ...,
\sigma_{n} = 0
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V_1 = [v_1, v_2, ..., v_r],
V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\)&lt;/span&gt;. Also
let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_1 =
\begin{bmatrix}
\sigma_1 \\
&amp;amp; \sigma_2 \\
&amp;amp; &amp;amp; \ddots \\
&amp;amp; &amp;amp; &amp;amp; \sigma_r
\end{bmatrix}
\]&lt;/span&gt; Complement &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_1\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to get the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma =
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T A)\)&lt;/span&gt; is of dimension &lt;span class=&#34;math inline&#34;&gt;\(n-r\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; comprises of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvectors whose eigenvalues are
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n -
r\)&lt;/span&gt; independent columns and &lt;span class=&#34;math inline&#34;&gt;\(A^T A V_2
= 0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; spans
&lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^TA)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1 V_1^T + V_2 V_2^T \\
A = A I = A V_1 V_1^T + A V_2 V_2^T = A V_1 V_1^T
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Construction of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
u_i \coloneq \frac{1}{\sigma_i} A v_i, i = 1, 2, ..., r \\
U_1 \coloneq [u_1, u_2, ..., u_r]
\end{gather}
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(AV_1 = U_1\Sigma_1\)&lt;/span&gt;.
Also, &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s columns are
orthonormal: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
u_i^T u_j &amp;amp;= (\frac{1}{\sigma_i} A v_i)^T (\frac{1}{\sigma_j} A v_j)
\\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} v_i^T A^T A v_j \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} v_i^T \lambda_j v_j \\
&amp;amp;= \frac{\sigma_j}{\sigma_i} v_i^T v_j \\
&amp;amp;=
\begin{cases}
0 &amp;amp; i \ne j \\
1 &amp;amp; i = j
\end{cases}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s are within &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt; and are orthonormal as shown,
plus that &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s span the &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;’s complement, we have &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp = \Nul(A^T)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\{ u_{r+1}, u_{r+2}, \dots, u_{m} \}\)&lt;/span&gt; be
an orthonormal basis of &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T)\)&lt;/span&gt;
such that they are orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s column vectors. We construct &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
U_2 \coloneq [u_{r+1}, u_{r+2}, ..., u_{m}] \\
U \coloneq [U_1, U_2]
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof of &lt;span class=&#34;math inline&#34;&gt;\(U \Sigma V^T = A\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
U \Sigma V^T &amp;amp;= [U_1, U_2]
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\
\end{bmatrix}
\\
&amp;amp;= [U_1 \Sigma_1, 0]
\begin{bmatrix}
V_1^T \\
V_2^T \\
\end{bmatrix}
\\
&amp;amp;= U_1 \Sigma_1 V_1^T \\
&amp;amp;= A V_1 V_1^T \\
&amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;p&gt;Note that SVD reveals that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
essentially the addition of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;
rank-&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
U \Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T
\]&lt;/span&gt; We have shown the construction process of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(U \Sigma
V^T\)&lt;/span&gt; in the above proof, i.e. the existence of such
decomposition, with which we can investigate into &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Notice that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A A^T &amp;amp;= U \Sigma V^T V \Sigma U^T \\
A A^T &amp;amp;= U \Sigma^2 U^T \\
A A^T U &amp;amp;= U \Sigma^2 U^T U \\
A A^T U &amp;amp;= U \Sigma^2
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is a
diagonal matrix, we can conclude that &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; contains the eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; contains the eigenvalues of
&lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; as its diagonal entries.
Similarly &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^T A &amp;amp;= V \Sigma U^T U \Sigma V^T \\
A^T A &amp;amp;= V \Sigma^2 V^T \\
A^T A V &amp;amp;= V \Sigma^2 V^T V \\
A^T A V &amp;amp;= V \Sigma^2
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; contains the
eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; contains the eigenvalues of
&lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; as its diagonal entries.&lt;/p&gt;
&lt;p&gt;It is easy to verify that &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; actually have the same
eigenvalues. And if &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is an
eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A v\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; with the same eigenvalue. The
columns of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are called the
&lt;strong&gt;left-singular vectors&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and the columns of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are called the &lt;strong&gt;right-singular
vectors&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The time complexity of SVD is &lt;span class=&#34;math inline&#34;&gt;\(O(\min\{
m^2 n, n^2 m \})\)&lt;/span&gt;, depending on whether &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(A^T
A\)&lt;/span&gt; is used to solve &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;eckart-young-mirsky-theorem&#34;&gt;Eckart-Young-Mirsky Theorem&lt;/h2&gt;
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(r
\le \min(m,n)\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X = U
\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\diag(\sigma_1, \sigma_2, ..., \sigma_r)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star
= U \Sigma_k V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k =
\diag(\sigma_1, \sigma_2, ..., \sigma_k)\)&lt;/span&gt;, when distance between
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Notice that in this case, &lt;span class=&#34;math display&#34;&gt;\[
Y^\star = \sum_{i=1}^k \sigma_k u_k v_k^T
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||U A||_F = ||A U||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= \tr((U A)^T U A) \\
&amp;amp;= \tr(A^T U U A) \\
&amp;amp;= \tr(A^T A) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||A
U||_F\)&lt;/span&gt;. Then for any &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||U \Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^T U \Sigma V^T V - U^T Y V||_F^2 \\
&amp;amp;= ||\Sigma - U^T Y V||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^T Y V\)&lt;/span&gt;, which is
also of rank &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||\Sigma - Z||_F^2 = \sum_{ij}(\Sigma_{ij} -
Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^r (\sigma_{i} - Z_{ii})^2 + \sum_{i&amp;gt;r}^{\min(m,n)}
Z_{ii}^2 + \sum_{i \ne j} Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \begin{cases}
\sigma_i, &amp;amp; 1 \le i \le r \\
0, &amp;amp; r \le i \le \min(m,n)
\end{cases} \\
Z_{ij} = 0, 1 \le i \le M, 1 \le j \le N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U \Sigma_k V^T\)&lt;/span&gt;. Q.E.D.&lt;/p&gt;
&lt;p&gt;Note that Eckart-Young-Mirsky theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;svd-and-diagonalization&#34;&gt;SVD and Diagonalization&lt;/h2&gt;
&lt;p&gt;It is easy to mix up SVD with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalization&lt;/a&gt;. Notably,
diagonalization factorizes square matrix while SVD factorizes any
matrix. Also, not all square matrices can be diagonalized but all
matrices can be applied with SVD. Finally, SVD can be interpreted as
rotation-scaling-rotation because &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(V^T\)&lt;/span&gt; are orthogonal. But in
diagonalization, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}\)&lt;/span&gt; are not necessarily orthogonal,
unless in the case of real symmetric matrix.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6251584.html&#34;&gt;奇异值分解(SVD)原理与在降维中的应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/22237507&#34;&gt;奇异值的物理意义是什么？
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Singular%20Value%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Singular
Value Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gram-Schmidt Orthogonalization</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/</guid>
      <description>

&lt;h2 id=&#34;gram-schmidt-orthogonalization&#34;&gt;Gram-Schmidt
Orthogonalization&lt;/h2&gt;
&lt;p&gt;The Gram-Schmidt process is a simple algorithm for producing
orthogonal basis for any nonzero subspace of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given a basis &lt;span class=&#34;math inline&#34;&gt;\(\{ \x_1, \dots, \x_p
\}\)&lt;/span&gt; for a nonzero subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, define&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \newcommand{\v}{\mathrm{v}}
\v_1 &amp;amp;= \x_1 \\
\v_2 &amp;amp;= \x_2 - \frac{\x_2 \cdot \v_1}{\v_1 \cdot \v_1} \v_1 \\
\v_3 &amp;amp;= \x_3 - \frac{\x_3 \cdot \v_1}{\v_1 \cdot \v_1} \v_1 -
\frac{\x_3 \cdot \v_2}{\v_2 \cdot \v_2} \v_2 \\
&amp;amp;\vdots \\
\v_p &amp;amp;= \x_p - \frac{\x_p \cdot \v_1}{\v_1 \cdot \v_1} \v_1 -
\frac{\x_p \cdot \v_{p-1}}{\v_{p-1} \cdot \v_{p-1}} \v_{p-1}
\end{aligned}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(\{ \v_1, \dots, \v_p
\}\)&lt;/span&gt; is an orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\span}[1]{\mathrm{Span}\{#1\}}
\span{\v_1, \dots,\v_k} = \span{\x_1, \dots, \x_k} \text{\quad for $1
\le k \le p$}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;qr-factorization&#34;&gt;QR Factorization&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix with linearly
independent columns, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be
factored as &lt;span class=&#34;math inline&#34;&gt;\(A = QR\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m
\times n\)&lt;/span&gt; matrix whose columns form an orthonormal basis for
&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathrm{Col}}A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; upper triangular invertible matrix with positive
entries on its diagonal.&lt;/p&gt;
&lt;p&gt;Such factorization can be realized by Gram-Schmidt orthogonalization.
The columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, say denoted as
&lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots, \x_n\)&lt;/span&gt;, form the basis
of &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt;. Apply the Gram-Schmidt
process to construct an orthogonal basis &lt;span class=&#34;math inline&#34;&gt;\(\{
\v_1, \dots, \v_n \}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\Col
A\)&lt;/span&gt; and let &lt;span class=&#34;math display&#34;&gt;\[
Q = [\v_1, \dots, \v_n]
\]&lt;/span&gt; For every &lt;span class=&#34;math inline&#34;&gt;\(k = 1, \dots,
n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\x_k\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(\span{\x_1, \dots, \x_k} = \span{\v_1, \dots,
\v_k}\)&lt;/span&gt;. So there are constants &lt;span class=&#34;math inline&#34;&gt;\(r_{1k}, \dots, r_{kk}\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\x_k = [\v_1, \dots, \v_k]
\begin{bmatrix}
r_{1k} \\
\vdots \\
r_{kk}
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt; is nonzero or else
&lt;span class=&#34;math inline&#34;&gt;\(\x_k\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(\span{\x_1, \dots, \x_{k-1}}\)&lt;/span&gt;, which
violates the linear independence condition of columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. We can assume &lt;span class=&#34;math inline&#34;&gt;\(r_{kk} &amp;gt; 0\)&lt;/span&gt;; otherwise we can multiply
both &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\v_k\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; without compromising previous
conditions. Let &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\r}{\mathrm{r}} \r_k =
\begin{bmatrix}
r_{1k} \\
\vdots \\
r_{kk} \\
0 \\
\vdots \\
0
\end{bmatrix}
\]&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(\x_k = Q \r_k\)&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(k = 1, \dots, n\)&lt;/span&gt;. Then &lt;span class=&#34;math display&#34;&gt;\[
A = [\v_1, \dots, \v_n] [\r_1, \dots, \r_n] = Q R
\]&lt;/span&gt; The fact that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is
invertible follows easily the fact that &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;’s columns are linearly independent.
Because &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th element of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column, i.e. &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt;, is positive, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;’s diagonal entries are positive.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eckart-Young-Mirsky Theorem</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</link>
      <pubDate>Fri, 07 Jan 2022 12:40:50 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</guid>
      <description>
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(R
\le \min\{M,N\}\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X =
U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
diag(\sigma_1, \sigma_2, ..., \sigma_R)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star
= U\Sigma_K V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_K =
diag(\sigma_1, \sigma_2, ..., \sigma_K)\)&lt;/span&gt;, when distance between
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||UA||_F = ||AU||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= tr((UA)^TUA) \\
&amp;amp;= tr(A^TUUA) \\
&amp;amp;= tr(A^TA) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||AU||_F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K
\le R\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||U\Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^TU\Sigma V^TV - U^TYV||_F^2 \\
&amp;amp;= ||\Sigma - U^TYV||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^TYV\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is also of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||\Sigma - Z||_F^2 \\
&amp;amp;= \sum_{ij}(\Sigma_{ij} - Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^R(\sigma_{i} - Z_{ii})^2 +
\sum_{i&amp;gt;R}^{\min\{M,N\}}Z_{ii}^2 + \sum_{i \ne j}Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \sigma_{i}, i = 1, 2, \dots, K \\
Z_{ii} = \sigma_{i}, i = K, K + 1, \dots, R - 1 \\
Z_{ii} = 0, i = R, R + 1, \dots, \min\{M,N\} \\
Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \dots, N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U\Sigma_KV^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Eckart-Young-Mirsky Theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt; .&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Independent Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</link>
      <pubDate>Sun, 19 Dec 2021 10:16:43 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</guid>
      <description>

&lt;h2 id=&#34;assumption-and-derivation&#34;&gt;Assumption and Derivation&lt;/h2&gt;
&lt;p&gt;Suppose observations are the linear combination of signals. Also
suppose that the number of signal sources is equal to the number of
linearly mixed channel. Given observations (along the time axis &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{x^{(i)}\}_{i=1}^M, x^{(i)} \in
\R^{N \times 1}\)&lt;/span&gt;, independent component analysis finds the &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; mixing matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X
= AS\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are the
propagation weights from a signal source to observers. Therefore, they
are considered independent (and are thus called independent
components).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is called mixing matrix and
&lt;span class=&#34;math inline&#34;&gt;\(W = A^{-1}\)&lt;/span&gt; is called unmixing
matrix because &lt;span class=&#34;math inline&#34;&gt;\(S = WX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = U\Sigma V^T\)&lt;/span&gt; by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. Assume observations are
pre-centered:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^Mx^{(i)} = 0
\]&lt;/span&gt; Assume that signals are independent and the variance of each
signal is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (taking the advantage
of scale/sign ambiguity): &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I
\]&lt;/span&gt; Then the covariance of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;
can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
C &amp;amp;= \frac{1}{M}x^{(i)}(x^{(i)})^T \\
&amp;amp;= \frac{1}{M}As^{(i)}(As^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(U\Sigma V^Ts^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(s^{(i)})^TV\Sigma^T U^T \\
&amp;amp;= U\Sigma V^TIV\Sigma^T U^T \\
&amp;amp;= U\Sigma^2U^T \text{ (by property of SVD, note that $\Sigma$ is $n
\times n$ diagonal)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If our assumptions are correct, then the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the concatenated eigenvectors of the
matrix &lt;span class=&#34;math inline&#34;&gt;\(CC^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; is the diagonal matrix
consisting of corresponding eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(C^TC\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(CC^T = C^TC = C^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In other words, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be solved by eigen
decomposition. Define &lt;span class=&#34;math inline&#34;&gt;\(\hat x^{(i)} =
\Sigma^{-1}U^Tx^{(i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat C &amp;amp;= \frac{1}{M}\hat x^{(i)}(\hat x^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(\Sigma^{-1}U^Tx^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^T(\frac{1}{M}x^{(i)}(x^{(i)})^T)U\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^TU\Sigma^2U^TU\Sigma^{-1} \\
&amp;amp;= I \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = WX = A^{-1}X = V\Sigma^{-1}U^TX = V\hat X
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The remaining job is to find the &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. We resort to multi-information, a
generalization of mutual-information from Information Theory to judge
how close a distribution is to statistical independence for multiple
variables. &lt;span class=&#34;math display&#34;&gt;\[
I(s) = \sum_{s \in \mathcal{S}}p(s) \log \frac{p(s)}{\prod_jp(s_j)}
\]&lt;/span&gt; It is a non-negative quantity that reaches the minimum if and
only if all variables are statistically independent.&lt;/p&gt;
&lt;p&gt;The multi-information can be written as a function of entropy, which
is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(s) = -\sum_{s \in \mathcal{S}}p(s) \log p(s)
\]&lt;/span&gt; The multi-information can be written as the difference between
the sum of entropies of marginal distribution and the entropy of joint
distribution &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(s) &amp;amp;= \sum_jH(s_j) - H(s) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(V\hat x) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log|V|) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log1) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(\hat x) \\
\end{aligned}
\]&lt;/span&gt; Because we are assuming signal are independent from each
other, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^\star &amp;amp;= \arg \min_V\sum_jH((V\hat x)_j) - H(\hat x) \\
&amp;amp;= \arg \min_V\sum_jH((V\hat x)_j)
\end{aligned}
\]&lt;/span&gt; Calculating entropy and then taking derivative is no easy task
and therefore ICA algorithms focus on approximations or equivalences to
the above equation. For example, it can be approximated by finding the
rotation that maximizes the expected log-likelihood of the observed data
by assuming that the source distributions are known.&lt;/p&gt;
&lt;h3 id=&#34;non-gaussian-and-pca&#34;&gt;Non-Gaussian and &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The key assumption of ICA is that signals are non-Gaussian. We are
trying to recover the &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (by finding
&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;) to be as non-Gaussian as
possible. &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; consists of
independent components because its covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\hat C\)&lt;/span&gt; is shown to be an identity matrix
(and thus diagonal). &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is
reconstructed by the linear combination of independent components. Thus
it will be the most non-Gaussian when each variable is formed by exactly
one component of &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; by Central
Limit Theorem, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;’s components
are independent.&lt;/p&gt;
&lt;p&gt;Consider the case when &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;
consists of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent Gaussian,
which invalidate the above analysis. If we forcibly calculate &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;, due to the property of
multi-variate Gaussian that its iso-density maps are spherical, then any
&lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix will be
a solution to Equation (9), including the left singular matrix of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. the concatenated eigenvectors of
&lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, which is exactly the
projection basis obtained in PCA. If any &lt;span class=&#34;math inline&#34;&gt;\(N
\times N\)&lt;/span&gt; rotation matrix can be a solution, there are infinite
many solutions to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and thus ICA
will just fail.&lt;/p&gt;
&lt;p&gt;The conclusion drawn above is based on the ideal case, i.e. many
enough observations. Even in the real case where signals are Gaussian,
we may get a unique solution to &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html&#34;&gt;Independent
Component Analysis: A Tutorial (tkk.fi)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>RANSAC</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/ransac/</link>
      <pubDate>Wed, 20 Apr 2022 16:38:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/ransac/</guid>
      <description>

&lt;p&gt;Outliers (noises) in the data can diverge the regression model to
reduce prediction errors for them, instead of the majority real data
points. &lt;strong&gt;RAN&lt;/strong&gt;dom &lt;strong&gt;SA&lt;/strong&gt;mple
&lt;strong&gt;C&lt;/strong&gt;onsensus is a methodology to robustly fit the model in
the presence of outliers.&lt;/p&gt;
&lt;p&gt;RANSAC does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;randomly sample a subset of data of an fairly enough amount for
training;&lt;/li&gt;
&lt;li&gt;fit a model to the this subset;&lt;/li&gt;
&lt;li&gt;determine data points in the whole data set as inliers or outliers
by comparing the residuals (prediction errors) to a threshold. The set
of inliers is called a consensus set;&lt;/li&gt;
&lt;li&gt;repeat above for some iterations and retrain the final model with
the largest consensus set (since inliers should be the majority).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters of RANSAC include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;: number of points to fit
the model;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;: threshold of the
residual;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: proportion the
outliers;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;: probability of
success (at least one iteration is finished with no outlier);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;: number of iterations to
be determined.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has no
outliers)} = (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has at least one
outlier)} = 1 - (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(all T subsets have outliers)} =
(1 - (1 - e)^s)^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T &amp;lt; 1 -
\delta \\
T &amp;gt; \log\frac{1 - \delta}{1 - (1 - e)^s}
\end{gather}
\]&lt;/span&gt; The threshold &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is
usually set as the median absolute deviation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/xingshansi/p/6763668.html&#34;&gt;随机抽样一致算法（Random
sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</link>
      <pubDate>Fri, 07 Jan 2022 13:50:38 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</guid>
      <description>

&lt;h3 id=&#34;two-class&#34;&gt;Two-class&lt;/h3&gt;
&lt;p&gt;Assume we have a set of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-dimensional samples &lt;span class=&#34;math inline&#34;&gt;\(D = \{x^{(i)}\}^M_{i=1}\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; of which belong to Class &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt; to Class &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(M_1 + M_2
= M\)&lt;/span&gt;). We seek to obtain a scalar &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; by projecting &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto a unit vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)} =
v^Tx^{(i)}\)&lt;/span&gt;. Of all the possibilities we would like to select
the one that maximizes the separability of the scalars between
classes.&lt;/p&gt;
&lt;p&gt;The mean of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is &lt;span class=&#34;math display&#34;&gt;\[
\mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} =
c]x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} \\
\tilde \mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]v^T
x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} = v^T\mu_c\\
\]&lt;/span&gt; The scatter of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is defined as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_c &amp;amp;= \sum_{i=1}^M \mathbb I[y^{(i)} = c](x^{(i)} - \mu_c)(x^{(i)}
- \mu_c)^T \\
\tilde s^2_c &amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](z^{(i)} -
\tilde u_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)}
- u_c) \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} -
u_c)^Tv \\
&amp;amp;= v^TS_cv
\end{aligned}
\]&lt;/span&gt; FLD suggests in &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space
maximizing the distance among the means of classes (inter-class scatter)
and minimizing the variance over each class (within-class scatter), i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_v J(v) &amp;amp;\coloneq \frac{(\tilde \mu_1 - \tilde \mu_2)^2}{\tilde
s^2_1 + \tilde s^2_2} \\
&amp;amp;= \frac{(v^T\mu_1 - v^T\mu_2)^2}{v^TS_1v + v^TS_2v} \\
&amp;amp;= \frac{v^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv}{v^T(S_1 + S_2)v} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_W = S_1 + S_2\)&lt;/span&gt; is called
within-class scatter, &lt;span class=&#34;math inline&#34;&gt;\(S_B = (\mu_1 -
\mu_2)(\mu_1 - \mu_2)^T\)&lt;/span&gt; is called inter-class scatter. Then,
&lt;span class=&#34;math display&#34;&gt;\[
J(v) = \frac{v^TS_Bv}{v^TS_Wv} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial v} &amp;amp;= v^TS_Wv \frac{\partial
v^TS_Bv}{\partial v} - v^TS_Bv \frac{\partial v^TS_Wv}{\partial v}&amp;amp;
\\
&amp;amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv  \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv &amp;amp;= 0 \\
S_Bv - \frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv &amp;amp;= 0 \\
S_Bv &amp;amp;= J(v) S_Wv
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_W\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(S^{-1}_WS_B\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_Bv = J(v) v
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_Bv = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv
= \alpha(\mu_1 - \mu_2)\)&lt;/span&gt; always points to the same direction as
&lt;span class=&#34;math inline&#34;&gt;\((\mu_1 - \mu_2)\)&lt;/span&gt;. Thus we can
immediately give a &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and verify:
&lt;span class=&#34;math display&#34;&gt;\[
v = S^{-1}_W(\mu_1 - \mu_2)
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_B\underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v = S^{-1}_W\alpha(\mu_1
- \mu_2) = \underbrace{\alpha}_{J(v)} \underbrace{S^{-1}_W(\mu_1 -
\mu_2)}_v \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-class&#34;&gt;Multi-class&lt;/h3&gt;


</description>
    </item>
    
    <item>
      <title>Bias-variance Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</guid>
      <description>
&lt;p&gt;The notation used is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 18%&#34;/&gt;
&lt;col style=&#34;width: 81%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Symbol&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Notation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal
D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_\mathcal
D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the observation of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal
D\)&lt;/span&gt;, affected by noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the real value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar
y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the mean of the real values&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the model learned with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar
f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the expectation of prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l(f(x),
y_\mathcal D)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the loss function, chosen to be squared
error&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By assuming that the observation errors averages to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, the expectation of the error will be
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E_{x \sim \mathcal D}&amp;amp;[l(f(x), y_\mathcal D)] = E[(f(x) - y_\mathcal
D)^2] = E\{[(f(x) - \bar f(x) + (\bar f(x) - y_\mathcal D)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y_\mathcal D)^2] +
2E[(f(x) - \bar f(x))(\bar f(x) - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E\{[(\bar f(x) - y) + (y - y_\mathcal
D)]^2\} \\
&amp;amp;\quad + 2\underbrace{E[f(x) - \bar f(x)]}_0 E[\bar f(x) -
y_\mathcal D] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y)^2] + E[(y -
y_\mathcal D)^2] + 2E[(\bar f(x) - y)(y - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar
f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;\quad + 2E[\bar f(x) - y] \underbrace{E[y - y_\mathcal D]}_0 \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar
f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x)
- \bar y)^2] + E[(\bar y - y)^2] + 2E[(\bar f(x) - \bar y)(\bar y - y)]
\\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x)
- \bar y)^2] + E[(\bar y - y)^2] \\
&amp;amp;\quad + 2E[\bar f(x) - \bar y]\underbrace{E[\bar y - y]}_0 \\
&amp;amp;= \underbrace{E[(f(x) - \bar f(x))^2]}_{variance} +
\underbrace{E[(\bar f(x) - \bar y)^2]}_{bias^2} + \underbrace{E[(y -
y_\mathcal D)^2]}_{noise} + \underbrace{E[(\bar y - y)^2]}_{scatter} \\
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://medium.com/analytics-vidhya/5-ways-to-achieve-right-balance-of-bias-and-variance-in-ml-model-f734fea6116&#34;&gt;5
ways to achieve right balance of Bias and Variance in ML model | by
Niwratti Kasture | Analytics Vidhya | Medium&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的函数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 31 Mar 2022 10:08:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</guid>
      <description>
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一一维随机变量，&lt;span class=&#34;math inline&#34;&gt;\(f: \R \to \R\)&lt;/span&gt;为一函数，那么&lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;的期望以及分布情况会是什么样的呢？&lt;/p&gt;
&lt;p&gt;我们这里只讨论&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是单调函数的情况，令&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X)\)&lt;/span&gt;，那么 &lt;span class=&#34;math display&#34;&gt;\[
P_Y(y) = P_Y(Y \le y) = P_X(f(X) \le y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递增， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \le f^{-1}(y)) = P_X(f^{-1}(y)) \\
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial P_X(f^{-1}(y))}{\partial y} \\
&amp;amp;= p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递减， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \ge f^{-1}(y)) = 1 - P_X(X \le f^{-1}(y)) = 1 -
P_X(f^{-1}(y)) \\
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial [1 - P_X(f^{-1}(y))]}{\partial y} \\
&amp;amp;= -p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之，&lt;span class=&#34;math inline&#34;&gt;\(p_Y(y) = p_X(f^{-1}(y)) \cdot
|(f^{-1})^\prime (y)|\)&lt;/span&gt;。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Real Symmetric Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</link>
      <pubDate>Sun, 30 Jan 2022 13:46:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</guid>
      <description>

&lt;h2 id=&#34;real-symmetric-matrix&#34;&gt;Real Symmetric Matrix&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real-valued symmetric matrix.
We have its properties as follows.&lt;/p&gt;
&lt;h3 id=&#34;real-valued-eigenvalues-and-eigenvectors&#34;&gt;Real-valued
Eigenvalues and Eigenvectors&lt;/h3&gt;
&lt;p&gt;Its eigenvalues and thus eigenvectors are real-valued. Suppose by
contradiction that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has some
imaginary eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and
the corresponding imaginary eigenvector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A x &amp;amp;= \lambda x \\
A(x_\text{real} + x_\text{img}) &amp;amp;= (\lambda_\text{real} +
\lambda_\text{img})(x_\text{real} + x_\text{img}) \\
A x_\text{real} + A x_\text{img} &amp;amp;= (\lambda_\text{real}
x_\text{real} + \lambda_\text{img} x_\text{img}) + (\lambda_\text{real}
x_\text{real} + \lambda_\text{img} x_\text{real}) \\
\end{aligned}
\]&lt;/span&gt; Denoting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s and
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s complex conjugate by &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; respectively, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\left.
\begin{aligned}
&amp;amp;A\bar x = A x_\text{real} - A x_\text{img} \\
&amp;amp;= (\lambda_\text{real} x_\text{real} + \lambda_\text{img}
x_\text{img}) \\
&amp;amp;\quad- (\lambda_\text{real} x_\text{real} + \lambda_\text{img}
x_\text{real}) \\
&amp;amp;= \bar \lambda \bar x
\end{aligned}
\right\} \Rightarrow
\begin{aligned}
(A\bar x)^T &amp;amp;= (\bar \lambda \bar x)^T \\
\bar x^T A^T &amp;amp;= \bar \lambda \bar x^T \\
\bar x^T A &amp;amp;= \bar \lambda \bar x^T
\end{aligned}
\end{gather}
\]&lt;/span&gt; Left-multiply &lt;span class=&#34;math inline&#34;&gt;\(\bar x^T\)&lt;/span&gt; on
both sides of &lt;span class=&#34;math inline&#34;&gt;\(Ax = \lambda x\)&lt;/span&gt; to
give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar x^T \lambda x = \lambda \bar x^T x
\]&lt;/span&gt; Right-multiply &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on both
side of &lt;span class=&#34;math inline&#34;&gt;\(\bar x^TA = \bar \lambda \bar
x^T\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar \lambda \bar x^T x
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda \bar x^T x
= \lambda \bar x^T x\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\bar
x^Tx\)&lt;/span&gt; is real-value, &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda =
\lambda\)&lt;/span&gt;. In other words, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is real-valued and thus so is
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;sum-of-real-symmetric-matrices&#34;&gt;Sum of Real Symmetric
Matrices&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; be two real symmetric matrices. Let
&lt;span class=&#34;math inline&#34;&gt;\(\lambda^-, \lambda^+\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s smallest and largest eigenvalue of
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu^-, \mu^+\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;’s smallest and largest eigenvalue.
Denote &lt;span class=&#34;math inline&#34;&gt;\(\gamma^-, \gamma^+\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(A+B\)&lt;/span&gt;’s smallest and largest eigenvalue.
Then it can be &lt;a href=&#34;https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/eigenvectors/sum.html&#34;&gt;derived&lt;/a&gt;
that &lt;span class=&#34;math display&#34;&gt;\[
\lambda^- + \mu^- \le \gamma^- \le \gamma^+ \le \lambda^+ + \mu^+
\]&lt;/span&gt; ### Orthogonal Eigenvectors&lt;/p&gt;
&lt;p&gt;Its eigenvectors corresponding to different eigenvalues are
orthogonal. Arbitrarily taking &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’ s
two eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2\)&lt;/span&gt; and their
eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, \lambda_1
\ne \lambda_2\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= v_1^TA^Tv_2 \\
&amp;amp;= v_1^TAv_2 \\
&amp;amp;= v_1^T \lambda_2v_2 \\
&amp;amp;= \lambda_2v_1^Tv_2 \\
\end{aligned} \\
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= \lambda_1v_1^Tv_2
\end{aligned}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda_1v_1^Tv_2 &amp;amp;= \lambda_2v_1^Tv_2 \\
(\lambda_1 - \lambda_2)v_1^Tv_2 &amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ne \lambda_2\)&lt;/span&gt;,
we have &lt;span class=&#34;math inline&#34;&gt;\(v_1^Tv_2 = 0\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; are orthogonal.&lt;/p&gt;
&lt;h3 id=&#34;diagonalizable&#34;&gt;Diagonalizable&lt;/h3&gt;
&lt;p&gt;It has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent
eigenvectors and thus &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt;. To show it,
eigenvectors in different eigenspaces are orthogonal and thus linearly
independent; and eigenvectors in the same eigenspace are also linearly
independent because they form the basis of this eigenspace.&lt;/p&gt;
&lt;h4 id=&#34;easily-invertible&#34;&gt;“Easily Invertible”&lt;/h4&gt;
&lt;p&gt;Further on the diagonalizable property, suppose &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^{-1}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is not invertible, by &lt;a href=&#34;Eigenvectors%20and%20Eigenvalues.md#Eigenvalues:%20Rank,%20Trace%20and%20Determinant&#34;&gt;the
relation between the matrix rank and the eigenvalues&lt;/a&gt;, some of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;’s entries on the diagonal are
zero. By adding &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^\prime &amp;amp;= P\Lambda P^{-1} + \lambda PIP^{-1} \\
&amp;amp;= P(\Lambda + \lambda I)P^{-1}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;
complements all the zero entries on the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(A^\prime\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; nonzero eigenvalues and is invertible.
A singular symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
becomes invertible by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda
I\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;orthogonally-diagonalizable&#34;&gt;Orthogonally Diagonalizable&lt;/h4&gt;
&lt;p&gt;Its diagonalization can be in the form of &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(P^TP = I\)&lt;/span&gt;, by properly selecting the
orthonormal eigenvectors.&lt;/p&gt;
&lt;p&gt;Eigenvectors from different eigenspaces are already orthogonal.
Eigenvectors from the same eigenspace are independent but not
necessarily orthogonal. However, the linear combination of these
homo-spatial independent eigenvectors is still an eigenvector. Thus we
can apply the Gram-Schmidt process to these eigenvectors and obtain the
orthogonal basis for this eigenspace.&lt;/p&gt;
&lt;p&gt;Finally, we pull together all the orthogonal eigenvectors, normalize
them to unit vector, and get the orthonormal matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Orthogonal diagonalization is &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalization&lt;/a&gt;, as well as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. In fact, an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is orthogonally diagonalizable if and
only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a symmetric matrix.
Such orthogonal diagonalization is also referred to as &lt;strong&gt;spectral
decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;commutativity&#34;&gt;Commutativity&lt;/h3&gt;
&lt;p&gt;If the product of two symmetric matrices &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is symmetric, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; commute, i.e. &lt;span class=&#34;math inline&#34;&gt;\(AB = BA\)&lt;/span&gt;. This is simply because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A B &amp;amp;= (A B)^T &amp;amp;&amp;amp;\iff \\
A B &amp;amp;= B^T A^T &amp;amp;&amp;amp;\iff \\
A B &amp;amp;= B A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;Covariance matrix is a special kind of real symmetric matrix. It is
in the form of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;. It is
positive semi-definite and thus its eigenvalues are non-negative.&lt;/p&gt;
&lt;p&gt;In fact, matrices of this form are positive semi-definite.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zybuluo.com/fsfzp888/note/1112344&#34;&gt;real-valued
eigenvalues and orthogonal eigenvectors&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Squares</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</guid>
      <description>
&lt;p&gt;Suppose we are solving the &lt;span class=&#34;math inline&#34;&gt;\(Ax =
b\)&lt;/span&gt; problem. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; does not
always lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. However, we can try to find within
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s column space a vector &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\hat x\)&lt;/span&gt; best approximates &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. By best approximation we mean to
minimize the &lt;span class=&#34;math inline&#34;&gt;\(||Ax - b||\)&lt;/span&gt; over all
&lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The best approximation can be achieved when &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b =
\mathop{proj}_{Col(A)}b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of finding a orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, computing &lt;span class=&#34;math inline&#34;&gt;\(\hat b\)&lt;/span&gt; and then solving &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b\)&lt;/span&gt;, we can derive &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in this way: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
(b - \hat b) \perp Col(A) \iff (b - \hat b) \in Nul(A^T)\iff A^T(b -
\hat b) = 0 \\
A^T(b - Ax) = 0 \\
A^TAx = A^Tb \label{solution}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will show that if columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, then the least-square
solution &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; is uniquely given
by &lt;span class=&#34;math inline&#34;&gt;\((A^TA)^{-1}A^Tb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Firstly, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = Nul(A^TA)\)&lt;/span&gt;.
This is because &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = 0 \Rightarrow A^TAx = A^T0 = 0 \\
A^TAx = 0 \iff x^TA^TAx = 0 \iff (Ax)^TAx = 0 \Rightarrow Ax = 0
\end{gather}
\]&lt;/span&gt; When columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are
independent, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt; so that
&lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt;, which indicates that
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{solution}\)&lt;/span&gt; has the
unique solution. Conversely, as an aside, when &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt;, which indicates that columns
of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>大数定律和中心极限定理</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link>
      <pubDate>Fri, 20 May 2022 09:27:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)\)&lt;/span&gt;及方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X)\)&lt;/span&gt;存在，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
P(|X-\E(X)| \ge \epsilon) \le \frac{\Var(X)}{\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为连续型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \int_{|x - \E(x)| \ge \epsilon}
p(x)dx \\
&amp;amp;\le \mathop \int_{|x - \E(x)| \ge \epsilon} \bigg( \frac{X -
\E(x)}{\epsilon} \bigg)^2 p(x)dx \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \int_{|x - \E(x)| \ge \epsilon}
\big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \int_{x \in X} \big( X - \E(x)
\big)^2 p(x)dx \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为离散型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \sum_{|x - \E(x)| \ge \epsilon}
P(x) \\
&amp;amp;\le \mathop \sum_{|x - \E(x)| \ge \epsilon} \bigg( \frac{x -
\E(x)}{\epsilon} \bigg)^2 P(x) \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \sum_{|x - \E(x)| \ge \epsilon}
\big( x - \E(x) \big)^2 P(x) \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \sum_{x \in X} \big( x - \E(x)
\big)^2 P(x) \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是作为证明其它大数定理的基础工具。&lt;/p&gt;
&lt;h3 id=&#34;依概率收敛&#34;&gt;依概率收敛&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;p&gt;随机变量是事件的映射，当试验次数足够多时，事件的频率会&lt;strong&gt;依概率收敛&lt;/strong&gt;到该事件的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) =
1\)&lt;/span&gt;，则称该随机变量序列依概率收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{P}{\to} c\)&lt;/span&gt;。或者，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) =
0\)&lt;/span&gt;。 ### Markov不等式&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式其实是Markov不等式的一个特例。令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一非负随机变量、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为一非负实数，Markov不等式描述的是以下关系：
&lt;span class=&#34;math display&#34;&gt;\[
P(X \ge \alpha) \le \frac{\E(X)}{\alpha}
\]&lt;/span&gt; 以连续型随机变量为例，证明如下： &lt;span class=&#34;math display&#34;&gt;\[
P(X \ge \alpha) = \int_{x \ge \alpha} p(x) \d x \le \int_{x \ge \alpha}
\frac{x}{\alpha} p(x) \d x \le \int_x \frac{x}{\alpha} p(x) \d x =
\frac{\E(X)}{\alpha}
\]&lt;/span&gt; 由于&lt;span class=&#34;math inline&#34;&gt;\(|X - \E(x)| \ge \epsilon \iff
(X - \E(X))^2 \ge \epsilon^2\)&lt;/span&gt;，将Markov不等式中的的&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\((X -
\E(X))^2\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\(\epsilon^2\)&lt;/span&gt;，即可得到Chebyshev不等式。不过由于Markov不等式有随机变量非负的要求，适用范围就小了一些；而且同样，Markov不等式的这种估计也是很粗糙的。&lt;/p&gt;
&lt;h2 id=&#34;弱大数定律weak-law-of-large-numbers&#34;&gt;弱大数定律（Weak Law of
large numbers）&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev大数定律&#34;&gt;Chebyshev大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,\dots\)&lt;/span&gt;两两不相关，若存在常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\Var(X_i) \le c \ne +\infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n}
\sum_{i=1}^n \E(X_i)| &amp;lt; \epsilon) = 1
\]&lt;/span&gt; 亦即&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac{1}{n}
\sum_{i=1}^n X_i \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n
\E(X_i)\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;p&gt;由于该随机序列两两不相关，故根据期望及方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\E(\frac{1}{n} \sum_{i=1}^n X_i) =  \frac{1}{n} \sum_{i=1}^n
\E(X_i),\quad \Var(\frac{1}{n} \sum_{i=1}^N X_i) = \frac{1}{n^2}
\sum_{i=1}^n \Var(X_i) \le \frac{c}{n}
\]&lt;/span&gt; 根据Chebyshev不等式， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 \le P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n
\E(X_i)| \ge \epsilon) &amp;lt; \frac{\Var(\frac{1}{n} \sum_{i=1}^N
X_i)}{\epsilon^2} \le \frac{c}{n \epsilon} \\
\underbrace{\lim_{n \to \infty} 0}_0 \le \lim_{n \to \infty}
P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge
\epsilon) \le \underbrace{\lim_{n \to \infty} \frac{c}{n \epsilon}}_0
\Rightarrow\\
\lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n}
\sum_{i=1}^n \E(X_i)| \ge \epsilon) = 0
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;khinchin大数定律&#34;&gt;Khinchin大数定律&lt;/h3&gt;
&lt;h4 id=&#34;相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu, \Var(X_i) = \sigma^2 \ne \infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt;
\epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家Khinchin证明该定律依然成立，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt;
\epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim
B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - p| &amp;lt; \epsilon)
= 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。&lt;/p&gt;
&lt;h2 id=&#34;中心极限定理&#34;&gt;中心极限定理&lt;/h2&gt;
&lt;p&gt;注意，在本节中，我们用&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;表示标准正态分布的分布函数。&lt;/p&gt;
&lt;h3 id=&#34;lindburg-levy中心极限定理&#34;&gt;Lindburg-Levy中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu, \Var(X_i) = \sigma^2 \ne \infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - n \mu}{\sqrt n \sigma}
\le x) = \Phi(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;de-moivre-laplace中心极限定理&#34;&gt;de
Moivre-Laplace中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，且&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim
B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}} \le
x) = \Phi(x)
\]&lt;/span&gt; 显然de
Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。&lt;/p&gt;
&lt;p&gt;前面的Bernoulli大数定律告诉我们可以用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n} \sum_{i=1}^n
X_i\)&lt;/span&gt;（频率）近似&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;（概率），而至于近似程度如何，却不得而知。de
Moivre-Laplace中心极限定理则告诉我们当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;足够大时，近似程度如何： &lt;span class=&#34;math display&#34;&gt;\[
P(|\frac{1}{n}\sum_{i=1}^n X_i - p| \le \epsilon) =
P(|\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}}| \le \frac{\sqrt n
\epsilon}{\sqrt{p(1-p)}}) \approx 2\Phi(\frac{\sqrt n
\epsilon}{\sqrt{p(1-p)}}) - 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上式实际是在用正态分布近似二项分布（多个伯努利分布随机变量加和为伯努利分布），比如在&lt;a href=&#34;https://www.mathsisfun.com/data/quincunx-explained.html&#34;&gt;Galton
Board游戏&lt;/a&gt;中，我们就可以应用de
Moivre-Laplace中心极限定理来近似实际概率。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>三大分布与正态总体的抽样分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</link>
      <pubDate>Sat, 09 Jul 2022 22:21:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布都由正态分布衍生而来，常见统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。&lt;/p&gt;
&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;gamma函数&#34;&gt;Gamma函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x) = \int_0^{+\infty} e^{-t} t^{x-1} dt \quad (x &amp;gt; 0)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt;函数具有&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x + 1) = x \Gamma(x)\)&lt;/span&gt;的性质：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = \int_0^{+\infty} e^{-t} t^{x} dt = [-e^{-t} t^x]
\bigg|^{+\infty}_{t=0} - \int_0^{+\infty} -e^{-t} xt^{x-1} dt
\]&lt;/span&gt; 根据洛必达法则，&lt;span class=&#34;math inline&#34;&gt;\(\lim_{t \to
+\infty} = \frac{-t^x}{e^t} = \lim_{t \to +\infty} \frac{x!}{e^t} =
0\)&lt;/span&gt;，故 &lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = 0 + x\int_0^{+\infty} e^{-t} t^{x-1} dt = x \Gamma(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1) = \int_0^{+\infty} e^{-t}
dt = 1\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;为正整数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x) = x!\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2) = \sqrt
\pi\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x = 2k +
1\)&lt;/span&gt;为正奇数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\frac{x}{2}) =
\sqrt \pi \prod_{i=0}^{k-1} \frac{2 * i + 1}{2}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\Gamma (\frac{1}{2}) = \int_0^{+\infty} e^{-t} t^{-\frac{1}{2}} \d t
\stackrel{u = t^\frac{1}{2}}{\Longrightarrow} \int_0^{+\infty} e^{-u^2}
u^{-1} \;2u \d u
= 2 \int_0^{+\infty} e^{-u^2} \d u
= \int_{-\infty}^{+\infty} e^{-u^2} \d u  \\
\notag \\
\begin{aligned}
&amp;amp;\Gamma^2(\frac{1}{2}) = (\int_{-\infty}^{+\infty} e^{-u^2} \d u)^2
\\
&amp;amp;= (\int_{-\infty}^{+\infty} e^{-u^2} du)(\int_{-\infty}^{+\infty}
e^{-v^2} \d v) \\
&amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(u^2+v^2)}
\d u \d v \\
&amp;amp;\downarrow_{u = r\sin\theta, v = r\cos\theta} \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} e^{-r^2}\; r \d r \d \theta \\
&amp;amp;= \int_{0}^{2\pi} [-\frac{1}{2}e^{-r^2}] \bigg|_{r=0}^{+\infty} \d
\theta \\
&amp;amp;= \int_{0}^{2\pi} \frac{1}{2} \d \theta \\
&amp;amp;= \pi \\
&amp;amp;\Gamma (\frac{1}{2}) = \sqrt \pi
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinehw.math.ksu.edu/math340book/chap3/gamma.php&#34;&gt;The
Gamma Function&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=XAoe4th0F1k&#34;&gt;The derivation of
&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2)\)&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;chi2分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为相互独立的标准正态分布随机变量，即&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim N(0,1)\)&lt;/span&gt;则称&lt;span class=&#34;math inline&#34;&gt;\(Y = X_1^2 + \dots +
X_n^2\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布，记作&lt;span class=&#34;math inline&#34;&gt;\(Y \sim \chi^2(n)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\E[Y] = n, \Var[Y] = 2n\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时，容易得到&lt;span class=&#34;math inline&#34;&gt;\(\forall y \le 0, P_Y(y) = 0, p_Y(y)= 0\)&lt;/span&gt;，
$$ &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\begin{split}
\forall y &amp;gt; 0, P_Y(y) &amp;amp;= 2P_X(\sqrt y) - 1 \\
\end{split} \\

\begin{split}
&amp;amp;p_Y(y) = 2P_X&amp;#39;(\sqrt y) \frac 1 {2 \sqrt y} \\
&amp;amp;= \frac 1 {\sqrt {2\pi y}}  e^{-\frac 1 2 y} \\
&amp;amp;= \frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1}
e^{-\frac y 2}
\end{split}
\end{align}\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
$\chi^2(1)$分布的密度函数为：
\]&lt;/span&gt; p_Y(y) =&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
\frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y
2}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=k\)&lt;/span&gt;时，令&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_k\)&lt;/span&gt;表示一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维空间中的点， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_Y(y) = P_Y(Y \le y) &amp;amp;= \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\;
\d x_1 \dots \d x_k \\
&amp;amp;= \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}}
{(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
\end{aligned}
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k x_i^2 \le
y\)&lt;/span&gt;的积分区域。可以看出，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal
V\)&lt;/span&gt;对应一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维球体，且其半径&lt;span class=&#34;math inline&#34;&gt;\(R = \sqrt y\)&lt;/span&gt;。对此，作&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/&#34;&gt;高维球坐标变换&lt;/a&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P_Y(Y \le y) = \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1
\dots \d x_k = \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots +
x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2}
\int_0^\sqrt{y} \\
&amp;amp;\quad\quad\quad\frac{e^{-\frac 1 2 (r^2\cos^2 \varphi_1 +
r^2\sin^2 \varphi_1 \cos^2 \varphi_2 + \dots +
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \cos^2 \varphi_{k-1} +
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \sin^2 \varphi_{k-1})} }
{(2\pi)^{k / 2}}\\
&amp;amp;\quad\quad\quad r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3}
\dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2}
\int_0^\sqrt{y} \frac{e^{-\frac 1 2 r^2}} {(2\pi)^{k / 2}}
r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots
\sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^\sqrt{y} \underbrace{ \int_0^{2\pi} \underbrace{\int_0^\pi
\dots \int_0^\pi}_{k-2} \frac{1} {(2\pi)^{k / 2}} \sin(\varphi_1)^{k-2}
\sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})\ \d \varphi_1 \dots \d
\varphi_k}_{c_k} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;= c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;是和&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;相关的常数项，并且由于&lt;span class=&#34;math inline&#34;&gt;\(P_Y(Y \le \infty) = 1\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 &amp;amp;= c_k \int_0^\infty e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;\Downarrow_{r = \sqrt{2t}} \\
1 &amp;amp;= c_k \int_0^\infty e^{-t} \sqrt{2t}^{k-1} \frac{1}{\sqrt{2t}} \d
t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \int_0^\infty e^{-t} t^{(k-2)/2} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \Gamma(\frac{k}{2}) \\
c_k &amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故可得密度函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p_Y(y) = \frac{\d P_Y(Y \le y)}{\d y} \\
&amp;amp;= \frac{\d [c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d
r]}{\d y} \\
&amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})} e^{-\frac y 2}
y^\frac{k-1}{2} \frac{1}{2\sqrt y} \\
&amp;amp;= \frac{1} {2^{\frac k 2} \Gamma(\frac{k}{2})} e^{-\frac y 2}
y^{\frac{k}{2} - 1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最终可得密度函数如下： &lt;span class=&#34;math display&#34;&gt;\[
p_Y(y) =
\begin{cases}
\frac 1 {2^\frac{n}{2} \Gamma(\frac n 2)} e^{-\frac y 2} y^{\frac n 2 -
1}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}
\]&lt;/span&gt; 另外，该密度函数也可以通过数学归纳法验证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statlect.com/probability-distributions/chi-square-distribution&#34;&gt;Chi
Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://vtechworks.lib.vt.edu/bitstream/handle/10919/34329/10Apxb.pdf?sequence=12&#34;&gt;Generating
Function of Chi Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/30020592&#34;&gt;正向推导&lt;/a&gt; || &lt;a href=&#34;https://www.bilibili.com/video/BV1e54y1v7e5&#34;&gt;数学归纳法&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;t分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim N(0,1), Y \sim
\chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z =
\frac{X}{\sqrt{Y/n}}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z
\sim t(n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \frac{\Gamma((n+1)/2)} {\sqrt{n\pi} \Gamma(n/2)} \big( 1 +
\frac{z^2} n \big)^{-(n+1)/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;f分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim \chi^2(m), Y \sim
\chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z =
\frac{X/m}{Y/n}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z
\sim F(m,n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \begin{cases}
\frac{\Gamma((m+n)/2} {\Gamma(m/2) \Gamma(n/2)} {m \choose n}^{\frac m
2} z^{\frac m 2 - 1} (1 + \frac m n z)^{-\frac{m+n}{2}}, &amp;amp;z &amp;gt; 0
\\
0, &amp;amp;\text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;正态总体的抽样分布&#34;&gt;正态总体的抽样分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,
\sigma^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar X
= \frac 1 n \sum_{i=1}^n X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar
X)^2\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X \sim N(\mu, \frac{\sigma^2}{n}) \\
X_i - \bar X \sim N(0, \frac{2(n-1)^2 \sigma^2}{n^2}) \\
\frac{\sum_{i=1}^n (X_i - \bar X)^2}{\sigma^2} \sim \chi^2(n - 1),\text
即 \frac{(n-1) S^2}{\sigma^2} = \frac{n S_n^2}{\sigma^2} \sim
\chi^2(n-1) \\
\text{$\bar X$与$S^2$相互独立，$\bar X$与$S_n^2$相互独立}
\label{independence} \\
\frac{(\bar X - \mu)\sqrt{n}}{S} \sim t(n-1) \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;实际上&lt;span class=&#34;math inline&#34;&gt;\(\eqref{independence}\)&lt;/span&gt;与“总体为正态分布”互为充要条件。&lt;/p&gt;
&lt;h3 id=&#34;两个独立正态总体的抽样分布&#34;&gt;两个独立正态总体的抽样分布&lt;/h3&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_{n_X}\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu_X,
\sigma_X^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar
X = \frac 1 {n_X} \sum_{i=1}^{n_X} X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_X^2 = \frac{1}{n_X-1} \sum_{i=1}^{n_X} (X_i -
\bar X)^2\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots,
Y_{n_Y}\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu_Y,
\sigma_Y^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar
Y = \frac 1 {n_Y} \sum_{i=1}^{n_Y} Y_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_Y^2 = \frac{1}{n_Y-1} \sum_{i=1}^{n_Y} (Y_i -
\bar Y)^2\)&lt;/span&gt;；且两个正态总体相互独立，令&lt;span class=&#34;math inline&#34;&gt;\(S_w = \frac{1}{n_X + n_Y - 2} (\sum_{i=1}^{n_X}
(X_i - \bar X)^2 + \sum_{i=0}^{n_Y} (Y_i - \bar Y)^2)\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X - \bar Y \sim N(\mu_X - \mu_Y, \frac{\sigma_X^2}{n_X} +
\frac{\sigma_Y^2}{n_Y}) \\
X_i - Y_j \sim N(\mu_X - \mu_Y, \sigma_X^2 + \sigma_Y^2) \\
\frac{(n_X - 1) S_X^2}{\sigma_X^2} + \frac{(n_Y - 1) S_Y^2}{\sigma_Y^2}
\sim \chi^2(n_X + n_Y - 2) \\
\frac{S_X^2 / \sigma_X^2}{S_Y^2 / \sigma_Y^2} = \frac{S_X^2 /
S_Y^2}{\sigma_X^2 / \sigma_Y^2} \sim F(n_X - 1, n_Y - 1) \\
\text{当$\sigma_X^2 = \sigma_Y^2 = \sigma^2$时，} \frac{\bar X - \bar Y
- (\mu_1 - \mu_2)}{S_w \sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}} \sim t(n_X
+ n_Y - 2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;分布计算器&#34;&gt;分布计算器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html&#34;&gt;Normal
Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/chisq.html&#34;&gt;Chi-Square
Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/t.html&#34;&gt;Student’s
t-Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/f.html&#34;&gt;F-Distribution
Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Whitening</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/</link>
      <pubDate>Thu, 11 Aug 2022 17:52:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/</guid>
      <description>

&lt;h2 id=&#34;whitening&#34;&gt;Whitening&lt;/h2&gt;
&lt;p&gt;Data whitening is the process of converting a random vector &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with only first-order correlation into
a new random vector &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; such that the
covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is an
identity matrix. Data whitening usually has two steps: the decorrelation
step and the standardization step.&lt;/p&gt;
&lt;p&gt;To do it, we shall first apply the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Orthogonally Diagonalizable&#34;&gt;orthogonal diagonalization&lt;/a&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;’s covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_X\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma_X \Phi = \Phi \Lambda
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; contains the
normalized eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1} =
\Phi^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is
diagonal and contains the eigenvalues. Now let &lt;span class=&#34;math inline&#34;&gt;\(Y = \Phi^T X\)&lt;/span&gt;, we can verify that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Y &amp;amp;= \E \{ \Phi^T (\x - \mu_X) [\Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Phi^T (\x - \mu_X) (\x - \mu_X)^T \Phi] \\
&amp;amp;= \Phi^T \E [(\x - \mu_X) (\x - \mu_X)^T] \Phi \\
&amp;amp;= \Phi^T \Sigma_X \Phi = \Phi^T \Phi \Lambda = \Lambda
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Y\)&lt;/span&gt; is diagonal and
we finish the decorrelation step. To further make it an identity matrix
(the standardization step), we apply &lt;span class=&#34;math inline&#34;&gt;\(Z =
\Lambda^{-1/2} Y = \Lambda^{-1/2} \Phi^T X\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= \E \{\Lambda^{-1/2} \Phi^T (\x - \mu_X) [\Lambda^{-1/2}
\Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Lambda^{-1/2} \Phi^T (\x - \mu_X)  (\x - \mu_X)^T \Phi
\Lambda^{-1/2}] \\
&amp;amp;= \Lambda^{-1/2} \Phi^T \Sigma_X \Phi \Lambda^{-1/2} = I
\end{aligned}
\]&lt;/span&gt; The inverse of data whitening can be used to derive density
function of first-order correlated random variables, e.g. for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/#First-order correlated $n$-dimensional&#34;&gt;Gaussian case&lt;/a&gt;. Data whitening looks
a lot like &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;: they both
compute the eigen pairs; they both project the original data onto the
basis formed by eigenvectors; they both can be solved with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. But unlike PCA, data whitening
uses all the eigenvectors as the basis instead of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; most prominent ones. Therefore, data
whitening does not reduce the data’s dimensionality as PCA does.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Symbol Code</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/</guid>
      <description>

&lt;p&gt;Other than block code where symbols are encoded in chunks, symbol
code will assign each symbol a unique codeword. Among the codeword
schemes, we prefer those where no codeword is a prefix of any other
codeword. These are called &lt;strong&gt;prefix code&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem is how to give such scheme, or does it really exist?&lt;/p&gt;
&lt;h2 id=&#34;kraft-mcmillan-inequality&#34;&gt;Kraft-McMillan Inequality&lt;/h2&gt;
&lt;p&gt;Kraft-McMillan inequality reveals the relation&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and suppose there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. If &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^I 2^{-l_i} \le 1\)&lt;/span&gt;, then there
exists a set of uniquely-decodable prefix coding with these lengths as
their symbol codes’ lengths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The proof is done by construction. The
number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; should
be less than &lt;span class=&#34;math inline&#34;&gt;\(2^{l}\)&lt;/span&gt;, or else the
inequality will be violated. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\forall l = 0,1,\dots\)&lt;/span&gt;, we can loosely
arrange all the codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;
to be unique (since there are &lt;span class=&#34;math inline&#34;&gt;\(2^l\)&lt;/span&gt;
many distinct bit strings of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;). Then the uniqueness condition is
checked.&lt;/p&gt;
&lt;p&gt;Denote the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt;. For any two consecutive lengths
&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((l+1)\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;amp;\le\sum_{i=1}^I 2^{-l_i}\le 1 \\
C_{l+1} &amp;amp;\le 2^{l+1} - 2 C_l \\
C_{l+1} &amp;amp;\le 2(2^l - C_l) \\
\end{aligned}
\]&lt;/span&gt; This means we can append these unused &lt;span class=&#34;math inline&#34;&gt;\((2^l - C_l)\)&lt;/span&gt; codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to suit the number of codes of length
&lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;. Construction
completes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we have a set of uniquely-decodable prefix coding. Denote
the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. Then, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(S =
\sum_{i=1}^I 2^{-l_i} \le 1\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S^N &amp;amp;= (\sum_{i=1}^I 2^{-l_i})^N \\
  &amp;amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots +
l_{i_N})}
  \end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\((l_{i_1} + \dots +
l_{i_N})\)&lt;/span&gt; term can be treated as the length of encoding of &lt;span class=&#34;math inline&#34;&gt;\(a_{i_1} \dots a_{i_N}\)&lt;/span&gt; of arbitrary length
&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(l_\min = \min_i l_i, l_\max = \max_i l_i\)&lt;/span&gt;,
the above can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
  S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt; represents the
number of symbol codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;.
Since the coding is uniquely-decodable, &lt;span class=&#34;math inline&#34;&gt;\(C_l
\le 2^l\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(S &amp;gt; 1\)&lt;/span&gt;, the above
cannot hold for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.
Therefore &lt;span class=&#34;math inline&#34;&gt;\(S \le 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;source-coding-theorem-for-symbol-code&#34;&gt;Source Coding Theorem for
Symbol Code&lt;/h2&gt;
&lt;p&gt;For an ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, there exists
a prefix code &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; with expected
length satisfying &lt;span class=&#34;math display&#34;&gt;\[
H(X) \le L(C,X) &amp;lt; H(X) + 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We define the implicit probabilities
&lt;span class=&#34;math inline&#34;&gt;\(q_i = 2^{-l_i} / z\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z = \sum_i 2^{-l_i}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) &amp;amp;= \sum_i p_i l_i = -\sum_i [p_i \log (q_iz)] \\
&amp;amp;=\sum_i [p_i \log 1/q_i] - \log z \\
&amp;amp;\ge H(X)
\end{aligned}
\]&lt;/span&gt; The equality holds when &lt;span class=&#34;math inline&#34;&gt;\(z =
1\)&lt;/span&gt; (the code is complete) and &lt;span class=&#34;math inline&#34;&gt;\(q =
p\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(l_i = \log 1/p_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;From another perspective, suppose the coding is complete but not
optimal, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) &amp;amp;= -\sum_i [p_i \log (q_iz)] = -\sum_i [p_i \log (q_i)] \\
&amp;amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i
\log (q_i)] \\
&amp;amp;= H(X) + D_{KL}(p || q)
\end{aligned}
\]&lt;/span&gt; where the cost is the extra &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p || q)\)&lt;/span&gt; bits, which is brought by
instead treating &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as the real
distribution. &lt;span class=&#34;math inline&#34;&gt;\(D_{KL(p||q)}\)&lt;/span&gt; is
termed as relative entropy or the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Stirling&#39;s Approximation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/functional-analysis/stirlings-approximation/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/functional-analysis/stirlings-approximation/</guid>
      <description>

&lt;h2 id=&#34;stirlings-approximation&#34;&gt;Stirling’s Approximation&lt;/h2&gt;
&lt;p&gt;Stirling’s approximation, which states that &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(n+1) \sim \sqrt{2 \pi n} \left( \frac{n}{e}
\right)^n\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;,
is useful when estimating the order of &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt;. Notably, it is quite accurate even
when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is small.&lt;/p&gt;
&lt;p&gt;The authentic proof entails Gamma function and Laplace’s method.
However in integer case, Stirling’s approximation can be approached with
Poisson distribution. Start from a Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
P(r ; \lambda) = e^{-\lambda} \frac{\lambda^r}{r!}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda_1)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(Y \sim P(\lambda_2)\)&lt;/span&gt;, and suppose
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(X + Y \sim P(\lambda_1 + \lambda_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \Pr(X + Y = r) = \sum_{k=0}^r \Pr(X = k) \Pr(Y = r-k) \\
&amp;amp;= \sum_{k=0}^r e^{-\lambda_1} \frac{\lambda_1^k}{k!} e^{-\lambda_2}
\frac{\lambda_2^{r-k}}{(r-k)!} \\
&amp;amp;= \frac{e^{-(\lambda_1 + \lambda_2)}}{r!} \sum_{k=0}^r \frac{r!}{k!
(r-k)!} \lambda_1^k \lambda_2^{r-k} \\
&amp;amp;= e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^r}{r!}
\\
&amp;amp;= P(r; \lambda_1 + \lambda_2)
\end{aligned}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, a random variable &lt;span class=&#34;math inline&#34;&gt;\(X \sim
P(\lambda)\)&lt;/span&gt; (with integer &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) can be treated as the addition
of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; independent &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim P(1)\)&lt;/span&gt;. By the central limit
theorem, for a large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\mathbb \Pr(\frac{\underbrace{\sum_i Y_i}_X - \lambda}{\sqrt{\lambda}}
\le x) \simeq \Phi(x)
\]&lt;/span&gt; Or put it another way, the mass of the Poisson distribution
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; follows is well approximated by
the density of the Gaussian distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(x;\lambda) &amp;amp;\simeq N(x; \lambda, \lambda) \\
e^{-r} \frac{\lambda^r}{r!} &amp;amp;\approx \frac{1}{\sqrt{2\pi \lambda}}
e^{-\frac{(r - \lambda)^2}{2\lambda}}
\end{aligned}
\]&lt;/span&gt; Plug &lt;span class=&#34;math inline&#34;&gt;\(r = \lambda\)&lt;/span&gt; into
this formula and rearrange it to have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{-\lambda} \frac{\lambda^\lambda}{\lambda!} &amp;amp;\approx
\frac{1}{\sqrt{2\pi \lambda}} \\
\lambda! &amp;amp;\approx \sqrt{2\pi \lambda} \left( \frac{\lambda}{e}
\right)^\lambda
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/overview/</link>
      <pubDate>Fri, 22 Apr 2022 21:13:30 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/overview/</guid>
      <description>

&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;cross entropy&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; describe the
relationship between &lt;strong&gt;two different probability
measures/functions over the same event space&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/&#34;&gt;conditional entropy&lt;/a&gt; and
&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/mutual-information/&#34;&gt;mutual information&lt;/a&gt;, as well as
&lt;u&gt;joint entropy&lt;/u&gt;, describe the relationship between &lt;strong&gt;two
different random variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Since the notion of entropy is the basis of all other concepts in
this section and the entropy is more well-defined on discrete random
variable, they are more meaningful to be applied in discrete case,
though they are usually trivially extended to continuous case.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-entropy-and-cross-entropy&#34;&gt;KL-divergence, Entropy
and Cross Entropy&lt;/h2&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;D_\text{KL}(p||q) = -\mathrm{E}_{x \sim p} \log \frac{q(x)}{p(x)}
\\
&amp;amp;= -\mathrm{E}_{x \sim p} \log q(x) - [-\mathrm{E}_{x \sim p} \log
{p(x)}] \\
&amp;amp;= H(p||q) - H(p)
\end{align}
\]&lt;/span&gt; By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(-\log\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;D_\text{KL}(p||q) = \mathrm{E}_{x \sim p} [-\log \frac{q(x)}{p(x)}]
\\
&amp;amp;\ge -\log(\mathrm{E}_{x \sim p} \frac{q(x)}{p(x)}) \\
&amp;amp;= 0
\end{align}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) \ge H(p)
\]&lt;/span&gt; The equality holds if and only if &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-mutual-information&#34;&gt;KL-divergence and Mutual
Information&lt;/h2&gt;
&lt;p&gt;Given that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; have the same dimension, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&amp;amp;= \sum_{x,y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x} \sum_{y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x} p(x) D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{x} D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{y} D_{KL}[p(x|y), p(x)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;joint-entropy-and-conditional-entropy&#34;&gt;Joint Entropy and
Conditional Entropy&lt;/h2&gt;
&lt;p&gt;Joint entropy is just entropy, but with the random variable usually
broken into two separate components. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;H(X, Y) = \sum_{x,y} p(x,y) \log p(x,y) \\
&amp;amp;= \sum_{x,y} p(x,y) \log p(y|x) p(x) \\
&amp;amp;= \sum_{x,y} p(x,y) \log p(y|x) \\
&amp;amp;\quad + \sum_{x,y} p(x,y) \log p(x) \\
&amp;amp;= H(Y|X) + H(X)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>3-convex-function</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</guid>
      <description>

&lt;h2 id=&#34;convex-function&#34;&gt;Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R_+\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\R_+ = \R \cup \{
+\infty \}\)&lt;/span&gt; (in convex discussion, usually only &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt; is included) be an extended
real-valued function. Note that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
shouldn’t trivially be &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;
everywhere. We say that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
&lt;strong&gt;convex&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x_1, x_2
\in \R^n, \alpha \in [0,1]\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x_1 + (1-\alpha) x_2) \le \alpha f(x_1) + (1-\alpha) f(x_2)
\]&lt;/span&gt; Note that for &lt;span class=&#34;math inline&#34;&gt;\(x \in \R\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; +\infty, x + \infty = \infty + x =
\infty, +\infty \le +\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interestingly, convex functions on &lt;u&gt;an open domain&lt;/u&gt; are always
continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The &lt;strong&gt;epigraph&lt;/strong&gt; of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\epi f \triangleq \{ (x, t) \in \R^n \times \R:
f(x) \le t \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Definition: The &lt;strong&gt;effective domain&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\dom f \triangleq \{ x \in \R^n: f(x) &amp;lt; \infty
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that the real line does not contain &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. Therefore, the effective domain
of &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x\)&lt;/span&gt; is still &lt;span class=&#34;math inline&#34;&gt;\(\R\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt;
be a set. The &lt;strong&gt;indicator&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the function &lt;span class=&#34;math display&#34;&gt;\[
\mathbb 1_S (x) =
\begin{cases}
0, &amp;amp; x \in S \\
+\infty, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the indicator, we have &lt;span class=&#34;math display&#34;&gt;\[
\underset{\text{constrained}}{\inf_{x \in S} f(x)} \iff
\underset{\text{unconstrained}}{\inf_{x \in \R^n} f(x) + \mathbb 1_S(x)}
\]&lt;/span&gt; That is, we convert a constrained problem to a unconstrained
one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition (verify it): Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n
\mapsto \R_+\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
convex (as a function) if and only if &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; is convex (as a set). Moreover, let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be a set. Then &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is convex (as a set) if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb 1_S\)&lt;/span&gt; is convex (as a
function).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proposition above associates the convexity of a function with
that of its epigraph; and the convexity of a set with that of its
indicator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Jensen’s inequality&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R_+\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if &lt;span class=&#34;math inline&#34;&gt;\(f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m
\alpha_i f(x_i)\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(m \in
\N^+\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_m \in
\R^n\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots,
\alpha_m \ge 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^m \alpha_i = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Sufficiency is easy to show by interpreting the definition of convex
function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,m\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\big( x_i, f(x_i) \big) \in \epi f
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\epi f\)&lt;/span&gt; is convex,
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m \alpha_i \big( x_i, f(x_i) \big) =  \big( \sum_{i=1}^m
\alpha_i x_i, \sum_{i=1}^m \alpha_i f(x_i) \big) \in \epi f
\]&lt;/span&gt; which means &lt;span class=&#34;math display&#34;&gt;\[
f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving
Operations&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non-negative combination&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f_1, \dots, f_m\)&lt;/span&gt; be convex
functions, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots, \alpha_m \ge
0\)&lt;/span&gt; be non-negative scalars. Then, &lt;span class=&#34;math display&#34;&gt;\[
f \triangleq \sum_{i=1}^m \alpha_i f_i \text{ is convex.}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pointwise supremum&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; be an index set (either
finite or infinite) and &lt;span class=&#34;math inline&#34;&gt;\(\{ f_i: i \in I
\}\)&lt;/span&gt; be a collection of convex functions. Then &lt;span class=&#34;math display&#34;&gt;\[
f \triangleq \sup_{i \in I} f_i \text{ is convex.}
\]&lt;/span&gt; To show it, let &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2 \in
\R^n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in [0,
1]\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;f(\alpha x_1 + (1-\alpha) x_2) = \sup_{i \in I} f_i(\alpha x_1 +
(1-\alpha) x_2) \\
&amp;amp;\le \sup_{i \in I} \big( \alpha f_i(x_1) + (1-\alpha) f_i(x_2)
\big) \\
&amp;amp;\le [\alpha \sup_{i \in I} f_i(x_1)] + [(1-\alpha) \sup_{i \in I}
f_i(x_2)] \\
&amp;amp;= \alpha f(x_1) + (1-\alpha) f(x_2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Geometrically, pointwise supremum is intersecting the epigraphs of
&lt;span class=&#34;math inline&#34;&gt;\(f_i\)&lt;/span&gt;’s. &amp;gt; Example: Consider the
mapping &lt;span class=&#34;math inline&#34;&gt;\(f: \R^{m \times n} \supseteq X \to
||X||\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(||X||\)&lt;/span&gt; is the
largest singular value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Show
that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex. &amp;gt; &amp;gt; By the
&lt;em&gt;Courant-Fischer theorem&lt;/em&gt;, &amp;gt; &lt;span class=&#34;math display&#34;&gt;\[
&amp;gt; \begin{aligned}
&amp;gt; ||X|| &amp;amp;= \max_{u \in \R^m, v \in \R^n} u^T X v \\
&amp;gt; \text{s.t.} &amp;amp;\quad ||u||_2 = 1, ||v||_2 = 1
&amp;gt; \end{aligned}
&amp;gt; \]&lt;/span&gt; &amp;gt; Let &lt;span class=&#34;math inline&#34;&gt;\(f_{u,v}(X) = u^T X
v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I = \{ (u,v) \in \R^m \times
\R^n: ||u||_2 = 1, ||v||_2 = 1 \}\)&lt;/span&gt;. Then, &amp;gt; &lt;span class=&#34;math display&#34;&gt;\[
&amp;gt; f(X) = \max_{(u,v) \in I} f_{u,v}(X)
&amp;gt; \]&lt;/span&gt; &amp;gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(f_{u,v}(X)\)&lt;/span&gt; is linear and thus convex in
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. It follows directly from the
pointwise supremum principle that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
is convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Composition with increasing function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(g: \R^n \mapsto \R\)&lt;/span&gt; be
convex, &lt;span class=&#34;math inline&#34;&gt;\(h: \R \mapsto \R\)&lt;/span&gt; be convex.
&lt;span class=&#34;math inline&#34;&gt;\(h \circ g\)&lt;/span&gt; is not generally convex.
To verify it, take &lt;span class=&#34;math display&#34;&gt;\[
h(x) = -x, g(x) = x^2
\]&lt;/span&gt; But if &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is convex as
well as increasing, then &lt;span class=&#34;math inline&#34;&gt;\(h \circ g\)&lt;/span&gt;
is convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Restriction on lines&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a point &lt;span class=&#34;math inline&#34;&gt;\(x_0 \in \R^n\)&lt;/span&gt; and a
direction &lt;span class=&#34;math inline&#34;&gt;\(h \in \R^n \setminus \{ 0
\}\)&lt;/span&gt;, we call the set &lt;span class=&#34;math display&#34;&gt;\[
\{ x_0 + t h: t \in \R \}
\]&lt;/span&gt; a line through &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; in the
direction &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be a function.
Define&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde f_{x_0, h}(t) \triangleq f(x_0 + t h)
\]&lt;/span&gt; as the &lt;strong&gt;restriction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; on the line&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{ x_0 + t h: t \in \R \}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\tilde f_{x_0, h}\)&lt;/span&gt; is convex for any
&lt;span class=&#34;math inline&#34;&gt;\(x_0 \in \R^n\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(h \in \R^n \setminus \{ 0 \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;differentiable-convex-function&#34;&gt;Differentiable Convex
Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;
be differentiable, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt;
exists. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and
only if for every &lt;span class=&#34;math inline&#34;&gt;\(x, y \in \R^n\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
f(x) \ge f(y) + (\nabla f(y))^T (x - y) \tag{gradient inequality}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a fixed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, the right-hand
side of the gradient inequality is in essence an affine function of
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The graph of this affine
function is &lt;span class=&#34;math display&#34;&gt;\[
t(x) = f(y) + (\nabla f(y))^T (x - y) \\
\Updownarrow \\
\underbrace{[(\nabla f(y))^T, -1]}_{s^T} \underbrace{\begin{bmatrix}
x \\
t
\end{bmatrix}}_{z} - \underbrace{[(\nabla f(y))^T y - f(y)]}_{c} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, the normal vector &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; always points downwards (due to the
&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be twice
continuously differentiable, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla f,
\nabla^2 f\)&lt;/span&gt; exist and &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2
f\)&lt;/span&gt; is continuous. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
\nabla^2 f(x) \succcurlyeq 0, \forall x \in \R^n
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \mathcal{S}_{++}^n
\mapsto \R\)&lt;/span&gt; be given by &lt;span class=&#34;math inline&#34;&gt;\(f(X) = -\ln
\det (X)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_{++}^n\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; positive definite matrix. Show
that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex.&lt;/p&gt;
&lt;p&gt;This problem usually emerges when calculating the capacity of channel
in information theory. To show it, we construct &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s restriction on all the lines. Let
&lt;span class=&#34;math inline&#34;&gt;\(X_0 \in \mathcal{S}_{++}^n\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(H \in \mathcal{S}^n \setminus \{ 0
\}\)&lt;/span&gt;. Define &lt;span class=&#34;math display&#34;&gt;\[
g(t) \triangleq \tilde f(X_0 + t H) = -\ln \det(X_0 + t H)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(X_0 \in
\mathcal{S}_{++}^n\)&lt;/span&gt;, we can write &lt;span class=&#34;math inline&#34;&gt;\(X_0 = X_0^{1/2} X_0^{1/2}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X_0^{1/2} \in \mathcal{S}_{++}^n\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(X_0^{-1/2} = (X_0^{1/2})^{-1}\)&lt;/span&gt;.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
g(t) &amp;amp;= -\ln \det(X_0^{1/2} (I + t X_0^{-1/2} H X_0^{-1/2})
X_0^{1/2}) \\
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2})
\end{align*}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(X_0^{-1/2} H
X_0^{-1/2}\)&lt;/span&gt; is real symmetric. Let &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \dots, \lambda_n\)&lt;/span&gt; be its
eigenvalues. Then &lt;span class=&#34;math inline&#34;&gt;\((I + t X_0^{-1/2} H
X_0^{-1/2})\)&lt;/span&gt;’s eigenvalues are &lt;span class=&#34;math inline&#34;&gt;\(t
\lambda_1 + 1, \dots, t \lambda_n + 1\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
g(t) &amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H
X_0^{-1/2}) \\
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \sum_i \ln(t\lambda_i + 1)
\end{align*}
\]&lt;/span&gt; Now it is easy to verify that &lt;span class=&#34;math inline&#34;&gt;\(-\ln(t\lambda_i + 1)\)&lt;/span&gt;’s are convex, which
concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;differentiability-agnostic-convex-function&#34;&gt;Differentiability-agnostic
Convex Function&lt;/h2&gt;
&lt;p&gt;“Differentiability-agnostic” means that no premise on the
differentiability of the convex function is assumed. In this section, we
discuss several “differentiability-agnostic” topics.&lt;/p&gt;
&lt;h3 id=&#34;subgradient&#34;&gt;Subgradient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a function. A vector &lt;span class=&#34;math inline&#34;&gt;\(s \in
\R^n\)&lt;/span&gt; is called a &lt;strong&gt;subgradient&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; if &lt;span class=&#34;math display&#34;&gt;\[
f(x) \ge f(x_0) + s^T (x - x_0), \forall x \in \R^n
\]&lt;/span&gt; The set &lt;span class=&#34;math display&#34;&gt;\[
\partial f(x_0) \triangleq \{ s \in \R^n: \text{$s$ is a subgradient of
$f$ at $x_0$} \}
\]&lt;/span&gt; is called the &lt;strong&gt;sub-differential&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;. Sub-differential is always convex
and closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of subgradient is quite inspired from the gradient
inequality. The geometric interpretation of subgradient resembles that
of gradient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;
be convex. Then, &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is
non-empty, convex and compact (non-emptiness is due to the continuity
(which in turn is due to the openness of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;) and boundedness is due to
convexity). Moreover,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; if and only if &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x_0) = \{ \nabla f(x_0)
\}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; is the global minima if and
only if &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial f(x_0)\)&lt;/span&gt;;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The topological properties that the sub-differential possesses imply
that we can do projection on the sub-differential; besides we may apply
Weierstrass theorem to solve some optimization problem w.r.t. the
sub-differential (such as what is the minimal-norm subgradient).&lt;/p&gt;
&lt;h3 id=&#34;conjugate-function&#34;&gt;Conjugate Function&lt;/h3&gt;
&lt;p&gt;If two functions have the identical epigraphs, then these two
functions are identical. That’s the geometric view of a function.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be
convex. &lt;span class=&#34;math inline&#34;&gt;\(\epi f\)&lt;/span&gt; is known to be
convex. Suppose in addition that &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; is non-empty and closed. Recall that a non-empty, closed and
convex set can be written as the intersection of lower halfspaces that
contain it: &lt;span class=&#34;math display&#34;&gt;\[
\epi f = \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T,
y_0]^T, c)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s \in \R^n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_0 \in \R\)&lt;/span&gt;. We argue that &lt;span class=&#34;math inline&#34;&gt;\(y_0 \le 0\)&lt;/span&gt; because the last entry of some
&lt;span class=&#34;math inline&#34;&gt;\(z \in \epi f\)&lt;/span&gt; can be arbitrarily
large. We further argue that &lt;span class=&#34;math inline&#34;&gt;\(y_0 &amp;lt;
0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(f(x) = -\infty\)&lt;/span&gt;
everywhere. Without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(y_0 = -1\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T, y_0]^T,c) = H^-([-s^T/y_0,
-1]^T,c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice that this lower half-space &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T, -1]^T, c)\)&lt;/span&gt; is exactly the
epigraph of the affine function &lt;span class=&#34;math display&#34;&gt;\[
h_{s, c}(x) = s^T x - c
\]&lt;/span&gt; because for any &lt;span class=&#34;math inline&#34;&gt;\((x, y) \in \R^n
\times \R\)&lt;/span&gt; that belongs to &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T,
-1]^T, c)\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
s^T x - y &amp;amp;\le c \\
y &amp;amp;\ge s^T x - c
\end{aligned}
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\epi f
\subseteq H^-([s^T, -1]^T, c) = \epi{h_{s, c}}\)&lt;/span&gt; indicates that
&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in \R^n, f(x) \ge h_{s,
c}(x)\)&lt;/span&gt;. Consequently, &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\epi f &amp;amp;= \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T,
-1]^T,c) \\
&amp;amp;= \bigcap_{\epi{h_{s, c}} \supseteq \epi f} \epi{h_{s, c}} \\
&amp;amp;= \bigcap_{f \ge h_{s, c}} \epi{h_{s, c}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; can be
written as the pointwise supremum of the affine functions who are less
than &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f = \sup_{h \le f} h
\]&lt;/span&gt; The normal vectors of those affine functions that are less
than &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and that pass through &lt;span class=&#34;math inline&#34;&gt;\((x_0, f(x_0))\)&lt;/span&gt; essentially forms the
sub-differential of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider the set &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_f &amp;amp;= \{ (y, t) \in \R^n \times \R: y^T x - t \le f(x), \forall x
\in \R^n \} \\
&amp;amp;= \{ (y, t) \in \R^n \times \R: \sup_{x}(y^T x - f(x)) \le t \} \\
&amp;amp;\triangleq \epi{f^*}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
f^*(y) \triangleq \sup_{x}(y^T x - f(x))
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f^*(y)\)&lt;/span&gt; is called the
&lt;strong&gt;conjugate function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Difference Equation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</guid>
      <description>

&lt;h2 id=&#34;difference-equation&#34;&gt;Difference Equation&lt;/h2&gt;
&lt;p&gt;To solve difference equation like &lt;span class=&#34;math inline&#34;&gt;\(x_t =
a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \dots + a_0\)&lt;/span&gt;, we first
rewrite it into the matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left[ \begin{array} \\
x_t \\
x_{t-1} \\
\vdots \\
x_2 \\
x_1
\end{array} \right] =
\underbrace{
\left[
\begin{array} \\
a_{t-1} &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \vdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0
\end{array}
\right]
}_{A}
\left[ \begin{array} \\
x_{t-1} \\
x_{t-2} \\
\vdots \\
x_1 \\
x_0
\end{array} \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To solve it, we try to find the eigenvalues and eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A - \lambda I = \left[
\begin{array} \\
a_{t-1} - \lambda &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots  &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{array}
\right]
\]&lt;/span&gt; Apply Laplacian expansion along the first column to get its
determinant as &lt;span class=&#34;math display&#34;&gt;\[
\det (A - \lambda I) = (a_{t-1} -\lambda) (-\lambda)^{t-1} - \det
B_{t-1}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
B_{t-1} =
\underbrace{
\begin{bmatrix}
a_{t-2} &amp;amp; a_{t-3} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{bmatrix}
}_\text{$t-1$ columns}
\]&lt;/span&gt; Apply Laplacian expansion along the first column of &lt;span class=&#34;math inline&#34;&gt;\(B_{n-1}\)&lt;/span&gt; to give the following recurrence
relation: &lt;span class=&#34;math display&#34;&gt;\[
\left.
\begin{array} \\
\det B_{t-1} = a_{t-2} (-\lambda)^{t-2} - \det B_{t-2} \\
\det B_1 = a_0
\end{array}
\right\} \Rightarrow
\det B_{t-1} = (-1)^t (a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} +
\dots + a_0)
\]&lt;/span&gt; In all, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det (A - \lambda I) &amp;amp;= (a_{t-1} - \lambda)(-\lambda)^{t-1} - (-1)^t
(a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} + \dots + a_0) \\
&amp;amp;= (-1)^t (\lambda^t - a_{t-1} \lambda^{t-1} - a_{t-2} \lambda^{t-2}
- \dots - a_0)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After solving the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1
\ge \dots \ge \lambda_t\)&lt;/span&gt; and corresponding eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\v}{\mathrm{v}} \v_1, \dots,
\v_t\)&lt;/span&gt; from above equation, we can rewrite the vector formed by
the initial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; terms as the linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_t\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\x_0 = \left[
x_{t-1}, \cdots, x_0
\right]^T
= c_1 \v_1 + \dots c_t \v_t
\]&lt;/span&gt; Then for every &lt;span class=&#34;math inline&#34;&gt;\(n \ge t\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
x_n = \left[ A^{n-t+1} \x_0 \right]_0 = \left[ \lambda_1^{n-t+1} c_1
\v_1 + \dots + \lambda_t^{n-t+1} c_t \v_t\right]_0
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt; will be asymptotic to
&lt;span class=&#34;math inline&#34;&gt;\(\lambda_1^{n}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>统计量</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;定义：设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为取自总体的一组样本，若函数&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;不包含总体分布中的任何参数，则称&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;为&lt;strong&gt;统计量&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;样本均值和样本方差&#34;&gt;样本均值和样本方差&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{样本均值：}\bar X = \frac{1}{n} \sum_{i=1}^n X_i \\
\text{样本方差：}S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2 =
\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(m_k = \frac{1}{n} \sum_{i=1}^n
X_i^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a_k = \frac{1}{n} \sum_{i=1}^n (X_i - \bar
X)^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩&lt;/strong&gt;，这些也都是统计量。特别地，当&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;时，我们令&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq a_2 = \frac{1}{n} \sum_{i=1}^n
X_i^2 - \bar X^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;由于统计量是随机变量的函数，故统计量也是随机变量。设总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)
= \mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) =
\sigma^2\)&lt;/span&gt;，关于统计量有如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
&amp;amp; \E(\bar X) = \mu, \Var(\bar X) = \frac{\sigma^2}{n} \\
\notag \\
&amp;amp; \E(S^2) = \sigma^2, \E(S_n^2) = \frac{n-1}{n} \sigma^2 \\
\notag \\
&amp;amp; \bar X \stackrel{P}{\to} \mu, S^2 \stackrel{P}{\to} \sigma^2,
S_n^2 \stackrel{P}{\to} \sigma^2
\end{gather}
\]&lt;/span&gt; 有关证明如下： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\E(\bar X) = \E (\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n}
\sum_{i=1}^n \E(X_i) = \mu \\
\Var(\bar X) = \Var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2}
\sum_{i=1}^n \Var(X_i) = \frac{\sigma^2}{n}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}[t]
\E(S^2) &amp;amp;= \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n \E (X_i^2 ) - n \E(\bar X^2)
\big) \\
&amp;amp;\Downarrow_ {\E(X_i^2) = \Var(X_i) + \E^2(X_i) = \sigma^2 + \mu^2,
\E(\bar X^2) = \Var(\bar X) + \E^2(\bar X) = \frac{\sigma^2}{n} + \mu^2}
\\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n (\sigma^2 + \mu^2) - n
(\frac{\sigma^2}{n} + \mu^2) \big) \\
&amp;amp;= \sigma^2
\end{aligned}
\begin{aligned}[t]
\E(S_n^2) &amp;amp;= \E [\frac{n-1}{n} \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n
\bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar
X^2)] \\
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;归根结底，样本方差使用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;的原因是，其使用的“均值”为&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，这导致了一个自由度的缺失。而假设&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知，我们定义一个新的统计量&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;^2 = \frac{1}{n} \sum_{i=1}^N (X_i - \mu)^2
= \frac{1}{n} (n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n
X_i^2)\)&lt;/span&gt;，我们会发现&lt;span class=&#34;math inline&#34;&gt;\(\E(S&amp;#39;^2) =
\sigma^2\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E(S&amp;#39;^2) = \frac{1}{n} \E(n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n
X_i^2) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu \E(\bar X) + \sum_{i=1}^n \E
(X_i^2)) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu^2 + \sum_{i=1}^n (\sigma^2 + \mu^2))
\\
&amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt; 至于三个统计量的依概率收敛证明，根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/a&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \mu \\
\frac{1}{n} \sum_{i=1}^n X_i^2 \stackrel{P}{\to} \frac{1}{n}
\sum_{i=1}^n \E (X_i^2) = \sigma^2 + \mu^2
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt;
0\)&lt;/span&gt;，存在&lt;span class=&#34;math inline&#34;&gt;\(N_1, N_2 &amp;gt;
0\)&lt;/span&gt;，使得当&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; \max(N_1,
N_2)\)&lt;/span&gt;时，始终有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 &amp;lt; P(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 + \mu^2)| \ge
\epsilon / 2) &amp;lt; \delta / 2 \\
0 &amp;lt; P(|\mu^2 - \bar X^2| \ge \epsilon / 2) &amp;lt; \delta / 2 \\
\end{gather}
\]&lt;/span&gt; 记事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 +
\mu^2)| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\bar X^2
- \mu^2| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2 -
\sigma^2| \ge \epsilon / 2\)&lt;/span&gt;。由于&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 +
\mu^2)| + |\mu^2 - \bar X^2| \ge |\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar
X^2 - \sigma^2|\)&lt;/span&gt;，则事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;发生时，事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;至少发生其中之一，即事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;是事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;并集的子集。故&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
0 &amp;lt; P(\text{事件$C$}) \le P(\text{事件$A$ 或 事件$B$}) \le
P(\text{事件$A$}) + P(\text{事件$B$}) &amp;lt; \delta
\]&lt;/span&gt; 即&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; P(|\frac{1}{n}
\sum_{i=1}^n X_i^2 - \bar X^2 \ - \sigma^2| \ge \epsilon) &amp;lt;
\delta\)&lt;/span&gt;。又由于对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta &amp;gt; 0\)&lt;/span&gt;该结论都成立，故 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\lim_{n \to \infty} P(|\underbrace{\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar
X^2}_{S_n^2} - \sigma^2| \ge \epsilon) = 0 \iff \\
S_n^2 \stackrel{P}{\to} \sigma^2
\end{gathered}
\]&lt;/span&gt; 运用类似的&lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta\)&lt;/span&gt;语言，我们可以证明&lt;span class=&#34;math inline&#34;&gt;\(S^2 =
\frac{n}{n-1} S_n^2 \stackrel{P}{\to} \sigma^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;次序统计量&#34;&gt;次序统计量&lt;/h2&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\((X_{(1)}, \dots,
X_{(n)})\)&lt;/span&gt;为样本&lt;span class=&#34;math inline&#34;&gt;\((X_1, \dots,
X_n)\)&lt;/span&gt;排序后的结果，则&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)} = \min
(X_1, \dots, X_n), X_{(n)} = \max (X_1, \dots,
X_n)\)&lt;/span&gt;亦是统计量。&lt;/p&gt;
&lt;p&gt;记&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)},
X_{(n)}\)&lt;/span&gt;的概率密度函数分别为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(1)}}, p_{X_{(n)}}\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p_{X_{(1)}}(u) = n \big( 1 - P_X(u) \big)^{n-1} p_X(u) \\
p_{X_{(n)}}(u) = n \big( P_X(u) \big)^{n-1} p_X(u)
\end{gather}
\]&lt;/span&gt; 记&lt;span class=&#34;math inline&#34;&gt;\(X_{(k)}\)&lt;/span&gt;的概率密度函数为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(k)}}\)&lt;/span&gt;，则…&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>参数估计</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid>
      <description>

&lt;h2 id=&#34;点估计&#34;&gt;点估计&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim
p(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;为总体的一组样本，若用一个统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;来估计&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。&lt;/p&gt;
&lt;h3 id=&#34;矩估计&#34;&gt;矩估计&lt;/h3&gt;
&lt;p&gt;矩估计的思想就是就是替换思想，即用样本矩替换总体矩。设总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩（moment）&lt;span class=&#34;math inline&#34;&gt;\(\mu_k = \E[X^k]\)&lt;/span&gt;，总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;中心矩（moment about mean）&lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \E[(X - \mu)^k]\)&lt;/span&gt;；样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩为&lt;span class=&#34;math inline&#34;&gt;\(m_k = \frac 1 n \sum_{i=1}^n
X_i^k\)&lt;/span&gt;，样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩为&lt;span class=&#34;math inline&#34;&gt;\(a_k = \frac 1 n \sum_{i=1}^n (X_i - \bar
X)^k\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;仅以中心矩为例，如果未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta =
\varphi(\mu_1, \dots, \mu_p)\)&lt;/span&gt;，则其估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \varphi(m_1, \dots,
m_p)\)&lt;/span&gt;，这种估计总体未知参数的方法叫作矩估计法。矩估计往往不唯一，如设&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda)\)&lt;/span&gt;，则由于&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;；又&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\frac 1 n \sum_{i=1}^n X_i^2 - \bar
X^2\)&lt;/span&gt;。此时往往采用较低阶的矩来估计未知参数。&lt;/p&gt;
&lt;h3 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;设总体有分布律&lt;span class=&#34;math inline&#34;&gt;\(X \sim
P(X=x;\theta)\)&lt;/span&gt;或密度函数&lt;span class=&#34;math inline&#34;&gt;\(X \sim
p(x;\theta)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots,
x_n\)&lt;/span&gt;为取自总体的一组样本观测值，将样本的联合分布律或联合密度函数看作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的函数： &lt;span class=&#34;math display&#34;&gt;\[
L(\theta) = \prod_{i=1}^n P(X=x_i;\theta)\ \text或 \ L(\theta) =
\prod_{i=1}^n p(x_i;\theta)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta)\)&lt;/span&gt;又称作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的似然函数，似然函数满足关系式&lt;span class=&#34;math inline&#34;&gt;\(L(\hat \theta) = \max_{\theta}
L(\theta)\)&lt;/span&gt;的解&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计量。&lt;/p&gt;
&lt;p&gt;由于最大似然估计对样本使用较为充分，通常其方差较小。&lt;/p&gt;
&lt;h3 id=&#34;优良性评判&#34;&gt;优良性评判&lt;/h3&gt;
&lt;h4 id=&#34;无偏性unbiasedness&#34;&gt;无偏性（unbiasedness）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个无偏估计（量），否则则称作有偏估计（量）。如果有
&lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} \E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个渐进无偏估计（量）。渐进无偏亦记作&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta \stackrel{L_1}{\to}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值也许偏大，取另一些样本时估计量也许偏小，但多次取样本进行估计，平均来讲偏差为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即&lt;strong&gt;系统误差&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;有趣的是，有一些估计虽然不满足无偏性，但满足一致性，所以我们依旧会采用这些估计。比如在估计正态总体的方差时，令&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq
a_2\)&lt;/span&gt;，则最大似然估计为&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;，该估计不满足无偏性，但满足一致性；&lt;span class=&#34;math inline&#34;&gt;\(S_n^2/k\)&lt;/span&gt;形式（&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;为待定系数）的最小均方差估计为&lt;span class=&#34;math inline&#34;&gt;\(n S_n^2/(n+1)\)&lt;/span&gt;（&lt;a href=&#34;https://www.wikiwand.com/en/Mean_squared_error#Variance&#34;&gt;参此&lt;/a&gt;），也不满足无偏性，但满足一致性。&lt;/p&gt;
&lt;h4 id=&#34;最小方差minimum-variance&#34;&gt;最小方差（minimum-variance）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1 = \hat
\theta_2\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的两个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat \theta_1) \le \Var(\hat
\theta_2)\)&lt;/span&gt;，且至少有一个&lt;span class=&#34;math inline&#34;&gt;\(\theta \in
\Theta\)&lt;/span&gt;使得该不等式严格成立，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1\)&lt;/span&gt;比&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_2\)&lt;/span&gt;有效。&lt;/p&gt;
&lt;h4 id=&#34;一致性consistency&#34;&gt;一致性（consistency）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，若对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\hat \theta - \theta| &amp;gt; \epsilon) = 0 \\
\equiv \\
\lim_{n \to \infty} P(|\hat \theta - \theta| \le \epsilon) = 1
\]&lt;/span&gt; 则称估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;具有一致性，一致性描述的是一个估计量&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/#依概率收敛（convergence in probability）&#34;&gt;依概率收敛&lt;/a&gt;到真实值的过程，一致性亦记作&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta \stackrel{P}{\to}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;一致性是一个很基本（“基本”不是指“一致性是其他两条性质的必要条件”）的要求：随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。&lt;/p&gt;
&lt;h3 id=&#34;cramer-rao不等式&#34;&gt;Cramer-Rao不等式&lt;/h3&gt;
&lt;p&gt;实际上，点估计量不仅仅可以估计未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;本身（假设为一元情况），更可以估计未知参数的某个函数&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，即给定总体的一组样本&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;，用统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g = \hat g(X_1, \dots, X_n)\)&lt;/span&gt;估计&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;。估计量最好的效果便是达到最小方差无偏（minimum-variance
unbiased &amp;lt;MVU&amp;gt;）估计，Cramer-Rao不等式给出了点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g\)&lt;/span&gt;方差的一个下界。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{cr} \Var(\hat g) \ge (g&amp;#39;(\theta))^2 / (nI(\theta))
\end{equation}
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(I(\theta) = \int
[(\frac{\partial p(x;\theta)}{\partial \theta})^2 / p(x;\theta)] \d
x\)&lt;/span&gt;为Fisher Information。当&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)
= \theta\)&lt;/span&gt;，即只估计未知参数本身时，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat g) \ge 1 / (nI(\theta))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;成立有一定的条件，其本身就暗含了&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial p(x;\theta)}{\partial
\theta}\)&lt;/span&gt;存在及&lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\theta)\)&lt;/span&gt;存在的&lt;strong&gt;条件&lt;/strong&gt;。记
&lt;span class=&#34;math display&#34;&gt;\[
S = S(X_1, \dots, X_n, \theta) = \sum_{i=1}^n \frac{\partial \ln
p(X_i;\theta)} {\partial \theta} = \sum_{i=1}^n [\frac{\partial
p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\int p(x;\theta)\ \d x =
1\)&lt;/span&gt;，此式两边同时对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导，并&lt;strong&gt;假定&lt;/strong&gt;此处求导可以移至积分号内部，可得到&lt;span class=&#34;math inline&#34;&gt;\(\int \frac{\partial p(x;\theta)}{\partial \theta}
\d x = 0\)&lt;/span&gt;。根据&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/#Law of the Unconscious Statistician&#34;&gt;LOTUS&lt;/a&gt;，
&lt;span class=&#34;math display&#34;&gt;\[
\E [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
= \int [\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta)]
p(x;\theta)\ \d x
= \int \frac{\partial p(x;\theta)} {\partial \theta}\d x = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;的独立性，
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Var(S) &amp;amp;= \sum_{i=1}^n \Var [\frac{\partial p(X_i;\theta)}
{\partial \theta} / p(X_i;\theta)] \\
&amp;amp;= \sum_{i=1}^n \{ \E [\big (\frac{\partial p(X_i;\theta)} {\partial
\theta} / p(X_i;\theta) \big)^2] - \E^2 [\frac{\partial p(X_i;\theta)}
{\partial \theta} / p(X_i;\theta)] \} \\
&amp;amp;= \sum_{i=1}^n \E [\big (\frac{\partial p(X_i;\theta)} {\partial
\theta} / p(X_i;\theta) \big)^2] \\
&amp;amp;= n \int \big (\frac{\partial p(x;\theta)} {\partial \theta} /
p(x;\theta) \big)^2 p(x;\theta)\ \d x \\
&amp;amp;= n I(\theta)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根据协方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{cov_prop} [\Cov(\hat g, S)]^2 \le \Var(\hat g) \Var(S) =
\Var(\hat g) n I(\theta)
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又&lt;span class=&#34;math inline&#34;&gt;\(\E(S) = 0\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) = \E (\hat g S) &amp;amp;= \int \dots \int \hat g(x_1,
\dots, x_n) \sum_{i=1}^n [\frac{\partial p(x_i;\theta)} {\partial
\theta} / p(x_i;\theta)] \prod_{i=1}^n p(x_1;\theta)\ \d x_1 \dots \d
x_n \\
&amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \frac{\partial
p(x_1;\theta) \dots p(x_n;\theta)} {\partial \theta}\ \d x_1 \dots \d
x_n
\end{aligned}
\]&lt;/span&gt; &lt;strong&gt;假定&lt;/strong&gt;此处对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导可以移至积分号外部， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S)
&amp;amp;= \frac \partial{\partial \theta} \int \dots \int \hat g(x_1,
\dots, x_n) p(x_1;\theta) \dots p(x_n;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \frac \partial{\partial \theta} g(\theta) = g&amp;#39;(\theta)
\end{aligned}
\]&lt;/span&gt; 将上式重新带入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cov_prop}\)&lt;/span&gt;，从而得到&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/56411276/answer/204992057&#34;&gt;对Cramer-Rao不等式的理解&lt;/a&gt;
|| &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Wiki
(see the multi-variate case)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;区间估计&#34;&gt;区间估计&lt;/h2&gt;
&lt;p&gt;点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作&lt;strong&gt;置信区间&lt;/strong&gt;（confidence
interval）。&lt;/p&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim
f(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;为总体的一组样本，给定一个很小的数&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \alpha &amp;lt; 1\)&lt;/span&gt;，若有统计量&lt;span class=&#34;math inline&#34;&gt;\(\theta_l = \theta_l (X_1, \dots, X_n) \le
\theta_r(X_1, \dots, X_n) = \theta_r\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
P(\theta_l \le \theta \le \theta_r) \ge 1 - \alpha
\]&lt;/span&gt; 我们称&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\([\theta_l,
\theta_r]\)&lt;/span&gt;的&lt;strong&gt;置信水平&lt;/strong&gt;（confidence level），&lt;span class=&#34;math inline&#34;&gt;\(\theta_l\)&lt;/span&gt;为&lt;strong&gt;置信下限&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\theta_r\)&lt;/span&gt;为&lt;strong&gt;置信上限&lt;/strong&gt;。一般来说置信水平不唯一，因为若&lt;span class=&#34;math inline&#34;&gt;\(1 -
\alpha\)&lt;/span&gt;是某个区间的置信水平，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\alpha &amp;lt; \tilde \alpha &amp;lt; 1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(1 - \tilde
\alpha\)&lt;/span&gt;亦是该区间的置信水平。故一般的“置信水平”是这一系列置信水平中的最大者。&lt;/p&gt;
&lt;h3 id=&#34;枢轴变量法&#34;&gt;枢轴变量法&lt;/h3&gt;
&lt;p&gt;区间估计一般采用枢轴变量法，枢轴变量法的一般步骤为：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;（如&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;的一个函数&lt;span class=&#34;math inline&#34;&gt;\(G = G(\theta, \hat
\theta)\)&lt;/span&gt;（称作枢轴（pivot）函数），且&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;完全已知，且其分布与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关，&lt;/li&gt;
&lt;li&gt;对任何常数&lt;span class=&#34;math inline&#34;&gt;\(a &amp;lt; b\)&lt;/span&gt;，不等式&lt;span class=&#34;math inline&#34;&gt;\(a \le G(\theta, \hat \theta) \le
b\)&lt;/span&gt;能够改写成等价的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le
B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;仅与&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta,a,b\)&lt;/span&gt;有关，与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;取&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;的上&lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha / 2}) = 1 -
\alpha/2\)&lt;/span&gt;）及上&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{1 - \alpha / 2}) = \alpha /
2\)&lt;/span&gt;），此时有&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha/2}) - F(w_{1
- \alpha / 2}) = 1 - \alpha\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
P(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}) = 1 -
\alpha
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2} \le G(\theta, \hat
\theta) \le w_{\alpha/2}\)&lt;/span&gt;可改写为对应的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;的形式，且&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;仅与估计量和两个分位点有关，&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;就构成了&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在构造枢轴函数时，一般会使用一些现有结论，比如&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#中心极限定理&#34;&gt;中心极限定理&lt;/a&gt;的近似、&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/&#34;&gt;三大分布与正态总体的抽样分布&lt;/a&gt;等等。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Matrix Identity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</link>
      <pubDate>Wed, 22 Dec 2021 15:51:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</guid>
      <description>
&lt;p&gt;A useful matrix identity: &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; It can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} &amp;amp;= PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1} \\
\end{aligned}
\]&lt;/span&gt; Its reduced form: &lt;span class=&#34;math display&#34;&gt;\[
(I_N+AB)^{-1}A = A(I_M+BA)^{-1}
\]&lt;/span&gt; It can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(I_N+AB)^{-1}A &amp;amp;= A(I_M+BA)^{-1} \\
\iff A &amp;amp;= (I_N + AB)A(I_M + BA)^{-1} \\
\iff A &amp;amp;= (A + ABA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A(I_M + BA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>贝叶斯推断</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/</link>
      <pubDate>Sun, 14 May 2023 18:27:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/</guid>
      <description>

&lt;h2 id=&#34;贝叶斯公式&#34;&gt;贝叶斯公式&lt;/h2&gt;
&lt;p&gt;在全概率公式之下，有 &lt;span class=&#34;math display&#34;&gt;\[
P(B_i | A) = \frac{P(A B_i)}{P(A)} = \frac{P(B_i) P(A|B_i)}{P(A)} =
\frac{P(B_i) P(A|B_i)} {\sum_j P(B_j) P(A|B_j)}
\]&lt;/span&gt;
这便是贝叶斯公式。贝叶斯公式中的项目也有它们在贝叶斯学派中相应的称呼：
&lt;span class=&#34;math display&#34;&gt;\[
\text{posterior} = \frac{\text{prior} \times
\text{likelihood}}{\text{evidence}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯估计&#34;&gt;贝叶斯估计&lt;/h2&gt;
&lt;p&gt;在参数估计问题中，记&lt;span class=&#34;math inline&#34;&gt;\(D = \{ X_1, \dots,
X_n \}\)&lt;/span&gt;为样本、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;为参数，并用将&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;代入后验概率中的事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;代入后验概率中的&lt;span class=&#34;math inline&#34;&gt;\(B_i\)&lt;/span&gt;，我们得到：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(\theta | D) = \frac{P(\theta) P(D|\theta)} {\sum_j P(\theta_j)
P(D|\theta_j)} \\
\]&lt;/span&gt; 取决于&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和单个样本的取值是连续型或是离散型，上式中的&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;可代表密度函数或分布律，而分母中的求和运算应当在&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;取值连续的时候被替换在&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;所有可行范围内的积分运算。比较值得注意的一点是，贝叶斯推断为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;引入了先验分布，而在频率学派中，参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;是不存在什么先验分布的。&lt;/p&gt;
&lt;p&gt;由于贝叶斯学派假设参数服从某个分布，在先验、似然已知的情况下，我们可以求得后验的解析解、近似解，或者通过Metropolis-Hastings等算法对后验直接采样。无论采取哪种方式，我们都可以获得后验的（近似）密度函数。如此一来，贝叶斯估计其实天然是一种区间估计。除开&lt;u&gt;直接获取后验密度函数&lt;/u&gt;，我们再讨论一些贝叶斯方法中常见的其他的估计方法。&lt;/p&gt;
&lt;h3 id=&#34;最大后验估计&#34;&gt;最大后验估计&lt;/h3&gt;
&lt;p&gt;最大后验估计（maximum a posteriori estimation）得到的点估计是以下：
&lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = \arg \max_{\theta} \frac{P(\theta) P(D | \theta)}{\int
P(\theta&amp;#39;) P(D|\theta&amp;#39;) \d \theta&amp;#39;} = \arg \max_{\theta}
{P(\theta) P(D | \theta)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;最小均方差估计&#34;&gt;最小均方差估计&lt;/h3&gt;
&lt;p&gt;最小均方差估计（minimum mean squared error
estimation）得到的点估计是以下： &lt;span class=&#34;math display&#34;&gt;\[
\theta^\star = \arg \min_{\hat \theta} \E_{\theta \sim \text{posterior}}
[(\hat \theta - \theta)^2]
\]&lt;/span&gt; 换言之，此时的到的点估计&lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt;即为&lt;span class=&#34;math inline&#34;&gt;\(\E_{\theta \sim \text{posterior}}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;可信区间&#34;&gt;可信区间&lt;/h3&gt;
&lt;p&gt;可信区间（credible interval），或者叫最大后验密度（highest posterior
density）得到的是一个区间，该区间是使得随机变量落在该区间内的概率大于某一个数字（常用的有95%、98%）的最小区间。&lt;/p&gt;
&lt;h3 id=&#34;案例抛硬币&#34;&gt;案例：抛硬币&lt;/h3&gt;
&lt;p&gt;在具体的抛硬币案例中（抛&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;次硬币，其中&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次正面朝上，未知参数为正面朝上概率&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;）， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{likelihood: } P(n|p) = {N \choose n} p^n (1-p)^{N-n} \\
\text{prior: } \rho(p) = 1 \\
\text{evidence: } \int_0^1 P(n|p) \rho(p) \d p \\
\text{posterior: } \rho(p|n)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \rho(p|n) = \frac{P(n|p) \rho(p)}{\int_0^1 P(n|p) \rho(p) \d p} \\
&amp;amp;= \frac{{N \choose n} p^n (1-p)^{N-n} \rho(p)}{\int_0^1 {N \choose
n} x^n (1-x)^{N-n} \rho(x) \d x} \\
&amp;amp;= \frac{p^n (1-p)^{N-n} 1}{\int_0^1 x^n (1-x)^{N-n} 1 \d x} \\
&amp;amp;= \frac{p^{n+1-1} (1-p)^{N-n+1-1}} {\underbrace{\int_0^1 x^{n+1-1}
(1-x)^{N-n+1-1} \d x}_{\mathrm{Beta}(n+1, N-n+1)}} \\   
&amp;amp;= \mathrm{Beta}(p|n+1,N-n+1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(n+1,
N-n+1)\)&lt;/span&gt;表示&lt;strong&gt;&lt;em&gt;Beta函数&lt;/em&gt;&lt;/strong&gt;在&lt;span class=&#34;math inline&#34;&gt;\((n+1, N-n+1)\)&lt;/span&gt;处的取值；&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(p|\underbrace{n+1}_{a&amp;gt;0},
\underbrace{N-n+1}_{b&amp;gt;0})\)&lt;/span&gt;表示参数&lt;span class=&#34;math inline&#34;&gt;\(a =n+1,
b=N-n+1\)&lt;/span&gt;时的&lt;strong&gt;&lt;em&gt;Beta分布&lt;/em&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;需要注意的是&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(p|1,1)\)&lt;/span&gt;等价于&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;之间的均匀分布： &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{Beta}(p|1,1) = \text{Uniform}(p|0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在prior为Beta分布、likelihood为二项分布时，得到的posterior依旧是Beta分布（不过该Beta分布的参数和prior中Beta分布的参数有所不同），此时的prior和likelihood称作&lt;strong&gt;conjugate
distributions&lt;/strong&gt;，此时的prior称作likelihood的&lt;strong&gt;conjugate
prior&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Beta分布是二项分布的conjugate prior；高斯分布是高斯分布的conjugate
prior。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Quadratic Form</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</link>
      <pubDate>Fri, 05 May 2023 10:21:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</guid>
      <description>

&lt;h2 id=&#34;quadratic-form&#34;&gt;Quadratic Form&lt;/h2&gt;
&lt;p&gt;Quadratic form involves many concepts like real symmetric matrix,
positive definiteness and singular value decomposition. It can be quite
helpful to glue these things together.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;quadratic function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; variables, or say a vector &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, is the sum of second-order terms:
&lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \sum_{i=1}^n \sum_{j=1}^n c_{ij} x_i x_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above summation can be simplified as matrix product &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n \times
n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = c_{ij}\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; is called the
&lt;strong&gt;quadratic form&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In essence, there is a nicer formulation for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Firstly define the &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = \frac{1}{2} (c_{ij} + c_{ji})\)&lt;/span&gt;.
It suffices to show &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a real
symmetric matrix and &lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \x^T A \x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus the discussion of quadratic form usually encompasses the real
symmetric matrix.&lt;/p&gt;
&lt;h3 id=&#34;positive-definiteness&#34;&gt;Positive Definiteness&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be a real symmetric
matrix. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;strong&gt;positive
definite&lt;/strong&gt; if and only if the quadratic form of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive. Specifically, for every
&lt;span class=&#34;math inline&#34;&gt;\(\x \ne 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;A real symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
positive definite if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvalues are positive.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For every &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenpair &lt;span class=&#34;math inline&#34;&gt;\((\lambda, \v)\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(\v^T A \v = \lambda \v^T \v &amp;gt; 0 \Rightarrow
\lambda &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s spectral decomposition
as &lt;span class=&#34;math inline&#34;&gt;\(A = Q \Lambda Q^T\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Q Q^T = I\)&lt;/span&gt;. For every &lt;span class=&#34;math inline&#34;&gt;\(\x &amp;gt; 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\x^T A \x = \overbrace{\x^T Q}^{y^T} \Lambda \overbrace{Q^T \x}^{y} &amp;gt;
0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One possible reason why we would like to define positive definiteness
on real symmetric matrix is that, any real matrix can be easily
decomposed into the addition of a real symmetric matrix and a real
&lt;strong&gt;skew-symmetric&lt;/strong&gt; matrix whose quadratic form is zero:
&lt;span class=&#34;math display&#34;&gt;\[
A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\frac{A - A^T}{2}\)&lt;/span&gt;’s
quadratic form is zero and it makes no contribution to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s, the only component of interest will
be the real symmetric &lt;span class=&#34;math inline&#34;&gt;\(\frac{A +
A^T}{2}\)&lt;/span&gt;. So why not just focus on the real symmetric
matrix?&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的收敛</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</link>
      <pubDate>Wed, 13 Jul 2022 14:41:05 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</guid>
      <description>

&lt;h2 id=&#34;依概率收敛convergence-in-probability&#34;&gt;依概率收敛（convergence in
probability）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成的序列。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则意味着当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能总和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) =
1\)&lt;/span&gt;，抑或是，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n -
c| \ge \epsilon) =
0\)&lt;/span&gt;），则称该随机变量序列&lt;strong&gt;依概率收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{P}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt;
0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt;
0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有
&lt;span class=&#34;math display&#34;&gt;\[
1 - \delta &amp;lt; P(|X_n - c| &amp;lt; \epsilon) \le 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;依概率收敛的一个例子便是&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/a&gt;，即当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。&lt;/p&gt;
&lt;h2 id=&#34;几乎必然收敛almost-sure-convergence&#34;&gt;几乎必然收敛（almost-sure
convergence）&lt;/h2&gt;
&lt;p&gt;在某些情况下，若随机变量序列能够和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;几乎接近，我们说它几乎必然收敛。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(P(\lim_{n \to \infty} X_n = c) =
1\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;几乎必然收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{a.s.}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt;
0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有
&lt;span class=&#34;math display&#34;&gt;\[
P(|X_n - c| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，几乎必然收敛和依概率收敛是不等价的，因为&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty}
f(x_n)\)&lt;/span&gt;中的极限符号不总是能够交换到函数&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部，举个简单的例子： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\{ x_n \} = -\frac{1}{n}, \
f(x) = \begin{cases}
x^2 - 1, &amp;amp; -1 \le x &amp;lt; 0 \\
x, &amp;amp; x \ge 0
\end{cases} \\
\lim_{n \to \infty} f(x_n) = \lim_{n \to \infty}(\frac{1}{n^2}-1) = -1
\ne f(\lim_{n \to \infty} x_n) = f(0) = 0
\end{gathered}
\]&lt;/span&gt; 注意&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是右连续的，这也意味着，我们可以找到类似的右连续的分布函数&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，使得极限符号不能被移至&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;内部。也就是说，几乎必然收敛和依概率收敛是不等价的，而显然，几乎必然收敛是强于依概率收敛的。&lt;/p&gt;
&lt;h2 id=&#34;l_p收敛convergence-in-l_p&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛（convergence in &lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，对于某个&lt;span class=&#34;math inline&#34;&gt;\(p
&amp;gt; 0\)&lt;/span&gt;，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} \E(|| X_n - c||_p^p) =
0\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{L_p}{\to} c\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;均方收敛&#34;&gt;均方收敛&lt;/h3&gt;
&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛又称作均方收敛。根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/a&gt;， &lt;span class=&#34;math display&#34;&gt;\[
P(|X_n-\E(X_n)| \ge \epsilon) \le \frac{\Var(X_n)}{\epsilon^2} =
\frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2}
\]&lt;/span&gt; 在两边取&lt;span class=&#34;math inline&#34;&gt;\(n \to
\infty\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|X_n-\E(X_n)| \ge \epsilon) \le \lim_{n \to
\infty} \frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2} = 0
\]&lt;/span&gt;
即均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也强于依概率收敛；但均方收敛和几乎必然收敛之间并没有推导关系。&lt;/p&gt;
&lt;h2 id=&#34;依分布收敛convergence-in-distribution&#34;&gt;依分布收敛（convergence
in distribution）&lt;/h2&gt;
&lt;p&gt;前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，让&lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;的分布函数，如果存在一个分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} F_n(x) =
F(x)\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;依分布收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{d}{\to} F\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;收敛到随机变量&#34;&gt;“收敛到随机变量”&lt;/h2&gt;
&lt;p&gt;除了上述讨论的收敛到值、收敛到（分布）函数的情况外，另外一个比较有趣的话题是“收敛到随机变量”，或者说“两个随机变量相等”是一个怎样的概念？&lt;/p&gt;
&lt;p&gt;我们讨论概率的时候，会涉及到两个函数：一个是概率函数，另一个是随机变量这一从事件到数字的映射。方便起见我们令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;为两个随机变量，随机变量相等，则意味着这两个从事件到数字的映射相等，进而&lt;span class=&#34;math inline&#34;&gt;\(P(X = Y) = 1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;映射相等，意味着定义域、值域、映射关系完全相等。如果我有两个骰子，令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;表示第一个骰子掷出的点数、&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;表示第二个骰子掷出的点数，那么&lt;span class=&#34;math inline&#34;&gt;\(X =
Y\)&lt;/span&gt;吗？答案是不，因为这两个随机变量的定义域不相等：&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的定义域表示第一个骰子的所有可能事件，&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的定义域表示第二个骰子的所有可能事件；虽然两个骰子掷出的点数都只能是1、2、3、4、5、6，但这代表的仅是值域相同，而“第一个骰子掷出一”（注意这里避免使用阿拉伯数字，以表示它是一个事件）这个事件和“第二个骰子掷出一”是不一样的，因为&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;不会因为第二个骰子掷出一而取为1。&lt;/p&gt;
&lt;p&gt;至此，我们更加能够体会到“收敛到函数”的含义：只是碰巧该随机变量的分布函数在极限情况下与某个满足分布函数条件的函数相同而已。而“收敛到值”相较于“收敛到函数”，提供的信息更少，因为“收敛到值”只是告诉我们在极限情况下，该随机变量在某一处取值的概率趋近于&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;而已。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B&#34;&gt;随机变量的收敛&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Receiver Operator Characteristic</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/receiver-operator-characteristic/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/receiver-operator-characteristic/</guid>
      <description>

&lt;h2 id=&#34;receiver-operator-characteristic&#34;&gt;Receiver Operator
Characteristic&lt;/h2&gt;
&lt;p&gt;Receiver operator characteristic (ROC) curve connects the consecutive
TPR-FPR 2-D points, which are obtained by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ranking the testcases according to the probability of being
positive from high to low;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;repeatedly labelling the current testcase as positive and
re-computing the TPR-FPR;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and proceeding to the next.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that true positive rate (TPR, also the recall) and false
positive rate (FPR) are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{TPR} = \frac{\text{TP}}{\text{TP + FN}},\ \text{FPR} =
\frac{\text{FP}}{\text{FP} + \text{TN}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When we lower the threshold (from &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;) of the probability of estimating the
sample as positive, both TPR and FPR will increase from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; monotonically. The area under the curve
(AUC) of ROC depicts the probability that a random positive testcase has
a higher predicted probability of being positive than a random negative
testcase.&lt;/p&gt;
&lt;p&gt;To show it, note that in the process described above, when a positive
testcase is added as positive, TPR goes up by &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\text{\# total positive
samples}}\)&lt;/span&gt;; when a negative testcase is added as positive, FPR
goes up (right) by &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\text{\# total
negative samples}}\)&lt;/span&gt;. Suppose at iteration i there is an update
on FPR, we can accumulate the area by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\text{\# current positive samples}_i}{\text{\# total negative
samples} \times \text{\# total positive samples}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{\# current positive
samples}_i\)&lt;/span&gt; is the number of positive testcases that are added
before the negative testcase i. This holds for each negative testcase.
That is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \text{AUC-ROC} = \frac{\text{\# positive-negative pairs
where $+$ is ranked before $-$}}{\text{\# total negative samples} \times
\text{\# total positive samples}} \\ = \frac{\text{\# positive-negative
pairs where $+$ is ranked before $-$}}{\text{\# total positive-negative
pairs}} \end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;AUC-ROC reflects the model’s capability of telling positive testcase
from negative testcase.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&#34;&gt;Classification:
ROC Curve and AUC | Machine Learning | Google for Developers&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>4-linear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</guid>
      <description>

&lt;h2 id=&#34;linear-programming&#34;&gt;Linear Programming&lt;/h2&gt;
&lt;p&gt;Recall that the linear programming problem is &lt;span class=&#34;math display&#34;&gt;\[
\label{lp} \begin{aligned}
\min_{x} \quad&amp;amp; c^T x \\
\text{s.t.} \quad&amp;amp; a_i^T x \le b_i, i=1,\dots,m
\end{aligned} \tag{LP}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c \in \R^n\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(a_i \in \R^{n}, b_i \in \R,
i=1,\dots,m\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;polyhedron&lt;/strong&gt; is the intersection of a
finite set of halfspaces. A bounded polyhedron is called a
&lt;strong&gt;polytope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the feasible set of LP is a polyhedron. Interestingly, all
polyhedrons are convex. That is, the feasible set of LP is convex.&lt;/p&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;The standard form of LP is &lt;span class=&#34;math display&#34;&gt;\[
\label{standard-lp} \begin{aligned}
\min_{x \in \R_+^n} \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; A x = b \\
\end{aligned} \tag{Standard LP}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c \in \R^n, A \in \R^{m
\times n}, b \in \R^m\)&lt;/span&gt;. Standard LP has standard solutions.
Thus, the next question is how to convert an ordinary LP problem to a
standard one.&lt;/p&gt;
&lt;p&gt;Now write &lt;span class=&#34;math inline&#34;&gt;\(x = x^+ - x^-\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(x^+, x^- \in \R_+^n\)&lt;/span&gt;. Introduce
another slack variable &lt;span class=&#34;math inline&#34;&gt;\(s \in
\R_+^m\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lp}\)&lt;/span&gt; is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{x^+, x^- \ge 0, s \ge 0} \quad &amp;amp; c^T (x^+ - x^-) \\
\text{s.t.} \quad &amp;amp; A (x^+ - x^-) + s = b \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
A&amp;#39; = \left[\begin{array}{c:c}
A &amp;amp; -A &amp;amp; I
\end{array}\right] \in \R^{m \times (2n+m)} \\
x&amp;#39; = \begin{bmatrix}
x^+ \\
\hdashline
x^- \\
\hdashline
s
\end{bmatrix} \in \R_+^{2n+m},
c&amp;#39; = \begin{bmatrix}
c \\
\hdashline
-c \\
\end{bmatrix} \in \R_+^{2n}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{x&amp;#39; \in \R_+^{2n+m}} \quad &amp;amp; (c&amp;#39;)^T x&amp;#39; \\
\text{s.t.} \quad &amp;amp; A&amp;#39; x&amp;#39; = b \\
\end{aligned}
\]&lt;/span&gt; which observes the formality of standard LP. Here lays down
again the standard LP problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\label{primal} \begin{aligned}
v_p^* = \min_{x \in \R_+^n} \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; A x = b \\
\end{aligned} \tag{P}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The first question&lt;/strong&gt; to ask is whether there is a
optimal solution to it. The answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;
is feasible, then either 1) the optimal value &lt;span class=&#34;math inline&#34;&gt;\(v^* = -\infty\)&lt;/span&gt; (no optimal solution),or 2)
it has an optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas’ Lemma&lt;/h3&gt;
&lt;p&gt;But &lt;strong&gt;the second question&lt;/strong&gt; arises: how to certify that
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is infeasible? We
meet the similar dilemma to that when dealing with set-set separation:
it is easy to test the feasibility of any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;; but it is prohibitive to test the
feasibility of all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s just to show
that the original problem is infeasible.&lt;/p&gt;
&lt;p&gt;The idea is that, given the feasible polyhedron &lt;span class=&#34;math inline&#34;&gt;\(P \triangleq \{ x \in \R_+^n: Ax = b \}\)&lt;/span&gt;
of the original problem, we construct another auxiliary polyhedron &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; such that exactly one of the following
holds:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P \ne \emptyset, Q =
\emptyset\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P = \emptyset, Q \ne
\emptyset\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Farkas’ lemma&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A \in \R^{m \times n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R^m\)&lt;/span&gt; be given. Then exactly one of
the following systems is solvable:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A x = b, x \ge 0\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le 0, b^T y &amp;gt; 0\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A x = b, x \ge 0\)&lt;/span&gt; means that
&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a non-negative linear
combinations of &lt;span class=&#34;math inline&#34;&gt;\(a_1, \dots, a_n\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le 0\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; forms an obtuse angle to &lt;span class=&#34;math inline&#34;&gt;\(a_1, \dots, a_n\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(b^T y &amp;gt; 0\)&lt;/span&gt; means &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; forms an acute angle with &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that the above two statements cannot be solvable at the same
time. or else this gives rise to the contradiction for some &lt;span class=&#34;math inline&#34;&gt;\(x_0 \ge 0, y_0 \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{x_0^T}_{\ge 0} \underbrace{A^T y_0}_{\le 0} = y_0^T A x =
\underbrace{y_0^T b}_{&amp;gt; 0}
\]&lt;/span&gt; We then claim that the above two statements cannot be
unsolvable at the same time. Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is unsolvable.&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(b \notin A^+ \triangleq \{
Ax: x \ge 0 \}\)&lt;/span&gt;. It can be verified that &lt;span class=&#34;math inline&#34;&gt;\(A^+\)&lt;/span&gt; is non-empty (&lt;span class=&#34;math inline&#34;&gt;\(0 \in A^+\)&lt;/span&gt;), closed (closeness is not
generally preserved under affine transformation; but in this case it is
&amp;lt;refer to &lt;a href=&#34;../3-lp.pdf&#34;&gt;this handout&lt;/a&gt;&amp;gt;) and convex
(because convexity is preserved under affine transformation &lt;span class=&#34;math inline&#34;&gt;\(A^+ = A \R_+^n\)&lt;/span&gt;). Then by &lt;em&gt;point-set
separation theorem&lt;/em&gt;, for any &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt;, there exists a &lt;span class=&#34;math inline&#34;&gt;\(y_0 \in
\R^m\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{x \ge 0} y_0^T A x &amp;lt; y_0^T b
\]&lt;/span&gt; Take &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
y_0^T b &amp;gt; 0
\]&lt;/span&gt; We claim that &lt;span class=&#34;math inline&#34;&gt;\(A^T y_0 \le
0\)&lt;/span&gt;. Suppose on the contrary that there exists an &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\((A^T y_0)_i\)&lt;/span&gt; is positive. Then for any
&lt;span class=&#34;math inline&#34;&gt;\(\lambda \ge 0\)&lt;/span&gt;, take &lt;span class=&#34;math inline&#34;&gt;\(x = \lambda e_i\)&lt;/span&gt; and give &lt;span class=&#34;math display&#34;&gt;\[
y_0^T A x = x^T A^T y_0 = \underbrace{\lambda}_{\ge 0} \underbrace{(A^T
y_0)_i}_{&amp;gt; 0} &amp;lt; y_0^T b
\]&lt;/span&gt; which is impossible when &lt;span class=&#34;math inline&#34;&gt;\(\lambda
\to \infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; is solvable when
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is unsolvable. &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; cannot be unsolvable at the same
time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We construct &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m: A^T y \le 0, b^T y &amp;gt; 0
\}\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; is
not a polyhedron yet, because &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m:
b^T y &amp;gt; 0 \}\)&lt;/span&gt; is open and is not a half-space. However,
observe that (verify it) &lt;span class=&#34;math display&#34;&gt;\[
A^T y \le 0, b^T y &amp;gt; 0 \text{ is solvable} \iff A^T y \le 0, b^T y =
1 \text{ is solvable}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m: b^T y = 1
\}\)&lt;/span&gt; can be rewritten as &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in
\R^m: b^T y \ge 1, b^T y \le 1 \}\)&lt;/span&gt; which is an intersection of
halfspaces. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(Q \triangleq \{ y \in
\R^m: A^T y \le 0, b^T y \ge 1, b^T \le 1 \}\)&lt;/span&gt; is non-empty if
and only if &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; is non-empty.
Moreover, &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an intersection of
half-spaces as desired.&lt;/p&gt;
&lt;p&gt;Now, we convert the infeasibility of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; into the feasibility of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;. &lt;strong&gt;The third question&lt;/strong&gt;
is, suppose we verify that &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is
feasible, then given a solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;, how to certify its
optimality? The idea is to establish a lower bound on the optimal value
&lt;span class=&#34;math inline&#34;&gt;\(v^*_p\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;duality&#34;&gt;Duality&lt;/h3&gt;
&lt;p&gt;Consider a &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt; such that
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le c\)&lt;/span&gt;, then for any &lt;span class=&#34;math inline&#34;&gt;\(x \in \R_+^n\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(b \triangleq A x\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
b^T y = x^T A^T y \le c^T x
\]&lt;/span&gt; The above holds for &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;
of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; as well.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
b^T y \le c^T x_p^* = v_p^*
\]&lt;/span&gt; We can try to find the largest lower bound w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\label{dual} \begin{aligned}
v_d^* = \max_y &amp;amp;\quad b^T y \\
\text{s.t.} &amp;amp;\quad A^T y \le c
\end{aligned} \tag{D}
\]&lt;/span&gt; Automatically, &lt;span class=&#34;math inline&#34;&gt;\(v_d^* \le
v_p^*\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is the also the dual of
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; in that &lt;span class=&#34;math inline&#34;&gt;\(v_p^* \ge v_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt; of LP. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
c^T \bar x \ge b^T \bar y
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = -\infty\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is infeasible.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = +\infty\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is infeasible.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; is feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;, and the &lt;strong&gt;duality
gap&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Delta(\bar x, \bar y)
\triangleq c^T \bar x - b^T \bar y\)&lt;/span&gt; is zero, then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is optimal for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; is optimal for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What about the converses of conclusions above?&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is
infeasible, then either &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
-\infty\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;
is infeasible.&lt;/p&gt;
&lt;p&gt;It is rather easy to construct a problem such that both the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; are infeasible, since the
constraints &lt;span class=&#34;math inline&#34;&gt;\(Ax = b, x \ge 0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le c\)&lt;/span&gt; are quite independent.
The following will give an example: &lt;span class=&#34;math display&#34;&gt;\[
A =
\begin{bmatrix}
-1 &amp;amp; -1 \\
1 &amp;amp; 1
\end{bmatrix},
b = [1, 1]^T,
c = [-1, -1]^T
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is
infeasible, then either &lt;span class=&#34;math inline&#34;&gt;\(v_d^* =
+\infty\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is
infeasible.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality&lt;/strong&gt; for LP. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; has an optimal solution
&lt;span class=&#34;math inline&#34;&gt;\(x_p^*\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; has an optimal solution
&lt;span class=&#34;math inline&#34;&gt;\(y_d^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = c^T x_p^* = b^T y_d^* =
v_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that &lt;span class=&#34;math display&#34;&gt;\[
Ax = b, x \ge 0, c^T x - v_p^* &amp;lt; 0 \tag{I}
\]&lt;/span&gt; is unsolvable. Consider &lt;strong&gt;homogenizing&lt;/strong&gt; the
above system: &lt;span class=&#34;math display&#34;&gt;\[
Ax - bt = 0, c^T x - v_p^* t &amp;lt; 0, x \ge 0, t \ge 0 \tag{II}
\]&lt;/span&gt; We argue that &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt; is solvable if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; is solvable. If &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\((\bar x, 1)\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; too. If &lt;span class=&#34;math inline&#34;&gt;\((\bar x, \bar t)\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt;, we discuss by case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar t &amp;gt; 0\)&lt;/span&gt;. In this case,
&lt;span class=&#34;math inline&#34;&gt;\(\bar x / \bar t\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar t = 0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math inline&#34;&gt;\(x^* + \bar x\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; can be rewritten
as &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A &amp;amp; -b \\
-A &amp;amp; b \\
-I &amp;amp; 0 (n \times 1) \\
0 (1 \times m) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
x \\
t
\end{bmatrix} \le 0,
[-c^T, v_p^*]
\begin{bmatrix}
x \\
t
\end{bmatrix} &amp;gt; 0
\]&lt;/span&gt; By &lt;em&gt;Farkas’ lemma&lt;/em&gt;, the following system is solvable:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A^T &amp;amp; -A^T &amp;amp; -I &amp;amp; 0 (m\times 1) \\
-b^T &amp;amp; b^T &amp;amp; 0 (1 \times n) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
z_1 \\
z_2 \\
z_3 \\
z_4
\end{bmatrix} =
\begin{bmatrix}
-c \\
v_p^*
\end{bmatrix},
\begin{bmatrix}
z_1 \\
z_2 \\
z_3 \\
z_4
\end{bmatrix}
\ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This means &lt;span class=&#34;math display&#34;&gt;\[
A^T (z_1 - z_2) - z_3 = -c \Rightarrow \\
A^T (z_2 - z_1) \le c \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-b^T (z_1 - z_2) - z_4 = v_p^* \Rightarrow \\
b^T (z_2 - z_1) \ge v_p^*
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(y^* \triangleq z_2 - z_1\)&lt;/span&gt;
is a feasible solution to the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Plus the weak duality which
states &lt;span class=&#34;math inline&#34;&gt;\(b^T y^* \le v_p^*\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b^T y^* = v_p^*\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(y_d^* = y^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c^T x_p^* = b^T y_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;complementary slackness&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, they are optimal for
their respective problems if and only if &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{\bar x_i}_{\substack{\text{$i$-th} \\ \text{primal
variable}}}\ \underbrace{(c - A^T \bar y)_i}_{\substack{\text{$i$-th }
\\ \text{dual constraint}}} = 0
\]&lt;/span&gt; Proof: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
c^T \bar x - b^T \bar y &amp;amp;= c^T x - (A \bar x)^T \bar y \\
0 &amp;amp;= \bar x^T (c - A^T \bar y)
\end{aligned}
\]&lt;/span&gt; Plus that &lt;span class=&#34;math inline&#34;&gt;\(\bar x \ge 0, c - A^T
\bar y \ge 0\)&lt;/span&gt;, we can conclude with the complementary
slackness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After establishing the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;’s optimal value with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;’s optimal value, by strong
duality, to find optimal solutions to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is equivalent to find a
feasible solution to &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = b, x \ge 0 \tag{primal constraint} \\
A^T y \le c \tag{dual constraint} \\
c^T x = b^T y \tag{zero duality gap}
\end{gather}
\]&lt;/span&gt; Note that the zero duality gap is in essence equivalent to
&lt;span class=&#34;math display&#34;&gt;\[
x^T (c - A^T y) = 0 \tag{complementary slackness}
\]&lt;/span&gt; An optimization problem is converted to feasibility problem.
Particularly, an LP optimization problem is no harder than an LP
feasibility problem.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-lp-problem&#34;&gt;Examples of LP Problem&lt;/h2&gt;
&lt;h3 id=&#34;vertex-cover&#34;&gt;Vertex Cover&lt;/h3&gt;
&lt;p&gt;Given a graph &lt;span class=&#34;math inline&#34;&gt;\(G = (V, E)\)&lt;/span&gt; and a
cost function &lt;span class=&#34;math inline&#34;&gt;\(c: V \mapsto \R^+\)&lt;/span&gt;,
find a vertex cover that minimizes overall cost. Here we say that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq V\)&lt;/span&gt; is vertex cover if every
edge has at least one endpoint in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. The cost of a vertex cover the sum of
costs of the vertices contained.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(x_i \in \{ 0,1 \}\)&lt;/span&gt; be an
indicator variable defined as &lt;span class=&#34;math display&#34;&gt;\[
x_i =
\begin{cases}
1, &amp;amp; \text{if vertex $i$ is in the cover} \\
0, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt; Then we have the following integer programming problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{vc} \begin{aligned}
v^* = \min_x \quad &amp;amp;\sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp;x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp;x_i \in \{ 0,1 \}, \forall i=1,\dots,|V|
\end{aligned} \tag{Vertex Cover}
\]&lt;/span&gt; The problem above is NP-hard. One typical way to tackle it is
to relax the constraint to make it easier. &lt;span class=&#34;math display&#34;&gt;\[
\label{vc-lp-I} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp; 0 \le x_i \le 1, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC I}
\]&lt;/span&gt; Note that we can further drop the &lt;span class=&#34;math inline&#34;&gt;\(x_i \le 1\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-I}\)&lt;/span&gt; because we can always let
&lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39; = 1\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x_i &amp;gt; 1\)&lt;/span&gt; without violating the
constraints but with smaller objective (because &lt;span class=&#34;math inline&#34;&gt;\(c \ge 0\)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[
\label{vc-lp-II} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp; 0 \le x_i, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC II}
\]&lt;/span&gt; Once &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; solves
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;, we need to round
&lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; to give a feasible
&lt;span class=&#34;math inline&#34;&gt;\(x_\text{rd}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd} = c^T x_\text{rd} \approx
v^*\)&lt;/span&gt;. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(v^* \ge
v_\text{lp}^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v^* \le
v_\text{rd}\)&lt;/span&gt;. The question is if we can upper-bound &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd}\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\alpha v^*\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\alpha \ge 1\)&lt;/span&gt;. Here &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is called the
&lt;strong&gt;approximation ratio&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(P = \{ x \in \R^n: a_i^T
x \le b, i=1,\dots,m \}\)&lt;/span&gt; be a polyhedron and let &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in P\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(I(\bar x) = \{ i: a_i^T x = b \}\)&lt;/span&gt; be the
active index set. We say that &lt;span class=&#34;math inline&#34;&gt;\(\bar
x\)&lt;/span&gt; is a &lt;strong&gt;vertex&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\{ a_i:
i \in I(\bar x) \}\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;
linearly-independent vectors. Alternatively, the system &lt;span class=&#34;math display&#34;&gt;\[
a_i^T x = b_i, i \in I(\bar x)
\]&lt;/span&gt; has a unique solution (which is &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The equality constraint ensures that the solution is on the boundary
of the feasible set, or rather, on a hyperplane. &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; linearly-independent vectors ensure
that the solution is the intersection of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; hyperplanes, which is why it is called
&lt;u&gt;vertex solution&lt;/u&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If an LP has a vertex feasible solution and is bounded
(which means the optimal value is finite), then it has a vertex optimal
solution (??).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(Q = \{ x \in \R^{|V|}: x_i +
x_j \ge 1, \forall (i,j) \in E; x \ge 0 \}\)&lt;/span&gt;, which is exactly
the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; has a vertex solution (??). Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be one of such vertex solutions.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\forall i, \bar x_i \in \{ 0, 1/2, 1 \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the theorem and proposition above, and because &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt; is bounded (verify it),
we can have a vertex optimal solution &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;. Next, we choose to
round the optimal vertex solution &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; in this way: if &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{lp}_i}^*\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{rd}_i} = 0\)&lt;/span&gt;; else &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{rd}_i} = 1\)&lt;/span&gt;. Note that this
rounding method satisfies the constraint of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc}\)&lt;/span&gt;. The largest divergence of
&lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(v_\text{lp}^*\)&lt;/span&gt; happens when &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; is full of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, in which case &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd} = 2 v_\text{lp}^*\)&lt;/span&gt;. Thus, we
can conclude that &lt;span class=&#34;math display&#34;&gt;\[
v_\text{lp}^* \le v^* \le c^T x_\text{rd} \le 2 v_\text{lp}^* \le 2 v^*
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The approximation ratio in this problem is &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;max-flow&#34;&gt;Max Flow&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&#34;&gt;cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;cooperative-game-theory&#34;&gt;Cooperative Game Theory&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N} = \{ 1,\dots,n
\}\)&lt;/span&gt; be the set of players and &lt;span class=&#34;math inline&#34;&gt;\(v: 2^N
\mapsto \R^+\)&lt;/span&gt; be the worth function, which satisfies &lt;span class=&#34;math inline&#34;&gt;\(v(\emptyset) = 0\)&lt;/span&gt;. The subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; is called
&lt;strong&gt;coalition&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt; be the &lt;strong&gt;allocation vector&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; be the payoff assigned to player
&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x(\emptyset) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x(S) \triangleq \sum_{i \in S} x_i\)&lt;/span&gt; for
non-empty coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We say that
coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; &lt;strong&gt;improves
upon&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(v(S) &amp;gt; x(S)\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The allocation vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is said to be in the
&lt;strong&gt;core&lt;/strong&gt; if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x(\mathcal{N}) = v(\mathcal{N}) \\
\forall S \subseteq \mathcal{N}, x(S) \ge v(S) \label{core-inequality}
\end{gather}
\]&lt;/span&gt; which means no &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; improves
upon &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is essential to understand &lt;strong&gt;the implication behind the
game&lt;/strong&gt;. This “cooperative game” is actually preventing players
from cooperation, or collusion, or called forming a coalition. The
allocation vector is part of this “evil scheme”. The fact of matter is,
the solver of this problem is trying to pay off players (in total the
amount is no more than the value he thinks the whole coalition &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; deserves) such that for every
possible coalition, there will be some member who thinks it unfair
because the share (equal share for everyone!) he gets from the coalition
is no more than the amount he would otherwise earns himself. &lt;span class=&#34;math display&#34;&gt;\[
x(S) = \sum_{i \in S} x_i \ge v(S)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x(S)\)&lt;/span&gt; is additive w.r.t.
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, which exactly represents the
payoff every player grabs (snout in the trough) covertly and separately.
There must be some &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(x_i \ge v(S)/|S|\)&lt;/span&gt;. The situation worsens
when the &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; relation is
strict.&lt;/p&gt;
&lt;p&gt;The question is, is the core non-empty for a given &lt;span class=&#34;math inline&#34;&gt;\((\mathcal{N}, v)\)&lt;/span&gt;, or rather, can such
scheme exist?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Treasure hunt. &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; people
discovered treasures in the mountain. 2 people are needed to bring one
piece out and thus &lt;span class=&#34;math inline&#34;&gt;\(v(S) = \lfloor
\frac{|S|}{2} \rfloor\)&lt;/span&gt;. Is the core empty?&lt;/p&gt;
&lt;p&gt;First consider the case when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;
is even. &lt;span class=&#34;math inline&#34;&gt;\(x = [1/2, \dots, 1/2]\)&lt;/span&gt; is
in the core.&lt;/p&gt;
&lt;p&gt;Now simply consider the case when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is odd. The fact is that the core is
empty in this case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;. The cooperative
game &lt;span class=&#34;math inline&#34;&gt;\((\mathcal{N}, v)\)&lt;/span&gt; has a
non-empty core if and only if for every set of numbers &lt;span class=&#34;math inline&#34;&gt;\(\{ y_S \}_{S \subseteq \mathcal{N}}\)&lt;/span&gt;,
whose elements are indexed by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt;’s subset, such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall S \subseteq \mathcal{N}, y_S \ge
0\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall i \in \mathcal{N}, \sum_{S: i
\in S} = 1\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{S \subseteq \mathcal{N}} y_S v(S)
\le v(\mathcal{N})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_S\)&lt;/span&gt; can be interpreted as the
amount of time each player in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;
spent on the coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;,
justifying the non-negativity constraint. The total amount of time
player &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; spent on different
coalitions is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y_S v(S)\)&lt;/span&gt; can be understood as the
proportional outcome from partial-commitment.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Consider the following optimization problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp} \tag{Coop. Game} \begin{aligned}
\min \quad &amp;amp; [x(\mathcal{N}) \triangleq \sum_{i \in N} x_i] \\
\text{s.t.} \quad &amp;amp; x(S) \ge v(S), \forall S \subseteq \mathcal{N}
\end{aligned}
\]&lt;/span&gt; Observe that the constraints naturally imply the inequality
conditions &lt;span class=&#34;math inline&#34;&gt;\(\eqref{core-inequality}\)&lt;/span&gt;
for an allocation to be in the core. &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt; is automatically embedded into &lt;span class=&#34;math inline&#34;&gt;\(x(\{ i \}) \ge v(\{ i \}) \ge 0,
i=1,\dots,n\)&lt;/span&gt;. Other than that, the minimizing objective together
with the constraint &lt;span class=&#34;math inline&#34;&gt;\(x(\mathcal{N}) \ge
v(\mathcal{N})\)&lt;/span&gt; implies &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; is lower-bounded by &lt;span class=&#34;math inline&#34;&gt;\(v(\mathcal{N})\)&lt;/span&gt;. Also note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; is always feasible because we
can assign each &lt;span class=&#34;math inline&#34;&gt;\(x(S)\)&lt;/span&gt; sufficiently
large to surpass &lt;span class=&#34;math inline&#34;&gt;\(v(S)\)&lt;/span&gt;. Therefore,
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; always has an optimal
solution.&lt;/p&gt;
&lt;p&gt;To rewrite &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; to LP
form, let &lt;span class=&#34;math display&#34;&gt;\[
e = [1,\dots,1]^T,
A =
\begin{bmatrix}
\mathbb{1}_n(\emptyset)^T \\
\vdots \\
\mathbb{1}_n(S)^T \\
\vdots \\
\mathbb{1}_n(\mathcal{N})^T
\end{bmatrix},
b =
\begin{bmatrix}
v(\emptyset) \\
\vdots \\
v(S) \\
\vdots \\
v(\mathcal{N})
\end{bmatrix}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_\mathcal{N}(S)\)&lt;/span&gt; is the
indicator function that returns the vector in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; whose &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th entry indicates if &lt;span class=&#34;math inline&#34;&gt;\(i \in S\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; becomes &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-lp} \tag{Coop. Game LP} \begin{aligned}
v^* = \min \quad &amp;amp; e^T x \\
\text{s.t.} \quad &amp;amp; A x \ge b
\end{aligned}
\]&lt;/span&gt; We don’t rush to convert it to standard LP form yet; otherwise
we need to introduce a slack variable. On the other hand, we formulate
the following LP problem, whose optimal value is &lt;strong&gt;of the same
magnitude as but of different sign to&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-lp}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-primal} \tag{Coop. Game P} \begin{aligned}
v_p^* = \max \quad &amp;amp; -e^T x \\
\text{s.t.} \quad &amp;amp; -A x \le -b
\end{aligned}
\]&lt;/span&gt; That is, &lt;span class=&#34;math inline&#34;&gt;\(v^* = -v_p^*\)&lt;/span&gt;.
Recall that the primal and the dual are relative. The above problem has
the dual &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-dual} \tag{Coop. Game D} \begin{aligned}
v_d^* = \min_{y \ge 0} \quad &amp;amp; -b^T y \\
\text{s.t.} \quad &amp;amp; A^T y = e
\end{aligned}
\]&lt;/span&gt; which is in the standard LP form.&lt;/p&gt;
&lt;p&gt;Note that columns of &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; are
exactly those indicator vectors. Thus, &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt;’s columns and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;’s entries can both be indexed by sets.
Also, rows of &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; are indexed by
elements. Since every entry of &lt;span class=&#34;math inline&#34;&gt;\(A^T
y\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, interpreting &lt;span class=&#34;math inline&#34;&gt;\(A^T y\)&lt;/span&gt; as the row-column dot-product, the
constraint in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-dual}\)&lt;/span&gt; is
exactly &lt;span class=&#34;math display&#34;&gt;\[
\forall i \in \mathcal{N}, \sum_{S: i \in S} y_S = 1
\]&lt;/span&gt; By strong duality, we have &lt;span class=&#34;math display&#34;&gt;\[
v^* = x^*(\mathcal{N}) = \sum_{S} y_S^* v(S) = -v_d^* = -v_p^*
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\forall S \subseteq \mathcal{N},
x^*(S) \ge v(S)\)&lt;/span&gt; by the constraint of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-lp}\)&lt;/span&gt;. By
&lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;’s condition, we have &lt;span class=&#34;math inline&#34;&gt;\(x^*(\mathcal{N}) = \sum_{S} y_S^* v(S) \le
v(\mathcal{N})\)&lt;/span&gt;. Therefore, we conclude that &lt;span class=&#34;math inline&#34;&gt;\(x^*(\mathcal{N}) = v(\mathcal{N})\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; is in the core.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be inferred that there are lots of duality in game theory,
given its mini-max nature.&lt;/p&gt;
&lt;h2 id=&#34;solving-lp&#34;&gt;Solving LP&lt;/h2&gt;
&lt;p&gt;The algorithm for solving LP problems generally falls into two
methods: simplex method and interior point method.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Note</title>
      <link>https://chunxy.github.io/courses/energy-efficient-computing/images/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/energy-efficient-computing/images/</guid>
      <description>

&lt;h1 id=&#34;implementation-trickcomm-lower-bound&#34;&gt;Implementation Trick&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id=&#34;gemm&#34;&gt;GEMM&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ge&lt;/strong&gt;neral &lt;strong&gt;M&lt;/strong&gt;atrix
&lt;strong&gt;M&lt;/strong&gt;ultiplication describes the implementation tricks that
speeds up computation in neural network. Matrix multiplication is a
classical, fundamental and established field in both math and computer
science. And this is the reason why much effort and interest have been
put into how to further speed up it and how to convert other kinds of
operations into it.&lt;/p&gt;
&lt;h3 id=&#34;im2col&#34;&gt;Im2Col&lt;/h3&gt;
&lt;h4 id=&#34;single-feature-map-and-kernel&#34;&gt;Single Feature Map and
Kernel&lt;/h4&gt;
&lt;p&gt;A normal convolution operation will slide a &lt;strong&gt;window&lt;/strong&gt;
of the same size as &lt;strong&gt;kernel&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(F: k_h \times k_w\)&lt;/span&gt;) through the
&lt;strong&gt;feature map&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(I: i_h \times
i_w\)&lt;/span&gt;) in a row-major order (for simplicity, we will take that
stride is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and padding is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;). This causes problem because numbers
in a single convolution operation will span multiple columns, due to
which the spatial locality cannot be exploited.&lt;/p&gt;
&lt;p&gt;Since the convolution operation is in essence doing the “sum of
products”, we may just as well treat the convolution as dot product
between two vectors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./im2col.png&#34; style=&#34;zoom:50%;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;To realize it, we can squeeze the kernel into a &lt;span class=&#34;math inline&#34;&gt;\(k_h k_w \times 1\)&lt;/span&gt; column vector and each
window on the feature map to a &lt;span class=&#34;math inline&#34;&gt;\(1 \times k_h
k_w\)&lt;/span&gt; row vector (in memory, a column vector &lt;code&gt;v[N][1]&lt;/code&gt;
is no difference from a row vector &lt;code&gt;v[1][N]&lt;/code&gt;). Then we stack
these row vectors vertically in the order as their original window would
appear in the convolution. This newly synthesized matrix &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is usually called &lt;strong&gt;lowered
matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As for implementation, we don’t do this “window by window”, because
usually the feature map has a large width, which still introduce the
same issue of not exploiting the spatial locality when accessing across
rows. For each position in the feature map, we can identify all the
positions that it will appear in the lowered matrix in one off. Since
kernel size is usually small, accessing this lowered matrix across rows
causes less cache misses than that in the input.&lt;/p&gt;
&lt;p&gt;Treating &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(l_h \times l_w \times k_h \times k_w\)&lt;/span&gt;, we
fill up its entries in following way:&lt;/p&gt;
&lt;div class=&#34;sourceCode&#34; id=&#34;cb1&#34;&gt;&lt;pre class=&#34;sourceCode c&#34;&gt;&lt;code class=&#34;sourceCode c&#34;&gt;&lt;span id=&#34;cb1-1&#34;&gt;&lt;a href=&#34;#cb1-1&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; i_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-2&#34;&gt;&lt;a href=&#34;#cb1-2&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; k_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; k_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-3&#34;&gt;&lt;a href=&#34;#cb1-3&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-4&#34;&gt;&lt;a href=&#34;#cb1-4&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; l_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; i_h &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt; k_h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; l_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; i_w &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt; k_w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-5&#34;&gt;&lt;a href=&#34;#cb1-5&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-6&#34;&gt;&lt;a href=&#34;#cb1-6&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; im2lower&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_w&lt;span class=&#34;op&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;l_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;l_w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_w&lt;span class=&#34;op&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-7&#34;&gt;&lt;a href=&#34;#cb1-7&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_h&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-8&#34;&gt;&lt;a href=&#34;#cb1-8&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_w&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-9&#34;&gt;&lt;a href=&#34;#cb1-9&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt;k_h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-10&#34;&gt;&lt;a href=&#34;#cb1-10&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt;k_w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-11&#34;&gt;&lt;a href=&#34;#cb1-11&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          &lt;span class=&#34;cf&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;||&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;gt;=&lt;/span&gt; l_h&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-12&#34;&gt;&lt;a href=&#34;#cb1-12&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          &lt;span class=&#34;cf&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;||&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;gt;=&lt;/span&gt; l_w&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-13&#34;&gt;&lt;a href=&#34;#cb1-13&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;][-&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][-&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-14&#34;&gt;&lt;a href=&#34;#cb1-14&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-15&#34;&gt;&lt;a href=&#34;#cb1-15&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-16&#34;&gt;&lt;a href=&#34;#cb1-16&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-17&#34;&gt;&lt;a href=&#34;#cb1-17&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-18&#34;&gt;&lt;a href=&#34;#cb1-18&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb1-19&#34;&gt;&lt;a href=&#34;#cb1-19&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;normal-input-and-filter&#34;&gt;Normal Input and Filter&lt;/h4&gt;
&lt;p&gt;By far we only consider the case of a single convolution. More often
that not, the real-world convolution is done in batch with multiple
channels, meaning that the input &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;
is of shape &lt;span class=&#34;math inline&#34;&gt;\(i_n \times i_c \times i_w \times
i_w\)&lt;/span&gt; and filter &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is of
shape &lt;span class=&#34;math inline&#34;&gt;\(f_c \times i_c \times k_h \times
k_w\)&lt;/span&gt;. Each of &lt;span class=&#34;math inline&#34;&gt;\(f_c\)&lt;/span&gt; output
channels is obtained as the sum of the one-on-one convolution between
each of the &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; kernels and each of
the &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; channels (which is in
accordance with PyTorch’s &lt;code&gt;Conv2d&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now we consider the case where there are multiple channels. We
simplify a bit by still setting &lt;span class=&#34;math inline&#34;&gt;\(i_n =
1\)&lt;/span&gt;. As a result, the input &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is of shape &lt;span class=&#34;math inline&#34;&gt;\(i_c \times i_h \times i_w\)&lt;/span&gt; and filter
&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; still remains &lt;span class=&#34;math inline&#34;&gt;\(f_c \times i_c \times k_h \times k_w\)&lt;/span&gt;.
Suppose each feature map contains &lt;span class=&#34;math inline&#34;&gt;\(d = o_h
\times o_w\)&lt;/span&gt; unique kernel windows. Then each transformed channel
should be of shape &lt;span class=&#34;math inline&#34;&gt;\(d \times k_h k_w\)&lt;/span&gt;
(the order of symbols in shortened multiplication matters too; in this
case, &lt;span class=&#34;math inline&#34;&gt;\(k_h k_w\)&lt;/span&gt; indicates we squeeze
by row); each transformed kernel should be of shape &lt;span class=&#34;math inline&#34;&gt;\(k_h k_w \times 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The input contains &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; such
transformed &lt;span class=&#34;math inline&#34;&gt;\(d \times k_h k_w\)&lt;/span&gt;
channels. Instead of doing matrix multiplication &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; times, we can concatenate these &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; matrices horizontally to give a
single &lt;span class=&#34;math inline&#34;&gt;\(d \times (i_c k_h k_w)\)&lt;/span&gt;
lowered matrix &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The filter contains &lt;span class=&#34;math inline&#34;&gt;\(f_c \times
i_c\)&lt;/span&gt; such transformed &lt;span class=&#34;math inline&#34;&gt;\(k_h k_w \times
1\)&lt;/span&gt; kernels. For each output channel, we can concatenate
corresponding &lt;span class=&#34;math inline&#34;&gt;\(i_c\)&lt;/span&gt; kernels
vertically to facilitate the one-on-one convolution, which gives a
single &lt;span class=&#34;math inline&#34;&gt;\((i_c k_h k_w) \times f_c\)&lt;/span&gt;
transformed filter &lt;span class=&#34;math inline&#34;&gt;\(F&amp;#39;\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now &lt;span class=&#34;math inline&#34;&gt;\(I \circledast F\)&lt;/span&gt; becomes
&lt;span class=&#34;math inline&#34;&gt;\(L \times F&amp;#39;\)&lt;/span&gt;. The output shape
is &lt;span class=&#34;math inline&#34;&gt;\(d \times f_c\)&lt;/span&gt;. Each input image
becomes a &lt;span class=&#34;math inline&#34;&gt;\(d \times 1\)&lt;/span&gt; column vector
finally, which is exactly what “Im2Col” means. This &lt;span class=&#34;math inline&#34;&gt;\(d \times 1\)&lt;/span&gt; vector can be transposed and
then reshaped into &lt;span class=&#34;math inline&#34;&gt;\(o_h \times o_w\)&lt;/span&gt;
to recover the convolution result (no actual transformation has to be
done, just to interpret it this way). Then by applying the Im2Col trick
repeatedly, we can chain up and handle consecutive convolutional
layers.&lt;/p&gt;
&lt;div class=&#34;sourceCode&#34; id=&#34;cb2&#34;&gt;&lt;pre class=&#34;sourceCode c&#34;&gt;&lt;code class=&#34;sourceCode c&#34;&gt;&lt;span id=&#34;cb2-1&#34;&gt;&lt;a href=&#34;#cb2-1&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; i_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; i_c &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-2&#34;&gt;&lt;a href=&#34;#cb2-2&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; k_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; k_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; f_c &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-3&#34;&gt;&lt;a href=&#34;#cb2-3&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-4&#34;&gt;&lt;a href=&#34;#cb2-4&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; l_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; i_h &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt; k_h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; l_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; i_w &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt; k_w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-5&#34;&gt;&lt;a href=&#34;#cb2-5&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-6&#34;&gt;&lt;a href=&#34;#cb2-6&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; im2lower&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i_c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_w&lt;span class=&#34;op&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;l_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;l_w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_w&lt;span class=&#34;op&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-7&#34;&gt;&lt;a href=&#34;#cb2-7&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; c &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; c &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_c&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; c&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-8&#34;&gt;&lt;a href=&#34;#cb2-8&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_h&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-9&#34;&gt;&lt;a href=&#34;#cb2-9&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_w&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-10&#34;&gt;&lt;a href=&#34;#cb2-10&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt;k_h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-11&#34;&gt;&lt;a href=&#34;#cb2-11&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt;k_w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-12&#34;&gt;&lt;a href=&#34;#cb2-12&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;            &lt;span class=&#34;cf&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;||&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;gt;=&lt;/span&gt; l_h&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-13&#34;&gt;&lt;a href=&#34;#cb2-13&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;            &lt;span class=&#34;cf&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;||&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;gt;=&lt;/span&gt; l_w&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-14&#34;&gt;&lt;a href=&#34;#cb2-14&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;            L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;h &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;c&lt;span class=&#34;op&#34;&gt;][-&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][-&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-15&#34;&gt;&lt;a href=&#34;#cb2-15&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-16&#34;&gt;&lt;a href=&#34;#cb2-16&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-17&#34;&gt;&lt;a href=&#34;#cb2-17&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-18&#34;&gt;&lt;a href=&#34;#cb2-18&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-19&#34;&gt;&lt;a href=&#34;#cb2-19&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-20&#34;&gt;&lt;a href=&#34;#cb2-20&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-21&#34;&gt;&lt;a href=&#34;#cb2-21&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-22&#34;&gt;&lt;a href=&#34;#cb2-22&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; ker2col&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; F&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;f_c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_w&lt;span class=&#34;op&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; K&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i_c&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;f_c&lt;span class=&#34;op&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-23&#34;&gt;&lt;a href=&#34;#cb2-23&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_c&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-24&#34;&gt;&lt;a href=&#34;#cb2-24&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; k_h&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-25&#34;&gt;&lt;a href=&#34;#cb2-25&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; k_w&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-26&#34;&gt;&lt;a href=&#34;#cb2-26&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; f &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; f &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; f_c&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; f&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-27&#34;&gt;&lt;a href=&#34;#cb2-27&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;          K&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;f&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; F&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;f&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;w&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-28&#34;&gt;&lt;a href=&#34;#cb2-28&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-29&#34;&gt;&lt;a href=&#34;#cb2-29&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-30&#34;&gt;&lt;a href=&#34;#cb2-30&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-31&#34;&gt;&lt;a href=&#34;#cb2-31&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb2-32&#34;&gt;&lt;a href=&#34;#cb2-32&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;mecmec&#34;&gt;MEC&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Im2Col costs much extra space because most entries in the feature map
appears &lt;span class=&#34;math inline&#34;&gt;\(k^2\)&lt;/span&gt; times in the
transformed lowered matrix. The
&lt;strong&gt;m&lt;/strong&gt;emory-&lt;strong&gt;e&lt;/strong&gt;fficient
&lt;strong&gt;c&lt;/strong&gt;omputation method improves on this by putting the
entries of (say vertically) adjacent windows in one row so that entries
can be reused. By doing so, the transformed kernel row vector will slide
through each row of obtained matrix to compute convolution result.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./mec-basic.svg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Again in single feature map and kernel case, the filter &lt;span class=&#34;math inline&#34;&gt;\(F: k_h \times k_w\)&lt;/span&gt; is squeezed to a &lt;span class=&#34;math inline&#34;&gt;\(k_h k_w \times 1\)&lt;/span&gt; column vector. The input
&lt;span class=&#34;math inline&#34;&gt;\(I: i_h \times i_w\)&lt;/span&gt; is converted to
the lowered matrix &lt;span class=&#34;math inline&#34;&gt;\(L: l_h \times
l_w\)&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
l_h = o_w \triangleq (i_w - k_w) / s_w + 1 \\
l_w = i_h k_w, \text{assuming that $k_w \ge s_w$}\\
\]&lt;/span&gt; Note that a &lt;span class=&#34;math inline&#34;&gt;\(k_w\)&lt;/span&gt;-width bar
in &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; (like &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;) expands to a row in &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, and finally a row in &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt;. Treating &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(o_w
\times i_h \times k_w\)&lt;/span&gt;, we fill up its entries in following
way:&lt;/p&gt;
&lt;div class=&#34;sourceCode&#34; id=&#34;cb3&#34;&gt;&lt;pre class=&#34;sourceCode c&#34;&gt;&lt;code class=&#34;sourceCode c&#34;&gt;&lt;span id=&#34;cb3-1&#34;&gt;&lt;a href=&#34;#cb3-1&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i_h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; i_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-2&#34;&gt;&lt;a href=&#34;#cb3-2&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; k_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; s_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-3&#34;&gt;&lt;a href=&#34;#cb3-3&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; o_w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;i_w &lt;span class=&#34;op&#34;&gt;-&lt;/span&gt; k_w&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;/&lt;/span&gt; s_w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-4&#34;&gt;&lt;a href=&#34;#cb3-4&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// const int l_h = o_w, l_w = i_h * k_w;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-5&#34;&gt;&lt;a href=&#34;#cb3-5&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; mec&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_w&lt;span class=&#34;op&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;double&lt;/span&gt; L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;o_w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i_h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k_w&lt;span class=&#34;op&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-6&#34;&gt;&lt;a href=&#34;#cb3-6&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; o_w&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; w&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-7&#34;&gt;&lt;a href=&#34;#cb3-7&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; i_h&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; h&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-8&#34;&gt;&lt;a href=&#34;#cb3-8&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; k_w&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-9&#34;&gt;&lt;a href=&#34;#cb3-9&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;co&#34;&gt;// I can be transposed first for better spatial locality.&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-10&#34;&gt;&lt;a href=&#34;#cb3-10&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        L&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;w&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; I&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;h&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;s_w &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt; w &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-11&#34;&gt;&lt;a href=&#34;#cb3-11&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-12&#34;&gt;&lt;a href=&#34;#cb3-12&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-13&#34;&gt;&lt;a href=&#34;#cb3-13&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb3-14&#34;&gt;&lt;a href=&#34;#cb3-14&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;memory-layout&#34;&gt;Memory Layout&lt;/h4&gt;
&lt;p&gt;Memory layout is not a standalone method to speed up matrix
computation, but that it cornerstones most implementation tricks.
Different methods may assume different memory layouts.&lt;/p&gt;
&lt;p&gt;In convention, the input is of shape &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times C \times H \times W}\)&lt;/span&gt;,
which is the assumed layout of Im2Col. However, the preferred layout for
convolution is usually &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times H
\times W \times C}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The justification is that, we usually would parallelize by accessing
a window of pixels across all the channels. Though the window spans
multiple columns in both cases, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N
\times C \times H \times W}\)&lt;/span&gt; would separate these &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; windows apart but &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times H \times W \times C}\)&lt;/span&gt;
would otherwise bring all the &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;
channel values of a pixel in a row, in which case the spatial locality
can be exploited.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times H \times W \times
C}\)&lt;/span&gt; is the assumed layout by MEC. That is, &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is of shape &lt;span class=&#34;math inline&#34;&gt;\(i_n \times i_h \times i_w \times i_c\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is of shape &lt;span class=&#34;math inline&#34;&gt;\(k_h \times k_w \times i_c \times f_c\)&lt;/span&gt;.
Given &lt;span class=&#34;math inline&#34;&gt;\(i_n \times i_c\)&lt;/span&gt; number of
&lt;span class=&#34;math inline&#34;&gt;\(o_w \times i_h k_w\)&lt;/span&gt; matrices, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is obtained by interleaving
horizontally across channels, and stacked vertically across images. As a
result, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is of shape &lt;span class=&#34;math inline&#34;&gt;\(i_n o_w \times i_h k_w i_c\)&lt;/span&gt;. Accordingly,
&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is squeezed to &lt;span class=&#34;math inline&#34;&gt;\(F&amp;#39;: k_h k_w i_c \times f_c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The example below shows a &lt;span class=&#34;math inline&#34;&gt;\(3 \times 7
\times 7 \times 1\)&lt;/span&gt; input and &lt;span class=&#34;math inline&#34;&gt;\(3
\times 3 \times 1 \times 1\)&lt;/span&gt; filter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./mec-batch.svg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Given the transformed &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(F&amp;#39;\)&lt;/span&gt;, there are two methods for
the remaining dot product (convolution). One is to do &lt;span class=&#34;math inline&#34;&gt;\(L[0 : i_n o_w, s_w i_h k_w h : s_w i_h k_w h + k_h
k_w c] \times F&amp;#39;\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(h = 0,
\dots, o_h\)&lt;/span&gt;, resulting in a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{H \times N \times W \times C}\)&lt;/span&gt;
layout (note that each bar in &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;
expands to a row in &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt;), illustrated
at the upper right in the above figure. If we want to chain up
convolutional layers, we need to convert this layout to &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times H \times W \times
C}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another is to do &lt;span class=&#34;math inline&#34;&gt;\(L[o_w n : o_w (n+1), s_w
i_h k_w h : s_w i_h k_w h + k_h k_w c]\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(h = 0, \dots, o_h, n = 0,\dots,i_n\)&lt;/span&gt;,
resulting in a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{N \times H \times W
\times C}\)&lt;/span&gt; layout, illustrated at the lower right in the above
figure (the tensor is not properly drawn though; it should have been of
shape &lt;span class=&#34;math inline&#34;&gt;\(3 \times 5 \times 5\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2 id=&#34;direct-convmmdirect-convdirect-mm-1direct-mm-2direct-mm-3&#34;&gt;Direct
Conv/MM&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;GEMM usually requires extra time and space to do transformation.
In-place convolution/matrix multiplication also have room for
improvement.&lt;/p&gt;
&lt;h3 id=&#34;loop-reordering&#34;&gt;Loop Reordering&lt;/h3&gt;
&lt;p&gt;Nested loop is very common in computation. If the effective
statements only appear in the innermost loop, the nesting level of loop
indices can be usually be changed without causing side effect.&lt;/p&gt;
&lt;p&gt;One reason to shuffle the loop indices is to better exploit the
spatial locality. Another is to exploit the temporal locality, or called
input reuse. As an example,&lt;/p&gt;
&lt;div class=&#34;sourceCode&#34; id=&#34;cb4&#34;&gt;&lt;pre class=&#34;sourceCode c&#34;&gt;&lt;code class=&#34;sourceCode c&#34;&gt;&lt;span id=&#34;cb4-1&#34;&gt;&lt;a href=&#34;#cb4-1&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// before reordering&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-2&#34;&gt;&lt;a href=&#34;#cb4-2&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; matmul&lt;span class=&#34;op&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-3&#34;&gt;&lt;a href=&#34;#cb4-3&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  memset&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;op&#34;&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-4&#34;&gt;&lt;a href=&#34;#cb4-4&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-5&#34;&gt;&lt;a href=&#34;#cb4-5&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-6&#34;&gt;&lt;a href=&#34;#cb4-6&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; k &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; k &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; k&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-7&#34;&gt;&lt;a href=&#34;#cb4-7&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        C&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;+=&lt;/span&gt; A&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt; B&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;k&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-8&#34;&gt;&lt;a href=&#34;#cb4-8&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-9&#34;&gt;&lt;a href=&#34;#cb4-9&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-10&#34;&gt;&lt;a href=&#34;#cb4-10&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-11&#34;&gt;&lt;a href=&#34;#cb4-11&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-12&#34;&gt;&lt;a href=&#34;#cb4-12&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-13&#34;&gt;&lt;a href=&#34;#cb4-13&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// after reordering&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-14&#34;&gt;&lt;a href=&#34;#cb4-14&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;void&lt;/span&gt; matmul_ikj&lt;span class=&#34;op&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-15&#34;&gt;&lt;a href=&#34;#cb4-15&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  memset&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;C&lt;span class=&#34;op&#34;&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-16&#34;&gt;&lt;a href=&#34;#cb4-16&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; i&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-17&#34;&gt;&lt;a href=&#34;#cb4-17&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; k &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; k &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; k&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-18&#34;&gt;&lt;a href=&#34;#cb4-18&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;cf&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;dt&#34;&gt;int&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j &lt;span class=&#34;op&#34;&gt;&amp;lt;&lt;/span&gt; n&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt; j&lt;span class=&#34;op&#34;&gt;++)&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-19&#34;&gt;&lt;a href=&#34;#cb4-19&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        C&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;+=&lt;/span&gt; A&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;i&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;k&lt;span class=&#34;op&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt; B&lt;span class=&#34;op&#34;&gt;[&lt;/span&gt;k&lt;span class=&#34;op&#34;&gt;][&lt;/span&gt;j&lt;span class=&#34;op&#34;&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-20&#34;&gt;&lt;a href=&#34;#cb4-20&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;      &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-21&#34;&gt;&lt;a href=&#34;#cb4-21&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-22&#34;&gt;&lt;a href=&#34;#cb4-22&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;  &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb4-23&#34;&gt;&lt;a href=&#34;#cb4-23&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before reordering, when innermost loop traverses over &lt;code&gt;k&lt;/code&gt;,
there will be one output reuse (&lt;code&gt;C[i][j]&lt;/code&gt;), one cache hit
(&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache miss (&lt;code&gt;B[k][j]&lt;/code&gt;. After
reordering, when innermost loop traverses over &lt;code&gt;j&lt;/code&gt;, there
will be one input reuse (&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache hit
(&lt;code&gt;B[k][j]&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;loop-unrolling&#34;&gt;Loop Unrolling&lt;/h3&gt;
&lt;p&gt;At the end of a loop, there is usually a branch checking to determine
whether to exit the loop or not. Loop unrolling tries to reduce the
number of branch checking in loop (see &lt;a href=&#34;https://www.wikiwand.com/en/Duff&amp;#39;s_device&#34;&gt;Duff’s device&lt;/a&gt;).
If the number of loops is known in advance, as is usually the case in a
&lt;code&gt;for&lt;/code&gt; loop, the number of branch checking can be reduced by
repeating the loop statements several times:&lt;/p&gt;
&lt;div class=&#34;sourceCode&#34; id=&#34;cb5&#34;&gt;&lt;pre class=&#34;sourceCode c&#34;&gt;&lt;code class=&#34;sourceCode c&#34;&gt;&lt;span id=&#34;cb5-1&#34;&gt;&lt;a href=&#34;#cb5-1&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;/* count &amp;gt; 0 and count % 8 == 0 assumed */&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-2&#34;&gt;&lt;a href=&#34;#cb5-2&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// before unrolling&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-3&#34;&gt;&lt;a href=&#34;#cb5-3&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;send&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;to&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; from&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; count&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-4&#34;&gt;&lt;a href=&#34;#cb5-4&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-5&#34;&gt;&lt;a href=&#34;#cb5-5&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;register&lt;/span&gt; count&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-6&#34;&gt;&lt;a href=&#34;#cb5-6&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-7&#34;&gt;&lt;a href=&#34;#cb5-7&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-8&#34;&gt;&lt;a href=&#34;#cb5-8&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-9&#34;&gt;&lt;a href=&#34;#cb5-9&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(--&lt;/span&gt;count &lt;span class=&#34;op&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-10&#34;&gt;&lt;a href=&#34;#cb5-10&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-11&#34;&gt;&lt;a href=&#34;#cb5-11&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-12&#34;&gt;&lt;a href=&#34;#cb5-12&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;co&#34;&gt;// after unrolling&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-13&#34;&gt;&lt;a href=&#34;#cb5-13&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;send&lt;span class=&#34;op&#34;&gt;(&lt;/span&gt;to&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; from&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; count&lt;span class=&#34;op&#34;&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-14&#34;&gt;&lt;a href=&#34;#cb5-14&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;dt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to&lt;span class=&#34;op&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-15&#34;&gt;&lt;a href=&#34;#cb5-15&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;dt&#34;&gt;register&lt;/span&gt; count&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-16&#34;&gt;&lt;a href=&#34;#cb5-16&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-17&#34;&gt;&lt;a href=&#34;#cb5-17&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;dt&#34;&gt;register&lt;/span&gt; n &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; count &lt;span class=&#34;op&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-18&#34;&gt;&lt;a href=&#34;#cb5-18&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;cf&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-19&#34;&gt;&lt;a href=&#34;#cb5-19&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-20&#34;&gt;&lt;a href=&#34;#cb5-20&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-21&#34;&gt;&lt;a href=&#34;#cb5-21&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-22&#34;&gt;&lt;a href=&#34;#cb5-22&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-23&#34;&gt;&lt;a href=&#34;#cb5-23&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-24&#34;&gt;&lt;a href=&#34;#cb5-24&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-25&#34;&gt;&lt;a href=&#34;#cb5-25&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-26&#34;&gt;&lt;a href=&#34;#cb5-26&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;        &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;to &lt;span class=&#34;op&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;from&lt;span class=&#34;op&#34;&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-27&#34;&gt;&lt;a href=&#34;#cb5-27&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;    &lt;span class=&#34;op&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;cf&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;(--&lt;/span&gt;n &lt;span class=&#34;op&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;op&#34;&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span id=&#34;cb5-28&#34;&gt;&lt;a href=&#34;#cb5-28&#34; aria-hidden=&#34;true&#34; tabindex=&#34;-1&#34;&gt;&lt;/a&gt;&lt;span class=&#34;op&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Manual loop unrolling makes little sense nowadays since modern CPU
can smartly predict the correct branch and modern compiler can
automatically optimize the loop code.&lt;/p&gt;
&lt;h3 id=&#34;write-caching&#34;&gt;Write Caching&lt;/h3&gt;
&lt;h3 id=&#34;tilingtiled-mmtiled-mm-multithreaded&#34;&gt;Tiling&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Tiling utilizes the matrix multiplication property that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
A =
\begin{pmatrix}
A_{11} &amp;amp; A_{12} \\
A_{21} &amp;amp; A_{22}
\end{pmatrix},
B =
\begin{pmatrix}
B_{11} &amp;amp; B_{12} \\
B_{21} &amp;amp; B_{22}
\end{pmatrix} \\ \\
A B =
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp;amp; A_{11} B_{12} + A_{12} B_{22} \\
A_{21} B_{11} + A_{22} B_{21} &amp;amp; A_{21} B_{12} + A_{22} B_{22} \\
\end{pmatrix}
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Tiling divides the matrix into blocks that can better fit in the
cache line, so that the temporal locality can be exploited.&lt;/p&gt;
&lt;h3 id=&#34;vectorization-simdvectorization&#34;&gt;Vectorization (SIMD)&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id=&#34;array-packing&#34;&gt;Array Packing&lt;/h3&gt;
&lt;h2 id=&#34;dataflow-optimization&#34;&gt;Dataflow Optimization&lt;/h2&gt;
&lt;h3 id=&#34;systolic-arraysystolic-array&#34;&gt;Systolic Array&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Systolic array is dataflow trick implemented in hardware level to
speed up compute-bound task. It is an analog to the heart beat: in
systolic array setting, memory is the heart, which bumps data (blood) to
(usually a regular array of) processing elements (cells) and then
recycle (processing result).&lt;/p&gt;
&lt;p&gt;A whole bandwidth of data would certainly entail a bunch of PEs to
digest. These PEs can have local memory and execution kernel, which
means they can be any kind of computing devices. PEs also connect to
each for passing data. All that’s left to do is to properly orchestrate
the data flow.&lt;/p&gt;
&lt;p&gt;The crux is that, instead of bumping one piece of data to a single
processing element (PE), bringing the memory bandwidth to the utmost
utilization would be more efficient. Other than that, once data is
brought out from memory, it and its intermediate result can be used
effectively at each PE it passes through.&lt;/p&gt;
&lt;h2 id=&#34;elimination-of-multiplicationmnnfaster-mm&#34;&gt;Elimination of
Multiplication&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multiplication is way more time-consuming than addition. Therefore
generally, we are willing to trade off with more additions for less
multiplications. The two algorithms below can reduce the number of
multiplications in matrix computation.&lt;/p&gt;
&lt;h3 id=&#34;strassens-algorithmstrassen-implstrassen-analysis&#34;&gt;Strassen’s
Algorithm&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Firstly we look at the &lt;strong&gt;Strassen’s algorithm&lt;/strong&gt; of
matrix computation. Suppose we do the matrix multiplication on two
square matrices &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; following the idea of blockwise
multiplication. We split the two matrices into &lt;span class=&#34;math display&#34;&gt;\[
M = \begin{bmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{bmatrix},
N = \begin{bmatrix}
E &amp;amp; F \\
G &amp;amp; H
\end{bmatrix}
\]&lt;/span&gt; Then we calculate the intermediate matrices &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
S_1 &amp;amp;= (B-D)(G+H) \\
S_2 &amp;amp;= (A+D)(E+H) \\
S_3 &amp;amp;= (A-C)(E+F) \\
S_4 &amp;amp;= (A+B)H \\
S_5 &amp;amp;= A(F-H) \\
S_6 &amp;amp;= D(G-E) \\
S_7 &amp;amp;= (C+D)E \\
\end{align*}
\]&lt;/span&gt; And the final result will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{bmatrix}
\begin{bmatrix}
E &amp;amp; F \\
G &amp;amp; H
\end{bmatrix}
=
\begin{bmatrix}
S_1 + S_2 - S_4 + S_6 &amp;amp; S_4 + S_5 \\
S_6 + S_7 &amp;amp; S_2 - S_3 + S_5 - S_7
\end{bmatrix}
\]&lt;/span&gt; The derivation of matrix multiplication with Strassen’s
algorithm can be formulated as &lt;span class=&#34;math display&#34;&gt;\[
T(n) =
\begin{cases}
\Theta(1), &amp;amp; \text{if $n=1$;} \\
7 \Theta(\frac{n}{2}) + \Theta(n^2), &amp;amp; \text{if $n&amp;gt;1$.}
\end{cases}
\]&lt;/span&gt; The master theorem provides an asymptotic analysis for
divide-and-conquer recurrence like this. Let &lt;span class=&#34;math inline&#34;&gt;\(T(n)\)&lt;/span&gt; be a monotonically increasing
function that satisfies &lt;span class=&#34;math display&#34;&gt;\[
T(n) = a T(\frac{n}{b}) + f(n) \\
T(1) = c
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(a \ge 1, b \ge 2, c &amp;gt;
0\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(f(n) \in \Theta(n^d)\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(d \ge 0\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
T(n) = \begin{cases}
\Theta(n^d) &amp;amp; \text{if $a &amp;lt; b^d$} \\
\Theta(n^d \log n) &amp;amp; \text{if $a = b^d$} \\
\Theta(n^{\log_b a}) &amp;amp; \text{if $a &amp;gt; b^d$} \\
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The time complexity of Strassen’s algorithm is &lt;span class=&#34;math inline&#34;&gt;\(\Theta(n^{\log_2 7}) \approx
\Theta(n^{2.8074})\)&lt;/span&gt;. There are 7 multiplications and 18
additions (recall that subtraction is addition in computer arithmetics)
in Strassen’s algorithm. In usual blockwise matrix multiplication, these
numbers are 8 and 4. These extra 14 additions in Strassen’s algorithm
may drag down its performance when input size is small. Other than that,
the memory access pattern of Strassen’s algorithm is quite chaotic.
There are many temp matrices of different shapes generated during the
execution. Besides, floating-point errors will accumulate in Strassen’s
large number of additions. These factors may constitute the reason why
Stassen algorithm is not widely adopted.&lt;/p&gt;
&lt;h3 id=&#34;winograd-algorithmwinograd&#34;&gt;Winograd Algorithm&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In 1-D convolution, let &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; be the
length of the output vector and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;
be the length of kernel. The baseline implementation would require &lt;span class=&#34;math inline&#34;&gt;\(mr\)&lt;/span&gt; multiplications. But it is argued that
the minimum number of required multiplications is &lt;span class=&#34;math inline&#34;&gt;\(m + r - 1\)&lt;/span&gt; (denoted as &lt;span class=&#34;math inline&#34;&gt;\(F(m,r)\)&lt;/span&gt; the corresponding algorithm).&lt;/p&gt;
&lt;p&gt;Similarly in 2-D convolution, let &lt;span class=&#34;math inline&#34;&gt;\(m
\times n\)&lt;/span&gt; be the output dimension and &lt;span class=&#34;math inline&#34;&gt;\(r \times s\)&lt;/span&gt; be the kernel dimension. The
baseline implementation would require &lt;span class=&#34;math inline&#34;&gt;\(m n r
s\)&lt;/span&gt; multiplications. But the minimum number of required
multiplications is &lt;span class=&#34;math inline&#34;&gt;\((m + r - 1)(n + s -
1)\)&lt;/span&gt; (denoted as &lt;span class=&#34;math inline&#34;&gt;\(F(m \times n, r
\times s)\)&lt;/span&gt; the corresponding algorithm).&lt;/p&gt;
&lt;p&gt;The Winograd paper documents the following algorithm for &lt;span class=&#34;math inline&#34;&gt;\(F(2,3)\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
F(2,3) =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 \\
d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}
\begin{bmatrix}
g_0 \\
g_1 \\
g_2
\end{bmatrix}
=
\begin{bmatrix}
m_1 + m_2 + m_3 \\
m_2 - m_3 - m_4
\end{bmatrix}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
m_1 = (d_0 - d_2) g_0, m_2 = (d_1 + d_2) \frac{g_0 + g_1 + g_2}{2} \\
m_4 = (d_1 - d_3) g_2, m_3 = (d_2 - d_1) \frac{g_0 - g_1 + g_2}{2}
\]&lt;/span&gt; Actually, this can be written in matrix form as &lt;span class=&#34;math display&#34;&gt;\[
Y =A^T [(G g) \odot (B^T d)]
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
B^T =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; -1 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; -1
\end{bmatrix},
G =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
\frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} \\
\frac{1}{2} &amp;amp; -\frac{1}{2} &amp;amp; \frac{1}{2} \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} \\
A^T =
\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; -1 &amp;amp; -1
\end{bmatrix}\\
g =
\begin{bmatrix}
g_0 &amp;amp; g_1 &amp;amp; g_2
\end{bmatrix}^T \\
d =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}^T
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(F(m \times m, r \times r)\)&lt;/span&gt;
can be built upon &lt;span class=&#34;math inline&#34;&gt;\(F(m, r)\)&lt;/span&gt;. For
example, &lt;span class=&#34;math inline&#34;&gt;\(F(2 \times 2, 3 \times 3)\)&lt;/span&gt;
is &lt;span class=&#34;math display&#34;&gt;\[
Y&amp;#39; = A^T \left[ [G g G^T] \odot [B^T d B] \right] A
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(F(m \times n, r \times s)\)&lt;/span&gt;
can be built upon &lt;span class=&#34;math inline&#34;&gt;\(F(m, r)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F(n, s)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Winograd algorithm is great. One problem is that the &lt;span class=&#34;math inline&#34;&gt;\(A,B,G\)&lt;/span&gt; matrix are too specific. For
input/kernel of different sizes, &lt;span class=&#34;math inline&#34;&gt;\(A,B,G\)&lt;/span&gt; will be greatly different. For a
convolutional neural network that involves inputs/kernels of varying
sizes, Winograd algorithm is not suitable for acceleration on
special-purpose hardware, which is usually dedicated to a fixed type of
computation.&lt;/p&gt;
&lt;h2 id=&#34;sparse-computationsparse&#34;&gt;Sparse Computation[^sparse]&lt;/h2&gt;
&lt;p&gt;Structured sparsity can reduce computation when there is zero in the
multiplicands. But this involves zero-check for each element of the
filter, which might ruin the CPU pipeline however. To avoid zero-check
in a sparse matrix, we may as well store the positions of all the
nonzero elements. During computation, only involved nonzero elements
will be multiplied and accumulated to obtain the final result.&lt;/p&gt;
&lt;h3 id=&#34;sparse-matrix-multiplication&#34;&gt;Sparse Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;For a sparse matrix, we either store it in &lt;strong&gt;compressed sparse
row&lt;/strong&gt; (CSR) or &lt;strong&gt;compressed sparse column&lt;/strong&gt; (CSC)
(or other formats&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;). That is, for every row/column, we
maintain a linked list whose nodes store the nonzero value and its
offset in this row/column.&lt;/p&gt;
&lt;p&gt;The compression is done after training. The choice of CSR or CSC in
matrix multiplication seems arbitrary, so long as one matrix is CSR and
the other is CSC. But in practice, convolution is done row-by-row. We
need to access the feature map across rows more often than across
columns. Thus, feature map is stored in CSR and the filter is stored in
CSC.&lt;/p&gt;
&lt;h3 id=&#34;sparse-sparse-convolution&#34;&gt;Sparse-sparse Convolution&lt;/h3&gt;
&lt;p&gt;Sometimes not only the filter, but also the feature map is sparse,
e.g. in point cloud case. We may as well apply the Im2Col and then apply
the sparse matrix multiplication. But better still, we hope to directly
apply the sparse convolution&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt;
on the original feature map.&lt;/p&gt;
&lt;p&gt;But does sparse-sparse convolution always work? It depends. The crux
is that we don’t just carry out a single sparse-sparse convolution.
Modern models consist of multiple layers. We hope the output of each
layer is sparse so that sparse-sparse convolution can be chained up. If
the output of any intermediate layer is not sparse enough, the whole
process becomes pointless.&lt;/p&gt;
&lt;h2 id=&#34;tensor-virtual-machine&#34;&gt;Tensor Virtual Machine&lt;/h2&gt;
&lt;p&gt;Current neural network frameworks translate models into a computation
graph in ONNX format, which in turn is translated into hardware
instructions by the compilers from different manufacturers
(e.g. TensorRT for NVIDIA GPU, MNN for ARM Cortex-A CPU, OpenVINO for
Intel CPU).&lt;/p&gt;
&lt;h2 id=&#34;cuda&#34;&gt;CUDA&lt;/h2&gt;
&lt;p&gt;Ideally, the computation can be parallelized for greater speedup.
CUDA provides such API for parallel computation on GPU. One key concept
of CUDA is its granularity of execution: grid -&amp;gt; block -&amp;gt; thread
(from coarsest to finest).&lt;/p&gt;
&lt;h1 id=&#34;model-trick&#34;&gt;Model Trick&lt;/h1&gt;
&lt;p&gt;Other than implementation tricks, the matrix computation can be sped
up by multiplication with zero. Block of zeros usually allows us to jump
a series of block computation when using the implementation tricks
mentioned before. Other than that, it saves space due to the sparse
matrix storage model (a kind of &lt;strong&gt;model compression&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;It would be great if there are multiple blocks of zeros in the
matrix. Better still, these zeros won’t undermine the model results
much.&lt;/p&gt;
&lt;h2 id=&#34;sparsification&#34;&gt;Sparsification&lt;/h2&gt;
&lt;p&gt;Sparsification tries to zero parameters in block during training. It
does so by adding special regularization term to the loss function.
Typical sparsity analysis assumes the linear-regression-like problem.
Denote the original loss as &lt;span class=&#34;math inline&#34;&gt;\(\ell_w(X,
Y)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is the MSE
loss function, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the data, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the target (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may be a feature map or label vector)
and &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is the model parameters
(interpreted as a vector). Then, the problem is formulated as &lt;span class=&#34;math display&#34;&gt;\[
\min_{w} \ell_w(X, Y) \triangleq ||Y - X w||_F
\]&lt;/span&gt; We may force an extra &lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt;
norm term on &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\hat \ell_w(X, Y) \triangleq \ell_w(X, Y) + \lambda ||w||_p
\]&lt;/span&gt; Note that &lt;span class=&#34;math display&#34;&gt;\[
||w||_0 = \sum_{i} \mathbb{1}[w_i \ne 0] \\
||w||_1 = \sum_{i} |w_i| \\
||w||_2 = \sqrt{\sum_{i} x_i^2} \\
||w||_\infty = \max_{i} |x_i|
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(l_0\)&lt;/span&gt; norm is a direct
attempt to penalize nonzero parameters. However, &lt;span class=&#34;math inline&#34;&gt;\(\lambda ||w||_0\)&lt;/span&gt; is not continuous at
points when there is a zero entry in &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;; when &lt;span class=&#34;math inline&#34;&gt;\(\forall i, w_i \ne 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda ||w||_0\)&lt;/span&gt; does not contribute
gradient at all. There is no analytical way to determine &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; should be zero or not. The only
course open is to manually set each &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; to zero. But this method is
prohibitive in terms of the complexity: there are &lt;span class=&#34;math inline&#34;&gt;\(2^{|w|}\)&lt;/span&gt; combinations to try (there are &lt;a href=&#34;https://www.wikiwand.com/en/Matching_pursuit&#34;&gt;orthogonal matching
pursuit&lt;/a&gt; &amp;lt;??&amp;gt; and &lt;a href=&#34;https://www.jmlr.org/papers/volume19/17-194/17-194.pdf&#34;&gt;other
methods&lt;/a&gt; that try to approximate it though). Thus, the &lt;span class=&#34;math inline&#34;&gt;\(l_0\)&lt;/span&gt; norm is ruled out for
consideration.&lt;/p&gt;
&lt;p&gt;Then &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm pops up. &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm is good since there is an
analytical solution to its gradient w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. (why does &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm add sparsity?)&lt;/p&gt;
&lt;h3 id=&#34;structured-sparsity&#34;&gt;Structured Sparsity&lt;/h3&gt;
&lt;p&gt;Better zero parameters is that zero parameters appear block-wise.
Block-wise zero entries are the essence of sparsity. However, the Lasso
term does not guarantee zero entries appear in block. To amend it,
&lt;strong&gt;group Lasso&lt;/strong&gt; trick is invented and the regularized loss
becomes &lt;span class=&#34;math display&#34;&gt;\[
\hat \ell_w(X, Y) = \ell_w(X, Y) + \sum_j \lambda_j ||\beta_j||_1
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; is the
&lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt; norm of &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th group of parameters that usually
spatially near.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./structured-sparsity.svg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Structured sparsity precedes random sparsity, because random sparsity
does not secure a regular memory access pattern, so that there will be a
poor cache locality. The figure above illustrates from irregular
structured sparsity to regular structured sparsity&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt;.
Obviously, regular structured sparsity is preferred so long as it won’t
undermine the model performance much.&lt;/p&gt;
&lt;h3 id=&#34;nonlinearity-approximationnonlinear-approx&#34;&gt;Nonlinearity
Approximation&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As mentioned, typical sparsification analyzes the linear regression
problem, which has two components: 1) the model as a linear function; 2)
the MSE loss function. The MSE loss function does well, not among the
best though, in most tasks. But mere linear model won’t do
generally.&lt;/p&gt;
&lt;p&gt;Most nonlinearity in nonlinear model comes from the element-wise
function mapping on the tensor obtained by linear transformation of
input, e.g. the response on feature map obtained by convolution (in this
case, &lt;span class=&#34;math inline&#34;&gt;\(X,w,X w\)&lt;/span&gt; will be the
transformed input, kernel and convolution result respectively, as shown
in &lt;a href=&#34;#GEMM&#34;&gt;GEMM&lt;/a&gt; section). Our focus is still on sparsity of
the linear component in the nonlineear model.&lt;/p&gt;
&lt;p&gt;The objective becomes &lt;span class=&#34;math display&#34;&gt;\[
\min_w ||Y - f(X w)||_F
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a nonlineear
function like &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{ReLU}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;pruning&#34;&gt;Pruning&lt;/h2&gt;
&lt;p&gt;Pruning is a kind of post-processing trick, which happens after
training, to make parameters more “zero”.&lt;/p&gt;
&lt;h3 id=&#34;channel-pruning&#34;&gt;Channel Pruning&lt;/h3&gt;
&lt;p&gt;Channel pruning boils down to the following optimization problem:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i
\right\Vert_F^2 \\
\text{s.t.} \quad &amp;amp; ||\beta||_0 \le c&amp;#39;
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note here that &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; is not the
feature map at &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th channel, but
instead the sampled window of kernel size (&lt;span class=&#34;math inline&#34;&gt;\(k_h \times k_h\)&lt;/span&gt;) on &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th feature map. It is known that this
problem is NP-hard because of the &lt;span class=&#34;math inline&#34;&gt;\(l_0\)&lt;/span&gt; norm term in the constraint. In
practice, it can be relaxed to (why and really?) &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i
\right\Vert_F^2 \\
\text{s.t.} \quad &amp;amp; ||\beta||_1 \le c&amp;#39;&amp;#39; \and \forall i,
||W_i||_F = 1
\end{aligned}
\]&lt;/span&gt; Note that kernels &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is
also included for optimization. This is because after pruning, kernels
may still be fine-tuned a bit to preserve the accuracy.&lt;/p&gt;
&lt;p&gt;The above formula can be optimized in an alternative fashion:
i.e. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is fixed and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is to be optimized; then &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is fixed and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is to be optimized. We also add a
regularization term for each &lt;span class=&#34;math inline&#34;&gt;\(W_i\)&lt;/span&gt;.
This is because optimizing &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is
materially a linear regression task, which easily has infinitely many
solution due to the dimension of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; unless there is an extra
constraint.&lt;/p&gt;
&lt;h3 id=&#34;low-rank-decomposition&#34;&gt;Low-rank Decomposition&lt;/h3&gt;
&lt;p&gt;Take the matrix multiplication as an example to appreciate the idea
of low-rank decomposition: &lt;span class=&#34;math display&#34;&gt;\[
X \times W \Rightarrow X \times U \times V
\]&lt;/span&gt; We may decompose &lt;span class=&#34;math inline&#34;&gt;\(W: m \times
n\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(U: m \times r\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(N: r \times n\)&lt;/span&gt; where hopefully &lt;span class=&#34;math inline&#34;&gt;\(r &amp;lt; n,m\)&lt;/span&gt; (note that when &lt;span class=&#34;math inline&#34;&gt;\(r = \rank W\)&lt;/span&gt;, there exists &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; that can fully recover &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;). In this way, storage cost can be
saved and computation can be sped up.&lt;/p&gt;
&lt;p&gt;Note that our objective is not to reconstruct the matrix &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; (called &lt;strong&gt;matrix
approximation&lt;/strong&gt;) with some other lower-rank matrices. Or else
simply the singular value decomposition would do the job. Instead, we
are to reconstruct the model output (called &lt;strong&gt;matrix
regression&lt;/strong&gt;). Therefore, the problem is formulated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  X W||_F \\
\text{s.t.} \quad &amp;amp; W = U V \\
&amp;amp; \rank U, \rank V \le r
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;combination-with-other-methods&#34;&gt;Combination with Other
Methods&lt;/h4&gt;
&lt;p&gt;Low-rank decomposition can be combined with sparsity and nonlinearity
approximation to give the following ultimate pruning problem: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||_F \\
\text{s.t.} \quad &amp;amp; W = A + B \\
&amp;amp; ||A||_0 \le S \\
&amp;amp; \rank B \le L \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(|| \cdot ||_0\)&lt;/span&gt; is the
number of nonzero entries in the matrix.&lt;/p&gt;
&lt;p&gt;The sparsity comes from &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and
low-rank decomposition comes from &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. The above problem is NP-hard due to
the constraints on &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. We can relax these constraints to
other forms: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||_F \\
\text{s.t.} \quad &amp;amp; W = A + B \\
&amp;amp; ||A||_{21} \le S  \\
&amp;amp; ||B||_* \le L \\
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(|| \cdot ||_{21}\)&lt;/span&gt;
takes the &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm of the &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt; norms taken on each column of the
matrix; and &lt;span class=&#34;math inline&#34;&gt;\(|| \cdot ||_*\)&lt;/span&gt; is the
nuclear norm that equals to the sum of singular values.&lt;/p&gt;
&lt;p&gt;To solve it, alternating direction method of multipliers (ADMM) can
be adopted. The augmented Lagrangian function is (??) &lt;span class=&#34;math display&#34;&gt;\[
L(W,A,B,\Lambda) = ||Y -  f(X W)||_F + \lambda_1 ||A||_{21} + \lambda_2
||B||_* + \Lambda \odot (W - A - B) + \frac{\rho}{2}||W - A - B||_F^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;tensor-decomposition&#34;&gt;Tensor Decomposition&lt;/h4&gt;
&lt;p&gt;Tucker decomp, CP decomp, Tucker-2 (or TT) decomp&lt;/p&gt;
&lt;h2 id=&#34;quantizationquant-pytorch&#34;&gt;Quantization&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Model parameters are natively floating-point numbers. During
training, model parameters usually won’t be too large and won’t exceed
the range of &lt;code&gt;int8&lt;/code&gt;. One way to make the model smaller and
run faster is to map model parameters to integers of smaller size (say
from &lt;code&gt;fp32&lt;/code&gt; to &lt;code&gt;int16&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Other than the efficiency perspective, if the original values span a
very narrow range (say from -10 to 10), by mapping them to a wider range
of integers, better precision may even be obtained. This is mainly due
to that most floating-point number arithmetic suffer from precision
loss. Particularly, when two floating-point numbers of significant
difference adds or subtracts, a great loss in precision can occur
(called &lt;strong&gt;catastrophic cancellation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;After integer arithmetic, we can map these integer values back again.
The process involved is called
&lt;strong&gt;quantization/dequantization&lt;/strong&gt;. For a number &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, its quantization is &lt;span class=&#34;math inline&#34;&gt;\(Q(r) = \text{Int}(r/k) - b\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the scale and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the bias. To dequantize, &lt;span class=&#34;math inline&#34;&gt;\(\hat r = k(Q(r) + b)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./quant_sym.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./quant_asym.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;The first question to consider &lt;strong&gt;symmetric quantization or
asymmetric quantization&lt;/strong&gt; (the figure above). That is, should the
zero in the original domain mapped to zero or &lt;span class=&#34;math inline&#34;&gt;\(-b\)&lt;/span&gt; (because &lt;span class=&#34;math inline&#34;&gt;\(\text{Int}(0/k)-b = -b\)&lt;/span&gt;). In essence, this
is a question of choice of bias. Preferring to computation efficiency,
we hope bias is zero. To show it, consider the dequantization process of
the matrix product: &lt;span class=&#34;math display&#34;&gt;\[
A = k_A \times A_Q + b_A \\
B = k_B \times B_Q + b_B \\
A B = k_A k_B A_Q B_Q + k_A b_B A_Q ＋ k_B b_A B_Q + b_A b_B
\]&lt;/span&gt; It would have been cleaner if &lt;span class=&#34;math inline&#34;&gt;\(b_A,
b_B\)&lt;/span&gt; are zero. However, if the activations in the network are
mostly non-negative, like in the case where ReLU is used, symmetric
quantization would waste half of the quantization range.&lt;/p&gt;
&lt;p&gt;The second question is to consider using the &lt;strong&gt;restricted range
or the full range&lt;/strong&gt; of the target integer type. Take
&lt;code&gt;int8&lt;/code&gt; for an example, should we map numbers to &lt;span class=&#34;math inline&#34;&gt;\([-127, 127]\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\([-128, 127]\)&lt;/span&gt;? When symmetric quantization
is used, the answer is the restricted range. The reason is that, the
quantization of two numbers of the same magnitude but different signs,
should be of the same magnitude but different signs too. But had the
full range been used, supposing the floating-point range was &lt;span class=&#34;math inline&#34;&gt;\([-2.2, 2.2]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(2.2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-2.2\)&lt;/span&gt; would have been quantized into &lt;span class=&#34;math inline&#34;&gt;\(2.2 \times \frac{127}{2.2} = 127\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(-2.2 \times \frac{128}{2.2} = -128\)&lt;/span&gt;
respectively. The problem is that the scaling factors for positive
number and negative number are different due to the asymmetric range, in
which case a small bias would be introduced and leads to precision
loss.&lt;/p&gt;
&lt;p&gt;The third question is the timing of quantization. Should the
quantization happen during training (&lt;strong&gt;quantization-aware
training&lt;/strong&gt;) or after training (&lt;strong&gt;post-training
quantization&lt;/strong&gt;)? Accompanying this question is what the best
scale should be.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Post-training (static) quantization (PTQ)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./ptq.svg&#34; style=&#34;zoom: 25%;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;In PTQ, model are trained before quantized. After training, a small
subset of training data (called &lt;strong&gt;calibration data&lt;/strong&gt;) is
used to determine the scale (magnitude) and the clipping range
(quantization bit number). Notice that model parameters are fixed in
this process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quantization-aware training (QAT)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./qat.svg&#34; style=&#34;zoom:25%;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;In QAT, model parameters are quantized and then trained. QAT brings a
great save in computation during training. But this yields another
question: should the quantization happen during forward pass or during
back propagation?&lt;/p&gt;
&lt;p&gt;Quantization is used in forward pass but not in back propagation. The
reason not to update gradient in quantized domain is that, numbers in
quantized domain are integers but gradient is fractional. Another reason
not to back-propagate in quantized domain is that, the gradient might be
so large as to disturb the convergence of model, compared with that in
dequantized domain.&lt;/p&gt;
&lt;p&gt;The error is measured between the dequantized output and the target
output. Model parameters will update in dequantized format and will be
re-quantized for next round of training. There are arguments on both
sides for this approach. One good aspect is that it takes the
quantization error into consideration. But this causes the gradient
mismatch because the gradient of the parameters are computed with
quantized values but updated in dequantized format. In worst case, this
may cause the model to diverge.&lt;/p&gt;
&lt;p&gt;This training method reconciles with the idea of stochastic neuron,
where back propagation is done by &lt;strong&gt;straight-through
estimator&lt;/strong&gt; (STE)&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt;. There is another
method based on STE and called &lt;strong&gt;parameterized clipping
activation&lt;/strong&gt; (PACT)&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt;. The meat of PACT is a
learnable ReLU function: &lt;span class=&#34;math display&#34;&gt;\[
y = \mathrm{PACT}(x, \alpha) = 0.5 (|x| - |x-\alpha| + \alpha) =
\begin{cases}
0, &amp;amp; x \le 0 \\
x, &amp;amp; 0 &amp;lt; x &amp;lt; \alpha \\
\alpha, &amp;amp; x \ge \alpha
\end{cases}
\]&lt;/span&gt; Back-propagation is done w.r.t. the dequantized value &lt;span class=&#34;math inline&#34;&gt;\(y_{dq} \triangleq \lfloor y \cdot \frac{2^k -
1}{\alpha} \rfloor \frac{\alpha}{2^k - 1}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial y_{dq}}{\partial y}\)&lt;/span&gt; is set
to &lt;span class=&#34;math inline&#34;&gt;\(\frac{\text{range before
quant.}}{\text{range of quant. domain}}\)&lt;/span&gt; as would be in STE.
&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is learnable so that
clipping range can be dynamically adjusted: &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial y}{\partial \alpha} =
\begin{cases}
0, &amp;amp; x &amp;lt; \alpha \\
1, &amp;amp; x \ge \alpha
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The fourth question is the granularity of quantization. There are
usually three kinds of choices, namely channel-wise, layer-wise and
group-wise quantization.&lt;/p&gt;
&lt;h3 id=&#34;bnntnnbnn&#34;&gt;BNN/TNN&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Binarized and ternary neural network bring quantization to an
extreme. The reason to particularly split them out under &lt;a href=&#34;#quantization&#34;&gt;quantization&lt;/a&gt; is that, their multiplication and
addition logic are quite different. They use 1 bit and 2 bits for
clipping: BNN maps values to -1 and 1; TNN maps values to -1, 0 and 1.
Then the multiplication and addition only involve Boolean operations,
which will be much faster. Other than that, the training process of
BNN/TNN resembles that of quantization.&lt;/p&gt;
&lt;p&gt;The motivation behind is that model parameters are usually within
&lt;span class=&#34;math inline&#34;&gt;\([-1, 1]\)&lt;/span&gt;. So why not try just using
-1, 0 and 1? Given a weight matrix &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; (squeezed into an &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; vector), there is an analytical
solution to the best scaling factor &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and the quantized value &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; (squeezed into an &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; vector) for BNN:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B^* &amp;amp;= \arg \min_{B} \| W - kB \|_2 = \arg \min_{B} \| W - kB \|_2^2
\\
&amp;amp;= \arg \min_{B} W^T W - 2 k W^T B + k^2 B^T B \\
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is known and
&lt;span class=&#34;math inline&#34;&gt;\(B \in \{ -1, 1 \}^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W^T W\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B^T
B\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B^* = \arg \min_{B} k^2 n + c - 2 k W^T B = \mathrm{sign}(W) \\
k = \frac{W^T B^*}{n}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;knowledge-distillation&#34;&gt;Knowledge Distillation&lt;/h2&gt;
&lt;p&gt;Knowledge distillation is a model compression method in which a
smaller model is trained to mimic the pre-trained larger model. The
larger and the smaller model are referred to as “teacher” and “student”
respectively.&lt;/p&gt;
&lt;p&gt;There are three kinds of distillation methods:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Response-based knowledge&lt;/p&gt;
&lt;p&gt;This is perhaps the easiest one to think of: given the same input,
the output (usually a categorical distribution) should be the same.&lt;/p&gt;
&lt;p&gt;In this case, the distillation loss is set to be the cross entropy
between the distributions output by the teacher and the student.
Minimizing the cross entropy between the output distributions
equivalently minimizes the their KL-divergence, given that teacher’s
distribution is fixed.&lt;/p&gt;
&lt;p&gt;On the other hand, the student model can further be rectified by the
ground-truth loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Feature-based knowledge&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Relation-based knowledge&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;network-architecture-search&#34;&gt;Network Architecture Search&lt;/h2&gt;
&lt;p&gt;There are mainly two indexes for a network: one is the
&lt;strong&gt;latency&lt;/strong&gt; and the other is &lt;strong&gt;accuracy&lt;/strong&gt;. In
NAS, latency is cheaper to check upon than accuracy, since the training
is more time-consuming than a single pass of input.&lt;/p&gt;
&lt;p&gt;NAS is mostly based on heuristics. During training, we can save time
by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;early stop&lt;/li&gt;
&lt;li&gt;warm restart (parameter reuse)&lt;/li&gt;
&lt;li&gt;use the arrogate target like FLOPs (which is usually proportional to
latency)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for searching in the solution space, we can do with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grid search&lt;/li&gt;
&lt;li&gt;random sampling&lt;/li&gt;
&lt;li&gt;reinforcement learning&lt;/li&gt;
&lt;li&gt;evolutional algorithm&lt;/li&gt;
&lt;li&gt;Bayesian optimization like Gaussian process&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h1&gt;
&lt;p&gt;Related courses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.cs.ucsb.edu/~tyang/class/240a17/&#34;&gt;CS240A -
Applied Parallel Computing (ucsb.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hanlab.mit.edu/courses/2023-fall-65940&#34;&gt;MIT 6.5940
Fall 2023 TinyML and Efficient Deep Learning Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes footnotes-end-of-document&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1911.05662&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1706.06873&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://ieeexplore.ieee.org/document/10144741&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://tvm.apache.org/docs/how_to/optimize_operators/opt_gemm.html&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://ieeexplore.ieee.org/document/6877334&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://spatial-lang.org/dotprod&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;http://csapp.cs.cmu.edu/3e/waside/waside-blocking.pdf&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://penny-xu.github.io/blog/tiled-matrix-multiplication&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://docs.juliahub.com/LoopVectorization/4TogI/0.9.8/examples/matrix_multiplication/&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://www.youtube.com/watch?v=8zbh4gWGa7I&amp;amp;t=424s&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/2002.12418&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://www.youtube.com/watch?v=DruwS2_cVys&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://martin-thoma.com/strassen-algorithm-in-python-java-cpp/&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://www.diva-portal.org/smash/get/diva2:1219121/FULLTEXT01.pdf&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1509.09308&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://ceca.pku.edu.cn/docs/20191126112405101722.pdf&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1&#34;&gt;How
does sparse convolution work? | by Zhiliang Zhou | Towards Data
Science&lt;/a&gt;&lt;a href=&#34;#fnref17&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn18&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rancheng.github.io/Sparse-Convolution-Explained/&#34;&gt;Sparse
Convolution explained with code – Ran Cheng – Robotics, Vision,
Learning.&lt;/a&gt;&lt;a href=&#34;#fnref18&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/pdf/1705.08922.pdf&lt;a href=&#34;#fnref19&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1411.4229&lt;a href=&#34;#fnref20&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://pytorch.org/blog/quantization-in-practice/&lt;a href=&#34;#fnref21&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn22&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1308.3432&lt;a href=&#34;#fnref22&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1805.06085&lt;a href=&#34;#fnref23&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn24&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;https://arxiv.org/abs/1603.05279&lt;a href=&#34;#fnref24&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;


</description>
    </item>
    
    <item>
      <title>假设检验</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link>
      <pubDate>Tue, 06 Dec 2022 19:02:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid>
      <description>

&lt;h2 id=&#34;假设检验&#34;&gt;假设检验&lt;/h2&gt;
&lt;p&gt;参数估计是根据样本值得出参数的一个估计量，并且在区间估计中，我们还能得到一个这个参数&lt;strong&gt;置信区间&lt;/strong&gt;和相应的&lt;strong&gt;置信水平&lt;/strong&gt;。而在假设检验中，我们要做的则是根据某个&lt;strong&gt;假设&lt;/strong&gt;（hypothesis）以及给定的样本，决定是否接受这个假设。注意我们特地将假设和估计（estimation）区分开，因为这个假设并不是由样本得到的一个估计量，而是一条已有的断言（assertion）；我们要做的，则是给出在什么样的情况下（对应置信区间），我们能够以多高的信心否定这个断言（对应置信水平）。&lt;/p&gt;
&lt;p&gt;至于假设检验和区间估计的关系，其实我们会发现，如果本身就能对未知参数做有效的区间估计，那其实对它的假设检验设计，自然迎刃而解。&lt;/p&gt;
&lt;h3 id=&#34;一般检验方法&#34;&gt;一般检验方法&lt;/h3&gt;
&lt;p&gt;一般检验方法指Neyman-Pearson方法。&lt;/p&gt;
&lt;h4 id=&#34;建立假设&#34;&gt;建立假设&lt;/h4&gt;
&lt;p&gt;对要检验的问题，一般有一个&lt;strong&gt;原假设&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;（也叫零假设，null
hypothesis）以及一个&lt;strong&gt;备择假设&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;（alternative
hypothesis）。原假设一般是总体的某个未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;等于某个具体值&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
H_0: \theta = \theta_0
\]&lt;/span&gt; 这种只包含一个假设值（即&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;）的原假设又叫作&lt;strong&gt;简单原假设&lt;/strong&gt;（simple
hypothesis
null）。而备择假设一般和原假设互斥，它通常有以下三种形式：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta \ne
\theta_0\)&lt;/span&gt;，此时&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;为对立关系，我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;两侧的可能，这样的检测问题也称为双边检验（two-sided
test）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta &amp;gt;
\theta_0\)&lt;/span&gt;，此时我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;右侧的可能，这样的检测问题也称为右侧的单边检验；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta &amp;lt;
\theta_0\)&lt;/span&gt;，此时我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;左侧的可能，这样的检测问题也称为左侧的单边检验；&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;选择否定域形式&#34;&gt;选择否定域形式&lt;/h4&gt;
&lt;p&gt;根据已有的样本，我们能够给出未知参数的点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;（在假设检验中又称作&lt;strong&gt;检验统计量&lt;/strong&gt;&amp;lt;test
statistic&amp;gt;），如果&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;的距离小于某个&lt;strong&gt;临界值&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(c &amp;gt; 0\)&lt;/span&gt;（critical
value），我们就可以接受原假设（即便&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;不完全相等），否则则否定原假设。使得原假设被接受的样本所在的区域就被称作&lt;strong&gt;接受域&lt;/strong&gt;（acceptance
region）；使得原假设被否定的样本所在的区域就被称作&lt;strong&gt;否定域&lt;/strong&gt;（也叫拒绝域，rejection
region）；一般我们习惯先构造否定域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;，则剩余区域就为接受域&lt;span class=&#34;math inline&#34;&gt;\(\overline W\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
W = \{ (x_1, \dots, x_n) \big | |\hat \theta(x_1, \dots, x_n) -
\theta_0| &amp;gt; c \} \\
\overline W = W^c
\end{gather}
\]&lt;/span&gt;
对于某些参数，可能本身越小越好（比如故障率），所以我们仅需要进行右侧的单边检测，此时对应的否定域为
&lt;span class=&#34;math display&#34;&gt;\[
W = \{ (x_1, \dots, x_n) \big | \hat \theta(x_1, \dots, x_n) - \theta_0
&amp;gt; c \} \\
\]&lt;/span&gt; 此时可以等价认为&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \le
\theta_0\)&lt;/span&gt;，这种情况下，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;是一个&lt;strong&gt;复合原假设&lt;/strong&gt;（composite
hypothesis null），因为其包含的假设值不止一个。&lt;/p&gt;
&lt;p&gt;对于另外一些参数，可能本身越大越好（比如身高均值），所以我们仅需要左侧的单边检测，此时对应的否定域为
&lt;span class=&#34;math display&#34;&gt;\[
W = \{ (x_1, \dots, x_n) \big | \hat \theta(x_1, \dots, x_n) - \theta_0
&amp;lt; -c \} \\
\]&lt;/span&gt; 同理，此时可以等价认为&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta
\ge \theta_0\)&lt;/span&gt;，这种情况下，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;也是一个复合原假设。&lt;/p&gt;
&lt;h4 id=&#34;设定显著性水平&#34;&gt;设定显著性水平&lt;/h4&gt;
&lt;p&gt;给定假设，我们已经可以根据样本属于接受域还是否定域，做出接受或是否定假设的决策了。但和点估计中的问题一样，我们依然是基于样本提供的不完全信息做出的判断，所以我们的判断不总是正确的。这种判断会有四种结果：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;判断：接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;判断：否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;实际：&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立&lt;/td&gt;
&lt;td&gt;判断正确&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;第一类错误&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;实际：&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;成立&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;第二类错误&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;判断正确&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;通常较低的第一类错误风险&lt;span class=&#34;math inline&#34;&gt;\(P(\text{否定}H_0;
H_0\text{成立})\)&lt;/span&gt;和较低的第二类错误风险&lt;span class=&#34;math inline&#34;&gt;\(P(\text{接受}H_0;
H_1\text{成立})\)&lt;/span&gt;不可兼得，因为在检验统计量确定后，这两个概率主要是由临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;导出的否定域大小来控制的。而我们更希望降低第一类错误发生的风险（意味着更大的&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;），也就是说我们一旦否定，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;很大概率确实是不成立的；尽管这意味着我们在接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;有可能不成立——不过虚惊一场总好过后知后觉。所以实际应用中，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;往往对应了比较严重的结果，我们不希望在&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立时，我们却没有发现（即否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;）；或者&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;本身就对应了我们比较想否定的结果，这样我们否定时，它确实不成立的概率也更高。&lt;/p&gt;
&lt;p&gt;我们会将第一类错误发生的概率限制在&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;之内，这个&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;便是&lt;strong&gt;显著性水平&lt;/strong&gt;（significance
level）。显著性水平其实代表了我们对小概率事件的接受程度，即我们认为概率小于&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的事件应该是小概率事件，并且是不应该被正好碰上的；而此时在&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立的假设下，“给定的一组样本属于否定域”正是这样的一个小概率事件，如果碰上了这样的小概率事件，则有理由怀疑&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;不成立。&lt;/p&gt;
&lt;h4 id=&#34;确定临界值&#34;&gt;确定临界值&lt;/h4&gt;
&lt;p&gt;在确定显著性水平后，我们便可以进一步确定临界值，从而给出完整的否定域。此时我们调整临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，从而使得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(\text{否定}H_0; H_0\text{成立}) = P( (X_1, \dots, X_n) \in W;\theta =
\theta_0) \\
= P(|\hat \theta(X_1, \dots, X_n) - \theta_0| &amp;gt; c; \theta = \theta_0)
\le \alpha
\end{gathered}
\]&lt;/span&gt; 问题就变成了一个简单的分布问题。此处需要指出的是，&lt;span class=&#34;math inline&#34;&gt;\(P(\text{否定}H_0;
H_0\text{成立})\)&lt;/span&gt;不应写作&lt;span class=&#34;math inline&#34;&gt;\(P( (X_1,
\dots, X_n) \in W | \theta =
\theta_0)\)&lt;/span&gt;，因为此处讨论的是频率学派中的假设检验，频率学派中的未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;并没有先验分布。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P( (X_1, \dots, X_n) \in W;\theta =
\theta_0)\)&lt;/span&gt;又叫作&lt;strong&gt;功效函数&lt;/strong&gt;（power
function），记作&lt;span class=&#34;math inline&#34;&gt;\(\beta_W(\cdot)\)&lt;/span&gt;，它表示在未知参数取特定值时，一组随机样本属于否定域的概率。
前面我们之所以说单边检验等价于&lt;u&gt;原假设对应某个形式的复合假设&lt;/u&gt;（比如&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \le \theta_0\)&lt;/span&gt;或&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \ge
\theta_0\)&lt;/span&gt;），是因为这种情况下，有 &lt;span class=&#34;math display&#34;&gt;\[
\max_{h \in H_0} \beta_W(h) = \beta_W(\theta_0) \le \alpha
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;p值和p值检验法&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值和&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验法&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验法指的是数学家Fisher提出的检验方法。&lt;/p&gt;
&lt;p&gt;假设检验的&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值是在原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立的情况下，检验统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;出现其具体观测值&lt;span class=&#34;math inline&#34;&gt;\(z = \hat
\theta(x_1,\dots,x_n)\)&lt;/span&gt;或者比之更极端的值的概率，即&lt;span class=&#34;math inline&#34;&gt;\(p = P(\hat \theta = z; \theta =
\theta_0)\)&lt;/span&gt;（类似likelihood）。&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验中，我们检验&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值是否足够小，如果&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值小到一定程度，我们还是会否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，即&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p \le
\alpha\)&lt;/span&gt;，则我们在显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;下否定原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p &amp;gt;
\alpha\)&lt;/span&gt;，则我们在显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;下接受原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/291968/are-type-i-error-fwer-both-conditional-probabilities&#34;&gt;hypothesis
testing - Are type I error &amp;amp; FWER both conditional probabilities? -
Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/hezhijian/book/test.html&#34;&gt;第 3 章
假设检验 | 数理统计讲义 (bookdown.org)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>贝叶斯分类器</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link>
      <pubDate>Mon, 22 May 2023 09:43:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid>
      <description>

&lt;h2 id=&#34;贝叶斯分类器&#34;&gt;贝叶斯分类器&lt;/h2&gt;
&lt;p&gt;贝叶斯分类器（Bayes classifier）基于“能够根据标签&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;预测特征&lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;”的思想，在训练环节，它学习&lt;span class=&#34;math inline&#34;&gt;\(p(\x|y)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;；在预测环节，它给出&lt;span class=&#34;math inline&#34;&gt;\(\arg \max_y p(y|\x) = \arg \max_y p(\x|y)
p(y)\)&lt;/span&gt;。可以看出，贝叶斯分类器的两个关键部分便是“似然”和“先验”，其预测环节采用了最大后验估计的思想，从这个角度或许能够理解为什么它叫做“贝叶斯”分类器。&lt;/p&gt;
&lt;h3 id=&#34;朴素贝叶斯分类器&#34;&gt;朴素贝叶斯分类器&lt;/h3&gt;
&lt;p&gt;贝叶斯分类器在训练环节学习&lt;span class=&#34;math inline&#34;&gt;\(p(\x|y)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;可以直接由频率估计得来，而&lt;span class=&#34;math inline&#34;&gt;\(p(\x|y)\)&lt;/span&gt;这个条件概率则不太好求，尤其当&lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;为多元随机变量时，其各个成分之间的关系难以捕捉。&lt;/p&gt;
&lt;p&gt;为简化问题，朴素贝叶斯分类器（naive Bayes
classifier）做出这样的假设：&lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;的各个成分在&lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;给定的情况下，是相互独立的，此时有&lt;span class=&#34;math inline&#34;&gt;\(p(\x|y) = p(x_1|y) \dots
p(x_n|y)\)&lt;/span&gt;。进一步，为了求解各个成分在&lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;给定时的条件概率分布，我们还可以对条件概率的分布形式作出假设，比如假设&lt;span class=&#34;math inline&#34;&gt;\(p(x_i|y)\)&lt;/span&gt;服从伯努利分布、多项分布、高斯分布（分别对应伯努利朴素贝叶斯分类器&amp;lt;Bernoulli
naive Bayes classifier&amp;gt;、多项分布朴素贝叶斯分类器&amp;lt;multinomial
naive Bayes classifier&amp;gt;、高斯分布朴素贝叶斯&amp;lt;Gaussian naive Bayes
classifier&amp;gt;）等等。&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯最优分类器&#34;&gt;贝叶斯最优分类器&lt;/h2&gt;
&lt;p&gt;给定贝叶斯分类器&lt;span class=&#34;math inline&#34;&gt;\(f \in
\mathcal{H}\)&lt;/span&gt;，给定样本&lt;span class=&#34;math inline&#34;&gt;\((\x,y)\)&lt;/span&gt;，0-1损失函数&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;定义为， &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\I}{\mathbb{I}} l(f(\x), y) = \I[f(\x) \ne y] = 1 - \I[f(\x)
= y]
\]&lt;/span&gt; 我们的目标是最小化&lt;span class=&#34;math inline&#34;&gt;\(l(f) \triangleq
\E_{\x,y} l(f(\x), y)\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;l(f) = \sum_{\x} \sum_{y} p(\x,y) l(f(\x), y) \\
&amp;amp;= \sum_x p(\x) [\sum_y p(y|\x) l(f(\x), y)] \\
&amp;amp;= \E_\x [\sum_y p(y|\x) l(f(\x), y)]
\end{aligned}
\]&lt;/span&gt; 用&lt;span class=&#34;math inline&#34;&gt;\(f^\star\)&lt;/span&gt;表示贝叶斯最优分类器（Bayes
optimal classifier），则&lt;span class=&#34;math inline&#34;&gt;\(f^\star = \arg
\min_{f \in \mathcal{H}} \E_\x [\sum_y p(y|\x) l(f(\x),
y)]\)&lt;/span&gt;；故对于每一个样本&lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;，都应该有： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;f^\star(\x) = \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x)
l(f(\x), y)] \\
&amp;amp;= \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x) (1 - \I[f(\x) =
y])] \\
&amp;amp;= \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x) - \sum_y p(y|\x)
\I[f(\x) = y]] \\
&amp;amp;= \arg \min_{f \in \mathcal{H}} [1 - \sum_y p(y|\x) \I[f(\x) = y]]
\\
&amp;amp;= \arg \min_{f \in \mathcal{H}} [- \sum_y p(y|\x) \I[f(\x) = y]] \\
&amp;amp;= \arg \max_{f \in \mathcal{H}} [\sum_y p(y|\x) \I[f(\x) = y]] \\
&amp;amp;= \arg \max_{y \in \mathcal{Y}} p(y|\x)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;以&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;仅有两种取值举例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f^\star(x) &amp;amp;= \arg \max_{f \in \mathcal{H}} [p(y_1|\x) \I[f(\x) =
y_1] + p(y_2|\x) \I[f(\x) = y_2] ] \\
&amp;amp;= \arg \max_{y \in \mathcal{Y}} p(y|\x)
\end{aligned}
\]&lt;/span&gt; 以上结论在&lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;为连续型随机变量或者&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;为连续型随机变量时也成立。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://artint.info/html1e/ArtInt_181.html&#34;&gt;Artificial
Intelligence - foundations of computational agents – 7.3.3 Bayesian
Classifiers (artint.info)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/567299/what-does-it-mean-for-the-bayes-classifier-to-be-optimal&#34;&gt;machine
learning - What does it mean for the Bayes Classifier to be optimal? -
Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>隐马尔可夫模型</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 22 May 2023 09:43:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</guid>
      <description>

&lt;h2 id=&#34;隐马尔可夫模型&#34;&gt;隐马尔可夫模型&lt;/h2&gt;
&lt;p&gt;隐马尔可夫模型（Hidden Markov
Model）的对象是序列类型的数据，其基本假设为：该序列的观测值实际上是来自于完全关于当前时序下的状态（state，也可叫作隐变量&amp;lt;latent
variable&amp;gt;）取值的一个条件概率分布，而状态取值亦会随着时序发生变化，且其状态变化遵从马尔可夫过程。&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots, Y_T\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;个时序下的观测值，&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_T\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;个时序下的状态取值，对于任意&lt;span class=&#34;math inline&#34;&gt;\(i = 1, \dots, T\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim p(\cdot |
X_i)\)&lt;/span&gt;；一般来说，隐马尔科夫模型中假设状态取值和观测取值都是离散的，假设共有&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;种状态取值&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_N\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;种观测取值&lt;span class=&#34;math inline&#34;&gt;\(y_1, \dots,
y_M\)&lt;/span&gt;，令状态转移矩阵（transition matrix）为&lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt;的方阵&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，令观测概率矩阵（emission
matrix）为&lt;span class=&#34;math inline&#34;&gt;\(N \times M\)&lt;/span&gt;的矩阵&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;；令&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;为初始状态概率向量。&lt;/p&gt;
&lt;p&gt;隐马尔可夫模型即是由&lt;span class=&#34;math inline&#34;&gt;\(\pi, A,
B\)&lt;/span&gt;三者决定，所以其可以用三元组表示： &lt;span class=&#34;math display&#34;&gt;\[
\lambda = (\pi, A, B)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;隐马尔可夫模型有三类基本问题： 1. 概率问题。给定模型&lt;span class=&#34;math inline&#34;&gt;\(\lambda = (\pi, A, B)\)&lt;/span&gt;和观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y = (y_{o_1}, \dots,
y_{o_T})\)&lt;/span&gt;，计算模型&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;产生观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y\)&lt;/span&gt;的概率。 2. 学习问题。给定观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y = (y_{o_1}, \dots,
y_{o_T})\)&lt;/span&gt;，估计模型参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda =
(\pi, A, B)\)&lt;/span&gt;。 3.
预测问题（也叫解码&amp;lt;decoding&amp;gt;问题）。给定模型&lt;span class=&#34;math inline&#34;&gt;\(\lambda = (\pi, A, B)\)&lt;/span&gt;和观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y = (y_{o_1}, \dots,
y_{o_T})\)&lt;/span&gt;，求使得条件概率&lt;span class=&#34;math inline&#34;&gt;\(P(\X |
\Y)\)&lt;/span&gt;最大的状态序列&lt;span class=&#34;math inline&#34;&gt;\(\X\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;概率问题&#34;&gt;概率问题&lt;/h3&gt;
&lt;h4 id=&#34;暴力解法&#34;&gt;暴力解法&lt;/h4&gt;
&lt;p&gt;在预测问题中，最直接的想法便是首先枚举出所有的状态序列，然后根据状态序列计算观测序列的概率。某个状态序列&lt;span class=&#34;math inline&#34;&gt;\(\X = (x_{i_1}, \dots,
x_{i_T})\)&lt;/span&gt;出现的概率为 &lt;span class=&#34;math display&#34;&gt;\[
P(\X) = \pi_{i_1} a_{i_1, i_2} a_{i_2, i_3} \dots a_{i_{T-1}, i_T}
\]&lt;/span&gt; 给定这个状态序列&lt;span class=&#34;math inline&#34;&gt;\(\X\)&lt;/span&gt;的情况下，观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y\)&lt;/span&gt;出现的概率为 &lt;span class=&#34;math display&#34;&gt;\[
P(\Y | \X) = b_{i_1, o_1} b_{i_2, o_2} \dots b_{i_T, o_T}
\]&lt;/span&gt; 两者同时出现的联合概率为 &lt;span class=&#34;math display&#34;&gt;\[
P(\X, \Y) = P(\X) \times P(\Y | \X) = \pi_{i_1} a_{i_1, i_2} b_{i_1,
o_1} \dots a_{i_{T-1}, i_T} b_{i_T, o_T}
\]&lt;/span&gt; 然后对上式关于所有可能的状态序列求和，得到 &lt;span class=&#34;math display&#34;&gt;\[
P(\Y) = \sum_\X P(\X, \Y) = \sum_{i_1, i_2, \dots, i_T} \pi_{i_1}
a_{i_1, i_2} b_{i_1, o_1} \dots a_{i_{T-1}, i_T} b_{i_T, o_T}
\]&lt;/span&gt; 该式共有&lt;span class=&#34;math inline&#34;&gt;\(N^T\)&lt;/span&gt;项，每一项有&lt;span class=&#34;math inline&#34;&gt;\(O(T)\)&lt;/span&gt;次相乘，故整体复杂度为&lt;span class=&#34;math inline&#34;&gt;\(O(T N^T)\)&lt;/span&gt;，难以接受。&lt;/p&gt;
&lt;h4 id=&#34;前向算法&#34;&gt;前向算法&lt;/h4&gt;
&lt;p&gt;首先引入前向概率的概念。给定隐马尔可夫模型&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;及观测序列&lt;span class=&#34;math inline&#34;&gt;\(\Y = (y_{o_1}, \dots,
y_{o_T})\)&lt;/span&gt;，定义到时刻&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的观测序列为给定的&lt;span class=&#34;math inline&#34;&gt;\((y_{o_1}, \dots,
y_{o_t})\)&lt;/span&gt;且此时状态为&lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;的概率为前向概率，记作 &lt;span class=&#34;math display&#34;&gt;\[
\alpha_t(i) = P(Y_1 = y_{o_1}, \dots, Y_T = y_{o_t}, X_T = x_i; \lambda)
\]&lt;/span&gt; 初始情况下，即&lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;时，有
&lt;span class=&#34;math display&#34;&gt;\[
\alpha_1(i) = \pi_i a_{i, o_1}
\]&lt;/span&gt; 对&lt;span class=&#34;math inline&#34;&gt;\(t = 2, \dots, T\)&lt;/span&gt;，有
&lt;span class=&#34;math display&#34;&gt;\[
\alpha_t(i) = \left[ \sum_{j=1}^N \alpha_{t-1}(j) a_{j, i} \right] b_{i,
o_t}
\]&lt;/span&gt; 最终， &lt;span class=&#34;math display&#34;&gt;\[
P(\Y) = \sum_{i=1}^N \alpha_T(i)
\]&lt;/span&gt;
前向算法主要利用了状态变化的序列结构，记录了状态变化的中间过程，从而节省了计算时间。整个算法最耗时的步骤为递推这一步，&lt;span class=&#34;math inline&#34;&gt;\(\alpha_t(i)\)&lt;/span&gt;对应的加和表达式包含了&lt;span class=&#34;math inline&#34;&gt;\(O(N)\)&lt;/span&gt;个项，而对于每一个时刻&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;个这样的加和表达式，故&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;个时刻整体耗时为&lt;span class=&#34;math inline&#34;&gt;\(O(N^2 T)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;学习问题&#34;&gt;学习问题&lt;/h3&gt;
&lt;h4 id=&#34;有监督学习&#34;&gt;有监督学习&lt;/h4&gt;
&lt;p&gt;有监督学习是较好处理的一种问题，在这种情况下，&lt;span class=&#34;math inline&#34;&gt;\(\X, \Y\)&lt;/span&gt;都已知，需要我们估计模型参数&lt;span class=&#34;math inline&#34;&gt;\(\pi, A, B\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;设样本中状态由&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;转移到&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;的频数为&lt;span class=&#34;math inline&#34;&gt;\(A_{i, j}\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a_{i, j}\)&lt;/span&gt;的估计为 &lt;span class=&#34;math display&#34;&gt;\[
\hat a_{i, j} = \frac{A_{i, j}} {\sum_{k=1}^N A_{i, k}}
\]&lt;/span&gt; 设样本中状态为&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;且观测为&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;的频数是&lt;span class=&#34;math inline&#34;&gt;\(B_{i, j}\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(b_{i, j}\)&lt;/span&gt;的估计为 &lt;span class=&#34;math display&#34;&gt;\[
\hat b_{i, j} = \frac{B_{i, j}} {\sum_{k=1}^N B_{i, k}}
\]&lt;/span&gt; 至于初始状态向量&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;，若样本中由多条时序链，&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;亦可由相应频次估计而来。&lt;/p&gt;
&lt;h4 id=&#34;无监督学习&#34;&gt;无监督学习&lt;/h4&gt;
&lt;p&gt;多数情况下，状态——或者说隐变量&lt;span class=&#34;math inline&#34;&gt;\(\X\)&lt;/span&gt;——是没有办法观测到的，而这时就可以采用&lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/expectation-maximization/&#34;&gt;EM算法&lt;/a&gt;，通过优化似然函数的下界，找出最好的模型参数。EM算法在隐马尔科夫模型中具体实现由Baum和Welch提出，故实际求解算法被称作Baum-Welch算法。&lt;/p&gt;
&lt;p&gt;此问题的似然函数以及对数似然函数分别为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\Y; \lambda) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda) \\
\log P(\Y; \lambda) &amp;amp;= \log \sum_{\X} P(\X, \Y; \lambda)
\end{aligned}
\]&lt;/span&gt; 根据EM算法， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\log P(\Y; \lambda) = \log \E_{\X \sim P(\cdot | \Y; \lambda^t)}
P(\X, \Y; \lambda) \\
&amp;amp;\ge \E_{\X \sim P(\cdot | \Y; \lambda^t)} \log P(\X, \Y; \lambda)
\triangleq Q(\lambda^t, \lambda) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而下一轮迭代中，新的估计&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{t+1}\)&lt;/span&gt;为： &lt;span class=&#34;math display&#34;&gt;\[
\lambda^{t+1} = \arg \max_\lambda Q(\lambda^t, \lambda)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又由于 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;Q(\lambda^t, \lambda) = \E_{\X \sim P(\cdot | \Y; \lambda^t)} \log
P(\X, \Y; \lambda) \\
&amp;amp;= \sum_\X P(\X | \Y; \lambda^t) \log P(\X, \Y; \lambda) \\
&amp;amp;= \sum_\X \frac{P(\X, \Y; \lambda^t)}{P(\Y; \lambda^t)} \log P(\X,
\Y; \lambda) \\
&amp;amp;= \frac{\sum_\X P(\X, \Y; \lambda^t) \log P(\X, \Y; \lambda)}{P(\Y;
\lambda^t)} \\
\end{aligned}
\]&lt;/span&gt; 其中的&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{P(\Y;
\lambda^t)}\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;无关，所以 &lt;span class=&#34;math display&#34;&gt;\[
\lambda^{t+1} = \arg \max_\lambda \sum_\X P(\X, \Y; \lambda^t) \log
P(\X, \Y; \lambda)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意在接下来的讨论中，我们会令&lt;span class=&#34;math inline&#34;&gt;\(\X =
(x_{i_1}, \dots,
x_{i_T})\)&lt;/span&gt;，但为了整体简洁，我们不会在每一个&lt;span class=&#34;math inline&#34;&gt;\(\X\)&lt;/span&gt;出现的地方将此式展开。对上式做进一步展开，
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\sum_\X P(\X, \Y; \lambda^t) \log P(\X, \Y; \lambda) = \sum_\X
P(\X, \Y; \lambda^t) \log \pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \dots
a_{i_{T-1}, i_T} b_{i_T, o_T} \\
&amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \log \pi_{i_1} + \sum_{\X} P(\X,
\Y; \lambda^t) \sum_{t=1}^{T-1} a_{i_t, i_{t+1}} + \sum_{\X} P(\X, \Y;
\lambda^t) \sum_{t=1}^T \log b_{i_t, o_t}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们可以对其中的三项分别最大化，以达到整体最大化。&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;注意到&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;满足约束条件&lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^N \pi_j =
1\)&lt;/span&gt;，写出其拉格朗日函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L_\pi (\pi, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \log \pi_{i_1}
+ \gamma (\sum_{j=1}^N \pi_j - 1) \\
&amp;amp;= \sum_{j=1}^N P(\Y, i_1 = j; \lambda^t) \log \pi_{j} + \gamma
(\sum_{j=1}^N \pi_j - 1)
\end{aligned}
\]&lt;/span&gt; 对上式关于&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;求导并令其为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，得到对于任意&lt;span class=&#34;math inline&#34;&gt;\(j = 1, \dots, N\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{P(\Y, i_1 = j; \lambda^t)}{\pi_j} + \gamma &amp;amp;= 0 \\
P(\Y, i_1 = j; \lambda^t) + \pi_j \gamma &amp;amp;= 0 \label{pi-eq}
\end{align}
\]&lt;/span&gt; 对上式关于&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;求和，得到
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{j=1}^N P(\Y, i_1 = j; \lambda^t) + \sum_{j=1}^N \pi_j \gamma = 0
\\
\gamma = -P(\Y; \lambda^t)
\]&lt;/span&gt; 重新代入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{pi-eq}\)&lt;/span&gt;得到
&lt;span class=&#34;math display&#34;&gt;\[
\pi_j = \frac{P(\Y, i_1=j; \lambda^t)}{P(\Y; \lambda^t)}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;注意到&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;满足约束条件&lt;span class=&#34;math inline&#34;&gt;\(\forall j = 1, \dots, N, \sum_{k=1}^N a_{j, k} =
1\)&lt;/span&gt;，写出其拉格朗日函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L_A (A, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^{T-1}
\log a_{i_t, i_{t+1}} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^N a_{j, k} -
1) \\
&amp;amp;= \sum_{j=1}^N \sum_{k=1}^N \sum_{t=1}^{T-1} P(\Y, i_1=j, i_2=k;
\lambda^t) \log a_{j, k} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^N a_{j, k}
- 1)
\end{aligned}
\]&lt;/span&gt; 类似对&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的求解，我们可以得到 &lt;span class=&#34;math display&#34;&gt;\[
a_{j, k} = \frac{\sum_{t=1}^{T-1} P(\Y, i_1=j, i_2=k;
\lambda^t)}{\sum_{t=1}^{T-1} P(\Y, i_1=j; \lambda^t)}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;注意到&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;满足约束条件&lt;span class=&#34;math inline&#34;&gt;\(\forall j = 1, \dots, N, \sum_{k=1}^M b_{j, k} =
1\)&lt;/span&gt;，写出其拉格朗日函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L_B (B, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^T \log
b_{i_t, o_t} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^M b_{j, k} - 1) \\
&amp;amp;= \sum_{j=1}^N \sum_{t=1}^T P(\Y, i_t=j; \lambda^t) \log b_{j, o_t}
+ \sum_{j=1}^N \gamma_j (\sum_{k=1}^M b_{j, k} - 1)
\end{aligned}
\]&lt;/span&gt; 类似对&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的求解，我们可以得到 &lt;span class=&#34;math display&#34;&gt;\[
b_{j, k} = \frac{\sum_{t=1}^T P(\Y, i_t=j; \lambda^t) \mathbb{I}[o_t =
y_k]}{\sum_{t=1}^T P(\Y, i_t=j; \lambda^t)}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;预测问题&#34;&gt;预测问题&lt;/h3&gt;
&lt;p&gt;预测问题常用的算法是Viterbi算法，它是通过动态规划求解出一条最优路径。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>特征函数</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 09 May 2022 11:51:08 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid>
      <description>

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;h3 id=&#34;感性认知&#34;&gt;感性认知&lt;/h3&gt;
&lt;p&gt;根据泰勒级数我们可以得知，两个函数&lt;span class=&#34;math inline&#34;&gt;\(f(x),g(x)\)&lt;/span&gt;，如果它们各阶导数相等的越多，它们就越相似，换言之
&lt;span class=&#34;math display&#34;&gt;\[
\text{各阶导数都相同} \Rightarrow f(x) = g(x)
\]&lt;/span&gt; 可以说，函数的各阶导数即是它们的特征。&lt;/p&gt;
&lt;p&gt;对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即
&lt;span class=&#34;math display&#34;&gt;\[
\text{各阶矩都相同} \Rightarrow \text{随机变量对应的分布相同}
\]&lt;/span&gt; 对于随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，其特征函数定义为 &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{itX}\)&lt;/span&gt;的泰勒级数为 &lt;span class=&#34;math display&#34;&gt;\[
e^{itX} = 1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots +
\frac{(itX)^n}{n!}
\]&lt;/span&gt; 代入特征函数可得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\varphi(t) &amp;amp;= \E[1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots +
\frac{(itX)^n}{n!}] \\
&amp;amp;= \E[1] + \E[\frac{itX}{1!}] - \E[\frac{t^2X^2}{2!}] + \dots +
\E[\frac{(itX)^n}{n!}] \\
&amp;amp;= 1 + \frac{it \overbrace{\E[X]}^\text{一阶矩} }{1!} - \frac{t^2
\overbrace{\E[X^2]}^\text{二阶矩} }{2!} + \dots + \frac{(it)^n
\overbrace{\E[矩} }{n!} \\
\end{aligned}
\]&lt;/span&gt;
可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。&lt;/p&gt;
&lt;h3 id=&#34;理性认知&#34;&gt;理性认知&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}] = \int_{-\infty}^{+\infty} e^{itx} p(x)\; dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而对&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;进行逆傅里叶变换可得
&lt;span class=&#34;math display&#34;&gt;\[
F(t) = \int_{-\infty}^{+\infty} p(x) e^{-itx} dx
\]&lt;/span&gt; 可见二者互为共轭关系： &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \overline{F(t)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;应用&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;通过求&lt;span class=&#34;math inline&#34;&gt;\(t =
0\)&lt;/span&gt;时的各阶导数，可以快速求得各阶矩： &lt;span class=&#34;math display&#34;&gt;\[
\varphi^{(k)}(0) = i^k \E[X^k]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/23686709&#34;&gt;特征函数的理解&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mean Average Precision</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/</guid>
      <description>

&lt;h2 id=&#34;the-precision&#34;&gt;The Precision&lt;/h2&gt;
&lt;p&gt;The typical &lt;strong&gt;precision&lt;/strong&gt; and &lt;strong&gt;recall&lt;/strong&gt;
definition in a binary classification is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}},
\text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Precision determines, among all the samples that are identified as
positive, how many are really positive. Recall determines, among all the
samples that are positive, how many are successfully identified.&lt;/p&gt;
&lt;p&gt;Binary classification task will assign each case a score that shows
how confident the model is in the case is indeed positive. We can
arrange all the samples in descending order of this score. Then
beginning with an empty set, we add the sample one by one into this set.
Every time we add in a new sample, we can calculate the precision and
the recall within this set.&lt;/p&gt;
&lt;p&gt;During this process, recall will increase monotonically; but
precision may go up and down. We can draw a plot with regard to this two
numbers and this is the precision-recall curve (PRC). The area under PRC
usually indicates the goodness of the model, as that of a perfect model
will be 1.&lt;/p&gt;
&lt;h2 id=&#34;average-precision&#34;&gt;Average Precision&lt;/h2&gt;
&lt;p&gt;The “average precision” term usually appears in document retrieval
and object detection scenario. For document retrieval task,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{precision} = \frac{\{ \text{relevant documents} \} \cap \{
\text{retrieved documents} \} } {\{ \text{retrieved documents} \}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The question is where the “average” comes from. Similar to the
plotting of PRC, we may rank the retrieved documents according to the
relevance. Starting from an empty set of documents predicted as
relevant, we add these retrieved documents one by one. Then we can have
a series of precisions to average upon.&lt;/p&gt;
&lt;p&gt;On the other hand, in object detection task, predicted anchors are
firstly ranked according to its predicted objectness score. After that,
each anchor will be assigned a objectness label. The “positivity” of a
anchor is mainly determined by its intersection over union (IoU) with
the ground-truth bounding boxes. The rules for determining positivity is
complex. But as a result, we will have positive anchors, negative
anchors and those neither positive nor negative. During training, only
positive and negative anchors will contribute gradient. But in
precision, non-positive anchors are treated as “negative”. For a
detailed discussion of positivity in object detection, please refer &lt;a href=&#34;https://vignesh943628.medium.com/metrics-on-object-detection-b9fe3f1bac59&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can rank the anchors according to the objectness score. Then
similarly beginning with an empty set, we add the anchor one by one into
this set. Every time we add in a new anchor, we can calculate the
precision. By doing so, we obtain a series of precisions to average.&lt;/p&gt;
&lt;h3 id=&#34;mean-average-precision&#34;&gt;Mean Average Precision&lt;/h3&gt;
&lt;p&gt;We can also obtain a series of average precisions for different
classes of objects, yielding the concept of &lt;strong&gt;mean average
precision&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;average-mean-average-precision&#34;&gt;“Average Mean Average
Precision”&lt;/h3&gt;
&lt;p&gt;Moreover, we may change the IoU threshold to obtain a series of mean
average precisions to average upon; in some sense this is the “average
mean average precision”.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>5-conic-linear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</guid>
      <description>

&lt;p&gt;Though some problems can be relaxed to accommodate the linear
programming form, applications of LP are still limited due to its linear
constraints. To extend, a natural idea is to gradually allow for other
kinds of non-linear constraints. But that would go too far away from the
established theories for linear programming.&lt;/p&gt;
&lt;p&gt;In this post, we study the conic linear programming, which is a
phased open-up to non-linear problems.&lt;/p&gt;
&lt;h2 id=&#34;concept-preparation&#34;&gt;Concept Preparation&lt;/h2&gt;
&lt;h3 id=&#34;good-order-and-proper-cone&#34;&gt;“Good” Order and “Proper” Cone&lt;/h3&gt;
&lt;p&gt;Before exploring other non-linear problems, we first study the
properties of a “good” order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt;. between vectors. We expect it to
have the following basic three properties of a
&lt;strong&gt;partial-order&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\forall
u \in \R^n, u \succeq u\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Anti-symmetry&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v \in \R^n, u \succeq v, v \succeq u \to u = v
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transitivity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v,w \in \R^n, u \succeq v, v \succeq w \to u \succeq w
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than them, we expect &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; to further possess the following
two arithmetic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v \in \R^n, u \succeq v, \alpha \succeq 0, \alpha u \succeq
\alpha v
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Additivity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v,w,z \in \R^n, u \succeq v, w \succeq z \to u + w \succeq v +
z
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say the &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; order is
“good” if it satisfies the above five properties. The above five is from
the algebraic perspective. To illustrate it geometrically, consider the
set &lt;span class=&#34;math inline&#34;&gt;\(K \triangleq \{ x \in \R^n: x \ge 0
\}\)&lt;/span&gt;. We say &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a
&lt;strong&gt;“proper” cone&lt;/strong&gt; in that it is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;non-empty&lt;/strong&gt; and &lt;strong&gt;closed under
addition&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u, v \in K, u + v \in K
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;conic&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u \in K, \alpha &amp;gt; 0 \to \alpha u \in K \\
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;pointed&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
u, -u \in K \to u = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We further claim that a proper cone is &lt;strong&gt;convex&lt;/strong&gt; and
contains the zero vector (verify it).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The pointedness plus closeness (not closeness under addition)
implies that there is no complete line in this cone, which equivalently
means there is no non-trivial subspace (i.e., except &lt;span class=&#34;math inline&#34;&gt;\(\{ 0 \}\)&lt;/span&gt; and the universe) inside this
cone. To show it, let &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; be a closed
pointed cone, suppose on the contrary there exists &lt;span class=&#34;math inline&#34;&gt;\(u, v \in K\)&lt;/span&gt; such that for every &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in \R\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(u + \alpha(v - u) \in K\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 1\)&lt;/span&gt;, consider the sequence &lt;span class=&#34;math inline&#34;&gt;\(\{w_+^t\}_{t=1}^{\infty}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{w_-^t\}_{t=1}^{\infty}\)&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
w_+^t = \frac{u + t(v-u)}{\|u + t(v-u)\|_2}, w_-^t = \frac{u -
t(v-u)}{\|u - t(v-u)\|_2}
\]&lt;/span&gt; Now &lt;span class=&#34;math inline&#34;&gt;\(w_+^t, w_-^t \in K\)&lt;/span&gt;
for all &lt;span class=&#34;math inline&#34;&gt;\(t \ge 1\)&lt;/span&gt;. But &lt;span class=&#34;math inline&#34;&gt;\(w_+^t \to w \triangleq
\frac{v-u}{\|v-u\|_2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_-^t \to
-w\)&lt;/span&gt; . Since &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is closed, we
have &lt;span class=&#34;math inline&#34;&gt;\(w, -w \in K\)&lt;/span&gt;. However, since
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; has unit norm and is not zero
vector, it follows that &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is not
pointed, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: At times, people also refer to this as the
&lt;strong&gt;salient&lt;/strong&gt; property of a cone. A &lt;u&gt;proper cone&lt;/u&gt; is
variously defined on a subset of these properties (closeness, closeness
under addition, pointed, salient, and essentially, conic) depending on
the context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, the algebraic properties and the geometric properties can
derive each other. A good order and a proper cone have a one-to-one
relationship. Now we ask, in an arbitrary &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-d universe (or a finite-dimensional
Euclidean space), is &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; the only
good order, or equivalently, is &lt;span class=&#34;math inline&#34;&gt;\(\R_+^n\)&lt;/span&gt; the only proper cone? The answer
is no.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-proper-cone&#34;&gt;Examples of Proper Cone&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Lorentz cone / second-order cone / ice cream cone &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{Q}^{n+1} \triangleq \{ (t, x) \in \R \times \R^n: t \ge ||x||_2
\}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}^{n+1}\)&lt;/span&gt; is a
closed proper cone. Then what is the good order associated with this
proper cone?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Semi-definite cone &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{S}_+^n \triangleq \{ X \in \mathcal{S}^n: u^T X u \ge 0,
\forall u \in \R^n \}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^n\)&lt;/span&gt; is a
closed proper cone. Then what is the good order associated with this
proper cone?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Zero cone: &lt;span class=&#34;math inline&#34;&gt;\(\{ 0 \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New cones from old ones by Cartesian product&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(K_1, \dots, K_m\)&lt;/span&gt; be closed
proper cones (with non-empty interior). Then &lt;span class=&#34;math display&#34;&gt;\[
K \triangleq K_1 \times \dots \times K_m = \{ (x_1, \dots, x_m): x_i \in
K_i, \forall i=1,\dots,m \}
\]&lt;/span&gt; is a closed proper cone with non-empty interior (with
non-empty interior).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: &lt;span class=&#34;math inline&#34;&gt;\(\R_+^n, \mathcal{Q}^{n+1},
\mathcal{S}_+^n\)&lt;/span&gt; have non-empty interior: &lt;span class=&#34;math display&#34;&gt;\[
\intr(\R_+^n) = \R_{++}^n \\
\intr(\mathcal{Q}^{n+1}) = \{ (t, x) \in \R \times \R^n: t &amp;gt; ||x||_2
\} \\
\intr(\mathcal{S}_+^n) = \mathcal{S}_{++}^n \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conic-linear-programming&#34;&gt;Conic Linear Programming&lt;/h2&gt;
&lt;h3 id=&#34;formulation&#34;&gt;Formulation&lt;/h3&gt;
&lt;p&gt;Recall that linear programming is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\min &amp;amp; \quad c^T x \\
\text{s.t.} &amp;amp; \quad b - A x \ge 0
\end{align*}
\]&lt;/span&gt; Now replacing &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; with
the good order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; gives the
conic linear programming. &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\tag{CLP}
\min &amp;amp; \quad c^T x \\
\text{s.t.} &amp;amp; \quad b - A x \succeq 0
\end{align*}
\]&lt;/span&gt; The next question is, can we recover Farkas’ lemma and strong
duality in this setting? The answer is yes. And the good aspect of the
good order is that we can recover the conclusions in linear programming
verbatim. Next we generalize the LP problem and show the same result
applies.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; be a finite-dimensional
Euclidean space (e.g. &lt;span class=&#34;math inline&#34;&gt;\(\R^n,
\mathcal{S}^n\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\langle \cdot,
\cdot \rangle\)&lt;/span&gt; be the inner product on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; (e.g., on &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\langle
x, y \rangle = x^T y\)&lt;/span&gt;; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\langle X, Y \rangle = \tr(X^T Y)\)&lt;/span&gt;). Let
&lt;span class=&#34;math inline&#34;&gt;\(K \subseteq E\)&lt;/span&gt; be a &lt;strong&gt;closed
proper cone&lt;/strong&gt;. Then we can have a good order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. Consider the &lt;strong&gt;standard form of
CLP&lt;/strong&gt; as well as the primal problem: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{P} \label{primal}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; \langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall
i=1,\dots,m \\
&amp;amp; x \in K \subseteq E
\end{aligned}
\end{equation}
\]&lt;/span&gt; What is its dual? We mimic the procedure when we build the
lower bound for LP. Let &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;.
By &lt;u&gt;the linearity of inner product&lt;/u&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\langle b, y \rangle = b^T y =
\sum_{i=1}^m \langle a_i, x \rangle y_i =
\sum_{i=1}^m \langle y_i a_i, x \rangle =
\langle \sum_{i=1}^m y_i a_i, x \rangle
\]&lt;/span&gt; If we impose &lt;span class=&#34;math inline&#34;&gt;\(c \succeq
\sum_{i=1}^m y_i a_i\)&lt;/span&gt; like &lt;span class=&#34;math inline&#34;&gt;\(c \ge A^T
y\)&lt;/span&gt;, can we draw &lt;span class=&#34;math inline&#34;&gt;\(\langle c -
\sum_{i=1}^m y_i a_i, x \rangle \ge 0\)&lt;/span&gt; like &lt;span class=&#34;math inline&#34;&gt;\(x^T(c - A^T y) \ge 0\)&lt;/span&gt;? The answer is that,
this constraint is not enough in general. To reinforce the constraint,
construct the set &lt;span class=&#34;math display&#34;&gt;\[
K^* = \{ w \in E: \langle w, x \rangle \ge 0, \forall x \in K \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can draw some properties for &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is non-empty because
&lt;span class=&#34;math inline&#34;&gt;\(0 \in K^*\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is always closed and
convex (no matter what &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is,
because &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is the intersection of
hyperplanes which are closed and convex).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is conic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a closed proper cone
with non-empty interior, then so is &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; are heterogenous on the premise that
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a closed proper cone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We have (verify it) &lt;span class=&#34;math display&#34;&gt;\[
(\R_+^n)^* = \R_+^n, (\mathcal{Q}^{n+1})^* = \mathcal{Q}^{n+1},
(\mathcal{S}_+^n)^* = \mathcal{S}_+^n
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is called the
&lt;strong&gt;dual cone&lt;/strong&gt;. We impose that &lt;span class=&#34;math inline&#34;&gt;\(c
- \sum_{i=1}^m y_i a_i \in K^*\)&lt;/span&gt;. Then, the dual problem can be
written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{D} \label{dual}
\begin{aligned}
v_d^* = \sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in K^* \\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The constraint of CLP’s dual in essence describes that &lt;u&gt;an affine
map of the variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; belongs to a
cone&lt;/u&gt;. This is a good indicator on whether we are dealing with a
CLP.&lt;/p&gt;
&lt;h4 id=&#34;second-order-cone-programming&#34;&gt;Second-order Cone
Programming&lt;/h4&gt;
&lt;p&gt;Consider the second-order cone programming: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{SOCP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{Q}^{n+1}
\\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = [u_i,
\underbrace{a_{i,1}, \dots, a_{i, n}}_{\bar a_i^T}]^T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = [v, \underbrace{c_1, \dots,
c_n}_{d^T}]^T\)&lt;/span&gt;. The dual constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
c - \sum_{i=1}^m y_i a_i =
\begin{bmatrix}
v \\
d
\end{bmatrix} -
\sum_{i=1}^m y_i
\begin{bmatrix}
u_i \\
\bar a_i
\end{bmatrix}
=
\begin{bmatrix}
v - u^T y \\
d - A^T y
\end{bmatrix} \succeq 0, \\
\text{where }
\begin{aligned}[t]
A &amp;amp;= [\bar a_1, \dots, \bar a_n]^T, \\
u &amp;amp;= [u_1, \dots, u_n]^T, \\
\end{aligned}
\end{gathered}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
v - u^T y \ge \|d - A^T y\|_2
\]&lt;/span&gt; The left-hand and the right-hand side of the above are both an
affine function in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The above is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \\
(d - A^T y)^T &amp;amp; v - u^T y
\end{bmatrix}
\in \mathcal{S}_+^{n+1}
\]&lt;/span&gt; To show it, firstly the case when &lt;span class=&#34;math inline&#34;&gt;\(v - u^T y = 0\)&lt;/span&gt; trivially holds. On the
other hand, when &lt;span class=&#34;math inline&#34;&gt;\(v - u^T y &amp;gt; 0\)&lt;/span&gt;,
we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v - u^T y)^2 &amp;amp;\ge (d - A^T y)^T (d - A^T y) \\
\underbrace{(v - u^T y)}_{C} &amp;amp;\ge \underbrace{(d - A^T y)^T}_{B^T}
\underbrace{\frac{I_n}{v - u^T y}}_{A^{-1}} \underbrace{(d - A^T y)}_{B}
\end{aligned}
\]&lt;/span&gt; Consider the &lt;em&gt;Schur complement&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A &amp;amp; B \\
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}_+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}_+^m, C \in \mathcal{S}_+^n, \\
A - B C^{-1} B^T \in \mathcal{S}_+^m \text{ or } C - B^T A^{-1} B \in
\mathcal{S}_+^n
\end{gathered}
\]&lt;/span&gt; We have, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \\
(d - A^T y)^T &amp;amp; v - u^T y
\end{bmatrix}
\in \mathcal{S}_+^{n+1}
\]&lt;/span&gt; The converse similarly follows from &lt;em&gt;Schur complement&lt;/em&gt;.
Based on this result, we further claim that an SOCP is equivalent to an
semi-definite programming (SDP). But it is never necessary to solve SOCP
by converting it to SDP, which only increases the complexity.&lt;/p&gt;
&lt;h4 id=&#34;semi-definite-programming&#34;&gt;Semi-definite Programming&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{SDP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{S}_+^n \\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In terms of inclusion (and thus difficulty), LP =&amp;gt; QCQP =&amp;gt; SOCP
=&amp;gt; SDP =&amp;gt; CLP.&lt;/p&gt;
&lt;h3 id=&#34;weak-duality&#34;&gt;Weak Duality&lt;/h3&gt;
&lt;p&gt;Our previous derivation of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; implies the weak duality of
CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality of CLP&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(\langle c, \bar x \rangle \ge b^T \bar
y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
c - \sum_{i=1}^m y_i a_i \in K^* \Rightarrow \langle c - \sum_{i=1}^m
y_i a_i, x \rangle \ge 0 \\
\Downarrow \\
\begin{aligned}
\langle c, x \rangle &amp;amp;\ge \langle \sum_{i=1}^m y_i a_i, x \rangle \\
&amp;amp;= \sum_{i=1}^m \langle a_i, x \rangle y_i  \\
&amp;amp;= b^T y
\end{aligned}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We next investigate another method to convert between the primal and
dual in CLP. For a standard CLP problem, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}
\inf \quad &amp;amp; \langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall
i=1,\dots,m \\
&amp;amp; x \in K \subseteq E
\end{aligned} \quad
\substack{\langle a_i, x \rangle = b \iff b - \langle a_i, x \rangle \in
\{ 0 \} \\ \equiv} \quad
\begin{aligned}
-\sup \quad &amp;amp; -\langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \begin{bmatrix}
b \\
0
\end{bmatrix} -
\begin{bmatrix}
A \\
-I
\end{bmatrix} x
\in \{ 0 \} \times K
\end{aligned}
\end{gathered}
\]&lt;/span&gt; whose dual is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation*}
\begin{aligned}
-\inf \quad &amp;amp; [b^T, 0] [y^T, \tilde y^T]^T \\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
A^T &amp;amp; -I
\end{bmatrix}
\begin{bmatrix}
y \\
\tilde y
\end{bmatrix} = -c \\
&amp;amp;
\begin{bmatrix}
y \\
\tilde y
\end{bmatrix} \in (\{ 0 \} \times K)^*
\end{aligned}
\end{equation*}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\((\{ 0 \} \times K)^* =
\{ 0 \}^* \times K^* = \R \times K^*\)&lt;/span&gt;. And interestingly, from
above, we observe that the equality constraint is in essence a cone
constraint.&lt;/p&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas’ Lemma&lt;/h3&gt;
&lt;p&gt;Recall the Farkas’ lemma for linear systems. Exactly one of the
following two systems is solvable: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
Ax = b, x \ge 0 \\
A^T y \le 0, b^T y &amp;gt; 0
\end{gather*}
\]&lt;/span&gt; Farkas’ lemma secures a strong duality for LP. We would like
to do the same to CLP. First we mimic the two systems in CLP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\langle a_i, x \rangle = b_i, \forall i=1,\dots,m; x \in K \tag{I} \\
-\sum_{i=1}^m y_i a_i \in K^*; b^T y &amp;gt; 0 \tag{II}
\end{gather*}
\]&lt;/span&gt; Is it true that exactly one of the above two systems is
solvable? We first claim that they can’t be solvable at the same time.
Suppose on the contrary they both hold. By &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
b^T y =\sum_{i=1}^m \langle a_i, x \rangle y_i =\langle
\sum_{i=1}^m  y_i a_i, x \rangle &amp;amp;&amp;gt; 0 \\
\end{aligned}
\]&lt;/span&gt; But that &lt;span class=&#34;math inline&#34;&gt;\(-\sum_{i=1}^m y_i a_i \in
K^*\)&lt;/span&gt; implies &lt;span class=&#34;math inline&#34;&gt;\(\langle \sum_{i=1}^m
y_i a_i, x \rangle = -\langle -\sum_{i=1}^m y_i a_i, x \rangle \le
0\)&lt;/span&gt;, which is a contradiction. On the other hand, for CLP, it is
possible that neither &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(I)}\)&lt;/span&gt;
nor &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt; is solvable.&lt;/p&gt;
&lt;blockquote&gt;
Example: Let &lt;span class=&#34;math inline&#34;&gt;\(E = \mathcal{S}^2, K =
\mathcal{S}_+^2\)&lt;/span&gt;. Let &lt;span class=&#34;math display&#34;&gt;\[
A_1 = \begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 0
\end{bmatrix},
A_2 = \begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix},
b = \begin{bmatrix}
0 \\
2
\end{bmatrix}
\]&lt;/span&gt; Then $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{gathered}
\begin{cases}
\langle A_1, X \rangle = 0 \\
\langle A_2, X \rangle = 2 \\
X \in \mathcal{S}_+^2
\end{cases}
\iff
\begin{cases}
X_{11} = 0 \\
2 X_{12} = 2 \\
X \in \mathcal{S}_+^2
\end{cases}
\iff
\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; X_{22}
\end{bmatrix} \in \mathcal{S}_+^2
\text{, which is unsolvable} \\

\begin{cases}
-(y_1 A_1 + y_2 A_2) \in \mathcal{S}_+^2 \\
2 y_2 &amp;gt; 0
\end{cases}
\iff
\begin{cases}
\begin{bmatrix}
-y_1 &amp;amp; -y_2 \\
-y_2 &amp;amp; 0 \\
\end{bmatrix} \in \mathcal{S}_+^2 \\
y_2 &amp;gt; 0
\end{cases}
\text{, which is unsolvable}
\end{gathered}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is what goes wrong? Recall in the proof of the original
Farkas’ lemma, we apply the separation theorem to the setting &lt;span class=&#34;math display&#34;&gt;\[
b \notin \{ Ax: x \in \R_+^n \}
\]&lt;/span&gt; The right-hand set is always closed. However, for an arbitrary
closed proper cone &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, the set &lt;span class=&#34;math display&#34;&gt;\[
\{ (\langle a_1, x \rangle, \dots, \langle a_m, x \rangle): x \in K \}
\]&lt;/span&gt; is not always closed. Without closeness, we can’t properly
apply the &lt;em&gt;point-set separation theorem&lt;/em&gt;. For the same reason,
the optimum may not be attained in CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Continue with the &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
b\)&lt;/span&gt; in the previous example. Consider the set &lt;span class=&#34;math display&#34;&gt;\[
S = \{ (\langle A_1, X \rangle, \langle A_2, X \rangle): X \in
\mathcal{S}_+^2 \}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((\langle A_1, X \rangle, \langle
A_2, X \rangle) = (X_{11}, 2 X_{12})\)&lt;/span&gt; basically describes the
trajectory of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; varies in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^2\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
X = \begin{bmatrix}
X_{11} &amp;amp; X_{12} \\
X_{12} &amp;amp; X_{22}
\end{bmatrix}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(X_{11} = 0\)&lt;/span&gt;, the only
way to make &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; PSD is to make &lt;span class=&#34;math inline&#34;&gt;\(X_{12} = 0\)&lt;/span&gt;; if &lt;span class=&#34;math inline&#34;&gt;\(X_{11} &amp;gt; 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_{12}\)&lt;/span&gt; can be arbitrary because we can
take &lt;span class=&#34;math inline&#34;&gt;\(X_{22}\)&lt;/span&gt; sufficiently large such
that &lt;span class=&#34;math inline&#34;&gt;\(X_{11} X_{22} \ge X_{12}^2\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S = \{ (0, 0) \} \cup \{ (x, y): x &amp;gt; 0, y \in \R \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If only we can guarantee the closeness of transformation of a proper
cone!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;conic Farkas’ lemma&lt;/strong&gt;. Suppose there exists a
&lt;span class=&#34;math inline&#34;&gt;\(\bar y \in \R^m\)&lt;/span&gt; satisfying &lt;span class=&#34;math display&#34;&gt;\[
-\sum_{i=1}^m \bar y_i a_i \in \intr(K^*) \tag{Slater condition}
\]&lt;/span&gt; Then exactly one of the &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(I)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt; is solvable.&lt;/p&gt;
&lt;p&gt;The spirit of the Slater condition (and many its variants) is that
“some vector is in the interior of some set”. For this case, it
guarantees the closeness of &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;Recall the LP strong duality theorem: suppose that the primal is
feasible and is bounded below (alternatively the dual is feasible and is
bounded above); then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = v_d^*\)&lt;/span&gt;
and both the primal and the dual have optimal solutions. The question is
what about &lt;strong&gt;the duality gap and the attainment&lt;/strong&gt; of
CLP?&lt;/p&gt;
&lt;blockquote&gt;
Example: &lt;strong&gt;nonzero duality gap&lt;/strong&gt;. Consider the SDP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \inf \quad &amp;amp; y_1\\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
0 &amp;amp; y_1 &amp;amp; 0 \\
y_1 &amp;amp; y_2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1+y_1
\end{bmatrix} \in \mathcal{S}_3^+
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(y_1 = 0, y_2 = 0\)&lt;/span&gt; is the
only solution to it. Thus, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
0\)&lt;/span&gt;. It can be rewritten as $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
v_p^* = - \sup \quad &amp;amp; \underbrace{[-1, 0]}_{b^T} [y_1, y_2]^T \\
\text{s.t.} \quad &amp;amp;
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 \\
\end{bmatrix}
}_{C} -
y_1
\underbrace{
\begin{bmatrix}
0 &amp;amp; -1 &amp;amp; 0 \\
-1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; -1
\end{bmatrix}
}_{A_1} -
y_2
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; -1 \\
\end{bmatrix}
}_{A_2}

\in \mathcal{S}_3^+
\end{aligned}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Its dual is
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
v_d^* = -\inf \quad &amp;amp; X_{33} \\
\text{s.t.} \quad &amp;amp; -2 X_{12} - X_{33} = -1 \\
&amp;amp; -X_{22} = 0 \\
&amp;amp; X \in \mathcal{S}_+^3
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ The dual is feasible and &lt;span class=&#34;math inline&#34;&gt;\(X_{33}\)&lt;/span&gt; can only be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = -1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that the duality gap is nonzero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;non-attainment&lt;/strong&gt;. Consider the SOCP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \sup \quad &amp;amp; -y_1 \\
\text{s.t.} \quad &amp;amp; (y_1 + y_2, 1, y_1 - y_2) \in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; The constraint is &lt;span class=&#34;math display&#34;&gt;\[
y_1 + y_2 \ge \sqrt{1 + (y_1 - y_2)^2} \iff 4 y_1 y_2 \ge 1
\]&lt;/span&gt; This implies that &lt;span class=&#34;math inline&#34;&gt;\(y_1, y_2 &amp;gt;
0\)&lt;/span&gt;. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = 0\)&lt;/span&gt;
but cannot be attained. It can be rewritten as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \sup \quad &amp;amp; [-1, 0] [y_1, y_2]^T \\
\text{s.t. }\quad &amp;amp;
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix} -
y_1 \begin{bmatrix}
-1 \\
0 \\
-1
\end{bmatrix} -
y_2 \begin{bmatrix}
-1 \\
0 \\
1
\end{bmatrix}
\in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; Its dual is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_d^* = \inf \quad &amp;amp; z_2 \\
\text{s.t.} \quad &amp;amp; -z_1 - z_3 = -1 \\
&amp;amp; -z_1 + z_3 = 0 \\
&amp;amp; (z_1, z_2, z_3) \in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; The only solution is &lt;span class=&#34;math inline&#34;&gt;\((1/2, 0,
1/2)\)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = 0\)&lt;/span&gt; is
attained at this point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite the above, can we draw anything about the duality gap and the
attainment of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality of CLP&lt;/strong&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is bounded and satisfies
Slater’s condition, i.e. there exists a feasible &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \intr(K)\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = v_d^*\)&lt;/span&gt; and there exists an &lt;span class=&#34;math inline&#34;&gt;\(y^*\)&lt;/span&gt; that is optimal to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is bounded and satisfies
Slater’s condition, i.e. there exists a feasible &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(c - \sum_{i=1}^m \bar y_i a_i \in
\intr(K^*)\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
v_d^*\)&lt;/span&gt; and there exists an &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; that is optimal to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With Slater’s condition on one side, we can guarantee the zero
duality gap and the attainment only on the other side. The CLP strong
duality is much “weaker” than the LP strong duality. Refer to the
&lt;u&gt;non-attainment&lt;/u&gt; example.&lt;/p&gt;
&lt;p&gt;Pay attention what CLP strong duality does not say as well. Even
though the Slater’s condition is not satisfied, zero duality gap and
attainment can hold. Refer to the &lt;u&gt;nonzero duality gap&lt;/u&gt;
example.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>6-optimizaition-under-uncertainty</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</guid>
      <description>

&lt;p&gt;Firstly consider that in the previous (conic) linear programming
discussion, &lt;span class=&#34;math display&#34;&gt;\[
\tag{Problem} \begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
\]&lt;/span&gt; The coefficients &lt;span class=&#34;math inline&#34;&gt;\(\hat a_i, \hat b,
\hat c\)&lt;/span&gt; are given data. They correspond to the measurements in
the real world. But it is highly likely that these measurements are not
100% accurate. How do we take into account these uncertainties?&lt;/p&gt;
&lt;p&gt;Without loss of generality, we may assume that &lt;span class=&#34;math inline&#34;&gt;\(\hat c\)&lt;/span&gt; is deterministic, since the above
is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; t \\
\text{s.t.} \quad &amp;amp; \hat c^T x \le t \\
&amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
\]&lt;/span&gt; In this new problem, we bring all uncertainties into the
constraint and the coefficient &lt;span class=&#34;math inline&#34;&gt;\([0,\dots,0,1]\)&lt;/span&gt; for the variable &lt;span class=&#34;math inline&#34;&gt;\([x^T, t]\)&lt;/span&gt; is deterministic.&lt;/p&gt;
&lt;p&gt;The problem remains how to handle the uncertainty in the constraint
in &lt;span class=&#34;math inline&#34;&gt;\(\text{(Problem)}\)&lt;/span&gt;. There are
several viable methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stochastic optimization&lt;/p&gt;
&lt;p&gt;Stochastic optimization assumes that data follow a probability
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt; (unknown).
The constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta
\]&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt;.
This method is non-trivial, due to the integral of multi-dimensional
probability distribution function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Robust optimization&lt;/p&gt;
&lt;p&gt;The assumption is that data is drawn from a &lt;strong&gt;ambiguity
set&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}\)&lt;/span&gt;. We
require that &lt;span class=&#34;math display&#34;&gt;\[
\hat a_i^T x \le \hat b_i, \forall i=1,2,\dots, \forall (\hat a_i, \hat
b_i) \in \mathcal{U}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distributionally-robust optimization&lt;/p&gt;
&lt;p&gt;This is kind of the combination of the above two methods. The
assumption is that data follow a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt;, which in turn belongs to some
ambiguity set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}\)&lt;/span&gt;. The
constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\inf_{\mathbb{P} \in \mathcal{U}} \Pr(\hat a_i^T x \le \hat b_i) \ge 1 -
\delta, \forall i=1,2,\dots
\]&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt;
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;robust-linear-programming&#34;&gt;Robust Linear Programming&lt;/h2&gt;
&lt;p&gt;Consider the problem &lt;span class=&#34;math display&#34;&gt;\[
\label{rp} \tag{R} \begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; \underbrace{[\hat \alpha_i^T, \hat b_i]}_{\hat
a_i^T} \underbrace{[x^T, x&amp;#39;]^T}_z \le 0, \\
&amp;amp; \quad \hat a_i \in \mathcal{U}_i, i=1,\dots,m \\
&amp;amp; \underbrace{x&amp;#39;}_{z_{n+1}} = -1
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}_i \triangleq \{
y \in \R^{n+1}: y = u_i + B_i v, B_i \in \mathcal{S}_{++}^{n+1}, \|v\|
\le 1 \}\)&lt;/span&gt; is an ellipsoid.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{rp}\)&lt;/span&gt; is not LP
actually, as it may contain infinitely-many linear constraints. This is
usually hard. Just imagine its dual problem. For a primal that has
infinitely-many constraints, its dual has infinitely-many variables.&lt;/p&gt;
&lt;p&gt;The key question now is how to tackle &lt;span class=&#34;math inline&#34;&gt;\(\hat a_i^T z \le 0, \forall \hat a_i \in
\mathcal{U}_i\)&lt;/span&gt; (ignoring &lt;span class=&#34;math inline&#34;&gt;\(z_{n+1} =
-1\)&lt;/span&gt; for now). In essence, it is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left[ \sup_{\hat a_i \in \mathcal{U}_i} \hat a_i^T z \right] &amp;amp;\le 0
\iff \\
\left[ \sup_{\|v\| \le 1} (u_i + B_i v)^T z \right] &amp;amp;\le 0 \iff \\
u_i^T z + \left[ \sup_{\|v\| \le 1} v^T B_i z \right] &amp;amp;\le 0 \iff \\
y_i^T z + \|B_i z\|_2 &amp;amp;\le 0
\end{aligned}
\]&lt;/span&gt; This is exactly an SOCP constraint.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>7-quadratically-constrained-quadratic-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</guid>
      <description>

&lt;h2 id=&#34;quadratically-constrained-quadratic-programming&#34;&gt;Quadratically
Constrained Quadratic Programming&lt;/h2&gt;
&lt;p&gt;Consider the quadratically constrained quadratic programming: &lt;span class=&#34;math display&#34;&gt;\[
\label{qcqp} \tag{QCQP} \begin{aligned}
\inf \quad &amp;amp; x^T Q x \\
\text{s.t.} \quad &amp;amp; x^T A_i x \ge b_i, \forall i=1,\dots,m
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Q, A_1, \dots, A_m \in
\mathcal{S}^n\)&lt;/span&gt;. By far, no convexity is not assumed. But on the
other hand, we can apply the &lt;strong&gt;semi-definite relaxation&lt;/strong&gt;
technique to transform the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Observe that &lt;span class=&#34;math inline&#34;&gt;\(x^T Q x = \tr(x^T Q x) =
\tr(Q x x^T) = Q \bullet x x^T\)&lt;/span&gt;, which is linear in &lt;span class=&#34;math inline&#34;&gt;\(x x^T\)&lt;/span&gt;. We can apply the same trick to the
constraints so that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt; is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^* = \inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; \exists x \in \R^n, X = x x^T \\
\text{ non-convex??}
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\exists x \in \R^n, X =
x x^T \iff X \in \mathcal{S}_+^n, \rank(X) \le 1\)&lt;/span&gt;. That is.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; X \succeq 0 \\
&amp;amp; \rank(X) \le 1
\end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\(\rank\)&lt;/span&gt; function is not
convex. We may as well drop the rank constraint so that the
semi-definite relaxation of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\label{relaxed-qcqp} \tag{Relaxed QP} \begin{aligned}
v_R^* = \inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; X \succeq 0
\end{aligned}
\]&lt;/span&gt; Observe that &lt;span class=&#34;math inline&#34;&gt;\(v^* \ge
v_R^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-qcqp}\)&lt;/span&gt; is a SDP.&lt;/p&gt;
&lt;h3 id=&#34;max-cut-problem&#34;&gt;Max-cut Problem&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(G = (V,E)\)&lt;/span&gt; be an undirected
graph and &lt;span class=&#34;math inline&#34;&gt;\(w: E \mapsto \R_+\)&lt;/span&gt; be a
weight function on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. A subset
&lt;span class=&#34;math inline&#34;&gt;\(S \subseteq V\)&lt;/span&gt; defines a cut and the
value of a cut is &lt;span class=&#34;math display&#34;&gt;\[
w(S) \triangleq \sum_{(i,j) \in E, i \in S, j \notin S} w_{ij}
\]&lt;/span&gt; The goal is to find &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
V\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(w(S)\)&lt;/span&gt; is
maximized. The minimization problem is trivial, simply choosing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; gives the minimum value &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x_i
\in \{ -1, +1 \}\)&lt;/span&gt; be a binary variable indicating whether vertex
&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is in the cut (&lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt;) or not (&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;). Then, &lt;span class=&#34;math display&#34;&gt;\[
\label{mc} \tag{Max-cut} \begin{aligned}
v^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - x_i
x_j) \\
\text{s.t.} \quad &amp;amp; x_i^2 = 1, \forall i=1,\dots,n \\
\end{aligned}
\]&lt;/span&gt; Apply the SDR technique (there is a middle step to convert the
above to a QP) to get &lt;span class=&#34;math display&#34;&gt;\[
\label{relaxed-mc} \tag{Relaxed Max-cut} \begin{aligned}
v_\text{sdr}^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij}
(1 - X_{ij}) \\
\text{s.t.} \quad &amp;amp; X_{ii} = 1, \forall i=1,\dots,n \\
&amp;amp; X \succeq 0
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(v^* \le
v_\text{sdr}^*\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-mc}\)&lt;/span&gt; is solved with a
rank-one matrix &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt;, we can
automatically decompose it to give the optimal solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{mc}\)&lt;/span&gt;. The crux is how to preserve
the optimality when &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt; is of rank
higher than one. Here is an algorithm for it:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Solve &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-mc}\)&lt;/span&gt; to get
an optimal solution &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(X^* = U^T U\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(U \in \R^{n \times n}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(u_i \in \R^n\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. We have &lt;span class=&#34;math inline&#34;&gt;\(\|u_i\|_2^2 = u_i^T u_i = X_{ii}^* =
1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(r \in \R^n\)&lt;/span&gt; be a random
vector uniformly distributed on the sphere &lt;span class=&#34;math inline&#34;&gt;\(S^n = \{ x \in \R^n: \|x\|_2 = 1 \}\)&lt;/span&gt; (this
can be done by normalizing the samples from standard Gaussian in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39; = \sign(u_i^T r)\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sign(z)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(z \ge
0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; otherwise. Return
&lt;span class=&#34;math inline&#34;&gt;\(\{ x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt; as a
feasible solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{mc}\)&lt;/span&gt;.
This is also referred to as the &lt;strong&gt;hyperplane
rounding&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the first place, &lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39;\)&lt;/span&gt;s
are feasible. Let &lt;span class=&#34;math inline&#34;&gt;\(v&amp;#39;\)&lt;/span&gt; be the
objective value associated with the cut &lt;span class=&#34;math inline&#34;&gt;\(\{
x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt;. Clearly, &lt;span class=&#34;math inline&#34;&gt;\(v&amp;#39; \le v^*\)&lt;/span&gt;. To analyze its
approximation bound, firstly note that &lt;span class=&#34;math inline&#34;&gt;\(\{
x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt; is random. We can only consider the
&lt;span class=&#34;math inline&#34;&gt;\(\E[v&amp;#39;]\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[v&amp;#39;] = \frac{1}{2} \E[\sum_{(i,j) \in E} w_{ij} (1 - x_i&amp;#39;
x_j&amp;#39;)] \\
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \E[\frac{1 - x_i&amp;#39; x_j&amp;#39;}{2}] \\
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \Pr[\sign(u_i^T r) \ne \sign(u_j^T r)]
\\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u, v \in S^n\)&lt;/span&gt; be
arbitrary, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; be uniformly
distributed on &lt;span class=&#34;math inline&#34;&gt;\(S^{n-1}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\Pr[\sign(u^T r) \ne \sign(v^T r)] = \frac{\arccos(u^T v)}{\pi}
\]&lt;/span&gt; Under the above setting, for any &lt;span class=&#34;math inline&#34;&gt;\(z
\in [-1, 1]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(\cos \theta = z\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\arccos z}{\pi} = \frac{2 \theta}{\pi(1 - \cos \theta)}
\frac{1}{2} (1 - z) \\
\ge \alpha \cdot \frac{1}{2} (1-z)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \min_{0 \le \theta
\le \pi} \frac{2 \theta}{\pi (1 - \cos \theta)} &amp;gt; 0.878\)&lt;/span&gt;. As
a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[v&amp;#39;] = \sum_{(i,j) \in E} w_{ij} \frac{\arccos u_i^T
u_j}{\pi} \\
&amp;amp;\ge \sum_{(i,j) \in E} w_{ij} \alpha \cdot \frac{1}{2} (1 -
\underbrace{u_i^T u_j}_{X_{ij}^*}) \\
&amp;amp;= \alpha \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}^*) \\
&amp;amp;= \alpha v_\text{sdr}^* \ge \alpha v^*
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>8-nonlinear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</guid>
      <description>

&lt;h2 id=&#34;nonlinear-programming&#34;&gt;Nonlinear Programming&lt;/h2&gt;
&lt;p&gt;Recall the unconstrained optimization problem: &lt;span class=&#34;math display&#34;&gt;\[
\inf_{x \in \R^n} \quad f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a continuously differentiable function, &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \R^n\)&lt;/span&gt; be an arbitrary point. If
there exists s &lt;strong&gt;direction&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(d
\in \R^n \setminus \{ 0 \}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x)^T d &amp;lt; 0\)&lt;/span&gt;, then there
exists &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0 &amp;gt; 0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
f(\bar x + \alpha d) &amp;lt; f(\bar x), \forall \alpha \in (0, \alpha_0]
\]&lt;/span&gt; Here, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is called a
&lt;strong&gt;descent direction&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\bar
x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a necessary condition for &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; to be a local minima is that &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt; (&lt;strong&gt;first-order
necessary condition&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a convex and continuously differentiable function. Then,
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a global minima of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; if and only if &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;second-order sufficient condition&lt;/strong&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be a twice
continuously differentiable function. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2 f(\bar x) \succ 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a local minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;In constrained case, simple conditions does not apply, e.g. &lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \ge 1} x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \ge -1} x^2\)&lt;/span&gt;. Consider the
following constrained problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{primal} \tag{P} \begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; g_i(x) \le 0, i=1,\dots,r \\
&amp;amp; h_j(x) = 0, j=1,\dots,s
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f, g_i, h_j\)&lt;/span&gt; are
continuously differentiable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Fritz John necessary condition&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minimum of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;. Then there exist
&lt;strong&gt;multipliers&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(u \in
\R\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_r \in
\R\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(w_1, \dots, w_s \in \R\)&lt;/span&gt;
such that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
u \nabla f(\bar x) + \sum_{i=1}^r v_i \nabla g_i(\bar x) + \sum_{j=1}^s
w_j \nabla h_j(x) = 0 \tag{vanishing gradient} \\
[u, v_1, \dots, v_r, w_1, \dots, w_s] \ne 0 \tag{non-trivial solution}
\\
u, v_i \ge 0, i=1,\dots,r \tag{non-negativity} \\
v_i g_i(\bar x) = 0, i=1,\dots,r \tag{complementarity} \\
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; tells the importance
of the inequality constraint &lt;span class=&#34;math inline&#34;&gt;\(g_i(x)\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(v_i =
0\)&lt;/span&gt; implies that &lt;span class=&#34;math inline&#34;&gt;\(g_i(x) \le
0\)&lt;/span&gt; is well-fulfilled (strictly less than zero).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The implication is that, at a local minima, we should not be able to
find a direction that decreases the objective value as well as maintains
the feasibility. For simplicity of discussion, we drop the equality
constraints below.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Linear non-independence&lt;/p&gt;
&lt;p&gt;The vanishing gradient together with the non-trivial solution in
essence rules out the possibility of linear independence among the
gradients. This is easy to interpret. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x), \nabla g_1(\bar x), \dots,
\nabla g_r(\bar x)\)&lt;/span&gt; are linearly independent, it is easy to find
a direction &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(\Col^\perp(\nabla g_1(\bar x), \dots, \nabla
g_r(\bar x))\)&lt;/span&gt; that is acute to &lt;span class=&#34;math inline&#34;&gt;\(-\nabla f(\bar x)\)&lt;/span&gt;. Then moving along
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; will decrease the objective
function value but maintain the inequality constraints, which
contradicts that &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a local
minima.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Non-negativity + complementarity&lt;/p&gt;
&lt;p&gt;These two components should be discussed together and are a bit
intriguing. Please refer to &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0022247X67901631&#34;&gt;this
paper&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surely, the premise is that &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is
nonzero. When the only solution to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is zero, the corresponding solution
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will be a garbage point: it will not be
a local minima. In this case, &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar
x)\)&lt;/span&gt; must have a component that is orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(\nabla g_i(\bar x)\)&lt;/span&gt;’s. We can walk along
this component (or its opposite) to decrease &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; without compromising inequality
constraints. This motivates the study of &lt;strong&gt;constraint
qualification&lt;/strong&gt;, which aims to ensure a nonzero &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that in the above discussion, we ignore the effect of &lt;span class=&#34;math inline&#34;&gt;\(\nabla h_j(\bar x)\)&lt;/span&gt;’s. They only make the
choice of direction stricter, since the direction has to be orthogonal
to them.&lt;/p&gt;
&lt;h3 id=&#34;kkt-conditions-and-constraint-qualification&#34;&gt;KKT Conditions and
Constraint Qualification&lt;/h3&gt;
&lt;p&gt;One observation is that, if the constraint gradients are linearly
independent (though required dependent), there is no way to have &lt;span class=&#34;math inline&#34;&gt;\(u = 0\)&lt;/span&gt; or otherwise &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_r, u_1, \dots, u_s\)&lt;/span&gt; has to
be zero due to the linear independence, which in turn violates the
nonzero multiplier condition. How to “obtain” the contradicting linear
independence expectation and nonzero multiplier condition at the same
time?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Karush-Kuhn-Tucker conditions&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minima of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;. Let &lt;span class=&#34;math display&#34;&gt;\[
I(\bar x) = \{ i: g_i(\bar x) = 0 \}
\]&lt;/span&gt; be the index set on the &lt;strong&gt;active inequality
constraint&lt;/strong&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\{ \nabla
g_i(\bar x) \}_{i \in I} \cup \{ \nabla h_j(\bar x) \}_{j=1}^s\)&lt;/span&gt;
are linearly independent (&lt;strong&gt;linear-independence constraint
qualification&lt;/strong&gt;). Then, there exists &lt;span class=&#34;math inline&#34;&gt;\(v \in \R^r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^s\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(\bar x) + \sum_{i=1}^r \nabla g_i(x) + \sum_{j=1}^s \nabla
h_j(x) = 0 \\
v_i \ge 0, i=1,\dots,r \\
v_i g_i(\bar x) = 0, i=1,\dots,r
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: importance of CQ. Consider the problem &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf \quad &amp;amp; f(x_1, x_2) = x_1 \\
\text{s.t.} \quad &amp;amp; g_1(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - 1
\le 0 \\
&amp;amp; g_2(x_1, x_2) = (x_1 - 1)^2 + (x_2 + 1)^2 - 1 \le 0 \\
\end{aligned}
\]&lt;/span&gt; The only feasible and thus optimal solution is &lt;span class=&#34;math inline&#34;&gt;\(\bar x = (1, 0)\)&lt;/span&gt;. In this case, the active
inequality constraint gradient is &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
0 \\
-2
\end{bmatrix},
\begin{bmatrix}
0 \\
2
\end{bmatrix}
\]&lt;/span&gt; They are linearly dependent. Therefore, KKT conditions doesn’t
hold. Here is the reason why we intentionally separate the equality
constraints from inequality constraints. Though &lt;span class=&#34;math inline&#34;&gt;\(h(x) = 0 \iff h(x) \le 0 \land -h(x) \le
0\)&lt;/span&gt;, if we lay down the equality constraint as inequality
constraints, the gradient of &lt;span class=&#34;math inline&#34;&gt;\(h(x)\)&lt;/span&gt;
is always linearly dependent to that of &lt;span class=&#34;math inline&#34;&gt;\(-h(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One takeaway is that, even in this convex optimization problem, KKT
may not hold.&lt;/p&gt;
&lt;p&gt;Another takeaway is that, it is very convenient to “draw circles”
when finding counter examples related to constraint qualification.
Better still, leave only one feasible point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem with LICQ is that it is tedious to check for every
solution of &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt;. We may prefer
some kind of &lt;strong&gt;“looser” constraint qualifications&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Slater constraint qualification&lt;/strong&gt;. Suppose
that &lt;span class=&#34;math inline&#34;&gt;\(g_1, \dots, g_r\)&lt;/span&gt; are convex and
&lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots, h_s\)&lt;/span&gt; are affine. Let
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minima. Denote
the feasible region as &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Suppose
that there exists &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \in S\)&lt;/span&gt; such
that &lt;span class=&#34;math inline&#34;&gt;\(g_i(x&amp;#39;) &amp;lt; 0\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,r\)&lt;/span&gt;. Then, the KKT conditions are
necessary for optimality.&lt;/p&gt;
&lt;p&gt;This Slater condition quite resembles that in the conic Farkas’
lemma.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Suppose that &lt;span class=&#34;math inline&#34;&gt;\(g_1, \dots,
g_r\)&lt;/span&gt; are concave and &lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots,
h_s\)&lt;/span&gt; are affine. Then, the KKT conditions are necessary for
optimality.&lt;/p&gt;
&lt;p&gt;This theorem is especially useful when all the constraint functions
are affine (since affine functions are both convex and concave).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math inline&#34;&gt;\(A \in \R{m \times n}, b \in
\R^m, c \in \R^n\)&lt;/span&gt; be given. Consider &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; Ax = b \\
&amp;amp; x \ge 0
\end{aligned}
\]&lt;/span&gt; Beware of the dimension of constraint functions when
converting this LP problem to nonlinear programming problem: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; c^Tx \\
\text{s.t.} \quad &amp;amp; g_i(x) = -x_i = -e_i^T x \le 0, i=1,\dots,n \\
&amp;amp; h_i(x) = b_j - a_j^T x = 0, j=1,\dots,m
\end{aligned}
\]&lt;/span&gt; This is a linearly constrained problem; thus KKT conditions
are necessary for optimality: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
c - \sum_{i=1}^r v_i e_i - \sum_{j=1}^s w_i a_j = 0 \label{grad} \\
v_i \ge 0 \label{dual} \\
v_i x_i = 0, i=1,\dots,r \label{compl}
\end{gather}
\]&lt;/span&gt; From &lt;span class=&#34;math inline&#34;&gt;\(\eqref{grad}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
c - v - A^T w = 0
\]&lt;/span&gt; Given &lt;span class=&#34;math inline&#34;&gt;\(v \ge 0\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; and the complementarity from
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{compl}\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(c \ge A^T w\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((c - A^T w)_i x_i = 0\)&lt;/span&gt;. This essentially
recovers the sufficient and necessary conditions for the optimality of
LP. But KKT only tells the condition is necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is when are KKT conditions sufficient?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;KKT sufficient conditions&lt;/strong&gt;. Suppose that
&lt;span class=&#34;math inline&#34;&gt;\(f, g_1, \dots, g_r\)&lt;/span&gt; are convex and
&lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots, h_s\)&lt;/span&gt; are affine. Suppose
further that there exists &lt;span class=&#34;math inline&#34;&gt;\((\bar x, \bar v,
\bar w)\)&lt;/span&gt; satisfying the KKT conditions: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
g_i(\bar x) \le 0, h_j(\bar x) = 0, i=1,\dots,r, j=1,\dots,s \tag{primal
feasibility} \\
\tag{dual feasibility} \left. \begin{gathered}
\nabla f(\bar x) + \sum_{i=1}^r \bar v_i \nabla g_i(\bar x) +
\sum_{j=1}^s \bar w_i \nabla h_i(x) = 0 \\
\bar v_i \ge 0, i=1,\dots,r
\end{gathered} \right\} \\
\bar v_i g_i(x) = 0, i=1,\dots,r \tag{complentary slackness}
\end{gather}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a global
minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h3&gt;
&lt;p&gt;For simplicity, we rewrite the constrained problem as follows: &lt;span class=&#34;math display&#34;&gt;\[
\label{primalp} \tag{P} \begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; G(x) \le 0 \\
&amp;amp; H(x) = 0
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(G(x) = [g_1(x), \dots,
g_r(x)]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(x) = [h_1(x), \dots,
h_s(x)]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Observe that &lt;span class=&#34;math display&#34;&gt;\[
\eqref{primalp} \equiv \inf_{x \in \R^n} \sup_{v \in \R_+^r, w \in \R^s}
\underbrace{f(x) + v^T G(x) + w^T H(x)}_{L(x, v, w)}
\]&lt;/span&gt; The dual of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primalp}\)&lt;/span&gt; is then &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
v_d^* = \sup_{v \in \R_+^r, w \in \R^s} \inf_{x \in \R^n} L(x, v, w)
\label{dualp} \tag{D}
\end{equation}
\]&lt;/span&gt; Observe that &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{\inf_{x \in \R^n}L(x, \bar v, \bar w)}_{\theta(\bar v, \bar
w)} \le L(\bar x, \bar v, \bar w) \le \underbrace{\sup_{v \in \R_+^r, w
\in \R^s} L(\bar x, v, w)}_{\gamma(\bar x)}
\]&lt;/span&gt; This implies that &lt;span class=&#34;math display&#34;&gt;\[
v_d^* = \sup_{v \in \R_+^r, w \in \R^s} \theta(v, w) \le \inf_{x \in
\R^n} \gamma(x) = v_p^*
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primalp}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((\bar v, \bar w)\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dualp}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
f(\bar x) = \gamma(\bar x) \ge \theta(\bar v, \bar w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: illustration of weak duality. Consider a simple case: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; g(x) \le 0
\end{aligned} \\
\rule{6cm}{0.4pt} \\
\begin{aligned}
v_d^* = \sup_{v \ge 0} &amp;amp; \inf_{x \in \R^n} [f(x) + v g(x)] \\
\end{aligned}
\end{gathered}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G} = \{ (y,z): y =
g(x), z = f(x), x \in \R^n \}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta(v) &amp;amp;= \inf_{x \in \R^n} [f(x) + v g(x)] \\
&amp;amp;= \inf_{(y, z) \in \mathcal{G}} [z + v y]
\end{aligned}
\]&lt;/span&gt; This set is quite related to the proof of the &lt;strong&gt;strong
duality&lt;/strong&gt; under KKT sufficient condition together with Slater
condition.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/</guid>
      <description>&lt;p&gt;Positive semi-definite matrix involves many concepts like quadratic form, &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/&#34;&gt;real symmetric matrix&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;singular value decomposition&lt;/a&gt;. It can be quite helpful to glue these things together here.&lt;/p&gt;
&lt;h2 id=&#34;quadratic-form&#34;&gt;Quadratic Form&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;quadratic function&lt;/strong&gt; $f$ of $n$ variables, or say a vector $\x$ of length $n$, is the sum of second-order terms:
$$
f(\x) = \sum_{i=1}^n \sum_{j=1}^n c_{ij} x_i x_j
$$&lt;/p&gt;
&lt;p&gt;The above summation can be simplified as matrix product $\x^T A \x$ where $A$ is $n \times n$ and $a_{ij} = c_{ij}$. $\x^T A \x$ is called the &lt;strong&gt;quadratic form&lt;/strong&gt; of $A$.&lt;/p&gt;
&lt;p&gt;In essence, there is a nicer formulation for $A$. Firstly define the $n \times n$ matrix $A$ such that $a_{ij} = \frac{1}{2} (c_{ij} + c_{ji})$. It suffices to show $A$ is a real symmetric matrix and
$$
f(\x) = \x^T A \x
$$&lt;/p&gt;
&lt;p&gt;Thus the discussion of quadratic form usually encompasses the real symmetric matrix.&lt;/p&gt;
&lt;h2 id=&#34;positive-semi-definiteness&#34;&gt;Positive Semi-definiteness&lt;/h2&gt;
&lt;p&gt;Let $A$ be a real symmetric matrix. $A$ is &lt;strong&gt;positive definite&lt;/strong&gt; if and only if the quadratic form of $A$ is positive. Specifically, for every $\x \ne 0$, $\x^T A \x &amp;gt; 0$. $A$ is &lt;strong&gt;positive semi-definite&lt;/strong&gt; if and only if the quadratic form of $A$ is non-negative. Specifically, for every $\x \ne 0$, $\x^T A \x \ge 0$.&lt;/p&gt;
&lt;p&gt;One possible reason why we would like to define positive definiteness on real symmetric matrix is that, any real matrix can be easily decomposed into the addition of a real symmetric matrix and a real &lt;strong&gt;skew-symmetric&lt;/strong&gt; matrix whose quadratic form is zero:
$$
A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
$$
Since $\frac{A - A^T}{2}$&amp;rsquo;s quadratic form is zero and it makes no contribution to $A$&amp;rsquo;s, the only component of interest will be the real symmetric $\frac{A + A^T}{2}$. So why not just focus on the real symmetric matrix?&lt;/p&gt;
&lt;p&gt;Note that there is also &amp;ldquo;PSD&amp;rdquo; matrix that is not symmetric. It is not very easy to find a such matrix. As a guideline, drop the idea to find such a matrix whose eigenvalues are all real. But for an example,
$$
\x^T \begin{bmatrix}
1 &amp;amp; 1 \
-1 &amp;amp; 1
\end{bmatrix}
\x = x_1^2 + x_2^2 \ge 0
$$&lt;/p&gt;
&lt;p&gt;Interestingly, the real part of such matrix&amp;rsquo;s eigenvalues must be positive. Refer to &lt;a href=&#34;https://math.stackexchange.com/a/325412&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;psd-and-eigenvalues&#34;&gt;PSD and Eigenvalues&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A real symmetric matrix $A$ is positive semi-definite if and only if $A$&amp;rsquo;s eigenvalues are non-negative.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For every $A$&amp;rsquo;s eigenpair $(\lambda, \v)$, we have $\v^T A \v = \lambda \v^T \v \ge 0 \Rightarrow \lambda \ge 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Take $A$&amp;rsquo;s spectral decomposition as $A = Q \Lambda Q^T$ where $Q Q^T = I$. For every $\x &amp;gt; 0$, we have
$$
\x^T A \x = \overbrace{\x^T Q}^{y^T} \Lambda \overbrace{Q^T \x}^{y} \ge 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similarly we have that a real symmetric matrix $A$ is positive definite if and only if $A$&amp;rsquo;s eigenvalues are positive. Then it follows that a positive definite matrix is invertible, because its eigenvalues are positive and thus its determinant (product of eigenvalues) is positive.&lt;/p&gt;
&lt;h3 id=&#34;non-negative-diagonals&#34;&gt;Non-negative Diagonals&lt;/h3&gt;
&lt;p&gt;The diagonal entries of a PSD matrix $A$ must be non-negative. This is because for the basis vector $e_i = [0, \dots, \underset{i\text{-th}}{1}, \dots, 0]$, we have $e_i^T A e_i = a_{ii} \ge 0$. Furthermore, a PSD matrix is PD if and only if its main diagonal entries are positive.&lt;/p&gt;
&lt;p&gt;Specifically, if one diagonal entry of $A$ is zero, then the row and the column to which this diagonal entry belongs to must be zero.&lt;/p&gt;
&lt;p&gt;To show it, we firstly argue that $a_{ij}^2 \le a_{ii} a_{jj}$. For every $\lambda \in \R$, we have
$$
\begin{gathered}
(e_i + \lambda e_j)^T A (e_i + \lambda e_j) = e_i^T A e_i + \lambda (e_i^T A e_j + e_j^T A e_i) + \lambda^2 e_j^T A e_j \
= a_{ii} + 2 a_{ij} \lambda + a_{jj} \lambda^2 \ge 0
\end{gathered}
$$
Note the formula above is a quadratic function in $\lambda$. This quadratic form has at most one real root. Hence, $4 a_{ij}^2 \le 4 a_{ii} a_{jj} \Rightarrow a_{ij}^2 \le a_{ii} a_{jj}$. Therefore, if $j$-th diagonal entry of $A$ is zero, for any $i = 1,\dots,n$, we have $a_{ij}^2 \le 0 \Rightarrow a_{ij} = 0$.&lt;/p&gt;
&lt;p&gt;Another implication is that any $2 \times 2$ submatrix $\begin{bmatrix} a_{ii} &amp;amp; a_{ij} \ a_{ji} &amp;amp; a_{jj} \end{bmatrix}$ obtained from $A$ is always PSD. In fact, any submatrix obtained by removing the row and column of one of the main diagonal entry of a PSD matrix is PSD. Let $A_{kk}$ be the matrix obtained by removing $A$&amp;rsquo;s $k$-th row and column. To show it, let $\x \in \R^n$ and let $\bar \x = \x$ except that $\bar \x_k = 0$. $A_{kk}$&amp;rsquo;s quadratic form can be written as
$$
\sum_{i=1, i \ne k}^{n} \sum_{j=1, j \ne k}^n a_{ij} x_i x_j = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j \mathbb{1}[i,j \ne k] = \bar \x^T A \bar x \ge 0
$$&lt;/p&gt;
&lt;h3 id=&#34;decomposition&#34;&gt;Decomposition&lt;/h3&gt;
&lt;h4 id=&#34;definitive-decomposition&#34;&gt;Definitive Decomposition&lt;/h4&gt;
&lt;p&gt;A positive semi-definite matrix $A$ can be decomposed into the product of a square matrix $Q$ and this matrix&amp;rsquo;s transpose $Q^T$. In fact, a real symmetric matrix is positive semi-definite if and only if it can be decomposed this way.&lt;/p&gt;
&lt;p&gt;Consider that a real symmetric matrix can be orthogonally diagonalized:
$$
A = P \Lambda P^T
$$
When $A$ is PSD, its eigenvalues are all non-negative. Thus, $\Lambda$ can be written as $\Lambda^{1/2} \Lambda^{1/2}$ and
$$
\begin{aligned}
&amp;amp;A = P (\Lambda^{1/2} \Lambda^{1/2}) P^T \
&amp;amp;= P \Lambda^{1/2} (\Lambda^{1/2})^T P^T \
&amp;amp;= \underbrace{(P \Lambda^{1/2})}&lt;em&gt;{Q} \underbrace{(P \Lambda^{1/2})^T}&lt;/em&gt;{Q^T}
\end{aligned}
$$
$A$ is positive definite if and only if $Q$ is invertible.&lt;/p&gt;
&lt;h4 id=&#34;square-root&#34;&gt;Square Root&lt;/h4&gt;
&lt;p&gt;A real symmetric matrix $A$ is PSD if and only if there is a PSD matrix $B$ satisfying that $A = BB$. This $B$ is unique and is called &lt;strong&gt;non-negative square root&lt;/strong&gt; of $A$ (there are non-PSD matrix whose square also equals $A$). $B$ is usually denoted as $A^{1/2}$.&lt;/p&gt;
&lt;p&gt;Consider that a real symmetric matrix $A$ can be orthogonally diagonalized as $A = P \Lambda P^T$. Setting $B = P \Lambda^{1/2} P^T$ would give the non-negative square root of $A$.&lt;/p&gt;
&lt;p&gt;Note that this cannot be naively extrapolated to that &amp;ldquo;a real symmetric matrix can be decomposed (not necessarily PSD) into two identical symmetric matrices (not necessarily PSD)&amp;rdquo;. Just consider that, the product of two identical symmetric matrices are positive semi-definite; but not all real symmetric matrices are positive semi-definite.&lt;/p&gt;
&lt;p&gt;In fact, any &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt; matrix can have its square root, though not necessarily a PSD one.&lt;/p&gt;
&lt;h4 id=&#34;cholesky-decomposition&#34;&gt;Cholesky Decomposition&lt;/h4&gt;
&lt;p&gt;A PSD matrix $A$ can be written as $A = L L^T$, where $L$ is a lower-triangular matrix with non-negative diagonal. If $A$ is positive definite, then the diagonal of $L$ is positive and the Cholesky decomposition is unique.&lt;/p&gt;
&lt;h3 id=&#34;schur-complement&#34;&gt;Schur Complement&lt;/h3&gt;
&lt;p&gt;By applying Schur complement in PSD matrix, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix}
A &amp;amp; B \
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}&lt;em&gt;+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}&lt;/em&gt;+^m, C \in \mathcal{S}&lt;em&gt;+^n, \
A - B C^{-1} B^T \in \mathcal{S}&lt;/em&gt;+^m \text{ or } C - B^T A^{-1} B \in \mathcal{S}_+^n
\end{gathered}
$$&lt;/p&gt;
&lt;h3 id=&#34;sylvesters-condition&#34;&gt;Sylvester&amp;rsquo;s Condition&lt;/h3&gt;
&lt;p&gt;Sylvester&amp;rsquo;s criterion states that a $n \times n$ real symmetric matrix $M$ is positive-definite if and only if all the following matrices have a positive determinant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the upper left 1-by-1 corner of $M$,&lt;/li&gt;
&lt;li&gt;the upper left 2-by-2 corner of $M$,&lt;/li&gt;
&lt;li&gt;the upper left 3-by-3 corner of $M$,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;$M$ itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://adityam.github.io/stochastic-control/linear-algebra/postive-definite-matrix.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course Notes - 29  Positive definite matrices (adityam.github.io)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</guid>
      <description>&lt;h2 id=&#34;convex-conjugate&#34;&gt;Convex Conjugate&lt;/h2&gt;
&lt;p&gt;For a convex function $f$, its &lt;strong&gt;convex conjugate&lt;/strong&gt; $f^&lt;em&gt;$ is defined as
$$
f^&lt;/em&gt;(t) = \sup_x [x^T \cdot t - f(x)]
$$
By definition, a Convex Conjugate pair $(f,f^&lt;em&gt;)$ has the following property:
$$
f(x) + f^&lt;/em&gt;(t) \ge x^T \cdot t
$$
As a conjugate, $f^{&lt;strong&gt;} = f$:
$$
\begin{aligned}
f^{&lt;/strong&gt;}(t) &amp;amp;= \sup_x [x^T \cdot t - f^*(x)] \
&amp;amp;= \sup_x [x^T \cdot t - \sup_y [y^T \cdot x - f(y)]] \
&amp;amp;= \sup_x [x^T \cdot t + \inf_y [f(y) - y^T \cdot x]] \
&amp;amp;= \sup_x \inf_y [x^T (t-y) + f(y)]	\
&amp;amp;\Downarrow_\text{Mini-max Theorem} \
&amp;amp;= \inf_y \sup_x [x^T (t-y) + f(y)]
\end{aligned}
$$
The above reaches the infimum only if $y=t$. Otherwise, $\sup_x [x^T (t-y) + f(y)]$ can make it to infinity. Therefore,
$$
f^{**}(t) = \inf_y \sup_x [f(t)] = f(t)
$$
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_conjugate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wiki&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/188702379&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;凸优化-凸共轭&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
