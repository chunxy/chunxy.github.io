<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chunxy&#39; Notes | My Notes</title>
    <link>https://chunxy.github.io/notes/</link>
      <atom:link href="https://chunxy.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    <description>Chunxy&#39; Notes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Jul 2022 12:58:49 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/notes/media/icon_huc0707d156b6b3b9945e544e63d06d5e5_16450_512x512_fill_lanczos_center_3.png</url>
      <title>Chunxy&#39; Notes</title>
      <link>https://chunxy.github.io/notes/</link>
    </image>
    
    <item>
      <title>Common Distributions</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/common-distributions/</link>
      <pubDate>Fri, 20 May 2022 09:58:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/common-distributions/</guid>
      <description>

&lt;h2 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
P(x;\lambda) = e^{-\lambda} \frac{\lambda^x}{x!} \\
\E[X] = \lambda \\
\Var[X] = \lambda
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
P(x;p,N) = {N \choose x} p^x (1-p)^{N-x} \\
\E[X] = Np \\
\Var[X] = Np(1-p)
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;bernoulli-distribution&#34;&gt;Bernoulli Distribution&lt;/h2&gt;
&lt;p&gt;Bernoulli Distribution &lt;span class=&#34;math inline&#34;&gt;\(B(\{0,1\};p)\)&lt;/span&gt; is a special case of Binomial Distribution when &lt;span class=&#34;math inline&#34;&gt;\(N=1\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
B(1,p) = p, B(0,p) = 1 - p \\
\E[X] = p \\
\Var[X] = p(1 - p)
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Unconscious Statistics</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/unconscious-statistics/</link>
      <pubDate>Tue, 03 May 2022 10:50:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/unconscious-statistics/</guid>
      <description>

&lt;h2 id=&#34;law-of-the-unconscious-statistician&#34;&gt;Law of the Unconscious Statistician&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, the &lt;strong&gt;law of the unconscious statistician&lt;/strong&gt; (LOTUS), is a theorem used to calculate the expected value of a function &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt; of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; when one knows the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; but one does not know the distribution of &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the probability mass function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \sum_x g(x) p(x)
\]&lt;/span&gt; If the probability density function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)p(x)\ \d x
\]&lt;/span&gt; If the cumulative distribution function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)\ \d F(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;marginal-expectation&#34;&gt;Marginal Expectation&lt;/h2&gt;
&lt;p&gt;If the joint distribution of two random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is known, then the expectation of one component can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\E[X] = \int_{-\infty}^{+\infty} x p_X(x)\; \d x = \int_{-\infty}^{+\infty} x \int_{-\infty}^{+\infty} p(x,y)\; \d y\; \d x = = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xp(x,y)\ \d y\ \d x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\E [X] = \E{y \sim p_Y} [\E_{x \sim p(X|Y=y)}]  = \int_{-\infty}^{+\infty} p(y) \bigg( \int_{-\infty}^{+\infty} x p(x|y)\ \d x \bigg) \d y
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/questions/185729/expected-value-of-a-marginal-distribution-when-the-joint-distribution-is-given&#34;&gt;StackExchange Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-non-negative-random-variables&#34;&gt;Expectation of Non-negative Random Variables&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable whose value is non-negative, and &lt;strong&gt;its expectation exists&lt;/strong&gt;, and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is continuous, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[b]
\E (X) &amp;amp;= \int_{0}^{+\infty} x p(x)\ \d x = \int_{0}^{+\infty} x\ \d \big( P(x) - 1 \big) \\
&amp;amp;= [x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty} - \int_0^{+\infty} \big( P(x) - 1 \big)\ \d x
\end{aligned}
\]&lt;/span&gt; because the expectation exists, the above expression and especially the &lt;span class=&#34;math inline&#34;&gt;\([x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty}\)&lt;/span&gt; term must converge: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
[x \big( P(x) - 1 \big)]\bigg|_{x=0} = 0 \\
[P(x) - 1]\bigg|_{x \to +\infty} = 0 \Rightarrow [x \big( P(x) - 1 \big)]\bigg|_{x \to +\infty} = 0
\end{gather}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \int_{0}^{+\infty} \big (1 - P(x) \big)\ \d x
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is discrete and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; only takes on integer values, supposing the max value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E(X) &amp;amp;= \sum_{k=0}^{N} [k P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [(\sum_{j=0}^{k-1} 1) P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [\sum_{j=0}^{k-1} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} [\sum_{k=j+1}^{N} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} P(X &amp;gt; j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/305031/expectation-when-cumulative-distribution-function-is-given&#34;&gt;StackExchange Discussion&lt;/a&gt;|&lt;a href=&#34;https://en.wikipedia.org/wiki/Summation_by_parts&#34;&gt;Summation by Parts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-and-quantile-function&#34;&gt;Expectation and Quantile Function&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be the PDF and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; be the CDF of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Q = F^{-1}\)&lt;/span&gt; be the inverse of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is also called the &lt;strong&gt;quantile function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\int_0^1 Q(p)\ \d p \stackrel{p=F(x)}{\Longrightarrow} = \int_{-\infty}^{+\infty} x f(x)\ \d x = \E(X) 
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/a/18439&#34;&gt;StackExchange Answer&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/entropy/</link>
      <pubDate>Thu, 28 Apr 2022 22:47:16 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/entropy/</guid>
      <description>

&lt;p&gt;The Entropy of discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (probability mass function) is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p) = -\mathrm{E}_{x \sim p}\log p(x)
\]&lt;/span&gt; The Entropy of continuous distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; (probability density function) is usually similarly defined as &lt;span class=&#34;math display&#34;&gt;\[
H(q) = -\E_{x\sim q}\log q(x) = -\int q(x)\log q(x)\d x
\]&lt;/span&gt; This is actually the &lt;strong&gt;Differential Entropy&lt;/strong&gt; introduced by Shannon. In fact, it is not a good continuous analog of discrete entropy and it was not rigorously derived. For example, this formula can be negative. Therefore, in the case of Entropy, the random variable had better be discrete, although the differential entropy is widely used.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;The entropy of a &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional Gaussian distribution $p(x) =  $ can be derived as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
H(p) &amp;amp;\triangleq -\int p(x) \log p(x) \d x = -\int p(x) [-\frac 1 2 (x-\mu)^T \Sigma^{-1} (x-\mu) - \frac 1 2 \log |2\pi\Sigma|] \d x \\
&amp;amp;= \frac 1 2 \int p(x) (x-\mu)^T \Sigma^{-1}(x-\mu) \d x + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 \int p(x) x^T \Sigma^{-1} x \d x + \frac 1 2 \int p(x) \mu^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad - \frac 1 2 \int p(x) \mu^T \Sigma^{-1} x \d x - \frac 1 2 \int p(x) x^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= [\frac 1 2 \tr(\Sigma^{-1} \Sigma) + \frac 1 2 \mu^T \Sigma^{-1} \mu] + \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad - \frac 1 2 \mu^T \Sigma^{-1} \mu - \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 n + \frac 1 2 \log |2\pi\Sigma|
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier</title>
      <link>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</link>
      <pubDate>Fri, 07 Jan 2022 13:00:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</guid>
      <description>

&lt;h2 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h2&gt;
&lt;p&gt;Standard form of the optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Denote the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; that satisfies all the requirements in original problem as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(L:\R^N \times \R^r \times \R^s \mapsto \R\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(dom(L) = \mathcal X \times \R^r \times \R^s\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^r\lambda_if_i(x) + \sum_{i=1}^s\mu_ih_i(x)
\]&lt;/span&gt; Define the function &lt;span class=&#34;math inline&#34;&gt;\(P: \mathcal X \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu)
\]&lt;/span&gt; The primal problem is &lt;span class=&#34;math display&#34;&gt;\[
\min_{x \in \mathcal X} P(x)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(P(x) =f_0(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt;. Thus the primal problem is equivalent to the original problem. Denote its optimal value as &lt;span class=&#34;math inline&#34;&gt;\(p^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define the Lagrangian dual function &lt;span class=&#34;math inline&#34;&gt;\(D: \R^r \times \R^s \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
D(\lambda, \mu) = \inf_{x \in \mathcal X}L(x, \lambda, \mu)
\]&lt;/span&gt; The Lagrangian dual problem is &lt;span class=&#34;math display&#34;&gt;\[
\max_{\lambda,\mu;\lambda_i \ge 0} D(\lambda, \mu) \\
\]&lt;/span&gt; Denote its optimal value as &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt;. We always have &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;For any feasible &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathcal X, (\lambda, \mu) \in dom(D)\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu) \ge L(x,\lambda,\mu) \ge \inf_{x \in \mathcal X}L(x, \lambda, \mu) =D (\lambda, \mu)
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt; is condition called weak duality because it always holds. &lt;span class=&#34;math inline&#34;&gt;\(p^\star = d^\star\)&lt;/span&gt; is called a strong duality because it does not hold in general. But it does hold in general in [[Convex Optimization|convex optimization]] where &lt;span class=&#34;math inline&#34;&gt;\(f_1,\dots,f_r\)&lt;/span&gt; are convex and &lt;span class=&#34;math inline&#34;&gt;\(h_1,\dots,h_s\)&lt;/span&gt; are affine transformations of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, i.e. in the form of &lt;span class=&#34;math inline&#34;&gt;\(Ax + b\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;Assume a strong duality holds, &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is the primal optima, &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star, \mu^\star)\)&lt;/span&gt; is the dual optima, then &lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) \\
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
f_0(x^\star) = D(\lambda^\star, \mu^\star) = \inf_{x \in \mathcal X}L(x,\lambda^\star,\mu^\star) \le L(x^\star,\lambda^\star,\mu^\star) \\
f_0(x^\star) = P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x^\star,\lambda,\mu) \ge L(x^\star,\lambda^\star,\mu^\star) \\
L(x^\star,\lambda^\star,\mu^\star) \le f_0(x^\star) \le L(x^\star,\lambda^\star,\mu^\star)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) = L(x^\star,\lambda^\star,\mu^\star)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;karush-kuhn-tucker-conditions&#34;&gt;Karush-Kuhn-Tucker Conditions&lt;/h4&gt;
&lt;p&gt;In standard optimization problem, KKT conditions refer to the below four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Primal Constraints: &lt;span class=&#34;math inline&#34;&gt;\(f_1(x) \dots,f_r(x) \le 0, h_1(x),\dots,h_s(x) = 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dual Constraints: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\dots,\lambda_r \ge 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complementary Constraints: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1f_1(x),\dots,\lambda_rf_r(x) = 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gradient of Lagrangian w.r.t &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; vanishes: &lt;span class=&#34;math display&#34;&gt;\[
\nabla f_0(x) + \sum_{i=1}^r\lambda_i\nabla f_i(x) + \sum_{i=1}^s\mu_i\nabla h_i(x) = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the strong duality holds and &lt;span class=&#34;math inline&#34;&gt;\((x,\lambda,\mu)\)&lt;/span&gt; is the optima, they must satisfy the KKT conditions.&lt;/p&gt;
&lt;p&gt;Note that KKT conditions do not imply the existence of a strong duality or optima point.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Squares</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</guid>
      <description>
&lt;p&gt;Suppose we are solving the &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt; problem. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; does not always lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. However, we can try to find within &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s column space a vector &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\hat x\)&lt;/span&gt; best approximates &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. By best approximation we mean to minimize the &lt;span class=&#34;math inline&#34;&gt;\(||Ax - b||\)&lt;/span&gt; over all &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The best approximation can be achieved when &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b = \mathop{proj}_{Col(A)}b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of finding a orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, computing &lt;span class=&#34;math inline&#34;&gt;\(\hat b\)&lt;/span&gt; and then solving &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b\)&lt;/span&gt;, we can derive &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in this way: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
(b - \hat b) \perp Col(A) \iff (b - \hat b) \in Nul(A^T)\iff A^T(b - \hat b) = 0 \\
A^T(b - Ax) = 0 \\
A^TAx = A^Tb
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will show that if columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, then the least-square solution &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; is uniquely given by &lt;span class=&#34;math inline&#34;&gt;\((A^TA)^{-1}A^Tb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Firstly, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = Nul(A^TA)\)&lt;/span&gt;. This is because &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = 0 \iff A^TAx = A^T0 = 0 \\
A^TAx = 0 \iff x^TA^TAx = 0 \iff (Ax)^TAx = 0 \iff Ax = 0
\end{gather}
\]&lt;/span&gt; When columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt;, which indicates that equation (1) has the unique solution. Conversely, when &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt;, which indicates that columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eigenvectors and Eigenvalues</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;An eigenvector of a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(\rm x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\rm x = \lambda \rm x\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the corresponding eigenvalue.&lt;/p&gt;
&lt;h3 id=&#34;similarity&#34;&gt;Similarity&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrices, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is similar to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; if there is an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(PAP^{-1} = B\)&lt;/span&gt;, or equivalently &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}BP = A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are similar, then they have the same characteristic polynomial and hence then the same eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
B - \lambda I = PAP^{-1} - \lambda PP^{-1} = P(A - \lambda I)P^{-1} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det(B - \lambda I) &amp;amp;= \det(P(A - \lambda I)P^{-1}) \\
&amp;amp;= \det(P) \cdot \det(A - \lambda I) \cdot \det(P^{-1}) \\
&amp;amp;= \det(A - \lambda I)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independence-between-eigenvectors&#34;&gt;Independence between Eigenvectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Theorem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are the eigenvectors corresponding to the distinct eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, ..., \lambda_r\)&lt;/span&gt; of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\{v_1, v_2, ..., v_r\}\)&lt;/span&gt; is linearly independent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proof&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose instead these vectors are dependent. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the least index such that &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is a linear combination of previous vectors. Then there exists scalars &lt;span class=&#34;math inline&#34;&gt;\(c_1, c_2, ..., c_r\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
c_1v_1 + c_2v_2 + \cdots + c_pv_p = v_{p+1}
\]&lt;/span&gt; Multiply both sides with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to obtain &lt;span class=&#34;math display&#34;&gt;\[
c_1\lambda_1v_1 + c_2\lambda_2v_2 + \cdots + c_p\lambda_pv_p = \lambda_{p+1}v_{p+1}
\]&lt;/span&gt; Multiply both sides of &lt;span class=&#34;math inline&#34;&gt;\((1)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{p+1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
c_1\lambda_{p+1}v_1 + c_2\lambda_{p+1}v_2 + \cdots + c_p\lambda_{p+1}v_p = \lambda_{p+1}v_{p+1}
\]&lt;/span&gt; Subtract &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\((2)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
c_1(\lambda_1 - \lambda_{p+1})v_1 + c_2(\lambda_2 - \lambda_{p+1})v_2 + \cdots + c_p(\lambda_p - \lambda_{p+1})v_p = 0
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\{v_1, v_2, ..., v_p\}\)&lt;/span&gt; is independent, the weights in &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; are all zero. None of &lt;span class=&#34;math inline&#34;&gt;\((\lambda_i - \lambda_{p+1})\)&lt;/span&gt; is zero, so &lt;span class=&#34;math inline&#34;&gt;\(c_i = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\((1)\)&lt;/span&gt; says &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which is impossible.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; is nonzero so that equation (1) can hold&lt;/p&gt;
&lt;h3 id=&#34;diagonalization&#34;&gt;Diagonalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Theorem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has n independent eigenvectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(AP = PD\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(P = [\rm p_1, \rm p_2, ..., \rm p_N]\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix, &lt;span class=&#34;math display&#34;&gt;\[
AP = [A\mathrm p_1, A\mathrm p_2, ..., A\mathrm p_N] = [D_{11}\mathrm p_1, D_{22}\mathrm p_2, ..., D_{NN}\mathrm p_N]
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;s are independent, which indicates that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;s are &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent eigenvectors&lt;/p&gt;
&lt;p&gt;From this, we could also see that if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, then &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; must be the matrix stacked with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; must be the diagonal matrix filled with corresponding eigenvalues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Easy to show.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;linear-transformation&#34;&gt;Linear Transformation&lt;/h4&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then the transformation &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x \mapsto A\mathrm x\)&lt;/span&gt; in coordinate system formed by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s columns can be viewed as &lt;span class=&#34;math inline&#34;&gt;\([\mathrm x]_{\cal B} \mapsto D[\mathrm x]_\cal B\)&lt;/span&gt; in coordinate system &lt;span class=&#34;math inline&#34;&gt;\(\cal B\)&lt;/span&gt;, which is formed by &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;’s columns.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues&#34;&gt;Eigenvalues&lt;/h3&gt;
&lt;h4 id=&#34;rank-trace-and-determinant&#34;&gt;Rank, trace and determinant&lt;/h4&gt;
&lt;p&gt;Since the characteristic equation of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix involves a polynomial of degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the equation always has exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; roots, counting multiplicities, including complex roots. There are some relations between eigenvalues and matrix’s rank, trace and determinant: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\rank = \text{the number of non-zero, real eigenvalues, including multiplicities} \\
\tr = \text{sum of the eigenvalues} \\
\det = \text{product of the eigenvalues}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[The Eigen Decomposition.pdf]]&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Machine Learning Bullet Points</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</link>
      <pubDate>Sun, 19 Dec 2021 13:59:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</guid>
      <description>

&lt;p&gt;This article lists out miscellaneous points besides the machine learning algorithms that are worth noting.&lt;/p&gt;
&lt;h3 id=&#34;data-and-feature&#34;&gt;Data and Feature&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data splitting method&lt;/li&gt;
&lt;li&gt;Feature extraction&lt;/li&gt;
&lt;li&gt;Feature normalization&lt;/li&gt;
&lt;li&gt;Imbalanced data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discriminative and generative model&lt;/li&gt;
&lt;li&gt;Regularization and overfitting&lt;/li&gt;
&lt;li&gt;Multi-class classification with binary classifier&lt;/li&gt;
&lt;li&gt;Unsupervised learning
&lt;ul&gt;
&lt;li&gt;Discover inherent properties (latent variables) in the data&lt;/li&gt;
&lt;li&gt;Including:
&lt;ul&gt;
&lt;li&gt;[[Clustering]]&lt;/li&gt;
&lt;li&gt;[[Dimension Reduction|Dimension reduction]]&lt;/li&gt;
&lt;li&gt;Manifold embedding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;


</description>
    </item>
    
    <item>
      <title>Limit Computing Tricks</title>
      <link>https://chunxy.github.io/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/limit-computing-tricks/</link>
      <pubDate>Thu, 14 Jul 2022 13:42:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/limit-computing-tricks/</guid>
      <description>

&lt;h2 id=&#34;composited-exponential-functions&#34;&gt;Composited Exponential Functions&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x) = A, \lim_{x \to x_0}g(x) = B\)&lt;/span&gt;, then suppose &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x)^{g(x)}\)&lt;/span&gt; exists, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln(\lim_{x \to x_0}f(x)^{g(x)}) &amp;amp;= \lim_{x \to x_0}\ln(f(x)^{g(x)}) \\
&amp;amp;= \lim_{x \to x_0}g(x)\ln(f(x)) \\
&amp;amp;= \lim_{x \to x_0}g(x) \cdot \lim_{x \to x_0}\ln(f(x)) \\
&amp;amp;= B\ln\lim_{x \to x_0}f(x) \\
&amp;amp;= B\ln A \\
\lim_{x \to x_0}f(x)^{g(x)} &amp;amp;= e^{B\ln A} \\
&amp;amp;= (e^{\ln A})^B \\
&amp;amp;= A^B
\end{aligned}
\]&lt;/span&gt; Above deduction is not rigid, but if you want the result only, it is enough.&lt;/p&gt;
&lt;h2 id=&#34;composite-fractional-functions&#34;&gt;Composite Fractional Functions&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x) = A, \lim_{x \to x_0}g(x) = B\)&lt;/span&gt;, then suppose &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0} \frac {f(x)}{g(x)}\)&lt;/span&gt; exists, then, &lt;span class=&#34;math display&#34;&gt;\[
\lim_{x \to x_0} \frac {f(x)}{g(x)} = \frac A B
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;beta-function&#34;&gt;Beta-function&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y^{p-1} (1-y)^{q-1} = y^{p-1} \sum_{i=0}^{q-1} {q-1 \choose i} 1 ^{i} (-y)^{q-1-i} \\
= \sum_{i=0}^{q-1} {q-1 \choose i} 1 ^{i} (-y)^{q-1-i} \\
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的收敛</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</link>
      <pubDate>Wed, 13 Jul 2022 14:41:05 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</guid>
      <description>

&lt;h2 id=&#34;依概率收敛convergence-in-probability&#34;&gt;依概率收敛（convergence in probability）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能总和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) = 1\)&lt;/span&gt;，抑或是，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) = 0\)&lt;/span&gt;），则称该随机变量序列&lt;strong&gt;依概率收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{P}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt; 0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; 0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有： &lt;span class=&#34;math display&#34;&gt;\[
0 &amp;lt; P(|X_n - c| \ge \epsilon) &amp;lt; \delta
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随机变量是事件的映射，当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。&lt;/p&gt;
&lt;h2 id=&#34;几乎必然收敛almost-sure-convergence&#34;&gt;几乎必然收敛（almost-sure convergence）&lt;/h2&gt;
&lt;p&gt;在某些情况下，若随机变量序列能够和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;几乎接近，我们说它几乎必然收敛。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(P(\lim_{n \to \infty} X_n = c) = 1\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;几乎必然收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{a.s.}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; 0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有： &lt;span class=&#34;math display&#34;&gt;\[
P(|X_n - c| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然，几乎必然收敛是强于依概率收敛的。&lt;/p&gt;
&lt;h2 id=&#34;l_p收敛convergence-in-l_p&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛（convergence in &lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，对于某个&lt;span class=&#34;math inline&#34;&gt;\(p \ge 1\)&lt;/span&gt;，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\E[(\lim_{n \to \infty} X_n - c)^p] \to 0\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{L_p}{\to} c\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比较而言，&lt;span class=&#34;math inline&#34;&gt;\(L_P\)&lt;/span&gt;收敛的表达式中多了一层偏差的&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;次方的测度。&lt;/p&gt;
&lt;h3 id=&#34;均方收敛&#34;&gt;均方收敛&lt;/h3&gt;
&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(L_P\)&lt;/span&gt;收敛又称作均方收敛。根据[[大数定律和中心极限定理#Chebyshev不等式|Chebyshev不等式]]： &lt;span class=&#34;math display&#34;&gt;\[
P(|X-\E(X)| \ge \epsilon) \le \frac{\Var(X)}{\epsilon^2} = \frac{\E(X - \E(X))}{\epsilon^2}
\]&lt;/span&gt; 均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也比依概率收敛强。但它和几乎必然收敛之间并没有推导关系。&lt;/p&gt;
&lt;h2 id=&#34;依分布收敛convergence-in-distribution&#34;&gt;依分布收敛（convergence in distribution）&lt;/h2&gt;
&lt;p&gt;前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，让&lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;的分布函数，如果存在一个分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} F_n(x) = F(x)\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;依分布收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{d}{\to} F\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B&#34;&gt;随机变量的收敛&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Source Coding Theory</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</guid>
      <description>

&lt;h2 id=&#34;notations-and-concepts&#34;&gt;Notations and Concepts&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is a triple &lt;span class=&#34;math inline&#34;&gt;\((X, \newcommand{A}{\mathcal A} \newcommand{P}{\mathcal P} \A_X, \P_X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the outcome of a random variable, which takes on values from &lt;span class=&#34;math inline&#34;&gt;\(\A_X = \{a_1, a_1, \dots \}\)&lt;/span&gt;, that has probability &lt;span class=&#34;math inline&#34;&gt;\(\P_X = \{p_1, p_2, \dots \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The raw bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_0(X) = \log |\mathcal A_X|
\]&lt;/span&gt; &lt;strong&gt;The smallest &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_\delta\)&lt;/span&gt; is the smallest subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal A_X\)&lt;/span&gt; satisfying &lt;span class=&#34;math display&#34;&gt;\[
P(X \in S_\delta) \ge 1 - \delta
\]&lt;/span&gt; &lt;strong&gt;The essential bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X) = \log |S_\delta|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;shannons-source-coding-theorem&#34;&gt;Shannon’s source coding theorem&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be and ensemble with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; bits. Given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \delta &amp;lt; 1\)&lt;/span&gt;, there exists a positive integer &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; such that for &lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
|\frac{1}{N} H_\delta (X^N) - H| &amp;lt; \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;shannons-source-coding-theorem-verbose&#34;&gt;Shannon’s source coding theorem (verbose)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. random variables each with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; can be compressed into more than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits with negligible information loss as &lt;span class=&#34;math inline&#34;&gt;\(N \to \infty\)&lt;/span&gt;; conversely if they are compressed fewer than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits, it is virtually certain that there is information loss.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The random variable &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; defined for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; drawn from the ensemble &lt;span class=&#34;math inline&#34;&gt;\(Y = X^N\)&lt;/span&gt; which composes of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. &lt;span class=&#34;math inline&#34;&gt;\(X_1 \dots X_N\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be re-written as the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; information contents &lt;span class=&#34;math inline&#34;&gt;\(h_i = \log \frac 1 {P(X_i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots P(X_N)} = \frac 1 N (\log X_1 + \dots + \log X_N)
\]&lt;/span&gt; Each of these information contents is in turn a random variable with mean &lt;span class=&#34;math inline&#34;&gt;\(\bar h_i = H(X) = H\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{h_i}^2 = \sigma^2\)&lt;/span&gt;. The &lt;strong&gt;typical set&lt;/strong&gt; with parameters &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
T_{N \beta} = \{y \in \mathcal A_X^N: [\frac 1 N \log \frac 1 {P(y)} - H]^2 &amp;lt; \beta^2 \}
\]&lt;/span&gt; By the [[大数定律和中心极限定理#弱大数定律（Weak Law of large numbers）|Weak Law of Large Numbers]], &lt;span class=&#34;math inline&#34;&gt;\(P((y - H)^2 \ge \beta^2) = \frac{\sigma^2}{\beta^2 N}\)&lt;/span&gt;, and thus &lt;span class=&#34;math display&#34;&gt;\[
P(y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; As &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, the probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; falls in &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; draws near &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. We need to relate this to the theorem that for any given &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, there is a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \simeq NH\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;lt; N(H + \epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; is not the best sufficient subset for compression. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\log |T_{N \beta}|\)&lt;/span&gt; upper-bounds the &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N)\)&lt;/span&gt;. On the other hand, for all &lt;span class=&#34;math inline&#34;&gt;\(y \in T_{N \beta}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)} &amp;lt; P(y) &amp;lt; 2^{-N(H + \beta)}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;amp;&amp;lt; 1 \\
|T_{N \beta}| &amp;amp;&amp;lt; 2^{N(H + \beta)} \\
\end{align*}
\]&lt;/span&gt; If we set &lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N = N_0\)&lt;/span&gt; in a way such that &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{\epsilon^2 N_0} \le \delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(P(y \in T_{N_0 \epsilon}) \ge 1 - \delta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_{N_0 \epsilon}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset. Then, &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \le \log |T_{N_0 \epsilon}| \le N_0(H + \epsilon)\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(N \ge N_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H - \epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This part is reached by contradiction. Suppose instead there exists a &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;#39;\)&lt;/span&gt; such that there exists a sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; which results in &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^{N_0}) \le N_0(H - \epsilon)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon/2\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]&lt;/span&gt; Denote the associated subset by &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;. We are to disprove &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; can achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) = P(y \in S&amp;#39; \cap T_{N \beta}) + P(y \in S&amp;#39; \cap \overline{T_{N \beta}})
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39; \cap T_{N \beta}| \le |S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt;. The maximum value of the first term is obtained when &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap T_{N \beta}\)&lt;/span&gt; contains &lt;span class=&#34;math inline&#34;&gt;\(2^{N(H - 2\beta)}\)&lt;/span&gt; outcomes all with probability &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap \overline{T_{N \beta}} \subseteq \overline{T_{N \beta}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39; \cap \overline{T_{N \beta}}) \le P(y \in \overline{T_{N \beta}}) &amp;lt; \frac{\sigma^2}{\beta^2 N}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} + \frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; For arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt;, we can have &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \le 1 - \delta\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt;. We shall conclude that &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; cannot achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H - \epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kraft-mcmillan-inequality&#34;&gt;Kraft-McMillan Inequality&lt;/h2&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. If &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt; there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes’ lengths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The proof is done by construction. The number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; should be less than &lt;span class=&#34;math inline&#34;&gt;\(2^{l+1}\)&lt;/span&gt;, or else the above condition will be violated. Therefore we can loosely arrange all the codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; to be unique. Then the uniqueness condition is checked.&lt;/p&gt;
&lt;p&gt;Denote the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt;. For any two consecutive lengths &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;amp;\le 1 \\
C_{l+1} &amp;amp;\le 2^{l+1} - 2 C_l \\
C_{l+1} &amp;amp;\le 2(2^l - C_l) \\
\end{aligned}
\]&lt;/span&gt; This means we can append these unused &lt;span class=&#34;math inline&#34;&gt;\(2^l - C_l\)&lt;/span&gt; codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to meet the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;. Construction completes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we have a set of uniquely-decodable prefix coding. Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. Then, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sum_{i=1}^I 2^{-l_i} \le 1\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S^N &amp;amp;= (\sum_{i=1}^I 2^{-l_i})^N \\
  &amp;amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots + l_{i_N})}
  \end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\((l_{i_1} + \dots + l_{i_N})\)&lt;/span&gt; term can be treated as the length of encoding of &lt;span class=&#34;math inline&#34;&gt;\(a_{i_1} \dots a_{i_N}\)&lt;/span&gt; of arbitrary length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(l_\min = \min_i l_i, l_\max = \max_i l_i\)&lt;/span&gt;, the above can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
  S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt; represents the number of symbol codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. Since the coding is uniquely-decodable, &lt;span class=&#34;math inline&#34;&gt;\(C_l \le 2^l\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(S &amp;gt; 1\)&lt;/span&gt;, the above cannot hold for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(S \le 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;source-coding-theorem-for-symbol-codes&#34;&gt;Source coding theorem for symbol codes&lt;/h2&gt;
&lt;p&gt;For an ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, there exists a prefix code &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; with expected length satisfying &lt;span class=&#34;math display&#34;&gt;\[
H(X) \le L(C,X) &amp;lt; H(X) + 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We define the implicit probabilities &lt;span class=&#34;math inline&#34;&gt;\(q_i = 2^{-l_i} / z\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z = \sum_i 2^{-l_i}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) = \sum_i p_i l_i &amp;amp;= -\sum_i [p_i \log (q_iz)] \\
&amp;amp;=\sum_i [p_i \log 1/q_i] - \log z \\
&amp;amp;\ge H(X)
\end{aligned}
\]&lt;/span&gt; The equality holds when &lt;span class=&#34;math inline&#34;&gt;\(z = 1\)&lt;/span&gt; (the code is complete) and &lt;span class=&#34;math inline&#34;&gt;\(q = p\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(l_i = \log 1/p_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;From another perspective, suppose the coding is complete but not optimal, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) = -\sum_i [p_i \log (q_iz)] &amp;amp;= -\sum_i [p_i \log (q_i)] \\
&amp;amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i \log (q_i)] \\
&amp;amp;= H(X) + D_{KL}(p || q)
\end{aligned}
\]&lt;/span&gt; where the cost is the extra &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p || q)\)&lt;/span&gt; bits, which is brought by instead treating &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as the real distribution. &lt;span class=&#34;math inline&#34;&gt;\(D_{KL(p||q)}\)&lt;/span&gt; is termed as relative entropy or the [[KL-divergence]].&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</link>
      <pubDate>Thu, 12 May 2022 11:26:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</guid>
      <description>

&lt;h2 id=&#34;three-perspectives-on-nce&#34;&gt;Three Perspectives on NCE&lt;/h2&gt;
&lt;h3 id=&#34;non-parametric-estimation&#34;&gt;Non-parametric estimation&lt;/h3&gt;
&lt;p&gt;The traditional log-likelihood function will be &lt;span class=&#34;math inline&#34;&gt;\(\ell = \sum_x \ln p_\theta(x)\)&lt;/span&gt;. In NCE, we learn &lt;span class=&#34;math display&#34;&gt;\[
p_\theta(1|x) = \sigma(G(x;\theta) - \gamma) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}}
\]&lt;/span&gt; And corresponding loss function becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal {L} &amp;amp;= -\E_{x \sim \tilde p(x)} \ln p_\theta(1|x) - \E_{x \sim q(x)} \ln p_\theta(0|x) \\
&amp;amp;= -\int \tilde p(x) \ln p_\theta(1|x) dx - \int q(x) \ln p_\theta(0|x) dx \\
&amp;amp;= - \int [\tilde p(x) + q(x)] [\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln p_\theta(1|x) + \frac{q(x)}{\tilde p(x) + q(x)} \ln p_\theta(0|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P(y|x) = \begin{cases}\frac{\tilde p(x)}{\tilde p(x) + q(x)}, y=1 \\\frac{q(x)}{\tilde p(x) + q(x)},y=0\end{cases}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\label{loss} \begin{aligned} 
\arg \min_{\theta, \gamma} \mathcal{L} &amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln p_\theta(1|x) + \int \frac{q(x)}{\tilde p(x) + q(x)} \ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][P(1|x) \ln p_\theta(1|x) + P(0|x)\ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)][P(1|x) \ln \frac{1}{p_\theta(1|x)} + P(0|x)\ln \frac{1}{p_\theta(0|x)}]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)] H[P(y|x)||p_\theta(y|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since cross entropy &lt;span class=&#34;math inline&#34;&gt;\(H(p||q) \ge H(p)\)&lt;/span&gt; and the minimum is reached only when &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;, the global minimum for equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is reached when &lt;span class=&#34;math inline&#34;&gt;\(p_\theta(y|x) = P(y|x)\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_\theta(1|x) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}} &amp;amp;= \frac{\tilde p(x)}{\tilde p(x) + q(x)} = P(1|x) \\
\frac{q(x)}{\tilde p(x)} &amp;amp;= e^{-G(x;\theta) + \gamma} \\
\tilde p(x) &amp;amp;= \frac{q(x) e^{G(x;\theta)}}{e^\gamma}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; are learnt so that &lt;span class=&#34;math inline&#34;&gt;\(q(x) e^{G(x;\theta) - \gamma}\)&lt;/span&gt; fit the real distribution. It becomes more intuitive when &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a uniform distribution and &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(e^\gamma\)&lt;/span&gt; will be the learnt normalizing factor.&lt;/p&gt;
&lt;p&gt;https://kexue.fm/archives/5617/comment-page-1&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation-the-original-papers-view&#34;&gt;Maximum likelihood estimation (the original paper’s view)&lt;/h3&gt;
&lt;p&gt;The model is defined as &lt;span class=&#34;math inline&#34;&gt;\(\ln p_\theta(x) = \ln p^0_\alpha(x) + c\)&lt;/span&gt;. The MLE will maximize the objective function &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J_T(\theta) &amp;amp;= \frac{1}{T_d} [\sum_{i=1}^{T_d \\} \ln h(x_i;\theta) + \sum_{i=1}^{T_n} \ln (1 - h(y_i;\theta)) \text{, ($x_i$&amp;#39;s are samples, $y_i$&amp;#39;s are noises, $T_n = \nu T_d$)} \\
&amp;amp;\stackrel{P}\to J(\theta) \triangleq \E_{x \sim \tilde p} \ln h(x;\theta) + \nu \E_{x \sim q} \ln (1 - h(x;\theta)) \\
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
r_\nu(u) = \frac{1}{1 + \nu e^{-u}} \\
G(x; \theta) = \ln p_\theta(x) - \ln q(x) \\
h(x;\theta) = r_\nu(G(x;\theta)) = \frac{1}{1 + \nu e^{-G(x;\theta)}} \\
\end{gather}
\]&lt;/span&gt; Denote by &lt;span class=&#34;math inline&#34;&gt;\(\tilde J\)&lt;/span&gt; the objective function &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; seen as a function of &lt;span class=&#34;math inline&#34;&gt;\(f(.) = \ln p_\theta(.)\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f) = \E_{x \sim \tilde p} \ln r_\nu(f(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) - q(x)])
\]&lt;/span&gt; For &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; a perturbation of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f + \epsilon \phi) = \E_{x \sim \tilde p} \ln r_\nu(f(x) + \epsilon \phi(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) + \epsilon \phi - q(x)])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By Taylor’s expansion,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln r_\nu(u + \epsilon u_1 + \epsilon^2 u_2) &amp;amp;\approx \ln r_\nu (u) + r_{\frac{1}{\nu}}(-u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_\nu (u)r_\frac{1}{\nu}(-u)}{2}(\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln r_\nu (u) + \epsilon u_1r_{\frac{1}{\nu}}(-u) + \epsilon^2(u_2r_{\frac{1}{\nu}}(-u) - \frac{1}{2}u_1^2 r_{\frac{1}{\nu}}(-u)r_\nu (u)) + \Omicron \big(\epsilon^3 \big) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln \big(1 - r_v(u + \epsilon u_1 + \epsilon^2 u_2) \big) &amp;amp;\approx \ln \big(1 - r_v(u) \big) - r_v(u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_{\frac{1}{\nu}}(-u) r_\nu(u)}{2} (\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln(1 - r_v(u)) - \epsilon u_1 r_v(u) - \epsilon^2 \big( u_2 r_v(u) + \frac{1}{2} u_1^2 r_{\frac{1}{\nu}}(-u) r_\nu(u) \big) + \Omicron(\epsilon^3)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde J(f + \epsilon \phi) \approx &amp;amp;\E_{x \sim \tilde p} \ln r_\nu \big(f(x) + \epsilon \phi(x) - \ln q(x) \big) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) + \epsilon \phi - q(x)]) \\
= &amp;amp;\E_{x \sim \tilde p} \{\ln r_\nu \big(f(x) - \ln q(x) \big) + \epsilon \phi(x) r_{\frac{1}{\nu}} \big(\ln q(x) - f(x) \big) \} \\ 
&amp;amp;+\nu \E_{x \sim q} \{ \ln \big(1 - r_\nu[f(x) -\ln q(x)] \big) - \epsilon \phi(x) r_\nu \big( f(x) - \ln q(x) \big) \} + \Omicron(\epsilon^2) \\
= &amp;amp;\tilde J(f) + \epsilon \int \phi(x) \big(r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) - \nu r_\nu[f(x) - \ln q(x)] q(x) \big) + \Omicron(\epsilon^2)
\end{aligned}
\]&lt;/span&gt; The above equation attains the local maximum at &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; only if the term of order &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) &amp;amp;= \nu r_\nu[f(x) - \ln q(x)] q(x) \\
\frac{\nu \tilde p(x)}{\nu + e^{f(x) - \ln q(x)}} &amp;amp;= \frac{\nu q(x)}{1 + \nu e^{\ln q(x) - f(x)}} \\
\frac{\tilde p(x)q(x)}{\nu q(x) + p_\theta(x)} &amp;amp;= \frac{q(x) p_\theta(x)}{p_\theta(x) + \nu q(x)} \\
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is nonzero, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\tilde p(x)}{\nu q(x) + p_\theta(x)} = \frac{p_\theta(x)}{p_\theta(x) + \nu q(x)} \iff p_\theta(x) = \tilde p(x)\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;https://leimao.github.io/article/Noise-Contrastive-Estimation/&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Stonesjtu/Pytorch-NCE&#34;&gt;PyTorch-NCE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;other-reference&#34;&gt;Other Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/334772391&#34;&gt;NCE与语言模型&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jacobian Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</guid>
      <description>
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \to \R^m\)&lt;/span&gt;, with input &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt; and output &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f = 
\begin{cases}
y_1 = f_1(x_1, x_2, ..., x_n) \\
y_2 = f_2(x_1, x_2, ..., x_n) \\
... \\
y_m = f_m(x_1, x_2, ..., x_n) \\
\end{cases}
\]&lt;/span&gt; Then Jacobian matrix is &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J &amp;amp;= 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} &amp;amp; \frac{\partial f}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f}{\partial x_n} \\
\end{bmatrix} \\
&amp;amp;= 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp;amp; \frac{\partial f_m}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \\
\end{bmatrix}
\end{aligned}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear transformation, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y = Tx\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
J = T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear transformation and &lt;span class=&#34;math inline&#34;&gt;\(n = m\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; square matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
dy_1 \\
dy_2 \\
\vdots \\
dy_n \\
\end{bmatrix} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}
\begin{bmatrix}
dx_1 \\
dx_2 \\
\vdots \\
dx_n \\
\end{bmatrix}
\]&lt;/span&gt; That is, &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{
\begin{bmatrix}
dy_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dy_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dy_n
\end{bmatrix}}_A
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix} =
\underbrace{
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}}_J
\underbrace{
\begin{bmatrix}
dx_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dx_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dx_n
\end{bmatrix}}_B
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(JB\)&lt;/span&gt; are both diagonal. From above equation we can find that &lt;span class=&#34;math inline&#34;&gt;\(A = JB\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|A| &amp;amp;= |JB| \\
dy_1 \dots dy_n &amp;amp;= |J|dx_1 \dots dx_n \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spherical Coordinates</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</link>
      <pubDate>Mon, 09 May 2022 20:26:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</guid>
      <description>
&lt;p&gt;The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d. In &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d case, their conversion can be described as below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cartesian-to-spherical &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
r &amp;amp;= \sqrt{x_1^2 + \dots + x_k^2} &amp;amp;&amp;amp; \\
\varphi_1 &amp;amp;= \arccot \frac{x_1} {\sqrt{x_k^2 + \dots + x_2^2}} &amp;amp;&amp;amp;=  \arccos \frac{x_1} {\sqrt{x_k^2 + \dots + x_1^2}} \\
\varphi_2 &amp;amp;= \arccot \frac{x_2} {\sqrt{x_k^2 + \dots + x_3^2}} &amp;amp;&amp;amp;=  \arccos \frac{x_2} {\sqrt{x_k^2 + \dots + x_2^2}} \\
&amp;amp; \vdots &amp;amp;&amp;amp;\vdots\\
\varphi_{k-2} &amp;amp;= \arccot \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}} &amp;amp;&amp;amp;= \arccos \frac{x_{k-2}} {\sqrt{x_k^2 + x_{k-1}^2 + x_{k-2}^2}} \\
\varphi_{k-1} &amp;amp;= 2 \arccot \frac{x_{k-1} + \sqrt{x_k^2 + x_{k-1}^2}}{x_k} &amp;amp;&amp;amp;= 
\begin{cases}
\arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n \ge 0 \\
2\pi - \arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n &amp;gt; 0\\
\end{cases}
\end{alignat}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Spherical-to-Cartesian &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_1 &amp;amp;= r \cos(\varphi_1) \\
x_2 &amp;amp;= r \sin(\varphi_1) \cos(\varphi_2) \\
\notag &amp;amp;\vdots \\
x_{k-1} &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) \\
x_k &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\end{align}
\]&lt;/span&gt; The corresponding [[Jacobian Matrix]] is &lt;span class=&#34;math display&#34;&gt;\[
J^{(k)} = \left[ \begin{array}{ccccc|c}
\cos (\varphi_1) &amp;amp; -r \sin(\varphi_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\sin(\varphi_1) \cos(\varphi_2) &amp;amp; r \cos(\varphi_1) \cos(\varphi_2) &amp;amp; -r \sin(\varphi_1) \sin(\varphi_2) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ &amp;amp; \ddots &amp;amp; \vdots \\
\hline
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; -r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1})
\end{array} \right]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|J^{(2)}|\)&lt;/span&gt; can be easily derived as &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k)}|\)&lt;/span&gt; can be constructed from &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k-1)}|\)&lt;/span&gt;. Comparing &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt;,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and an extra row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\cos(\varphi_{k-1})\)&lt;/span&gt; term in each element than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\sin(\varphi_{k-1})\)&lt;/span&gt; term in each element than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; are totally the same on the region delimited by row &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, row &lt;span class=&#34;math inline&#34;&gt;\(k-2\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apply the [[Laplace Expansion]] along column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and combine the property of determinant to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J^{(k)}| =\ &amp;amp; \underbrace{0 + \dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \sin(\varphi_{k-1}) \big( \sin(\varphi_{k-1}) |J^{(k-1)}| \big)] + \\
&amp;amp; (-1)^{n+n} [r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) \big( \cos(\varphi_{k-1}) |J^{(k-1)}| \big)] \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \big( \sin_{\varphi_{k-1}}^2 + \cos{\varphi_{k-1}}^2 \big) |J^{(k-1)}| \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) |J^{(k-1)}| \\
\end{aligned}
\]&lt;/span&gt; By induction, &lt;span class=&#34;math display&#34;&gt;\[
|J^{(k)}| = r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\]&lt;/span&gt; Therefore when changing basis from orthogonal coordinate system to polar coordinate system, &lt;span class=&#34;math display&#34;&gt;\[
\d x_1 \dots \d x_k = r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2}) \d r \d \varphi_1 \dots \d \varphi_{k-1}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates&#34;&gt;Wiki&lt;/a&gt;|&lt;a href=&#34;https://wuli.wiki//online/SphCar.html&#34;&gt;3d Case Blog 1&lt;/a&gt;|&lt;a href=&#34;https://www.cnblogs.com/hans_gis/archive/2012/11/21/2755126.html&#34;&gt;3d Case Blog 2&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</guid>
      <description>

&lt;h2 id=&#34;continuous-time-fourier-transform&#34;&gt;Continuous-time Fourier Transform&lt;/h2&gt;
&lt;h3 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h3&gt;
&lt;p&gt;In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two basis is zero. Inner product can also be defined on some common interval between two functions, and thus the orthogonality.&lt;/p&gt;
&lt;h4 id=&#34;frequency-domain&#34;&gt;Frequency Domain&lt;/h4&gt;
&lt;p&gt;It is intuitive to model after the inner product between vectors. Function (signal) on its domain can be viewed as an “infinite-dimension” vector. We represent this infinity in the definition of function inner product by integration. In particular, given two functions &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, an interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[
\int\limits_{x \in I}s(x)g(x)dx
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are orthogonal on interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; if their inner product &lt;span class=&#34;math inline&#34;&gt;\(\int_{x \in I}s(x)g(x)dx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set of basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt; Euclidean space contains &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent orthogonal basis. For an “infinite-dimension” function space, there are an infinite number of basis, among which a group of sine and cosine functions satisfy. For integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and positive integers &lt;span class=&#34;math inline&#34;&gt;\(m, n\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = \pi, m = n, m, n \ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = 0, m \ne n, m, n \ge 1 \\
\end{array}
\right. \\
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = \pi, m = n, m, n \ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = 0, m \ne n, m, n \ge 1 \\
\end{array}
\right. \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx = 0, m, n \ge 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m = n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 - \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 + \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(nx) \cos(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin(2nx)}{2}dx \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m \ne n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) + \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} + \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) - \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} - \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin((n+m)x) + \sin((n-m)x)}{2}dx \\
&amp;amp;= -\frac{\frac{\cos((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} + \frac{\cos((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, we can use the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(x), \sin(x), \cos(2x), \sin(2x), \dots\)&lt;/span&gt; to fit any &lt;strong&gt;continuous function&lt;/strong&gt; on interval &lt;span class=&#34;math inline&#34;&gt;\([-\pi, \pi]\)&lt;/span&gt;. Or use &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(2\pi fx), \sin(2\pi fx), \cos(2\pi f2x), \sin(2\pi f2x), \dots\)&lt;/span&gt; to fit any function on interval &lt;span class=&#34;math inline&#34;&gt;\([\frac{-1}{2f} + \frac{k}{f}, \frac{1}{2f} + \frac{k}{f}]\)&lt;/span&gt;, which can be any interval depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (frequency) and the integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; approximated with such series up to level &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; can be written as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{split}
s_N(x) &amp;amp;= a_0 + \sum_{i=1}^N \big( \underbrace{a_n}_{A_n\sin(\varphi_n)} \cos(2\pi fnx) + \underbrace{b_n}_{A_n\cos(\varphi_n)} \sin(2\pi fnx) \big) \\
&amp;amp;= a_0 + \sum_{n=1}^N \bigg( A_n\sin(2\pi fnx + \varphi_n) \bigg) \text{, where} \\
\end{split} \\
\notag &amp;amp;A_n = \sqrt{a_n^2 + b_n^2}, \sin(\varphi_n) = \frac{a_n}{\sqrt{a_n^2 + b_n^2}}, \cos(\varphi_n) = \frac{b_n}{\sqrt{a_n^2 + b_n^2}}
\end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A_n\)&lt;/span&gt; can be interpreted as the amplitude, &lt;span class=&#34;math inline&#34;&gt;\(\varphi_n\)&lt;/span&gt; as the phase, &lt;span class=&#34;math inline&#34;&gt;\(nf\)&lt;/span&gt; as the frequency.&lt;/p&gt;
&lt;h4 id=&#34;complex-frequency-domain&#34;&gt;Complex Frequency Domain&lt;/h4&gt;
&lt;p&gt;By Euler’s Formula we have &lt;span class=&#34;math display&#34;&gt;\[
e^{ix} = \cos x + i\sin x
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(s_N(x)\)&lt;/span&gt; can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
s_N(x) &amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( \underbrace{a_n}_{A_n\cos \phi_n} \cos(2\pi fnx) + \underbrace{b_n}_{A_n\sin \phi_n} \sin(2\pi fnx) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( A_n\cos(2\pi fnx - \phi_n) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx - \phi_n) + i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx - \phi_n) - i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;\Downarrow_\text{by multiplication rule between complex numbers in polar form}  \\
&amp;amp;= &amp;amp;&amp;amp; \sum_{n=-N}^N c_ne^{i 2\pi fnx} \\
\end{alignat}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
c_n &amp;amp;= 
\begin{cases}
\frac{A_n}{2}(\cos \phi_n - i\sin \phi_n) = \frac{1}{2}(a_n - ib_n), &amp;amp;n &amp;gt; 0 \\
\overline{c_{|n|}} =\frac{A_n}{2}(\cos \phi_n + i\sin \phi_n), &amp;amp;n &amp;lt; 0 \\
a_0, &amp;amp;n = 0
\end{cases}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(N \to +\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; can be reconstructed as the Fourier Series: &lt;span class=&#34;math display&#34;&gt;\[
s(x) = \lim_{N \to +\infty} s_N(x) = a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg) \\
\]&lt;/span&gt; The problem comes how &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; can be computed. By the orthogonality mentioned before, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \d x = \frac{1}{f} a_0 \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \cos(2\pi fkx) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \cos (2\pi fkx) \d x = \frac{1}{2f} a_k \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \sin(2\pi fkx) \d x =\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \sin (2\pi fkx) \d x = \frac{1}{2f} b_k \\
\end{gather}
\]&lt;/span&gt; The computation of &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; can be combined together by the Euler’s Formula: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) e^{-i 2\pi fkx} \d x &amp;amp; = \int_{-1/2f+f/k}^{1/2f+f/k} (a_k \cos(2\pi fkx) + b_k \sin(2\pi fkx)) (\cos(2\pi fkx) - i \sin(2\pi fkx)) \d x \\
&amp;amp;= \frac{1}{2f} (a_k - i b_k) = \frac{1}{f} c_k
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;fourier-transform&#34;&gt;Fourier Transform&lt;/h3&gt;
&lt;p&gt;We have been through representing a continuous function on a certain interval using the Fourier Series. This can be quite useful for periodic functions. As long as we figure out the representation on its repeating interval, we obtain the representation on its whole domain. The problem is more of computing the factor for each sine and cosine function.&lt;/p&gt;
&lt;p&gt;The process of finding out factors for an arbitrary continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is called Fourier Transform. It transforms the function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; from the time domain to the frequency domain. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is periodic, it can be easily represented by the Fourier Series as discussed in previous section. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is not periodic, we can treat the periodic interval as &lt;span class=&#34;math inline&#34;&gt;\([-\infty, +\infty]\)&lt;/span&gt;. Its Fourier Transform and the inverse will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\hat s \stackrel{\mathcal F}\Longleftrightarrow s \\
\hat s(f) = \int_{-\infty}^{+\infty} s(t) e^{-i 2\pi f x} \d t \\
s(x) = \int_{-\infty}^{+\infty} \hat s(f) e^{i 2\pi f x} \d f
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;discrete-time-fourier-transform&#34;&gt;Discrete-time Fourier Transform&lt;/h2&gt;
&lt;p&gt;The domain (time axis) of the function (signal) is continuous in our discussion by now. When the time axis is discrete (and usually takes on a series of integers), we are facing the Discrete-time Fourier Transform. We will be using the term &lt;strong&gt;signal&lt;/strong&gt; instead of the function from now on.&lt;/p&gt;
&lt;p&gt;For a discrete signal &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, its Fourier Transform is &lt;span class=&#34;math display&#34;&gt;\[
\hat s(\omega) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i\omega k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the angular speed. &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is in the unit of radian/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(\omega)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; between two signal samples. &lt;span class=&#34;math display&#34;&gt;\[
\hat s(f) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i 2\pi f k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the “frequency”. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the unit of cycles/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(f)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; cycles between two signal samples.&lt;/p&gt;
&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Laplace Transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;-transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Fast Fourier Transform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://charlesliuyx.github.io/2018/02/18/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E8%AE%A9%E4%BD%A0%E6%B0%B8%E8%BF%9C%E5%BF%98%E4%B8%8D%E4%BA%86%E7%9A%84%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E8%A7%A3%E6%9E%90/&#34;&gt;傅立叶变换与群&lt;/a&gt;|&lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/1119070975&#34;&gt;如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎&lt;/a&gt;|&lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/334686351&#34;&gt;如何理解傅里叶变换公式？ - 马同学的回答 - 知乎&lt;/a&gt;|&lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_transform&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>特征函数</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 09 May 2022 11:51:08 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid>
      <description>

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;h3 id=&#34;感性认知&#34;&gt;感性认知&lt;/h3&gt;
&lt;p&gt;根据泰勒级数我们可以得知，两个函数&lt;span class=&#34;math inline&#34;&gt;\(f(x),g(x)\)&lt;/span&gt;，如果它们各阶导数相等的越多，它们就越相似，换言之 &lt;span class=&#34;math display&#34;&gt;\[
\text{各阶导数都相同} \Rightarrow f(x) = g(x)
\]&lt;/span&gt; 可以说，函数的各阶导数即是它们的特征。&lt;/p&gt;
&lt;p&gt;对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即 &lt;span class=&#34;math display&#34;&gt;\[
\text{各阶矩都相同} \Rightarrow \text{随机变量对应的分布相同}
\]&lt;/span&gt; 对于随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，其特征函数定义为 &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{itX}\)&lt;/span&gt;的泰勒级数为 &lt;span class=&#34;math display&#34;&gt;\[
e^{itX} = 1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots + \frac{(itX)^n}{n!}
\]&lt;/span&gt; 代入特征函数可得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\varphi(t) &amp;amp;= \E[1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots + \frac{(itX)^n}{n!}] \\
&amp;amp;= \E[1] + \E[\frac{itX}{1!}] - \E[\frac{t^2X^2}{2!}] + \dots + \E[\frac{(itX)^n}{n!}] \\
&amp;amp;= 1 + \frac{it \overbrace{\E[X]}^\text{一阶矩} }{1!} - \frac{t^2 \overbrace{\E[X^2]}^\text{二阶矩} }{2!} + \dots + \frac{(it)^n \overbrace{\E[X^n]}^\text{n阶矩} }{n!} \\
\end{aligned}
\]&lt;/span&gt; 可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。&lt;/p&gt;
&lt;h3 id=&#34;理性认知&#34;&gt;理性认知&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}] = \int_{-\infty}^{+\infty} e^{itx} p(x)\; dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而对&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;进行逆傅里叶变换可得 &lt;span class=&#34;math display&#34;&gt;\[
F(t) = \int_{-\infty}^{+\infty} p(x) e^{-itx} dx
\]&lt;/span&gt; 可见二者互为共轭关系： &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \overline{F(t)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;应用&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;通过求&lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt;时的各阶导数，可以快速求得各阶矩： &lt;span class=&#34;math display&#34;&gt;\[
\varphi^{(k)}(0) = i^k \E[X^k]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/23686709&#34;&gt;特征函数的理解&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gaussian Distribution</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/gaussian-distribution/</link>
      <pubDate>Sun, 08 May 2022 19:09:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/gaussian-distribution/</guid>
      <description>

&lt;h3 id=&#34;one-dimensional&#34;&gt;One-dimensional&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;-d random variable &lt;span class=&#34;math inline&#34;&gt;\(X \sim \mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt;, then its density function is &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty}p(x)dx &amp;amp;= \sqrt{(\int_{-\infty}^{+\infty}p(x)dx) \cdot (\int_{-\infty}^{+\infty}p(y)dy)} \\
&amp;amp;= \sqrt{\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}p(x)p(y)dxdy} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I^2 = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}p(x)p(y)dxdy\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-\infty}^{+\infty}p(x)dx = \sqrt{I^2} \\
I^2 = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{y - \mu}{\sigma})^2}dxdy
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = \frac{x-\mu}{\sigma}, v = \frac{y-\mu}{\sigma}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}u^2}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}v^2}\sigma^2dudv \\
&amp;amp;= \frac{1}{2\pi}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}(u^2+v^2)}dudv
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = r\sin\theta, v = r\cos\theta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{-\frac{1}{2}(u^2+v^2)}dudv &amp;amp;= 
\int_{0}^{2\pi}\int_{0}^{+\infty}e^{-\frac{1}{2}(r^2\sin^2\theta+r^2\cos^2\theta)}rdrd\theta \\
&amp;amp;= \int_{0}^{2\pi}\int_{0}^{+\infty}-e^{-\frac{1}{2}r^2}d(-\frac{1}{2}r^2)d\theta \\
&amp;amp;= \int_{0}^{2\pi}-e^{t}\Big|_{t=0}^{t=-\infty}d\theta \\
&amp;amp;= \int_{0}^{2\pi}d\theta \\
&amp;amp;= 2\pi
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I^2 = \frac{1}{2\pi}2\pi = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\int_{-\infty}^{+\infty}p(x)dx = \sqrt{I^2} = 1\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independent-standard-n-dimensional&#34;&gt;Independent standard &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;, suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i (i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution, we can derive the joint distribution density function for random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; to be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(z) &amp;amp;= p(z_1, z_2, ..., z_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z_i^2} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}z^Tz} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;n-dimensional&#34;&gt;N-dimensional&lt;/h3&gt;
&lt;p&gt;We have given the joint distribution function of independent &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional standard Gaussian distribution. What about these &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; dimensions are not independent with each other, but are only correlated in first order?&lt;/p&gt;
&lt;p&gt;Formally, &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are not independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i (i=1,...,n)\)&lt;/span&gt; observes &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu_i, \sigma_i^2)\)&lt;/span&gt;. We would like to linearly transform &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X_i, X_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(X_i (i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;’s covariance matrix is &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Z\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Z\)&lt;/span&gt; is [[Real Symmetric Matrix|real-symmetric]] and thus can be diagonalized into &lt;span class=&#34;math inline&#34;&gt;\(U\Lambda U^T\)&lt;/span&gt;. There is an invertible transformation matrix &lt;span class=&#34;math inline&#34;&gt;\(B^{-1} = \Lambda^{-\frac{1}{2}} U^T\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
X = B^{-1}(Z - \mu) \\
Z = BX + \mu \\
X \sim \mathcal{N}(0, I)
\]&lt;/span&gt; then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_X(x) &amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}x^Tx} 
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is to take on values in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, which is a subset of &lt;span class=&#34;math inline&#34;&gt;\(\R^{n}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P_Z(Z \in S) = \int_Sp_Z(z)dz
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(Z = f(X) = BX + \mu\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is invertible, the mapping &lt;span class=&#34;math inline&#34;&gt;\(X \to Z\)&lt;/span&gt; is one-to-one, therefore the multivariate [[Jacobian Matrix|Jacobian transformation]]: &lt;span class=&#34;math display&#34;&gt;\[
J(X \to Z) = B^{-1} \\
\]&lt;/span&gt; with its determinant &lt;span class=&#34;math inline&#34;&gt;\(J = |J(X \to Z)| = |B^{-1}| = |B|^{-1}\)&lt;/span&gt;. Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J| &amp;amp;= \sqrt{|B|^{-1}|B|^{-1}} \\
&amp;amp;= \sqrt{|B|^{-1}|B^T|^{-1}} \\
&amp;amp;= \sqrt{|BB^T|^{-1}} \\
&amp;amp;= |BB^T|^{-\frac{1}{2}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Z(Z \in S) &amp;amp;= P_X(X \in f^{-1}(S)) \\
&amp;amp;= \int_{f^{-1}(S)}p_X(x)dx \\
&amp;amp;= [\int_Sp_X(f^{-1}(z))|J|dz]_{x=f^{-1}(z)} \\
&amp;amp;= \int_Sp_X(f^{-1}(z))|J|dz \\
\int_Sp_Z(z)dz &amp;amp;= \int_Sp_X(f^{-1}(z))|J|dz \\
p_Z(z) &amp;amp;= p_X(f^{-1}(z))|J| \\
&amp;amp;= p_X(B^{-1}(z-\mu)|J| \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}(z-\mu)^T(B^{-1})^TB^{-1}(z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}(z-\mu)^T(B^T)^{-1}B^{-1}(z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}(z-\mu)^T(BB^T)^{-1}(z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}|BB^T|^\frac{1}{2}}e^{-\frac{1}{2}(z-\mu)^T(BB^T)^{-1}(z-\mu)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= E[(z-\mu)(z-\mu)^T] \\
&amp;amp;= E[BXX^TB^T] \\
&amp;amp;= BE[XX^T]B^T \\
&amp;amp;= BIB^T \\
&amp;amp;= BB^T
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, &lt;span class=&#34;math display&#34;&gt;\[
p(z) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_Z|}}e^{-\frac{1}{2}(z-\mu)^T\Sigma_Z^{-1}(z-\mu)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/40225646&#34;&gt;Integration&lt;/a&gt;|&lt;a href=&#34;https://zhuanlan.zhihu.com/p/58987388&#34;&gt;Multi-variate&lt;/a&gt;|&lt;a href=&#34;https://en.wikipedia.org/wiki/Integration_by_substitution&#34;&gt;Change of variable&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Contrastive Predictive Coding</title>
      <link>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</link>
      <pubDate>Fri, 29 Apr 2022 11:32:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</guid>
      <description>

&lt;h2 id=&#34;rationale&#34;&gt;Rationale&lt;/h2&gt;
&lt;p&gt;CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the data. The original data can either be the original data before the encoding, or the future data with different steps.&lt;/p&gt;
&lt;p&gt;CPC learns the representation by minimizing the following loss function: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\c}{\mathrm{c}}
\mathcal L_N = -\E_{t \sim \Phi} \log \frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} = -\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(t = (\ell, \x_1, \dots, \x_{N+1}, \c^*)\)&lt;/span&gt; is a tuple of random variables and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the distribution from which &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is drawn. &lt;span class=&#34;math inline&#34;&gt;\((\x:\c)_{1:N+1}\)&lt;/span&gt; are drawn from the joint distribution &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(\x,\c)\)&lt;/span&gt;. All &lt;span class=&#34;math inline&#34;&gt;\(\c_i\)&lt;/span&gt;’s but one randomly-chosen &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; are trimmed from the original samples. &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; is known but it is unknown which sample it is associated with. Denote by &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; the index of this unique sample we are trying to predict.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\x_{1:N+1}\)&lt;/span&gt; consists of one positive sample &lt;span class=&#34;math inline&#34;&gt;\(\x^*\)&lt;/span&gt; that is matched with &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; and more other independent negative (noise) samples &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt;’s that are not matched with &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed ratio of the number of negative samples to the number of positive samples. Let &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i|\x_{1:N+1},\c^*)\)&lt;/span&gt; represent the probability that &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt; is the positive sample given &lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots x_{N+1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\ell=i, \x_{1:N+1},\c^*)}{\sum_{j=1}^{N+1} P(\ell=j, \x_{1:N+1},\c^*)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i|\x_{1:N+1},\c^*) = P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)}{\sum_{j=1}^{N+1} [P(\x_j,\c^*) \prod_{k=1,k \ne j}^{N+1} P(\x_k)]} \\
&amp;amp;= \frac{\tilde p(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} \tilde p_X(\x_j)} {\sum_{j=1}^{N+1} [\tilde p(\x_j,\c^*) \prod_{k=1,k \ne j}^{N+1} \tilde p_X(\x_j)]} \\
&amp;amp;= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} \\
\end{aligned}
\]&lt;/span&gt; The loss function is in fact the expectation of Categorical Cross Entropy of correctly identifying the sample as positive. The minimum of loss function is thus reached when the two categorical distributions are identical. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_\theta(l = i|x_{1:N+1},\c^*) = \frac{f_\theta(\x_i,\c)}{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} = \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} = P(\ell=i|\x_{1:N+1},\c^*) \\
f_\theta(\x_i,\c) = \frac{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}{\sum_{\x^\prime \in X} \frac{p_d(\x^\prime|\c)}{p_n(\x^\prime)}} \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} \\
f_\theta(\x_i,\c) \propto \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;bounding-the-mutual-information&#34;&gt;Bounding the Mutual Information&lt;/h2&gt;
&lt;p&gt;CPC helps estimate the lower bound of the mutual information when optimizing the InfoNCE loss. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal L_N^{\text{opt}} &amp;amp;= -\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} {\sum_{\x&amp;#39; \in X} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)} + \sum_{\x&amp;#39; \in X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}} {\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} \sum_{\x&amp;#39; \in X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)} \big) \\
&amp;amp;\approx \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \E_{\tilde p_X(\x&amp;#39;)} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)} \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;\ge \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} [\log \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)}] + \log (N - 1) \\
&amp;amp;= -I(\x;\c^*) + \log (N - 1)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(\x;\c^\star) \ge \log(N-1) - \mathcal L^{\mathrm{opt}}_{N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://anilkeshwani.github.io/CPC/&#34;&gt;Paper Review&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=zNKMHj1eLa0&#34;&gt;CPC Formulation&lt;/a&gt;|&lt;a href=&#34;https://jxmo.io/posts/nce&#34;&gt;NCE and InfoNCE&lt;/a&gt;|&lt;a href=&#34;https://colab.research.google.com/github/google-research/google-research/blob/master/vbmi/vbmi_demo.ipynb#scrollTo=DUd0hFZ5Js8k&#34;&gt;Demo of Bounding Mutual Information&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Conditional Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</link>
      <pubDate>Wed, 13 Apr 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</guid>
      <description>
&lt;p&gt;The Conditional Entropy measures the the amount of information needed to describe the outcome of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that the value of another random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is known. The entropy of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(Y|X) = -\sum_{(x,y) \in X \times Y} p_{(X,Y)}(x,y) \log \frac{p_{(X,Y)}(x,y)}{p_X(x)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lipschitz Continuity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</link>
      <pubDate>Mon, 31 Jan 2022 00:02:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</guid>
      <description>

&lt;p&gt;For a continuous mapping &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, it is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous if there exists a number &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in dom(f)\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
||f(x) - f(y)|| \le K||x - y||
\]&lt;/span&gt; If the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous, we further say &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-smooth.&lt;/p&gt;
&lt;h3 id=&#34;lipschitz-constant&#34;&gt;Lipschitz Constant&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the minimum number to make the above condition hold, then &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is called the &lt;strong&gt;Lipschitz constant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Lipschitz constant for a general differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will be the maximum spectral norm of its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt; over its domain. &lt;span class=&#34;math display&#34;&gt;\[
||f||_{Lip} = \sup_x \sigma[\nabla f(x)] = \sup_x \sup_{||v||=1} \nabla f(x) \cdot v
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma[\nabla f(x)]\)&lt;/span&gt; denotes the spectral norm of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s gradient at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Lipschitz constant for a matrix transformation will be the matrix’s [[Spectral Normalization|spectral norm]].&lt;/li&gt;
&lt;li&gt;The Lipschitz constant for a &lt;span class=&#34;math inline&#34;&gt;\(\R \mapsto \R\)&lt;/span&gt; function will be its largest [[Subgradient#Properties|subgradient]] over its domain&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;composition-of-functions&#34;&gt;Composition of Functions&lt;/h3&gt;
&lt;p&gt;Suppose two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are Lipschitz continuous respectively. Then, &lt;span class=&#34;math display&#34;&gt;\[
\nabla (f \circ g)(x) = \nabla f [g(x)] \nabla g(x)
\]&lt;/span&gt; by the chain rule of derivatives. &lt;span class=&#34;math inline&#34;&gt;\(f \circ g\)&lt;/span&gt;’s Lipschitz constant will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sigma(\nabla (f \circ g)(x)) &amp;amp;= \sup_{||v|| = 1} ||\{\nabla f[g(x)] \nabla g(x)\} v|| \\
&amp;amp;=  \sup_{||v|| = 1} \{||\nabla g(x) v||\} \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;\le  \sup_{||v|| = 1} \sigma[\nabla g(x)] \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \sup_{||v|| = 1} \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \cdot \sigma\{\nabla f[g(x)]\}
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(||f \circ g||_{Lip} = ||f||_{Lip} \cdot ||g||_{Lip}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/&#34;&gt;Lipschitz Continuity, convexity, subgradients – Marco Tulio Ribeiro – (washington.edu)&lt;/a&gt; &lt;a href=&#34;https://xingyuzhou.org/blog/notes/Lipschitz-gradient&#34;&gt;Lipschitz continuous gradient · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Real Symmetric Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</link>
      <pubDate>Sun, 30 Jan 2022 13:46:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</guid>
      <description>
&lt;p&gt;Assume &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real-valued symmetric matrix, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;its eigenvalues and thus eigenvectors are real-valued&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has some imaginary eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, the corresponding imaginary eigenvector being &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Ax &amp;amp;= \lambda x \\
A(x_{real} + x_{img}) &amp;amp;= (\lambda_{real} + \lambda_{img})(x_{real} + x_{img}) \\
Ax_{real} + Ax_{img} &amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) + (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
\end{aligned}
\]&lt;/span&gt; Suppose &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s complex conjugate are &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; respectively. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A\bar x &amp;amp;= Ax_{real} - Ax_{img} \\
&amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) - (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
&amp;amp;= \bar \lambda \bar x \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(A\bar x)^T &amp;amp;= (\bar \lambda \bar x)^T \\
\bar x^TA^T &amp;amp;= \bar \lambda \bar x^T \\
\bar x^TA &amp;amp;= \bar \lambda \bar x^T
\end{aligned}
\]&lt;/span&gt; Left multiply &lt;span class=&#34;math inline&#34;&gt;\(\bar x^T\)&lt;/span&gt; on both sides of &lt;span class=&#34;math inline&#34;&gt;\(Ax = \lambda x\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar \lambda \bar x^T x \\
\]&lt;/span&gt; Also note that: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar x^T \lambda x = \lambda\bar x^Tx\\
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda \bar x^T x = \lambda \bar x^T x\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\bar x^Tx\)&lt;/span&gt; is real-value, &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda = \lambda\)&lt;/span&gt;. In other words, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is real-valued&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;its eigenvectors corresponding to different eigenvalues are orthogonal&lt;/p&gt;
&lt;p&gt;Arbitrarily take &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’ s two eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2\)&lt;/span&gt; and their eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2， \lambda_1 \ne \lambda_2\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= v_1^TA^Tv_2 \\
&amp;amp;= v_1^TAv_2 \\
&amp;amp;= v_1^T \lambda_2v_2 \\
&amp;amp;= \lambda_2v_1^Tv_2 \\
(Av_1)^Tv_2 &amp;amp;= \lambda_1v_1^Tv_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda_1v_1^Tv_2 &amp;amp;= \lambda_2v_1^Tv_2 \\
(\lambda_1 - \lambda_2)v_1^Tv_2 &amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ne \lambda_2\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(v_1^Tv_2 = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; are orthogonal&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors and thus [[Eigenvectors and Eigenvalues#Diagonalization|diagonalizable]]&lt;/p&gt;
&lt;p&gt;In this case, suppose &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^{-1}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is not invertible, by [[Eigenvectors and Eigenvalues#Eigenvalues|the relation between the matrix rank and the eigenvalues]], some of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;’s entries on the diagonal are zero. By adding &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^\prime &amp;amp;= P\Lambda P^{-1} + \lambda PIP^{-1} \\
&amp;amp;= P(\Lambda + \lambda I)P^{-1}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt; complements all the zero entries on the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(A^\prime\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; non-zero eigenvalues and is invertible. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; becomes invertible by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;its diagonalization can be in the form of &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^T\)&lt;/span&gt; by properly selecting the eigenvectors, drawing from its orthogonal eigenvectors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is positive semi-definite &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; its eigenvalues are non-negative&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zybuluo.com/fsfzp888/note/1112344&#34;&gt;real-valued eigenvalues and orthogonal eigenvectors&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spectral Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/spectral-normalization/</link>
      <pubDate>Sat, 29 Jan 2022 21:23:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/spectral-normalization/</guid>
      <description>

&lt;p&gt;Spectral Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
||A||_2 = \max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} = \sqrt{\lambda_{\max}(A^TA)} = \sigma_{\max}(A)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\rm z \in \R^N\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\max}(A^TA)\)&lt;/span&gt; is the maximum eigenvalue of matrix &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\max}(A)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s largest singular value.&lt;/p&gt;
&lt;p&gt;The equation on the right can be proved in this way: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} \iff \max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; We may force a constraint on &lt;span class=&#34;math inline&#34;&gt;\(\mathrm z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(||\mathrm z||^2_2 = 1\)&lt;/span&gt;. This is because &lt;span class=&#34;math display&#34;&gt;\[
\frac{||Ac\mathrm z||^2_2}{||c\mathrm z||^2_2} = \frac{c^2||A\mathrm z||^2_2}{c^2||\mathrm z||^2_2} = \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2} = ||A\mathrm z||^2_2 = \mathrm z^TA^TA\mathrm z \\
s.t. ||\mathrm z||^2_2 = 1
\end{gather}
\]&lt;/span&gt; This can be solved by Lagrange Multiplier, where the Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
L(\mathrm z, \lambda) = \mathrm z^TA^TA\mathrm z + \lambda(||\mathrm z||^2_2 - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extrapolation of the Spectral Normalization will be related to Rayleigh Quotient.&lt;/p&gt;
&lt;h3 id=&#34;power-iteration&#34;&gt;Power Iteration&lt;/h3&gt;
&lt;p&gt;We can apply a SVD to obtain a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s Spectral Norm, i.e. the square root of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s largest eigenvalue. There is also an iterative method to do so.&lt;/p&gt;
&lt;p&gt;Begin with an arbitrary vector &lt;span class=&#34;math inline&#34;&gt;\(v_0 = v \in \R^N\)&lt;/span&gt;, the iteration rule will be &lt;span class=&#34;math display&#34;&gt;\[
v_{t+1} = A^TAv_t
\]&lt;/span&gt; Unroll &lt;span class=&#34;math inline&#34;&gt;\(v_{t}\)&lt;/span&gt; to get &lt;span class=&#34;math inline&#34;&gt;\(v_t = (A^TA)^tv\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is real symmetric, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be written as a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s orthonormal eigenvectors: &lt;span class=&#34;math display&#34;&gt;\[
v = \sum_{i=1}^N \alpha_i u_i
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt; be arranged by corresponding eigenvector from large to small, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_t &amp;amp;= (A^TA)^t \sum_{i=1}^N \alpha_i u_i \\
&amp;amp;= \sum_{i=1}^N \alpha_i \lambda_i^t u_i \\
&amp;amp;= \alpha_1 \lambda_1^t \sum_{i=1}^N \frac{\alpha_i}{\alpha_1} (\frac{\lambda_i}{\lambda_1})^t u_i \\
&amp;amp;\to \alpha_1 \lambda_1^t u_1
\end{aligned}
\]&lt;/span&gt; Thus &lt;span class=&#34;math display&#34;&gt;\[
||A^TA \frac{v_t}{||v_t||}|| = ||A^TAu_1|| = ||\lambda_1 u_1|| = \lambda_1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://math.stackexchange.com/questions/2723629/why-is-the-maximum-rayleigh-quotient-equal-to-the-maximum-eigenvalue&#34;&gt;matrices - Why is the maximum Rayleigh quotient equal to the maximum eigenvalue? - Mathematics Stack Exchange&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Probability Estimation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/probability-estimation/</link>
      <pubDate>Sat, 22 Jan 2022 21:36:30 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/probability-estimation/</guid>
      <description>

&lt;h2 id=&#34;probability-estimation&#34;&gt;Probability Estimation&lt;/h2&gt;
&lt;h3 id=&#34;probability-function-estimation&#34;&gt;Probability Function Estimation&lt;/h3&gt;
&lt;h4 id=&#34;monte-carlo-method&#34;&gt;Monte Carlo Method&lt;/h4&gt;
&lt;h3 id=&#34;probability-density-function-estimation&#34;&gt;Probability Density Function Estimation&lt;/h3&gt;
&lt;h4 id=&#34;histogram&#34;&gt;Histogram&lt;/h4&gt;
&lt;h4 id=&#34;parzen-window&#34;&gt;Parzen Window&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/aminor/p/13851150.html&#34;&gt;作图直观理解Parzen窗估计（附Python代码） - aminor - 博客园 (cnblogs.com)&lt;/a&gt; &lt;a href=&#34;https://stats.stackexchange.com/questions/244012/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms/244023&#34;&gt;Can you explain Parzen window (kernel) density estimation in layman’s terms? - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Convex Optimization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</link>
      <pubDate>Fri, 07 Jan 2022 12:56:57 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</guid>
      <description>

&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convex Set&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt; is called a convex set if &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, x^{(2)} \in \mathcal X\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall \alpha \in [0,1], \alpha x^{(1)} + (1 - \alpha)x^{(2)} \in \mathcal X\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convex Function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is convex if &lt;span class=&#34;math inline&#34;&gt;\(dom(f)\)&lt;/span&gt; is a convex set and &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x^{(1)} + (1 - \alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1 - \alpha)f(x^{(2)}), \forall x^{(1)}, x^{(2)} \in dom(f), \alpha \in [0,1]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is concave is &lt;span class=&#34;math inline&#34;&gt;\(-f\)&lt;/span&gt; is convex.&lt;/p&gt;
&lt;p&gt;Differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with a convex domain is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + \nabla^T f(x)(y - x), \forall x, y \in dom(f)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is twice differentiable if &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2f(x)_{ij} = \frac{\partial^2f(x)}{\partial x_ix_j}\)&lt;/span&gt; exists at each &lt;span class=&#34;math inline&#34;&gt;\(x \in dom(f)\)&lt;/span&gt; the Hessian &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2f(x) \in \mathbb S^N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
\nabla^2f(x) \succeq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any local minima of a convex function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto R\)&lt;/span&gt; is also a global minima.&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a local minima. Then there exists a number &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x \in dom(f), ||x-y||_2 \le r \Rightarrow f(x) \ge f(y)\)&lt;/span&gt;. Suppose instead there exists a global minima &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Then it must hold that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||y-z||_2 &amp;gt; r \\
0 &amp;lt; \frac{r}{||y-z||_2} &amp;lt; 1
\end{gather}
\]&lt;/span&gt; There exists some &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \epsilon &amp;lt; r\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta = 1 - \frac{r - \epsilon}{||y-z||_2} &amp;lt; 1\)&lt;/span&gt;. Then the distance from &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to point &lt;span class=&#34;math inline&#34;&gt;\((\theta y + (1 - \theta)z)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
||y - (\theta y + (1 - \theta)z)||_2 = (1 - \theta)||y-z||_2 = \frac{r - \epsilon}{||y-z||_2}||y-z||_2 &amp;lt; r
\]&lt;/span&gt; That is to say &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \ge f(y)
\]&lt;/span&gt; However, since &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta &amp;lt; 1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the global minima, therefore, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z) &amp;lt; \theta f(y) + (1 - \theta)f(y) = f(y)
\]&lt;/span&gt; which contradicts that &lt;span class=&#34;math inline&#34;&gt;\(f(\theta y + (1 - \theta)z) \ge f(y)\)&lt;/span&gt; derived. In other words, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a global minima.&lt;/p&gt;
&lt;h3 id=&#34;standard-form-of-the-convex-optimization-problem&#34;&gt;Standard Form of the Convex Optimization Problem&lt;/h3&gt;
&lt;p&gt;Standard form of the optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Standard form of the convex optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;a_i^Tx = b_i (Ax = b), i = 1,\dots,s \\
&amp;amp;\text{$f_0,\dots,f_r$ are convex, and $a_0,\dots,a_s \in \R^N$}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 12:55:32 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</guid>
      <description>

&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be decomposed into &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $M \times M$, $\Sigma$ is diagonal, $V$ is $N \times N$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{M \times N}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = min(M, N)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;This is shown by construction.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Constructing &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real symmetric matrix, so it can be diagonalized by an orthogonal matrix. Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be this matrix: &lt;span class=&#34;math display&#34;&gt;\[
V^{-1}(A^TA)V = \Lambda \text{, where $\Lambda$ is the diagonal matrix consisting of $A^TA$&amp;#39;s eigenvalues}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; consists of orthonormal basis, we have &lt;span class=&#34;math inline&#34;&gt;\(V^TV = I\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
V^{T}(A^TA)V = \Lambda
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvalues are non-negative. Let &lt;span class=&#34;math inline&#34;&gt;\((\lambda, x)\)&lt;/span&gt; be one pair of its eigen, &lt;span class=&#34;math display&#34;&gt;\[
||Ax||^2 = (Ax)^TAx = x^T(A^TA)x = x^T\lambda x = \lambda||x||^2 \ge 0
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;’s columns be permuted such that their corresponding eigenvalues are ordered in &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_N \ge 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = \sqrt{\lambda_i}, i = 1, 2, ..., N\)&lt;/span&gt; (which are defined as &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s the singular values).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^TA) = r\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the number of its positive eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
\lambda_1, \lambda_2, ..., \lambda_r \ge 0, \lambda_r, \lambda_{r+1}, ..., \lambda_{N} = 0 \\
\sigma_1, \sigma_2, ..., \sigma_r \ge 0, \sigma_r, \sigma_{r+1}, ..., \sigma_{N} = 0
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V_1 = [v_1, v_2, ..., v_r], V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\)&lt;/span&gt;. Let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_1 = 
\begin{bmatrix}
\sigma_1 \\
&amp;amp; \sigma_2 \\
&amp;amp; &amp;amp; \ddots \\
&amp;amp; &amp;amp; &amp;amp; \sigma_r
\end{bmatrix}
\]&lt;/span&gt; Complement it with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to get the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma =
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; consists of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvectors whose eigenvalues are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
A^TAV_2 = 0
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^TA) = r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(N - r\)&lt;/span&gt; independent columns. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; spans the &lt;span class=&#34;math inline&#34;&gt;\(N(A^TA) = N(A)\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1V_1^T + V_2V_2^T
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
A = AI = AV_1V_1^T + AV_2V_2^T = AV_1V_1^T
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Constructing &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
u_i = \frac{1}{\sigma_i}Av_i, i = 1, 2, ..., r
\end{equation}
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
U_1 = [u_1, u_2, ..., u_r]
\]&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(AV_1 = U_1\Sigma_1\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
u_i^Tu_j &amp;amp;= (\frac{1}{\sigma_i}Av_i)^T(\frac{1}{\sigma_j}Av_j) \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j}v_i^TA^TAv_j \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j}v_i^T\lambda_jv_j \\
&amp;amp;= \frac{\sigma_j}{\sigma_i}v_i^Tv_j \\
&amp;amp;= 
\begin{cases}
0&amp;amp; i \ne j \\
1&amp;amp; i = j
\end{cases}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(u_i, i = 1, 2, ..., r\)&lt;/span&gt; is from &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt; and they are orthogonal as shown, and &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, they form the &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Col(A)^\perp\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt;’s complement. we have &lt;span class=&#34;math inline&#34;&gt;\(Col(A)^\perp = N(A^T)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_{r+1}, u_{r+2}, ..., u_{M}\}\)&lt;/span&gt; be an orthonormal basis of &lt;span class=&#34;math inline&#34;&gt;\(N(A^T)\)&lt;/span&gt;. They will be orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s column vectors.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
U_2 = [u_{r+1}, u_{r+2}, ..., u_{M}] \\
U = [U_1, U_2]
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prove that &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T = A\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
U\Sigma V^T &amp;amp;= [U_1, U_2] 
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= [U_1\Sigma_1, 0]
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= U_1\Sigma_1V_1^T \\
&amp;amp;= AV_1V_1^T \\
&amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;p&gt;There is another view on &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
AA^TU = [AA^TU_1, AA^TU_2]
\]&lt;/span&gt; Breaking it into two parts, &lt;span class=&#34;math inline&#34;&gt;\(\forall i = 1, 2, ..., r\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
AA^Tu_i &amp;amp;= AA^T\frac{1}{\sigma_i}Av_i \\
&amp;amp;= \frac{1}{\sigma_i}A(A^TA)v_i \\
&amp;amp;= \frac{1}{\sigma_i}A\lambda_iv_i \\
&amp;amp;= \lambda_i(\frac{1}{\sigma_i}Av_i) \\
&amp;amp;= \lambda_iu_i
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;’s stacked eigenvectors, ordered by corresponding eigenvalues. &lt;span class=&#34;math display&#34;&gt;\[
AA^TU_2 = 0 = 0U_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is the corresponding eigenvectors with eigenvalues being &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In all, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the stacked orthonormal eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;. It doesn’t matter how &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is permuted.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6251584.html&#34;&gt;奇异值分解(SVD)原理与在降维中的应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/22237507&#34;&gt;奇异值的物理意义是什么？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[[Singular Value Decomposition.pdf]]&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</link>
      <pubDate>Fri, 07 Jan 2022 12:42:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</guid>
      <description>
&lt;p&gt;Linear Discriminant Analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is. Take the Gaussian distribution as an example: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(x|y=C_j) &amp;amp;= \mathcal{N}(x;\mu_j, \Sigma) \\
&amp;amp;= \frac{1}{\sqrt{(2\pi)^n|\Sigma|}}e^{-\frac{1}{2}(x-\mu_j)^T\Sigma^{-1}(x-\mu_j)}
\end{aligned}
\]&lt;/span&gt; Then the classification function will be &lt;span class=&#34;math inline&#34;&gt;\(f_{LDA} = \arg \max\limits_{j}p(x|y=C_j)p(y=C_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;LDA’s parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\mu, \Sigma, \varphi)\)&lt;/span&gt; is also learned with Maximum Likelihood Estimation: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\theta) &amp;amp;= \prod_{i=1}^mp(x^{(i)}, y^{(i)};\theta) \\
&amp;amp;= \prod_{i=1}^mp(x^{(i)}|y^{(i)};\theta)p(y^{(i)})
\end{aligned}
\]&lt;/span&gt; The log-likelihood function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log L(\theta) \\
&amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) + \sum_{i=1}^m\log p(y^{(i)}, \varphi) \\
\end{aligned}
\]&lt;/span&gt; We can maximize above two summations separately.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_1(\mu,\Sigma) &amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) \\
&amp;amp;= -\frac{1}{2}\sum_{i=1}^m(x^{(i)}-\mu_{j|y^{(i)}})^T\Sigma^{-1}(x^{(i)}-\mu_{j|y^{(i)}}) + \log|\Sigma| + n\log2\pi\\
\end{aligned}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \mu_j} = \sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) \\
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) &amp;amp;= 0 \\
\Sigma^{-1}\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) &amp;amp;= 0 \\
\end{aligned}
\]&lt;/span&gt; Since the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is real-symmetric and invertible and thus its nullspace is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) = 0 \\
\mu_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \Sigma} = -\frac{1}{2}\sum_{i=1}^m(\Sigma^{-1} -\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) \\
\sum_{i=1}^m(\Sigma^{-1} -\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) = 0 \\
\Sigma^{-1}\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^mI = \Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\sum_{i=1}^m\Sigma =  \Sigma\Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_2(\varphi) &amp;amp;= \sum_{i=1}^m\log p(y^{(i)}; \varphi) \\
&amp;amp;= \sum_{i=1}^m\log\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} \\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(p(y^{(i)};\varphi) = \prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} = \sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\varphi_j\)&lt;/span&gt;. We chose the product notation because it could be easily “logged”.&lt;/p&gt;
&lt;p&gt;There is also a constraint in maximization of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;. We can form the corresponding Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
J(\varphi, \lambda) = \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j + \lambda(-1 + \sum_{j=1}^c\varphi_j)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\varphi_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\varphi_j} + \lambda = 0 \\
\varphi_j = -\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda}
\end{gather}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-\frac{\sum_{j=1}^c\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda} = 1 \\
\lambda = -M
\end{gather}
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
\varphi_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{M}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Matrix Identity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</link>
      <pubDate>Wed, 22 Dec 2021 15:51:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</guid>
      <description>
&lt;p&gt;A useful matrix identity &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} &amp;amp;= PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1} \\
\end{aligned}
\]&lt;/span&gt; Its reduced form &lt;span class=&#34;math display&#34;&gt;\[
(I_N+AB)^{-1}A = A(I_M+BA)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(I_N+AB)^{-1}A &amp;amp;= A(I_M+BA)^{-1} \\
\iff A &amp;amp;= (I_N + AB)A(I_M + BA)^{-1} \\
\iff A &amp;amp;= (A + ABA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A(I_M + BA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Frobenius Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/frobenius-normalization/</link>
      <pubDate>Mon, 20 Dec 2021 15:43:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/frobenius-normalization/</guid>
      <description>
&lt;p&gt;Frobenius Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
||A||_F = \sqrt{\sum_{ij}A_{ij}^2}
\]&lt;/span&gt; It can be found that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||A||_F^2 &amp;amp;= \sum_{ij}A_{ij}^2 \\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^nA_{ij}A_{ij} = \sum_{i=1}^n\sum_{j=1}^mA_{ji}A_{ji} \\
&amp;amp;= \sum_{i=1}^m(\sum_{j=1}^nA_{ij}A_{ji}^T) = \sum_{i=1}^n(\sum_{j=1}^mA_{ij}^TA_{ji}) \\
&amp;amp;= \sum_{i=1}^m(A_{i:}A_{:i}^T) = \sum_{i=1}^n(A_{i:}^TA_{:i})\\
&amp;amp;= \sum_{i=1}^m(AA^T)_{ii} = \sum_{i=1}^n(A^TA)_{ii}\\
&amp;amp;= tr(AA^T) = tr(A^TA)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Orthogonality and Projection</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</link>
      <pubDate>Mon, 20 Dec 2021 10:19:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</guid>
      <description>
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; are orthogonal to each other, then they are independent with each other.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; be an orthogonal basis for a subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, the weights in the linear combination &lt;span class=&#34;math display&#34;&gt;\[
y = c_1u_1 + c_2u_2 + ... + c_ku_k
\]&lt;/span&gt; are given by &lt;span class=&#34;math display&#34;&gt;\[
c_1 = \frac{y_1 \cdot u_1}{u_1 \cdot u_1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; and another vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, we wish to decompose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \alpha u\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is some vector orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
z &amp;amp;= y - \hat y \\
z \cdot u &amp;amp;= (y - \alpha u) \cdot u \\
y \cdot u - \alpha u \cdot u &amp;amp;= 0 \\
\alpha &amp;amp;= \frac{y \cdot u}{u \cdot u}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \frac{y \cdot u}{u \cdot u}u\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z = y - \hat y\)&lt;/span&gt; is called the component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(cu\)&lt;/span&gt; for any scalar &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is the projection onto the subspace &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; spanned by &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. In this sense, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{proj}_Ly\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; be a subspace of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, then each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; can be uniquely written in the form: &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W^\perp\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{proj}_Wy\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This projection can be found by finding an arbitrary orthogonal basis of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and then computing weights using equation (2).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also the closest point in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, in the sense that: &lt;span class=&#34;math display&#34;&gt;\[
||y - \hat y|| &amp;lt; ||y - v||, \forall v \in W \text{ and } v \ne \hat y
\]&lt;/span&gt; In this case, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is called the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by elements of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Above can be seen by &lt;span class=&#34;math display&#34;&gt;\[
y - v = (y - \hat y) + (\hat y - v)
\]&lt;/span&gt; which gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||y - v||^2 &amp;amp;= ||(y - \hat y) + (\hat y - v)|| \\
&amp;amp;= ||(y - \hat y)||^2 + ||(\hat y - v)||^2 + 2(y - \hat y) \cdot (\hat y - v) \\
&amp;amp;= ||(y - \hat y)||^2 + ||(\hat y - v)||^2 \text{($y - \hat y$ is in $W^\perp$, $\hat y - v$ is in $W$)} \\
&amp;amp;&amp;gt; ||(y - \hat y)||^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; has orthonormal columns if and only if &lt;span class=&#34;math inline&#34;&gt;\(U^TU = I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An orthogonal matrix is a square invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(U^{-1} = U^T\)&lt;/span&gt;. By its definition, it has orthonormal columns and orthonormal rows.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate System and Change of Basis</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</link>
      <pubDate>Sat, 18 Dec 2021 20:47:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</guid>
      <description>
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \{b_1, b_2, ..., b_n\}\)&lt;/span&gt; be a basis for a vector space &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Then for &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1, x_2, ..., x_n]^T\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, there exists a unique set of scalars &lt;span class=&#34;math inline&#34;&gt;\(q_1, q_2, ..., p_n\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
x = q_1b_1 + q_2b_2 + ... + q_nb_n
\]&lt;/span&gt; These scalars are called the coordinates of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{B} = 
\begin{bmatrix}
q_1 \\
\vdots \\
q_n
\end{bmatrix}
\]&lt;/span&gt; is the coordinate vectors of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. The mapping &lt;span class=&#34;math inline&#34;&gt;\(x \mapsto [x]_\mathcal{B}\)&lt;/span&gt; is called the coordinate mapping determined by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{B} = [b_1, b_2, ..., b_n]\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x = P_\mathcal{B}[x]_\mathcal{B}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt; both be an basis for an n-dimensional vector space &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then there is a unique &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow B}\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{C} = \mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B}
\]&lt;/span&gt; The columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow B}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;-coordinate vectors of the vectors in the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\mathop{P}\limits_\mathcal{C \leftarrow B} = [[b_1]_\mathcal{C}, [b_2]_\mathcal{C}, ..., [b_n]_\mathcal{C}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} &amp;amp;=  P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow B}
\begin{bmatrix}
q_1 \\
\vdots \\
q_n
\end{bmatrix} \\
&amp;amp;= P_\mathcal{C}(q_1[b_1]_\mathcal{C} + q_2[b_2]_\mathcal{C} + ... + q_n[b_n]_\mathcal{C}) \\
&amp;amp;= q_1P_\mathcal{C}[b_1]_\mathcal{C} + q_2P_\mathcal{C}[b_2]_\mathcal{C} + ... + q_nP_\mathcal{C}[b_n]_\mathcal{C} \\
&amp;amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\
&amp;amp;= x
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \mathcal{I}\)&lt;/span&gt; is the standard basis, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= [[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= P_\mathcal{C}[[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
&amp;amp;= [e_1, e_2, ..., e_n] \\
&amp;amp;= I
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{C}\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow I} = P_\mathcal{C}^{-1}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\([x]_\mathcal{B} = P_\mathcal{B}^{-1}x\)&lt;/span&gt; in equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \\
[x]_\mathcal{C} &amp;amp;= \mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
[x]_\mathcal{B} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C}  \\
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{B \leftarrow C} = (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}, \mathop{P}\limits_\mathcal{B \leftarrow C}\mathop{P}\limits_\mathcal{C \leftarrow B} = I\)&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Chebyshev Distance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/chebyshev-distance/</link>
      <pubDate>Sat, 18 Dec 2021 20:28:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measures/chebyshev-distance/</guid>
      <description>

&lt;h3 id=&#34;discrete-form&#34;&gt;Discrete form&lt;/h3&gt;
&lt;p&gt;Chebyshev distance is a specific form of Minkowski norm (&lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norm): &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= ||x - x^\prime||_p \\
&amp;amp;= (\sum_{i=1}^n|x_i - x^\prime_i|^p)^{1/p} \text{, where $p \to \infty$ }
\end{aligned}
\]&lt;/span&gt; Chebyshev distance is in fact &lt;span class=&#34;math inline&#34;&gt;\(\mathop{max}\limits_i (|x_i - x^\prime_i|)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = |x_i - x^\prime_i|\)&lt;/span&gt; and without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(a_1 = \max\limits_ia_i = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^n|x_i - x^\prime_i|^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^na_i^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(a_1^p\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
&amp;amp;= \lim_{p \to \infty}(a_1^p)^{1/p} \cdot \lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
&amp;amp;= a_1 \cdot \lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\forall i, a_1 &amp;gt; a_i\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\frac{a_i}{a_1} \le 1\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 \le &amp;amp;\sum_{i=1}^n(\frac{a_i}{a_1})^p \le n \\
1^{1/p} \le &amp;amp;(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le n^{1/p} \text{, where $p &amp;gt; 1$} \\
\lim_{p \to \infty}1^{1/p} \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le \lim_{p \to \infty}n^{1/p} \\
1 \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le 1 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(d_p(x, x^\prime) = a_1 \cdot 1 = a_1 = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;continuous-form&#34;&gt;Continuous form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; be continuous and bounded on interval &lt;span class=&#34;math inline&#34;&gt;\((a, b)\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = \sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;proof-1&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sup\limits_{x \in (a,b)}|f(x)|\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall\varepsilon &amp;gt; 0, \exists x_0 \in (a,b), |f(x_0)| &amp;gt; S - \varepsilon\)&lt;/span&gt;. By “continuous”, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{x \to x_0}|f(x)| &amp;gt; S - \varepsilon\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\exists\delta &amp;gt; 0, \forall x \in U(x_0, \delta), ||f(x)| - \lim\limits_{x \to x_0}|f(x)|| &amp;lt; \varepsilon \rightarrow |f(x)| &amp;gt; S - 2\varepsilon\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
&amp;amp;= \lim\limits_{p \to +\infty}(2\delta(S - 2\varepsilon)^p)^{1/p} \\
&amp;amp;= S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(U(x_0, \delta)\)&lt;/span&gt; in within the interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|f(x)|^p \ge 0\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} \\
&amp;amp;\ge S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is arbitrarily and positively valued, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \ge S\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \le \lim\limits_{p \to +\infty}(\int_a^b S^pdx)^{1/p} = \lim\limits_{p \to +\infty}((b - a)S^p)^{1/p} = S
\]&lt;/span&gt; then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = S = \sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/267575473&#34;&gt;p范数的极限（无穷范数）为什么是极大值范数？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</link>
      <pubDate>Fri, 17 Jun 2022 20:32:59 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</guid>
      <description>

&lt;h2 id=&#34;two-class-logistic-regression&#34;&gt;Two-class Logistic Regression&lt;/h2&gt;
&lt;p&gt;A linear classifier is a classifier with a linear separation hyperplane. That is to say, suppose the feature space is &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt;, then we will process the feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with a feature extractor function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = w^Tx + b\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R\)&lt;/span&gt;, and then operate on &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, without directly depending on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; anymore.&lt;/p&gt;
&lt;p&gt;Logistic Regression is a binary linear classifier, which takes a probabilistic approach. It maps the &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to a probability value between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; by applying a sigmoid function &lt;span class=&#34;math inline&#34;&gt;\(\sigma(z) = \frac{1}{1 + e^{-z}}\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(y = +1|x) &amp;amp;= \sigma(f(x)) \\
p(y = -1|x) &amp;amp;= 1 - \sigma(f(x)) = \sigma(-f(x)) \\
\end{aligned}
\]&lt;/span&gt; Or equivalently, &lt;span class=&#34;math inline&#34;&gt;\(p(y|x) = \sigma(yf(x))\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f_{LR}(x) = \arg \max\limits_{y \in \{-1, 1\}} \sigma(y(f(x)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Logistic Regression is a discriminative classifier because we are directly modelling &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;, with no intermediate step on &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = {x^{(i)}, y^{(i)}, i=1, \dots, M}\)&lt;/span&gt;, Logistic Regression is learning by maximizing the &lt;strong&gt;conditional&lt;/strong&gt; log-likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
l(w, b) = \log L(w, b) = \log \prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) = \sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(w^\star, b^\star) &amp;amp;= \arg \max\limits_{w, b}l(w, b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log \frac{1}{1 + e^{-y^{(i)}(w^Tx^{(i)}+b)}} \\
&amp;amp;= \arg \max\limits_{w, b}--\sum_{i=1}^M\log \frac{1}{1 + e^{-y^{(i)}(w^Tx^{(i)}+b)}} \\
&amp;amp;= \arg \min\limits_{w, b}-\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b)) \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(J(w, b) = -\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b))\)&lt;/span&gt; be the target function. There is no closed-form solution to this optimization problem. Rather, it is to be solved by iterative algorithm, e.g. gradient descent. For each iteration, parameters are updated by &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
w \leftarrow w - \eta\frac{\partial J}{\partial w} \\
b \leftarrow b - \eta\frac{\partial J}{\partial b}
\end{gather}
\]&lt;/span&gt; Specifically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial w} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))} \frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))} \frac{\partial (y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))} \sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial b} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))} \frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))} \frac{\partial (y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))} \sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The posterior obtained by binary [[Linear Discriminant Analysis]] has the form of &lt;span class=&#34;math display&#34;&gt;\[
p(y|x) = \frac{1}{1 + e^{-(wx + b)}}
\]&lt;/span&gt; This is not to say LDA and LR are the same in binary case. Note that LDA is a generative model which learns &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;, LR is a discriminative model which only learns &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;. One can discriminate with a generative model, but not reversely.&lt;/p&gt;
&lt;h2 id=&#34;multi-class-logistic-regression&#34;&gt;Multi-class Logistic Regression&lt;/h2&gt;
&lt;p&gt;One way to train use Logistic Regression in multi-class classification in to for each class &lt;span class=&#34;math inline&#34;&gt;\(i = {1, \dots, C}\)&lt;/span&gt;, assign a weight vector &lt;span class=&#34;math inline&#34;&gt;\(w^{(i)}\)&lt;/span&gt;, the probability is define by the softmax function: &lt;span class=&#34;math display&#34;&gt;\[
p(y = c|x) =\frac{exp(w^{(c)}\cdot x + b^{(c)})}{\sum_{i=1}^Cexp(w^{(i)} \cdot x + b^{(i)})}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(f_{LR_{multi-class}}(x) = \arg \max\limits_{y \in \{1,\dots,C\}}p(y|x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To prevent overfitting, a prior distribution is usually added to &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose each dimension of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is independently sampled from a Gaussian distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0, \frac{C}{2})\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
p(w) \propto \exp(-\frac{1}{C}w^Tw)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Laplace Expansion</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/</link>
      <pubDate>Fri, 17 Jun 2022 11:45:52 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/</guid>
      <description>
&lt;p&gt;Also known as Cofactor Expansion, Laplace Expansion is an expression of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix as the weighted sum of determinants of some &lt;span class=&#34;math inline&#34;&gt;\((n-1) \times (n-1)\)&lt;/span&gt; sub-matrices.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bounding Mutual Information</title>
      <link>https://chunxy.github.io/notes/papers/bounding-mutual-information/</link>
      <pubDate>Thu, 02 Jun 2022 14:01:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/bounding-mutual-information/</guid>
      <description>

&lt;h2 id=&#34;i_textba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;The very basic bound on the Mutual Information is based on the non-negativity of KL-divergence. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) = &amp;amp;\E_{p(x,y)} \log \frac{p(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} + \\
&amp;amp;\underbrace{\E_{p(x,y)} \log \frac{p(x|y)}{q(x|y)}}_{\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\
\ge &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log q(x|y) + H(X) \triangleq I_\text{BA}
\end{aligned}
\]&lt;/span&gt; This bound is not usually tractable since &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H(X)\)&lt;/span&gt; has no closed-form expression.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) = p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;i_textuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; is replaced with an unnormalized model, that is &lt;span class=&#34;math display&#34;&gt;\[
q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)}, \text{where } Z(y) = \E_{p(x)} e^{f(x,y)}
\]&lt;/span&gt; Substituting this back to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} \log \frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \triangleq I_\text{UBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that by scaling &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; term is cancelled.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{f(x,y)} &amp;amp;= \frac{p(x, y)}{p(x)p(y)} Z(y) \\
e^{f(x,y)} &amp;amp;= \frac{Z(y)}{p(y)} p(y|x) \\
f(x,y) &amp;amp;= \ln p(y|x) + \underbrace{\ln \frac{Z(y)}{p(y)}}_{c(y)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textdv&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By further applying Jensen’s inequality to the &lt;span class=&#34;math inline&#34;&gt;\(\E_{p(y)} Z(y)\)&lt;/span&gt; term in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log\E_{p(y)} [Z(y)] \triangleq I_\text{DV}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_texttuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt; function also has the following property: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\forall x,a&amp;gt;0,\log(x) &amp;amp;\le \frac{x}{a} + \log(a) - 1 &amp;amp;\iff \\
a + a\log(x) &amp;amp;\le x + a\log(a) &amp;amp;\iff \\
a\log(x) - x &amp;amp;\le a\log(a) - a \\
\end{aligned}
\]&lt;/span&gt; Therefore, we can insert the inequality &lt;span class=&#34;math inline&#34;&gt;\(\log Z(y) \le \frac{Z(y)}{a(y)} + \log a(y) - 1\)&lt;/span&gt; into the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; to get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) - 1 \big)] \triangleq I_\text{TUBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This bound is tight when&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(f(x,y) = \log p(y|x) + \underbrace{\log \frac{Z(y)}{p(y)}}_{c(y)}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(a(y) = Z(y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;i_textnwj&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By setting &lt;span class=&#34;math inline&#34;&gt;\(a(y) = e\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) - 1 \big)] \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - e^{-1}\E_{p(y)} Z(y)\triangleq I_\text{NWJ}
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt; is a special case of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, its bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(Z(y)\)&lt;/span&gt; self-normalizes to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. In this case &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x,y) &amp;amp;= \log p(y|x) + \log \frac{e}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(x|y)}{p(x)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textnce&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate the &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; given one sample from &lt;span class=&#34;math inline&#34;&gt;\(p(x_1)p(y|x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; additional independent samples &lt;span class=&#34;math inline&#34;&gt;\(x_{2:K} \sim p^{K-1}(x_{2:K})\)&lt;/span&gt;. For any random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is independent from &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_1,Z;Y) &amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\
&amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\
&amp;amp;= I(X_1;Y)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y) = I(X_1,X_{2:K};Y)\)&lt;/span&gt;. Bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; becomes bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1,X_{2:K};Y)\)&lt;/span&gt;, which can be estimated using any of the preceding methods.&lt;/p&gt;
&lt;p&gt;Set the critic to &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + \overbrace{\log \frac{e^{g(x,y)}} {a(y;x_{1:K})}}^{h(x_{1:K},y)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sample among &lt;span class=&#34;math inline&#34;&gt;\(x_{1:K}\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Usually &lt;span class=&#34;math inline&#34;&gt;\((x,y)_{1:K}\)&lt;/span&gt; are sampled from the same marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(x,y)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is then uniformly drawn among &lt;span class=&#34;math inline&#34;&gt;\(y_{1:K}\)&lt;/span&gt;. Substitute these to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\label{infonce} \begin{aligned} 
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\E_{p(y)} Z(y) \\
&amp;amp;= \E_{p(x_{1:K},y)} [1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [\E_{p(x_{1:K})} e^{1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [e\E_{p(x_{1:K})} \frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} \\
\end{aligned}
\]&lt;/span&gt; Further set &lt;span class=&#34;math inline&#34;&gt;\(a(y;x_{1:K}) = \frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}\)&lt;/span&gt;. Substitute this into the last term in equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{infonce}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} &amp;amp;= \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}}{\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} } \\
&amp;amp;\stackrel{P}{\to} \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}} {\E_{p(x_{1:K})} e^{g(x,y)} } \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} }] \\
&amp;amp;\approx \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log \frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)} }] \triangleq I_\text{NCE}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt; is tightly bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log K\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_{NCE} &amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log \frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)}} ] \\
&amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K (\log \frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} )] + \log K \\
&amp;amp;\le \E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{\sum_{i=1}^K e^{g(x_i, y_j)}} {e^{g(x_j,y_j)}} ]} + \log K
\end{aligned}
\]&lt;/span&gt; The equality is reached when &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + {h(x_{1:K},y)} = 1 + \log \frac{p(x|y)}{p(x)}\)&lt;/span&gt; as in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;. In this case, it can be derived that &lt;span class=&#34;math inline&#34;&gt;\(g(x,y) = g^\star(x,y) = \frac{p(y|x)}{p(y)}\)&lt;/span&gt;. And then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_{NCE} &amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{\frac{p(y_j|x_j)}{p(y_j)} + \sum_{i=1,i \ne j}^K \frac{p(y_j|x_i)}{p(y_j)}} {\frac{p(y_j|x_j)}{p(y_j)}} ]} + \log K \\
&amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K (1 + \frac{p(y_j)}{p(y_j|x_j)} \sum_{i=1,i \ne j}^K \frac{p(y_j|x_i)}{p(y_j)}) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(1 + \frac{p(y_j)}{p(y_j|x_j)} (K-1) \E_{p(y)} \frac{p(y|x_i)}{p(y)} \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[1 + \frac{1}{K} \sum_{j=1}^K \big( \frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big( \frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{p(y_j)}{p(y_j|x_j)} ]} - \log(K-1) + \log K \\
&amp;amp;= I(X_1;Y) - \log(K-1) + \log K
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Cross Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:46 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</guid>
      <description>
&lt;p&gt;The Cross Entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, instead of the true distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Cross Entropy of distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; relative to a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) = -\mathrm{E}_{x \sim p} \log q(x)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Function of Random Variable</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/function-of-random-variable/</link>
      <pubDate>Thu, 31 Mar 2022 10:08:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/function-of-random-variable/</guid>
      <description>

&lt;p&gt;Suppose we have a 1-D random variable &lt;span class=&#34;math inline&#34;&gt;\(X \sim p_X\)&lt;/span&gt; and a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, what will &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;’s distribution and expectation be like?&lt;/p&gt;
&lt;h3 id=&#34;distribution&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y = f(X)\)&lt;/span&gt; and suppose &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is monotonic, then &lt;span class=&#34;math display&#34;&gt;\[
P_Y(y) = P_Y(Y \le y) = P_X(f(X) \le y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is monotonically increasing: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_X(f(X) \le y) = P_X(X \le f^{-1}(y)) = P_X(f^{-1}(y)) \\ 
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial P_X(f^{-1}(y))}{\partial y} \\
&amp;amp;= P_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= P_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is monotonically decreasing: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_X(f(X) \le y) = P_X(X \ge f^{-1}(y)) = 1 - P_X(X \le f^{-1}(y)) = 1 - P_X(f^{-1}(y)) \\ 
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial [1 - P_X(f^{-1}(y))]}{\partial y} \\
&amp;amp;= -p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all, &lt;span class=&#34;math inline&#34;&gt;\(p_Y(y) = p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)|\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</link>
      <pubDate>Tue, 11 Jan 2022 20:26:40 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</guid>
      <description>

&lt;p&gt;If a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will converge by gradient descent.&lt;/p&gt;
&lt;h3 id=&#34;l-smoothness&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness&lt;/h3&gt;
&lt;p&gt;Qualitatively, smoothness means that the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; changes in a controlled, bounded manner. Quantitatively, smoothness assumes that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has a Lipschitz gradient. That means there exists an &lt;span class=&#34;math inline&#34;&gt;\(L &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
||\nabla f(x) - \nabla f(y)||_2 \le L||x - y||_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; We will say that such function is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth or strongly smooth. &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Proof&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\frac{L}{2}||x||^2_2 - f(x)\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With the smoothness, or the Lipschitz gradient, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from above.&lt;/p&gt;
&lt;p&gt;We will assume &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness in all cases below. Meanwhile, the local minima (in convex case, the global minima) is denoted as &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-convexity&#34;&gt;Convergence with Convexity&lt;/h3&gt;
&lt;p&gt;With convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with linear approximation: &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;Now consider a &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We adopt a fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt; so that the update in each iteration will be &lt;span class=&#34;math display&#34;&gt;\[
x_{k+1} = x_k - \frac{1}{L}\nabla f(x)
\]&lt;/span&gt; By the definition of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness, &lt;span class=&#34;math display&#34;&gt;\[
\label{diff} \begin{aligned} 
 f(x_{k+1}) &amp;amp;\le f(x_k) + (-\frac{1}{L}\nabla f(x_k))^T \nabla f(x_k) + \frac{L}{2}||-\frac{1}{L}\nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_k)\)&lt;/span&gt; holds in each iteration.&lt;/p&gt;
&lt;p&gt;By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
f(x^\star) \ge f(x_k) + (x^\star - x_k)^T \nabla f(x_k) \\
\label{convexity} f(x_k) \le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k)
\end{gather}
\]&lt;/span&gt; Substituting the result back to the right-hand side of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k) - \frac{1}{2M}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; (x_k - x^\star)^T L(x_k - x_{k+1}) - \frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; \frac{L}{2}(2(x_k - x^\star)^T(x_k - x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\
\end{aligned}
\]&lt;/span&gt; By using the fact that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\
2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2
\end{aligned}
\]&lt;/span&gt; We have &lt;span class=&#34;math display&#34;&gt;\[
\label{closer} f(x_{k+1}) - f(x^\star) \le \frac{L}{2}(||x_k - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This indicates that &lt;span class=&#34;math inline&#34;&gt;\(x_{k+1}\)&lt;/span&gt; is closer to the global minima &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k [f(x_{i+1}) - f(x^\star)] &amp;amp;\le \sum_{i=0}^k \frac{L}{2}(||x_i - x^\star||^2_2 - ||x_{i+1} - x^\star||^2_2) \\
&amp;amp;= \frac{L}{2}(||x_0 - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2) \\
&amp;amp;\le \frac{L}{2}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(f(x_{i+1}) \le f(x_i), \forall i=0,\dots,k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_i), \forall i=0,\dots,k\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k(f(x_{k+1}) - f(x^\star)) \le \frac{L}{2}||x_0 - x^\star||^2_2 \\
f(x_{k+1}) - f(x^\star) \le \frac{L}{2k}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;variant-step-size&#34;&gt;Variant Step Size&lt;/h5&gt;
&lt;p&gt;In real application scenario, it may be difficult to know &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; exactly. However, as long as we choose &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k \le \frac{1}{L}\)&lt;/span&gt; in each iteration, the convergence and the rate still holds. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\alpha_k \nabla f(x_k))^T \nabla f(x_k) + \frac{L}{2}||-\alpha_k \nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} = f(x_{k+1}) - f(x^\star) \ge 0\)&lt;/span&gt;, the above becomes &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} \le \eta_{k} - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2\)&lt;/span&gt;. From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{convexity}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\eta_k = f(x_k) - f(x^\star) \le (x_k - x^\star)^T \nabla f(x_k) \le ||x_k - x^\star||_2||\nabla f(x_k)||_2 \\
0 \le \frac{\eta_k}{||x_k - x^\star||_2} \le ||\nabla f(x_k)||_2 \\
-||\nabla f(x_k)||^2_2 \le -\frac{\eta^2_k}{||x_k - x^\star||^2_2} \\
\end{gather}
\]&lt;/span&gt; Substituting it back to give &lt;span class=&#34;math display&#34;&gt;\[
\eta_{k+1} \le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k) \eta^2_k}{||x_k - x^\star||^2_2}
\]&lt;/span&gt; From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{closer}\)&lt;/span&gt; we already have &lt;span class=&#34;math display&#34;&gt;\[
||x_{k+1} - x^\star||^2_2 \le ||x_k - x^\star||^2_2 \le \dots \le ||x_0 - x^\star||^2_2
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\eta_{k+1} &amp;amp;\le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k) \eta^2_k}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_k} &amp;amp;\le \frac{1}{\eta_{k+1}} - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot \frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot \frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \text{, by } \eta_k \ge \eta_{k+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k (\frac{1}{\eta_{i+1}} - \frac{1}{\eta_i}) &amp;amp;\ge \sum_{i=0}^k \frac{(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_0} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
f(x_{k+1} - f(x^\star)) &amp;amp;\le \frac{||x_0 - x^\star||^2_2}{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = \infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha^2_i &amp;lt; \infty\)&lt;/span&gt;, the Variant Step Size can be shown to converge. Particularly when &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \frac{1}{k}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
f(x_{k+1} - f(x^\star)) \le \Theta(\frac{1}{\log k})||x_0 - x^\star||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-strong-convexity&#34;&gt;Convergence with Strong Convexity&lt;/h3&gt;
&lt;h4 id=&#34;strong-convexity&#34;&gt;Strong Convexity&lt;/h4&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly convex if there exists a &lt;span class=&#34;math inline&#34;&gt;\(l &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Further, strong convexity is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(f(x) - \frac{l}{2}||x||^2_2\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With strong convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with a convex quadratic approximation.&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate-1&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size-1&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth above. That is &lt;span class=&#34;math display&#34;&gt;\[
\label{sm} f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; In this section we further assume that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth as well as strong convex: &lt;span class=&#34;math display&#34;&gt;\[
\label{sc} f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(F(y) = f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2\)&lt;/span&gt;. Take derivative of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla f(x) + l(y - x) = 0 \\
y_0 - x = -\frac{\nabla f(x)}{l}
\end{aligned}
\]&lt;/span&gt; Because the second derivative of &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(l \succeq 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; reaches global minimum at the local minima &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt;. Substitute &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; back to &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge F(y) \ge F(y_0) = f(x) - \frac{1}{2l}||\nabla f(x)||^2_2
\]&lt;/span&gt; This holds when &lt;span class=&#34;math inline&#34;&gt;\(y = x^\star\)&lt;/span&gt;. That is, &lt;span class=&#34;math display&#34;&gt;\[
f(x^\star) \ge f(x) - \frac{1}{2l}||\nabla f(x)||^2_2 \\
||\nabla f(x)||^2_2 \ge 2l(f(x) - f(x^\star))
\]&lt;/span&gt; The above inequality is referred to as Polyak-Lojasiewicz Inequality. By combining it with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, where we implicitly assume a fixed step-size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) + \frac{1}{2L}(-||\nabla f(x_k)||^2_2) \\ 
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) + \frac{1}{2L}(-2l(f(x) - f(x^\star))) \\
&amp;amp;= (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le (1 - \frac{l}{L}) (f(x_k) - f(x^\star)) \\
&amp;amp;\le (1 - \frac{l}{L})(1 - \frac{l}{L}) (f(x_{k-1}) - f(x^\star)) \\
&amp;amp;\le \dots \\
&amp;amp;\le (1 - \frac{l}{L})^{k+1} (f(x_0) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sm}\)&lt;/span&gt; and equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sc}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2 \le f&amp;amp;(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2 \\
l &amp;amp;\le L \\
\end{aligned}
\]&lt;/span&gt; Usually it would be the case that &lt;span class=&#34;math inline&#34;&gt;\(l &amp;lt; L\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; 1 - \frac{l}{L} &amp;lt; 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;non-convex-case&#34;&gt;Non-convex Case&lt;/h3&gt;
&lt;p&gt;Without convexity condition, we can still exploit the property of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness. Sum up on both sides of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(k = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} 
\sum_{i=0}^k f(x_{k+1}) &amp;amp;\le \sum_{i=0}^k f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k f(x_{k+1}) - f(x_k) &amp;amp;\le -\frac{1}{2L} \sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is a local minima, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) \le f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
f(x^\star) - f(x_0) &amp;amp;\le - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k ||\nabla f(x_k)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
\end{aligned}
\]&lt;/span&gt; In this case, the error is measured by the first derivative. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\tilde x = \arg \min_i ||\nabla f(x_i)||^2_2\)&lt;/span&gt;, where the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the closest to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k||\nabla f(\tilde x)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
||\nabla f(\tilde x)||^2_2 &amp;amp;\le \frac{2L(f(x_0) - f(x^\star))}{k} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[Convergence of Gradient Descent.pdf]]&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rkganti.wordpress.com/2015/08/21/convergence-rate-of-gradient-descent-algorithm/&#34;&gt;Convergence rate of gradient descent algorithm | bps/Hz (wordpress.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/strong-convexity&#34;&gt;Strong convexity · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mutual Information</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</link>
      <pubDate>Thu, 19 May 2022 12:20:04 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</guid>
      <description>

&lt;p&gt;Mutual Information of &lt;strong&gt;two random variables&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]&lt;/span&gt; By its definition, &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = I(Y;X)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \times p_Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;To better illustrate the formula of mutual information between two Gaussian-distributed random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. We can concatenate them to form, say an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, which is also Gaussian-distributed. Then the mutual information between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be computed as: &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\]&lt;/span&gt; The key to the derivation is that mutual information is the KL-divergence between the joint distribution and the product of the marginal distributions.&lt;/p&gt;
&lt;p&gt;The joint can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X:Y} = N(\underbrace{\mu_X:\mu_Y}_\mu,
\underbrace{
\begin{bmatrix} \
\Sigma_{X} &amp;amp; \Cov_{XY} \\
\Cov_{YX} &amp;amp; \Sigma_{Y} \\
\end{bmatrix}
}_\Sigma
)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The product of marginals can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X} \times p_{Y} = N(\mu_x:\mu_y,
\begin{bmatrix} \
\Sigma_{xx} &amp;amp; 0 \\
0 &amp;amp; \Sigma_{yy} \\
\end{bmatrix}
)
\]&lt;/span&gt; The probability density function of an &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;-dimensional Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(p(x&amp;#39;) = \frac{1}{\sqrt{|2\pi \Sigma&amp;#39;|}}e^{-\frac{1}{2}(x&amp;#39;-\mu&amp;#39;)^T\Sigma&amp;#39;^{-1}(x&amp;#39;-\mu&amp;#39;)}\)&lt;/span&gt;. The entropy of this Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 n&amp;#39; + \frac 1 2 \log |2\pi\Sigma&amp;#39;|\)&lt;/span&gt;. In view of above, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \times p_Y)
= \int p_{X:Y}(\underbrace{x:y}_z) \log \frac{p_{X:Y}(x:y)} {p_X(x) p_{Y}(y)} \d z \\
&amp;amp;= \int p_{X:Y}(\underbrace{x:y}_z) \log p_{X:Y}(x:y) \d z -
    \int p_{X:Y}(\underbrace{x:y}_z) \log p_X(x) \d z \\
&amp;amp;\quad\quad\quad -\int p_{X:Y}(\underbrace{x:y}_z) \log p_Y(y) \d z \\
&amp;amp;= \int p_{Z}(z) \log p_{Z}(z) \d z -
    \int p_{X}(x) \log p_X(x) \d x - 
    \int p_{Y}(y) \log p_Y(y) \d y \\
&amp;amp;= -(\log \sqrt{\det(2\pi \Sigma)} + \frac n 2) +
    (\log \sqrt{\det(2\pi \Sigma_{X}}) + \frac {n_X} 2) \\
&amp;amp;\quad\quad\quad +(\log \sqrt{\det(2\pi \Sigma_{Y}}) + \frac {n_Y} 2) \\    
&amp;amp;= \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/438607/mutual-information-between-subsets-of-variables-in-the-multivariate-normal-distr&#34;&gt;Mutual information between subsets of variables in the multivariate normal distribution - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.nyu.edu/~kleeman/infolect7.pdf&#34;&gt;Information Theory and Predictability Lecture 7: Gaussian Case&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Schmit-Gram Orthogonalization</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/schmit-gram-orthogonalization/</link>
      <pubDate>Fri, 01 Apr 2022 17:08:51 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/schmit-gram-orthogonalization/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</link>
      <pubDate>Fri, 07 Jan 2022 12:52:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</guid>
      <description>

&lt;p&gt;Support Vector Machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary. These points are called &lt;strong&gt;support vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hard-margin-svm&#34;&gt;Hard Margin SVM&lt;/h3&gt;
&lt;p&gt;Suppose the data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}), i=1, \dots, M\}\)&lt;/span&gt;, and is linearly separable. The separation hyperplane will be in the form of &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is above the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;gt; 0\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = -1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is below the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;lt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The distance (margin) from a data point to the separation hyperplane will be &lt;span class=&#34;math display&#34;&gt;\[
d^{(i)} = \frac{|w^Tx^{(i)} + b|}{||w||_2} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||_2}\\
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d^{(i)}\)&lt;/span&gt; is called the geometric distance, while &lt;span class=&#34;math inline&#34;&gt;\(|w^Tx^{(i)} + b|\)&lt;/span&gt; is called the functional distance.&lt;/p&gt;
&lt;p&gt;The margin of the hyperplane &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[
d = \min\limits_{i \in \{1,\dots,M\}} d^{(i)} = \min\limits_{i \in \{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; The maximum margin solution is found by solving &lt;span class=&#34;math display&#34;&gt;\[
\max\limits_{w,b} \min\limits_{i \in \{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; Suppose &lt;span class=&#34;math inline&#34;&gt;\((w^\prime, b^\prime)\)&lt;/span&gt; is one solution to the above. Then &lt;span class=&#34;math inline&#34;&gt;\((\lambda w^\prime, \lambda b^\prime)\)&lt;/span&gt; is also a solution, because &lt;span class=&#34;math display&#34;&gt;\[
\frac{y^{(i)}(\lambda (w^{\prime})^Tx^{(i)} + \lambda b^\prime)}{||\lambda w^\prime||_2} = \frac{\lambda y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{\lambda ||w^\prime||_2} = \frac{y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{||w^\prime||_2}
\]&lt;/span&gt; There is an extra degree of freedom in this problem. Therefore, we may impose &lt;span class=&#34;math display&#34;&gt;\[
\min\limits_{i \in \{1,\dots,M\}} y^{(i)}(w^Tx^{(i)}+b) = 1
\]&lt;/span&gt; to consume this freedom. Consequently, the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max \frac{1}{||w||_2} \iff \min \frac{1}{2}||w||^2_2 \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1, i = 1,\dots,M
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-margin-svm&#34;&gt;Soft Margin SVM&lt;/h3&gt;
&lt;p&gt;It may not happen that the real data are linearly-separable, or there may exist noisy samples that disrupt this linear separability. In such case, we may allow some samples to violate the margin. We define some slack variables &lt;span class=&#34;math inline&#34;&gt;\(\xi_i \ge 0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;gt; 0\)&lt;/span&gt; means that the sample is inside the margin (or even this sample will be misclassified), &lt;span class=&#34;math inline&#34;&gt;\(\xi_i = 0\)&lt;/span&gt; means that the sample is outside the margin.&lt;/p&gt;
&lt;p&gt;Of course, these slack variables should be as small as possible. Then the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\min \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, \xi_i \ge 0, i=1,\dots,M
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is a penalty factor on violating the margin. To some extent, it avoid overfitting.&lt;/p&gt;
&lt;p&gt;To solve this, first convert the problem into the standard form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \frac{1}{2}||w||^2_2 &amp;amp;+ C\sum_{i=1}^M\xi_i \\
s.t.\quad 1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b) &amp;amp;\le 0, i=1,\dots,M \\
-\xi_i &amp;amp;\le 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; This problem gives a strong duality. The solution will satisfy the KKT conditions. We first write down the Lagrangian: &lt;span class=&#34;math display&#34;&gt;\[
L(w,b,\xi,\lambda,\mu) = \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b)) - \sum_{i=1}^M\mu_i\xi_i
\]&lt;/span&gt; Then we form the dual problem: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\lambda,\mu;\lambda_i,\mu_i \ge 0} \min_{w,b,\xi}L(w,b,\xi,\lambda,\mu)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial L}{\partial w} = w - \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)}, \frac{\partial^2 L}{\partial w\partial w} = I \succeq 0 \\
\frac{\partial L}{\partial b} = -\sum_{i=1}^M\lambda_iy^{(i)}, \frac{\partial^2 L}{\partial b\partial b} = 0 \succeq 0 \\
\frac{\partial L}{\partial \xi} = C - \lambda - \mu, \frac{\partial^2 L}{\partial \xi\partial \xi} = 0 \succeq 0
\end{gather}
\]&lt;/span&gt; The Hessian matrices of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; are positive semi-definite. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min\limits_{w,b,\xi}L(w,\xi,\lambda,\mu)\)&lt;/span&gt; is obtained at its local minimum, i.e. where its first-order derivative meets &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
w = \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)} \\
\sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda + \mu = C
\]&lt;/span&gt; Substitute above back to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; to transform the dual problem into &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu} \frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + C\sum_{i=1}^M\xi_i - \sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}b) + \sum_{i=1}^M(\lambda_i - C)\xi_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda_i,\mu_i \ge 0, i=1,\dots,M \\
\Downarrow \\
\max_{\lambda,\mu}D(\lambda,\mu) = -\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; Since we know the optimal solution exists, instead of taking derivative of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to solve it, we assume the solution to above problem is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} \\
\mu^\star = C - \lambda^\star
\]&lt;/span&gt; Complementary constraints will give &lt;span class=&#34;math display&#34;&gt;\[
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star)) = 0 \\
\mu_i\xi_i = 0
\]&lt;/span&gt; There is at least one &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j &amp;gt; 0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} = 0\)&lt;/span&gt;, which means the primal constraints &lt;span class=&#34;math display&#34;&gt;\[
1 - \xi_i - y^{(i)}((w^\star)^Tx^{(i)} + b^\star) =  1 - \xi_i - y^{(i)}b^\star\le 0
\]&lt;/span&gt; won’t hold no matter what value &lt;span class=&#34;math inline&#34;&gt;\(b^\star\)&lt;/span&gt; takes. This specific &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j\)&lt;/span&gt;’s slackness complementary will give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
0 &amp;amp;= (1 - \xi^\star_j - y^{(j)}((w^\star)^Tx^{(j)} + b^\star)) \\
b^\star &amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - (w^\star)^Tx^{(j)} \\
&amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)}\cdot x^{(j)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
According to the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt; (not know yet), added with primal constraints and the complementary constraints, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0 \\
\xi_i \ge 0 \\
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star)) = 0 \\
\mu^\star_i\xi^\star_i = 0 \\
\lambda^\star_i + \mu^\star_i = C
\end{gather}
\]&lt;/span&gt; we can make interpretations on the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt;: $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\lambda^\star_i = 0 &amp;amp;\Rightarrow 
\begin{cases}
\mu^\star_i = C \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0
\end{cases}
 \Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \ge 1 \\
 0 &amp;lt; \lambda^\star_i &amp;lt; C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i &amp;gt; 0 \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 1 \\
\lambda^\star_i = C &amp;amp;\Rightarrow 
\begin{cases}
\mu^\star_i = 0 \Rightarrow \xi^\star_i \ge 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 1 \\

\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ They corresponds to cases where sample &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is outside margin, is on the margin, violates the margin, respectively.&lt;/p&gt;
&lt;p&gt;The key to the above derivation is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;, which is to be solved in Solving SVM section.&lt;/p&gt;
&lt;p&gt;Soft Margin SVM is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\min \frac{1}{C}||w||^2_2 + \sum_{i=1}^M\max(0, 1 - y^{(i)}(w^Tx^{(i)} + b))
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l_h(z) = \max(0, 1 - z)\)&lt;/span&gt; is called Hinge loss.&lt;/p&gt;
&lt;h3 id=&#34;kernel-svm&#34;&gt;Kernel SVM&lt;/h3&gt;
&lt;p&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \cdot x^{(j)}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda,\mu)\)&lt;/span&gt; is the inner product between two samples. We can make a table storing these inner products. This naturally introduces the kernel trick, which means we can manipulate the entry in this table by remapping the &lt;span class=&#34;math inline&#34;&gt;\((x^{(i)}, x^{(j)})\)&lt;/span&gt; so that the entry represents the inner product of some higher-dimensional features. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{(i)}), \phi(x^{(j)})
\]&lt;/span&gt; This saves us from explicitly defining the mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a higher-dimensional feature. This also saves the time of the computation of the inner products of these higher-dimensional features, than that of transform-then-inner-product method.&lt;/p&gt;
&lt;p&gt;SVM is a linear classifier, which means its decision boundary is a linear hyperplane in input feature space. By applying kernel trick, we implicitly map the input feature to a higher dimensional one. Therefore the decision boundary would become a linear hyperplane in this higher-dimensional space: &lt;span class=&#34;math display&#34;&gt;\[
w_\phi\phi(x) + b_\phi = 0
\]&lt;/span&gt; And thus this hyperplane is not linear in original feature space.&lt;/p&gt;
&lt;h4 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input feature &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1,\dots,x_N] \in \R^N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order polynomial: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x, x^\prime) = (x^Tx^\prime)^p = (\sum_{i=1}^Nx_ix^\prime_i)^p
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
K(x,x^\prime) = (\sum_{i=1}^Nx_ix^\prime_i)^2 = (\sum_{i=1}^N\sum_{j=1}^Nx_ix_jx^\prime_ix^\prime_j)^2 = \phi(x)\phi(x^\prime)
\]&lt;/span&gt; The transformation applied on original input feature will be &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = [x_1x_1,x_1x_2,\dots,x_1x_N,x_2x_1,\dots x_2x_N,\dots,x_Nx_1,\dots,x_Nx_N] \in \R^{N^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is similar to a Gaussian &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = e^{-\gamma||x - x^\prime||^2_2}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a hyper-parameter to be determined.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This kernel is the case where it is hard to define the transformation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; explicitly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-svm&#34;&gt;Solving SVM&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; is the key to the final solution and by far it is still remained unsolved: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}D(\lambda,\mu) = -\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; We attack it by Sequential Minimal Optimization.&lt;/p&gt;
&lt;h3 id=&#34;multi-class-svm&#34;&gt;Multi-class SVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 v.s. 1&lt;/p&gt;
&lt;p&gt;Train 1-vs-1 SVM for all pairs of classes. Then decide by majority voting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1 v.s. rest&lt;/p&gt;
&lt;p&gt;Train 1-vs-rest SVM for all classes (&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; belongs to the “1”). Choose the class with the largest geometric margin.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</link>
      <pubDate>Mon, 03 Jan 2022 14:50:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</guid>
      <description>

&lt;h3 id=&#34;convex-and-differentiable-function&#34;&gt;Convex and Differentiable Function&lt;/h3&gt;
&lt;p&gt;Given a convex, differentiable &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;, if we are at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is minimized along each coordinate axis, have we found a global minima? That is, does &lt;span class=&#34;math display&#34;&gt;\[
\forall d, i,f(x + d \cdot e_i) \ge f(x) \Rightarrow f(x) = \min_zf(z), \text{ where $e_i$ is the $i$-th standard basis ?}
\]&lt;/span&gt; The answer is yes. This is because &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(x) = \left[
\begin{array} \\
\frac{\partial f}{\partial x_1},
\cdots,
\frac{\partial f}{\partial x_n}
\end{array}
\right]
= 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convex-and-non-differentiable-function-but-non-smooth-separable&#34;&gt;Convex and Non-differentiable Function, but Non-smooth Separable&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is only convex but not differentiable, the above will not necessarily hold. However, if &lt;span class=&#34;math display&#34;&gt;\[
f(x) = g(x) + \sum_{i=1}^nh_i(x_i)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; is convex and differentiable, and each &lt;span class=&#34;math inline&#34;&gt;\(h_i(x_i)\)&lt;/span&gt; is convex, the global minima still holds. For any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;= g(y) - g(x) + \sum_{i=1}^nh_i(y_i) - \sum_{i=1}^nh_i(x_i) \\
&amp;amp;\ge \nabla g(x)^T(y - x) + \sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\
&amp;amp;= \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)]
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; obtains minimum along each axes, for any &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x + d \cdot e_k) &amp;amp;\ge f(x) \\
g(x + d \cdot e_k) + \sum_{i=1,i\ne k}^nh_i(x_i) + h_k(x_k + d) &amp;amp;\ge g(x) + \sum_{i=1}^nh_i(x_i) \\
g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) &amp;amp;\ge 0 \\
\end{aligned}
\]&lt;/span&gt; By [[Subgradient#Properties|the linearity of subgradient]], &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial(g_k + h_k) = \nabla_k g + \partial h_k \Rightarrow -\nabla_k g \in \partial h_k\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
h_k(y_k) - h_k(x_k) \ge -\nabla_k g(x)(y_k - x_k)
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
f(y) - f(x) = \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[Coordinate Descent.pdf]]&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zeyuan.wordpress.com/2016/06/13/coordinate-descent/&#34;&gt;Coordinate Descent in One Line, or Three if Accelerated | A Butterfly Valley (wordpress.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>协方差与相关系数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Sun, 01 May 2022 10:41:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description>

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;h3 id=&#34;协方差以二维随机变量为例&#34;&gt;协方差（以二维随机变量为例）&lt;/h3&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt;为二维随机变量，如果&lt;span class=&#34;math inline&#34;&gt;\(\mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}\)&lt;/span&gt;存在，则称 &lt;span class=&#34;math display&#34;&gt;\[
\notag \mathrm {Cov}(X, Y) \triangleq \mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}
\]&lt;/span&gt; 为随机变量X和Y的协方差。 在实际中计算协方差时，更多的是使用以下公式： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm {Cov}(X, Y) &amp;amp;= \mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\} \\
&amp;amp;= \mathrm E[XY - X\mathrm E(Y) - \mathrm E(X)Y + \mathrm E(X) \mathrm E(Y)] \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X)\mathrm E(Y) - \mathrm E(X)\mathrm E(Y) + \mathrm E(X) \mathrm E(Y) \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X) \mathrm E(Y)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;相关系数&#34;&gt;相关系数&lt;/h3&gt;
&lt;p&gt;协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们首先将随机变量标准化： &lt;span class=&#34;math display&#34;&gt;\[
X^\star = \frac{X - \E(X)}{\sqrt{\Var(X)}}，Y^\star = \frac{Y - \E(Y)}{\sqrt{\Var(Y)}}
\]&lt;/span&gt; 再求协方差&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{cov}(X^\star, Y^\star)\)&lt;/span&gt;，这便是随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的相关系数 &lt;span class=&#34;math display&#34;&gt;\[
\rho(X, Y) = \mathrm{Cov}(X^\star, Y^\star) = \frac{\mathrm{cov}(X, Y)}{\sqrt{\Var(X) \Var(Y)}}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>KL-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</guid>
      <description>

&lt;p&gt;KL-divergence, &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|q)\)&lt;/span&gt; is statistical distance, measuring how the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is different from the reference probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, both defined on &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathcal{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In information theory, it measures the &lt;strong&gt;relative entropy&lt;/strong&gt; from &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which is the average number of extra bits required to represent a message with &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In discrete form, &lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(p \| q) = -\sum_{x \in \mathcal{X}} p(x)\log \frac{q(x)}{p(x)} = \sum_{x \in \mathcal{X}} p(x)\log \frac{p(x)}{q(x)}
\]&lt;/span&gt; In continuous form, &lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(p \| q) = -\int_{x \in \mathcal{X}} p(x) \log \frac{q(x)}{p(x)}dx = \int_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are random variables, both of some &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional Gaussian distribution. Then the KL-divergence between them can be formulated as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_{KL}(p_X || p_Y) &amp;amp;= \int p_X(x) \log \frac{p_X(x)} {p_Y(x)} \d x = \int p_X(x) \log [
    \sqrt \frac{|\Sigma_X|}{|\Sigma_Y|} 
    \frac {
        e^{-\frac 1 2 (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X)}
    } {
        e^{-\frac 1 2 (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)}
    }
] 
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - 
\frac 1 2 \int p_X(x) [
    (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) + 
    (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - 
\frac 1 2 \int p_X(x) (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) \d x \\
&amp;amp;\quad -\frac 1 2 \int p_X(x) (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y) \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \int p_X(x) x^T \Sigma_X^{-1} x \d x + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \int p_X(x) x^T \Sigma_Y^{-1} x \d x + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \tr(\Sigma_X^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_X^{-1} \mu_X + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \tr(\Sigma_Y^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_Y^{-1} \mu_X + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y     \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y) + \mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu_X \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y)] \\
&amp;amp;\quad + \frac 1 2 [\mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu^T_X \Sigma_Y^{-1} \mu_X + \mu_X^T \Sigma_Y^{-1} \mu_Y - \mu_Y^T \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y) + (\mu_Y^T - \mu_X^T) \Sigma_Y^{-1} (\mu_Y - \mu_X)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</link>
      <pubDate>Fri, 07 Jan 2022 12:58:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</guid>
      <description>
&lt;p&gt;If a probabilistic model contains only observable variables, Maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables, or called latent variables. Latent variables are those that you cannot observe but you know its existence and effect in the model. In such case, Lagrange Multipliers may be hard to apply because of the existence of the “log of sum” term.&lt;/p&gt;
&lt;p&gt;Given samples &lt;span class=&#34;math inline&#34;&gt;\(X = \{x^{(1)}, x^{(2)}, ..., x^{(m)}\} \in \mathcal{X}\)&lt;/span&gt;, and their corresponding latent variables &lt;span class=&#34;math inline&#34;&gt;\(Z = \{z^{(1)}, z^{(2)}, ..., z^{(m)}\} \in \mathcal{Z}\)&lt;/span&gt;, denoting the data as &lt;span class=&#34;math inline&#34;&gt;\(D = \{(x^{(1)}, z^{(1)}), (x^{(2)}, z^{(2)}), ..., (x^{(m)}, z^{(m)})\}\)&lt;/span&gt; try to the find best parameters &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = arg\max_{\theta}\log(p(X;\theta))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The log-likelihood function is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log p(X;\theta) \\
&amp;amp;= \log\sum_{Z \in \mathcal{Z}}p(X, Z;\theta) \\
&amp;amp;= \log\sum_{Z \in \mathcal{Z}}p(Z;\theta)\frac{p(X, Z;\theta)}{p(Z;\theta)} \\
&amp;amp;\ge \sum_{Z \in \mathcal{Z}}p(Z;\theta)\log\frac{p(X, Z;\theta)}{p(Z;\theta)} \text{, by Jensen&amp;#39;s Inequality} \\
&amp;amp;= E_{Z \sim q}[\log\frac{p(X, Z;\theta)}{q(Z)}] \text{, where $q(Z) = p(Z;\theta)$}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we maximize the &lt;span class=&#34;math inline&#34;&gt;\(E_{Z \sim q}[\log\frac{p(X, Z;\theta)}{q(Z)}]\)&lt;/span&gt;, the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; can be maximized, which gives us a good guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; will increase monotonically. &lt;span class=&#34;math inline&#34;&gt;\(E_{z \sim q}[\log\frac{p(X, z;\theta)}{q(z)}]\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\((q, \theta)\)&lt;/span&gt;, firstly we maximize it w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, and then w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and then back and forth, making a Coordinate Ascent process.&lt;/p&gt;
&lt;p&gt;The first step of Coordinate Ascent, i.e. maximizing over &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, happens by having &lt;span class=&#34;math inline&#34;&gt;\(E_{Z \sim q}[\log\frac{p(X, Z;\theta)}{q(Z)}]\)&lt;/span&gt; reach its upper bound when the equality in Jensen’s Inequality holds, where &lt;span class=&#34;math inline&#34;&gt;\(\log\frac{p(X, Z;\theta)}{q(Z)}\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\frac{p(X, Z;\theta)}{q(Z)}\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(Z \in \mathcal{Z}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(X,Z;\theta) &amp;amp;= cq(Z) \\
\sum_Zp(X,Z;\theta) &amp;amp;= c\sum_zq(Z) \\
\sum_Zp(X,Z;\theta) &amp;amp;= c \\
\end{aligned}
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
q(Z) = \frac{p(X, Z;\theta)}{c} = \frac{p(X, Z;\theta)}{\sum_Zp(X,Z;\theta)} = \frac{p(X, Z;\theta)}{p(X;\theta)} = p(Z|X;\theta)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(Z|X;\theta)\)&lt;/span&gt; is a shorthand for &lt;span class=&#34;math inline&#34;&gt;\(\frac{p(X, Z;\theta)}{p(X;\theta)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
p(Z|X;\theta) = \frac{p(X, Z;\theta)}{p(X;\theta)} = \frac{\prod_{i=1}^m \sum_{z^{(i)}}p(x^{(i)}, z^{(i)})}{\prod_{i=1}^m p(x^{(i)})} = \prod_{i=1}^m \sum_{z^{(i)}}p(z^{(i)}|x^{(i)})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/306016801/answer/624061631&#34;&gt;good comparison of latent variables&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>统计量</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为取自总体的一个样本，若函数&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;不直接包含总体分布中的任何参数，则称&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;为&lt;strong&gt;统计量&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;样本均值和样本方差&#34;&gt;样本均值和样本方差&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\mathbb{样本均值}:\bar X = \frac{1}{n} \sum_{i=1}^n X_i \\
\mathbb{样本方差：}S^2 = \frac{1}{n-1} \sum_{i=1}^N (X_i - \bar X)^2 = \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_k = \frac{1}{n} \sum_{i=1}^n X_i^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(M_k = \frac{1}{n} \sum_{i=1}^n (X_i - \bar X)^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩&lt;/strong&gt;，当&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq M_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;由于统计量是随机变量的函数，故统计量也是随机变量。设总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的均值&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \mu, \Var(X) = \sigma^2\)&lt;/span&gt;，关于统计量有如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
&amp;amp; \E(\bar X) = \mu, \Var(\bar X) = \frac{\sigma^2}{n} \\
\notag \\
&amp;amp; \E(S^2) = \sigma^2, \E(S_n^2) = \frac{n-1}{n} \sigma^2 \\
\notag \\
&amp;amp; \bar X \stackrel{P}{\to} \mu, S^2 \stackrel{P}{\to} \sigma^2, S_n^2 \stackrel{P}{\to} \sigma^2
\end{gather}
\]&lt;/span&gt; 有关证明如下： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\E(\bar X) = \E (\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n} \sum_{i=1}^n \E(X_i) = \mu \\
\Var(\bar X) = \Var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2} \sum_{i=1}^n D(X_i) = \frac{\sigma^2}{n}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}[t]
\E(S^2) &amp;amp;= \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n \E (X_i^2 ) - n \E(\bar X^2) \big) \\
&amp;amp;\Downarrow_ {\E(X_i^2) = \Var(X_i) + \E^2(X_i) = \sigma^2 + \mu^2, \E(\bar X^2) = \Var(\bar X) + \E^2(\bar X) = \frac{\sigma^2}{n} + \mu^2} \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n (\sigma^2 + \mu^2) - n (\frac{\sigma^2}{n} + \mu^2) \big) \\
&amp;amp;= \sigma^2
\end{aligned}
\begin{aligned}[t]
\E(S_n^2) &amp;amp;= \E [\frac{n-1}{n} \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;归根结底，样本方差使用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;的原因是，其使用的“均值”为&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，这导致了一个自由度的缺失。&lt;/p&gt;
&lt;p&gt;根据[[大数定律和中心极限定理#相互独立同分布大数定律|相互独立同分布大数定律]]， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \mu \\
\frac{1}{n} \sum_{i=1}^n X_i^2 \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n \E (X_i^2) = \sigma^2 + \mu^2
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;次序统计量&#34;&gt;次序统计量&lt;/h2&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\((X_{(1)}, \dots, X_{(n)})\)&lt;/span&gt;为样本&lt;span class=&#34;math inline&#34;&gt;\((X_1, \dots, X_n)\)&lt;/span&gt;排序后的结果，则&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)} = \min (X_1, \dots, X_n), X_{(n)} = \max (X_1, \dots, X_n)\)&lt;/span&gt;亦是统计量。&lt;/p&gt;
&lt;p&gt;记&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)}, X_{(n)}\)&lt;/span&gt;的概率密度函数分别为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(1)}}, p_{X_{(n)}}\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p_{X_{(1)}}(u) = n \big( 1 - P_X(u) \big)^{n-1} p_X(u) \\
p_{X_{(n)}}(u) = n \big( P_X(u) \big)^{n-1} p_X(u)
\end{gather}
\]&lt;/span&gt; 记&lt;span class=&#34;math inline&#34;&gt;\(X_{(k)}\)&lt;/span&gt;的概率密度函数为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(k)}}\)&lt;/span&gt;，则…&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>f-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence can be treated as the generalization of the KL-divergence. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) p(x)\ \d x
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has to satisfy that &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a convex function. The reason for these two constraints is that we hope &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
D_f (p||q) = 0 \text{ when $p=q$} \\
\forall p, q, D_f (p||q) \ge 0
\end{gather}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \log x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence becomes KL-divergence.&lt;/p&gt;
&lt;h2 id=&#34;variational-f-divergence&#34;&gt;Variational &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; have no closed-form expression, it is difficult to compute the &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence. Therefore in practice &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence is computed with a variational expression: &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the convex conjugate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. The derivation is as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_f (p||q) &amp;amp;= \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int f^{**}(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int \sup_t [\frac{p(x)}{q(x)} t - f^*(t)] q(x)\ \d x \\
&amp;amp;= \int \sup_t[p(x) t - f^*(t) q(x)]\ \d x \\
&amp;amp;\Downarrow_{T(x) = \arg \sup_t[p(x) t - f^*(t) q(x)]} \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \int [p(x) T(x) - f^*(T(x)) q(x)]\ \d x \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</guid>
      <description>

&lt;p&gt;Given a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}),i=1,\dots,M\}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \in \R^N\)&lt;/span&gt; is the feature vector and &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} \in R\)&lt;/span&gt; is the output, learn a linear function &lt;span class=&#34;math inline&#34;&gt;\(f = w^Tx + b: \R^N \mapsto \R\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N, b \in \R\)&lt;/span&gt;, that best predicts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for any feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The “best” is usually measured by the Mean Square Error: &lt;span class=&#34;math display&#34;&gt;\[
MSE(f,\mathcal D) = \frac{1}{M}\sum_{i=1}^M(y^{(i)} - f(x^{(i)}))^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;ordinary-least-squares&#34;&gt;Ordinary Least Squares&lt;/h3&gt;
&lt;p&gt;OLS selects the linear regression parameters &lt;span class=&#34;math inline&#34;&gt;\(w, b\)&lt;/span&gt; the minimize the MSE: &lt;span class=&#34;math display&#34;&gt;\[
w^\star, b^\star = \arg \min_{w,b}\frac{1}{M}\sum_{i=1}^M(y^{(i)} - w^Tx^{(i)} - b)^2
\]&lt;/span&gt; This is equivalent to the below Least Squares problem. Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
X = 
\left [
\begin{array}{c|c}
(x^{(1)})^T &amp;amp; 1 \\
(x^{(2)})^T &amp;amp; 1 \\
\vdots &amp;amp; \vdots \\
(x^{(M)})^T &amp;amp; 1
\end{array}
\right ], 
W = \begin{bmatrix}
w \\
b \\
\end{bmatrix},
Y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(M)}
\end{bmatrix} \\
\\
XW =  Y
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may not lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Therefore we have to approximate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X\hat W = proj_{Col(X)}Y \Rightarrow Y &amp;amp;- X\hat W \in Nul(X^T) \Rightarrow\\
X^T(Y - X\hat W) &amp;amp;= 0 \\
X^TX\hat W &amp;amp;= X^TY
\end{aligned}
\]&lt;/span&gt; Columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are independent &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is not invertible, there are infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt;. We can get one specific &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; by enforcing regularization on &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; or adding more samples when &lt;span class=&#34;math inline&#34;&gt;\(M &amp;lt; N + 1\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible, there is unique solution that &lt;span class=&#34;math inline&#34;&gt;\(\hat W = (X^TX)^{-1}X^TY\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This gives the same result with minimizing the MSE: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \min_{W}MSE(W) = \frac{1}{M}(Y - XW)^T(Y - XW)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial MSE}{\partial W} &amp;amp;= \frac{\partial(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)}{\partial W} \\
&amp;amp;= \frac{\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\partial W} \\
&amp;amp;= -2X^TY + 2X^TXW \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
0 = -2X^TY + 2X^TXW^\star \\
X^TXW^\star = X^TY 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are chances that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is too large, making equation (7) too computationally expensive. In this case, we can use gradient descent. The update rule will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
W^{(t+1)} &amp;amp;= W^{(t)} - \frac{\eta}{2}\nabla MSE(W^{(t)}) \\
&amp;amp;= W^{(t)} - \eta(X^TXW^{(t)} -X^TY)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;a-probabilistic-view&#34;&gt;A Probabilistic View&lt;/h4&gt;
&lt;h5 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h5&gt;
&lt;p&gt;In classification task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the feature, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the label; in regression task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the ‘label’, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the ‘feature’. From a probabilistic point of view, we would like estimate &lt;span class=&#34;math inline&#34;&gt;\(p(feature|label)\)&lt;/span&gt;. In this case, we treat &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as the ‘feature’ composed of a deterministic function with a noise sampled from an identical and independent Gaussian distribution, i.e., for random variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X, \mathcal Y\)&lt;/span&gt; (to distinguish from matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), &lt;span class=&#34;math display&#34;&gt;\[
\mathcal Y = \mathcal XW + \epsilon, \text{ where }p(\epsilon;\sigma^2) = \mathcal N(\epsilon;0, \sigma^2)
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
p(Y|X;W,\sigma^2) = p(\epsilon=Y - XW;\sigma^2) = \mathcal N(Y - XW;0,\sigma^2I)
\]&lt;/span&gt; Then OLS can be attacked by Maximum Likelihood Estimation. The log-likelihood function will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(W,\sigma^2) &amp;amp;= \log(L(W, \sigma^2)) = \log(p(Y|X;W,\sigma^2)) = \log \mathcal N(Y - XW;0,\sigma^2I) \\
&amp;amp;= \log(\frac{1}{\sqrt{(2\pi)^M|\sigma^2I|}}e^{-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW)}) \\
&amp;amp;= -\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\arg \max_{W,\sigma^2}l(W, \sigma^2) &amp;amp;= \arg \max_{W,\sigma^2}-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi \\
&amp;amp;= \arg \min_{W,\sigma^2}\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) + \frac{M}{2}\log \sigma^2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{1}{\sigma^2}(X^TXW^\star - X^TY) = 0 \\
X^TXW^\star = X^TY
\end{gather}
\]&lt;/span&gt; Substitute the solution &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; back, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{\star2}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{M}{2\sigma^{\star2}} - \frac{(Y - XW^\star)^T(Y - XW^\star)}{2(\sigma^{\star2})^2} = 0 \\
\sigma^{\star2} = \frac{1}{M}(Y - XW^\star)^T(Y - XW^\star)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;maximizing-a-posteriori&#34;&gt;Maximizing a Posteriori&lt;/h5&gt;
&lt;p&gt;If we add a priori to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(W \sim \mathcal N(0,\frac{C}{2}I)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(p(W) \propto \exp(-\frac{1}{C}W^TW)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(W|X, Y) &amp;amp;= \frac{p(W, X, Y)}{p(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(X, W)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)P(X)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)}{P(Y|X)} \\
&amp;amp;\propto p(Y|X,W)p(W)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \max_W p(W|X,Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;By adding a regularization term to OLS objective we obtain the Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \frac{\alpha}{2}||W||^2_2
\]&lt;/span&gt; By regularization, we are “shrink” the amount of &lt;span class=&#34;math inline&#34;&gt;\(||W||_2\)&lt;/span&gt;, whose magnitude is controlled by the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We prefer smaller weights because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smaller weights are more robust to the perturbations of input&lt;/li&gt;
&lt;li&gt;there are better chances to zero out some dimensions of the input feature, which may be uninformative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The constant term before the sum of square errors that before &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not of significance. We choose them to be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; because this gives a nicer closed-form solution to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Take derivative of the objective w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-X^TY + X^TXW^\star + \alpha W^\star = 0 \\
(X^TX + \alpha I)W^\star = X^TY
\end{gather}
\]&lt;/span&gt; The “ridge” comes from the &lt;span class=&#34;math inline&#34;&gt;\(\alpha I\)&lt;/span&gt; term, which is added to the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h3&gt;
&lt;p&gt;In Ridge Regression, there is still chance that some weights are small but not zero, because the regularization term is small so far as the weights are fairly small. Think in this way: &lt;span class=&#34;math inline&#34;&gt;\(0.01^2 = 0.0001 \ll 0.01\)&lt;/span&gt;, though it is not rigid.&lt;/p&gt;
&lt;p&gt;To get better regularization, we can change the regularization term to &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \alpha||W||_1
\]&lt;/span&gt; The above problem can be efficiently solved using Coordinate Descent or [[Least Angle Regression]].&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6018889.html&#34;&gt;Lasso回归算法： 坐标轴下降法与最小角回归法小结 - 刘建平Pinard - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/76055830&#34;&gt;LASSO回归求解 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Subgradient</title>
      <link>https://chunxy.github.io/notes/articles/optimization/subgradient/</link>
      <pubDate>Tue, 11 Jan 2022 16:09:23 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/subgradient/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Subgradient of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
\partial f(x) = \{g|f(y) \ge f(x) + g^T(y - x), \forall y \in dom(f) \}
\]&lt;/span&gt; Or put it another way, the subgradient defines a hyper-plane &lt;span class=&#34;math display&#34;&gt;\[
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
= 0,
\left[
\begin{array} \\
y \in \R^n \\
t \in \R \\
\end{array}
\right]
\]&lt;/span&gt; of which &lt;span class=&#34;math display&#34;&gt;\[
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
\le 0, \forall (y, t) \in epi(f)
\]&lt;/span&gt; by definition: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
g^T(y - x) - (f(y) - f(x)) &amp;amp;\le 0 &amp;amp;\Rightarrow \\
g^T(y - x) - (t - f(x)) &amp;amp;\le 0, t \ge f(y) &amp;amp;\Rightarrow \\
g^T(y - x) - (t - f(x)) &amp;amp;\le 0, t \ge f(y) &amp;amp;\Rightarrow \\
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
&amp;amp;\le 0, \forall (y, t) \in epi(f)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;The subgradient &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is a set, instead of a single number like the gradient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Subgradient is a monotonic operator: &lt;span class=&#34;math display&#34;&gt;\[
(u - v)^T(y - x) \ge 0, \forall u \in \partial f(y), \forall v \in \partial f(x)
\]&lt;/span&gt; This can be shown by: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left.
\begin{array} \\
f(y) \ge f(x) + v^T(y - x) \\
f(x) \ge f(y) + u^T(x - y) \\
\end{array}
\right\}
&amp;amp;\Rightarrow 
f(y) + f(x) \ge f(x) + f(y) - (u - v)^T(y - x) \\
&amp;amp;\Rightarrow (u - v)^T(y - x) \ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x^\star = \arg \min_x f(x) \iff 0 \in \partial f(x^\star)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \{\nabla f(x)\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x) \and p \in \partial f(x)\)&lt;/span&gt;, then for &lt;span class=&#34;math inline&#34;&gt;\(y = x + r(p - \nabla f(x))\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;\ge p^T(y - x) \\
f(x + r(p - \nabla f(x))) - f(x) &amp;amp;\ge rp^T(p - \nabla f(x)) \\
\frac{f(x + r(p - \nabla f(x))) - f(x)}{r} &amp;amp;\ge p^T(p - \nabla f(x)) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(p - \nabla f(x))^T(p - \nabla f(x)) &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)}{r} - \nabla f(x)^T(p - \nabla f(x)) \\
||p - \nabla f(x)||^2_2 &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take limit on &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; on both sides to give &lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}||p - \nabla f(x)||^2_2 \le \lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big) \\
\]&lt;/span&gt; By the definition of gradient, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f&amp;amp;(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x)) \\
&amp;amp;= \mathcal{O}(||r(p - \nabla f(x))||^2_2) \\
&amp;amp;= ||p - \nabla f(x)||^2\mathcal{O}(r^2) \\
&amp;amp;= \mathcal{O}(r^2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big) = \lim_{r \to 0}\frac{\mathcal{O}(r^2)}{r} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||p - \nabla f(x)||^2_2 \le 0 \\
0 \le ||p - \nabla f(x)||^2_2 \le 0 \\
||p - \nabla f(x)||^2_2 = 0
\end{gather} 
\]&lt;/span&gt; , contradicting the assumption that &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x)\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(p = \nabla f(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \alpha_1 f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \alpha_1 \partial f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/139083837&#34;&gt;凸优化笔记16：次梯度 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Non-linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</link>
      <pubDate>Fri, 07 Jan 2022 12:46:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</guid>
      <description>

&lt;p&gt;The basic idea about non-linear regression is to perform the feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, followed by linear regression in the new feature space.&lt;/p&gt;
&lt;h3 id=&#34;polynomial-regression&#34;&gt;Polynomial Regression&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a scalar, the feature mapping in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order Polynomial Regression is &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = 
\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
\]&lt;/span&gt; The regression function and the parameters are then like those in linear regression: &lt;span class=&#34;math display&#34;&gt;\[
f(x) = [w_0,w_1,w_2,\dots,w^p]\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
= w^T\phi(x)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; captures all features in each degree from &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; up to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. In higher-dimensional input feature space, there are many more entries for feature mapping in each degree. Take a 2-D input for example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Degree 1: &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Degree 2: &lt;span class=&#34;math inline&#34;&gt;\([x_1^2, x_1x_2, x_2^2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Degree 3: &lt;span class=&#34;math inline&#34;&gt;\([x_1^3, x_1^2x_2, x_1x_2^2, x_2^3]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-trick-in-non-linear-regression&#34;&gt;Kernel Trick in Non-linear Regression&lt;/h3&gt;
&lt;p&gt;Many Linear Regression algorithms learning algorithms depend on the calculation of inner products between feature vectors, either during training or in prediction, without directly depending on the feature vector. We can transform the feature vector by applying a feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; to do the Non-linear Regression task.&lt;/p&gt;
&lt;p&gt;However, it is not necessary to explicitly define the feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; when only the inner products are needed. Instead, we can define a function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K: \R^N \times \R^N \mapsto \R\)&lt;/span&gt; that directly calculate the inner products of pseudo-transformed features: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = \phi_{pseudo}(x)^T\phi_{pseudo}(x^\prime)
\]&lt;/span&gt; This will be more flexible and computationally-efficient.&lt;/p&gt;
&lt;h4 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h4&gt;
&lt;p&gt;Recall Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \min_{W}\frac{1}{2}||Y - X^TW||^2_2 + \frac{\alpha}{2}||W||^2_2 \iff (XX^T + \alpha I_{N+1})W^\star = XY
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\((XX^T + \alpha I_{N+1})\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(W^\star = (XX^T + \alpha I_{N+1})^{-1}XY\)&lt;/span&gt;. By [[Matrix Identity|matrix identity]] &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; we can obtain that &lt;span class=&#34;math inline&#34;&gt;\(W^\star = X(X^TX + \alpha I_M)^{-1}Y\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(X \in \R^{(N+1)\times M}, (X^TX + \alpha I_M)^{-1}Y \in \R^{M}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; can be rewritten as a linear combination of columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; lies in the span of input vectors: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \sum_{i=1}^M\lambda_ix^{(i)}
\]&lt;/span&gt; For a new data &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we make prediction by &lt;span class=&#34;math display&#34;&gt;\[
y^* = (x^*)^TW = (x^*)^TX(X^TX + \alpha I_M)^{-1}Y
\]&lt;/span&gt; which totally depends on the inner products.&lt;/p&gt;
&lt;h4 id=&#34;support-vector-regression&#34;&gt;Support Vector Regression&lt;/h4&gt;
&lt;p&gt;Support Vector Regression learns a hyper-plane that incorporates within its margin as many points as possible. As a comparison, [[Support Vector Machine]] learns a hyper-plane that excludes outside its margin as many points as possible. Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b \le \epsilon + \xi_i, i=1,\dots,M \\
y^{(i)} - w^Tx^{(i)} - b \ge \epsilon + \xi^*_i, i=1,\dots,M \\
\xi_i, \xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a hyper-parameter to be determined. Transform the problem into the standard optimization form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i \le 0, i=1,\dots,M \\
w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i \le 0, i=1,\dots,M \\
-\xi_i, -\xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(w,\xi,\xi^*,\lambda,\mu,\nu,\nu^*) = &amp;amp;\frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
&amp;amp;+ \sum_{i=1}^M\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i) \\
&amp;amp;+ \sum_{i=1}^M\mu_i(w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i) \\
&amp;amp;- \sum_{i=1}^M\nu_i\xi_i -\sum_{i=1}^M\nu^*_i\xi^*_i
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[Support Vector Regression.pdf]]&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>参数估计</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid>
      <description>

&lt;h2 id=&#34;点估计&#34;&gt;点估计&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim p(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;为总体的一个样本，若用一个统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;来估计&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。&lt;/p&gt;
&lt;h3 id=&#34;矩估计&#34;&gt;矩估计&lt;/h3&gt;
&lt;p&gt;矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;span class=&#34;math inline&#34;&gt;\(\mu_k = \E(X^k)\)&lt;/span&gt;，样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩为&lt;span class=&#34;math inline&#34;&gt;\(A_k = \frac 1 n \sum_{i=1}^n X_i^k\)&lt;/span&gt;，如果未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta = \varphi(\mu_1, \dots, \mu_m)\)&lt;/span&gt;，则其估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \varphi(A_1, \dots, A_m)\)&lt;/span&gt;，这种估计总体未知参数的方法叫作矩估计法。&lt;/p&gt;
&lt;p&gt;矩估计法往往不唯一，如设&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda)\)&lt;/span&gt;，则由于&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;；又&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\frac 1 n \sum_{i=1}^n X_i^2 - \bar X^2\)&lt;/span&gt;。此时往往采用较低阶的矩来估计未知参数。&lt;/p&gt;
&lt;h3 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;设总体有分布律&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(X=x;\theta)\)&lt;/span&gt;或密度函数&lt;span class=&#34;math inline&#34;&gt;\(X \sim p(x;\theta)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_n\)&lt;/span&gt;为取自总体的一个样本的观测值，将样本的联合分布律或联合密度函数看作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的函数： &lt;span class=&#34;math display&#34;&gt;\[
L(\theta) = \prod_{i=1}^n P(X=x_i;\theta)\ \text或 \ L(\theta) = \prod_{i=1}^n p(x_i;\theta)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta)\)&lt;/span&gt;又称作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的似然函数，似然函数满足关系式&lt;span class=&#34;math inline&#34;&gt;\(L(\hat \theta) = \max_{\theta} L(\theta)\)&lt;/span&gt;的解&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计量&lt;/p&gt;
&lt;h3 id=&#34;优良性评判&#34;&gt;优良性评判&lt;/h3&gt;
&lt;h4 id=&#34;无偏性unbiased&#34;&gt;无偏性（Unbiased）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个无偏估计（量），否则则称作有偏估计（量）。如果有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} \E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个渐进无偏估计（量）。&lt;/p&gt;
&lt;p&gt;估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值偏大，取另一些样本时估计量偏小，多次取样本进行估计，平均来讲偏差为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即系统误差。&lt;/p&gt;
&lt;h4 id=&#34;最小方差minimum-variance&#34;&gt;最小方差（Minimum-variance）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1 = \hat \theta_2\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的两个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat \theta_1) \le \Var(\hat \theta_2)\)&lt;/span&gt;，且至少有一个&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;使得该不等式严格成立，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1\)&lt;/span&gt;比&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_2\)&lt;/span&gt;有效。&lt;/p&gt;
&lt;h4 id=&#34;一致性consistent&#34;&gt;一致性（Consistent）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，若对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\hat \theta - \theta| \ge \epsilon) = 0, \text{亦即} \hat \theta \stackrel{P}{\to} \theta
\]&lt;/span&gt; 则称估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;具有一致性。一致性是一个很基本的要求，随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。&lt;/p&gt;
&lt;h3 id=&#34;cramer-rao不等式&#34;&gt;Cramer-Rao不等式&lt;/h3&gt;
&lt;p&gt;实际上，点估计量不仅仅可以估计未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;本身（假设为一元情况），更可以估计未知参数的某个函数&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，即给定总体的一个样本&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;，用统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g = \hat g(X_1, \dots, X_n)\)&lt;/span&gt;估计&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;。估计量最好的效果便是达到最小方差无偏（minimum-variance unbiased &amp;lt;MVU&amp;gt;）估计，Cramer-Rao不等式给出了点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g\)&lt;/span&gt;方差的一个下界。 &lt;span class=&#34;math display&#34;&gt;\[
\label{cr} \Var(\hat g) \ge (g&amp;#39;(\theta))^2 / (nI(\theta))
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(I(\theta) = \int [(\frac{\partial p(x;\theta)}{\partial \theta})^2 / p(x;\theta)] \d x\)&lt;/span&gt;为Fisher Information。当&lt;span class=&#34;math inline&#34;&gt;\(g(\theta) = \theta\)&lt;/span&gt;，即只估计未知参数本身时，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat g) \ge 1 / (nI(\theta))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;成立有一定的条件，其本身就暗含了&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial p(x;\theta)}{\partial \theta}\)&lt;/span&gt;存在及&lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\theta)\)&lt;/span&gt;存在的&lt;strong&gt;条件&lt;/strong&gt;。记 &lt;span class=&#34;math display&#34;&gt;\[
S = S(X_1, \dots, X_n, \theta) = \sum_{i=1}^n \frac{\partial \ln p(X_i;\theta)} {\partial \theta} = \sum_{i=1}^n [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\int p(x;\theta)\ \d x = 1\)&lt;/span&gt;，此式两边同时对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导，并&lt;strong&gt;假定&lt;/strong&gt;此处求导可以移至积分号内部，可得到&lt;span class=&#34;math inline&#34;&gt;\(\int \frac{\partial p(x;\theta)}{\partial \theta} \d x = 0\)&lt;/span&gt;。根据[[Unconscious Statistics#Law of the Unconscious Statistician|LOTUS]]， &lt;span class=&#34;math display&#34;&gt;\[
\E [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] 
= \int [\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta)] p(x;\theta)\ \d x
= \int \frac{\partial p(x;\theta)} {\partial \theta}\d x = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;的独立性， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Var(S) &amp;amp;= \sum_{i=1}^n \Var [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \\
&amp;amp;= \sum_{i=1}^n \{ \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] - \E^2 [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \} \\
&amp;amp;= \sum_{i=1}^n \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] \\
&amp;amp;= n \int \big (\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta) \big)^2 p(x;\theta)\ \d x \\
&amp;amp;= n I(\theta)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根据协方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\label{cov_prop} [\Cov(\hat g, S)]^2 \le \Var(\hat g) \Var(S) = \Var(\hat g) n I(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又&lt;span class=&#34;math inline&#34;&gt;\(\E(S) = 0\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) = \E (\hat g S) &amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \sum_{i=1}^n [\frac{\partial p(x_i;\theta)} {\partial \theta} / p(x_i;\theta)] \prod_{i=1}^n p(x_1;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \frac{\partial p(x_1;\theta) \dots p(x_n;\theta)} {\partial \theta}\ \d x_1 \dots \d x_n 
\end{aligned}
\]&lt;/span&gt; &lt;strong&gt;假定&lt;/strong&gt;此处对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导可以移至积分号外部， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) 
&amp;amp;= \frac \partial{\partial \theta} \int \dots \int \hat g(x_1, \dots, x_n) p(x_1;\theta) \dots p(x_n;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \frac \partial{\partial \theta} g(\theta) = g&amp;#39;(\theta)
\end{aligned}
\]&lt;/span&gt; 将上式重新带入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cov_prop}\)&lt;/span&gt;，从而得到&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/56411276/answer/204992057&#34;&gt;对Cramer-Rao不等式的理解&lt;/a&gt;|&lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Wiki (see the multi-variate case)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;区间估计&#34;&gt;区间估计&lt;/h2&gt;
&lt;p&gt;点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作&lt;strong&gt;置信区间&lt;/strong&gt;（confidence interval）。&lt;/p&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim f(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;为总体的一个样本，给定一个很小的数&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \alpha &amp;lt; 1\)&lt;/span&gt;，若有统计量&lt;span class=&#34;math inline&#34;&gt;\(\theta_l = \theta_l (X_1, \dots, X_n) \le \theta_r(X_1, \dots, X_n) = \theta_r\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
P(\theta_l \le \theta \le \theta_r) \ge 1 - \alpha
\]&lt;/span&gt; 我们称&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\([\theta_;, \theta_r]\)&lt;/span&gt;的&lt;strong&gt;置信水平&lt;/strong&gt;（confidence level），&lt;span class=&#34;math inline&#34;&gt;\(\theta_l\)&lt;/span&gt;为&lt;strong&gt;置信下限&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\theta_r\)&lt;/span&gt;为&lt;strong&gt;置信上限&lt;/strong&gt;。一般来说置信水平不唯一，因为若&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;是某个区间的置信水平，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\alpha &amp;lt; \tilde \alpha &amp;lt; 1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(1 - \tilde \alpha\)&lt;/span&gt;亦是该区间的置信水平。故一般的“置信水平”是这一系列置信水平中的最大者。&lt;/p&gt;
&lt;p&gt;区间估计的一般步骤为：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;（如&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;的一个函数&lt;span class=&#34;math inline&#34;&gt;\(G = G(\theta, \hat \theta)\)&lt;/span&gt;（称作主元（pivot）函数），且&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;完全已知，且其分布与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关，&lt;/li&gt;
&lt;li&gt;对任何常数&lt;span class=&#34;math inline&#34;&gt;\(a &amp;lt; b\)&lt;/span&gt;，不等式&lt;span class=&#34;math inline&#34;&gt;\(a \le G(\theta, \hat \theta) \le b\)&lt;/span&gt;能够改写成等价的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;仅与&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta,a,b\)&lt;/span&gt;有关，与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;取&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;的上&lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{\alpha/2}\)&lt;/span&gt;及上&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2}\)&lt;/span&gt;，此时有&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha/2}) - F(w_{1-w_{\alpha/2}}) = 1 - \alpha\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
P(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}) = 1 - \alpha
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}\)&lt;/span&gt;可改写为对应的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;的形式，且&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;仅与估计量和两个分位点有关，&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;就构成了&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


</description>
    </item>
    
    <item>
      <title>Differentiation</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/differentiation/</link>
      <pubDate>Fri, 22 Apr 2022 21:13:30 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/differentiation/</guid>
      <description>

&lt;h2 id=&#34;distribution-or-random-variable&#34;&gt;Distribution or Random Variable?&lt;/h2&gt;
&lt;p&gt;Both [[Cross Entropy]] and [[KL-divergence]] describe the relationship between &lt;strong&gt;two distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both [[Conditional Entropy]] and [[Mutual Information]] describe the relationship between &lt;strong&gt;two random variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both [[Conditional Entropy]] and [[Cross Entropy]] would better be applied on discrete random variables.&lt;/p&gt;
&lt;p&gt;Both [[Mutual Information]] and [[KL-divergence]] can be applied on either discrete or continuous random variables&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-entropy&#34;&gt;KL-divergence and Entropy&lt;/h2&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= -\mathrm{E}_{x \sim p} \log \frac{q(x)}{p(x)} \\
&amp;amp;= -\mathrm{E}_{x \sim p} \log q(x) - (-\mathrm{E}_{x \sim p} \log {p(x)}) \\
&amp;amp;= H(p||q) - H(p)
\end{align}
\]&lt;/span&gt; By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(-\log\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= \mathrm{E}_{x \sim p} [-\log \frac{q(x)}{p(x)}] \\
&amp;amp;\ge -\log(\mathrm{E}_{x \sim p} \frac{q(x)}{p(x)}) \\
&amp;amp;= 0
\end{align}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) \ge H(p)
\]&lt;/span&gt; The equality holds if and only if &lt;span class=&#34;math inline&#34;&gt;\(q = p\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-mutual-information&#34;&gt;KL-divergence and Mutual Information&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;= \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&amp;amp;= \sum_{(x,y) \in X \times Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} p(x) D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{x \in X} D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{y \in Y} D_{KL}[p(x|y), p(x)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/clustering/</link>
      <pubDate>Sat, 09 Apr 2022 14:32:03 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/clustering/</guid>
      <description>

&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}), i=1,\dots,M\}\)&lt;/span&gt;, a clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; points into &lt;span class=&#34;math inline&#34;&gt;\(K (K \le M)\)&lt;/span&gt; clusters is a partitioning of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; disjoint groups &lt;span class=&#34;math inline&#34;&gt;\(\{C_1,\dots,C_K\}\)&lt;/span&gt;. Suppose we have a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that evaluates the clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; and returns lower score with better clustering. The best clustering will be &lt;span class=&#34;math display&#34;&gt;\[
\arg \min_{\mathcal C} f(\mathcal C)
\]&lt;/span&gt; The number of all possibilities of clustering with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements is called the Bell number, denoted as &lt;span class=&#34;math inline&#34;&gt;\(B_M\)&lt;/span&gt;. The calculation of the Bell number is based on dynamic programming. The number of ways to cluster &lt;span class=&#34;math inline&#34;&gt;\(M+1\)&lt;/span&gt; elements is the sum of number of ways to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; element and cluster it, with the rest belong to a new cluster&lt;/li&gt;
&lt;li&gt;Select &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; elements and cluster them, with the rest belong to a new cluster&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;li&gt;Select &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements and cluster them, with the rest belong to a new cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_{M+1} = \sum_{i=1}^M\binom{M}{0}B_i \\
B_0 = 1
\end{gather}
\]&lt;/span&gt; The exhaustive method will be computationally intractable. We need either an approximation algorithm or a scoring function with special properties.&lt;/p&gt;
&lt;h3 id=&#34;k-means&#34;&gt;K-means&lt;/h3&gt;
&lt;p&gt;K-means assumes there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; clusters. This greatly eliminates many possibilities described above. Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\min_{c_1,\dots,c_K}\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \text{ where }z^{(i)} = \arg \min_{j \in \{1,\dots,K\}}||x^{(i)}-c_j||^2_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}\)&lt;/span&gt; is also the cluster center index &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is assigned to. K-means’ objective is assign each point to the closest cluster center and minimize the within-cluster square errors. If &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is known, let &lt;span class=&#34;math inline&#34;&gt;\(C_j = \{x^{(i)}|z^{(i)} = j\}\)&lt;/span&gt; be the set of points assigned to Cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, the cluster center of Cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
c_j = \frac{1}{|C_j|}\sum_{x^{(i)} \in C_j}x^{(i)}
\]&lt;/span&gt; Initially, however, &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is not known. K-means solves this by randomly pick up initial cluster centers and enter the assign data points - update cluster centers loop, until the cluster centers converge or become satisfactory.&lt;/p&gt;
&lt;p&gt;Rewrite the objective of K-means: &lt;span class=&#34;math display&#34;&gt;\[
\min_{z,c}(l(z,c) = \sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2)
\]&lt;/span&gt; K-means is in essence a coordinate descent of the objective &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. The main loop of K-means is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assign data points to its nearest cluster center, i.e. minimizing over &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update cluster centers according to the points assigned to, i.e. minimizing over &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is monotonically decreasing after each step in above loop. &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is also bounded by &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and thus K-means will converge finally.&lt;/p&gt;
&lt;p&gt;One problem with K-means is that it assumes that each cluster has a circular shape because of the Euclidean distance it uses.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-mixture-model&#34;&gt;Gaussian Mixture Model&lt;/h3&gt;
&lt;p&gt;A cluster can be modelled by a multi-variate Gaussian with elliptical shape. The elliptical shape controlled by the covariance matrix. The location is controlled by the mean. Gaussian Mixture Model a weighted sum of Gaussians: &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^K\pi_j\mathcal N(x;\mu_j, \Sigma_j)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; is the prior that a sample is generated from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x;\mu_j, \Sigma_j)\)&lt;/span&gt; is the probability to generate the sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. Put it together, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; is the total probability of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; over its latent &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(z = j\)&lt;/span&gt; representing the prior condition that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is sampled from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^Kp(x, z = j) = \sum_{j=1}^Kp(x|z = j)p(z = j)
\]&lt;/span&gt; GMM is learned by &lt;span class=&#34;math display&#34;&gt;\[
\max_{\pi,\mu,\Sigma}\Big(L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)})\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\]&lt;/span&gt; The log-likelihood function form will be &lt;span class=&#34;math display&#34;&gt;\[
\max_{\pi,\mu,\Sigma}\Big(l(\pi,\mu,\Sigma) = \log L(\pi,\mu,\Sigma)\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-lagrange-multiplier&#34;&gt;Attempt with [[Lagrange Multiplier]]&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)}) = \prod_{i=1}^M\sum_{j=1}^Kp(z_j)p(x|z = z_j) = \prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\end{gather}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
J(\pi,\mu,\Sigma,\lambda) = -\sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) + \lambda(\sum_{j=1}^K\pi_j - 1)
\]&lt;/span&gt; This expression is just too hard to zero the derivatives. For example, writing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x^{(i)};\mu_j, \Sigma_j)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}_j\)&lt;/span&gt;, then take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial J}{\partial \pi_j} = -\sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_j, \Sigma_j)} + \lambda \\
\lambda = \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_1, \Sigma_1)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_1, \Sigma_1)} 
= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_2, \Sigma_2)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_2, \Sigma_2} 
= \dots 
= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_K, \Sigma_K)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_K, \Sigma_K}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-expectation-maximization&#34;&gt;Attempt with [[Expectation Maximization]]&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^M p(x^{(i)}) = \prod_{i=1}^M \sum_{j=1}^K p(x^{(i)}, z^{(i)} = j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)
\end{gather}
\]&lt;/span&gt; By Jensen’s Inequality, if &lt;span class=&#34;math inline&#34;&gt;\(\forall j \in \{1,\dots,K\}, \alpha_j \ge 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K\alpha_j = 1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\pi,\mu,\Sigma) &amp;amp;= \sum_{i=1}^M \log \sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j) \\
&amp;amp;= \sum_{i=1}^M \log \sum_{j=1}^K \alpha^{(i)}_j \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\alpha^{(i)}_j} \\
&amp;amp;\ge B(\alpha,\pi,\mu,\Sigma) \coloneq \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\alpha^{(i)}_j} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. If we can enlarge &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, we can guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is increasing in this process. If we fix &lt;span class=&#34;math inline&#34;&gt;\(\pi,\mu,\Sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; can be easily optimized by the equality condition of Jensen’s Inequality. The equality holds if and only if &lt;span class=&#34;math display&#34;&gt;\[
\frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\alpha^{(i)}_j} = c^{(i)}, i=1,\dots M, j=1,\dots,K
\]&lt;/span&gt; In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\mathcal N(x^{(i)};\mu_1, \Sigma_1) \pi_j + \dots + \mathcal N(x^{(i)};\mu_K, \Sigma_j) \pi_K}{\alpha^{(i)}_1 + \dots + \alpha^{(i)}_K} = c^{(i)} \\
c^{(i)} = \sum_{j=1}^K \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j \\
\begin{aligned}
\alpha^{(i)}_j &amp;amp;= \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\sum_{j=1}^K \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j} \\
&amp;amp;= \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{p(x^{(i)})} \\
&amp;amp;= \frac{p(x^{(i)}, z^{(i)} = j)}{p(x^{(i)})} \\
&amp;amp;= p(z^{(i)} = j|x^{(i)})
\end{aligned}
\end{gather}
\]&lt;/span&gt; If we fix &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B(\alpha,\pi,\mu,\Sigma) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j - \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \alpha^{(i)}_j \\
\end{aligned}
\]&lt;/span&gt; Grouping terms in &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; that are relevant to &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;, we are to &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_\pi \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j \\
s.t.\quad \sum_{i=1}^K \pi_j = 1
\end{gather}
\]&lt;/span&gt; We do this by Lagrangian Multipliers: &lt;span class=&#34;math display&#34;&gt;\[
J(\pi, \lambda) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j + \lambda(1 - \sum_{i=1}^K\pi_i )
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial J}{\partial \pi_k} =\sum_{i=1}^M \frac{\alpha^{(i)}_k}{\pi_k} - \lambda \\
\pi_k = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{\lambda}
\]&lt;/span&gt; With the knowledge that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K \pi_j = 1, \sum_{i=1}^M\alpha^{(i)}_j = 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\pi_k = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{M}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \mu_k} &amp;amp;= \frac{\partial \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\partial \mu_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k)}{\partial \mu_k} \\
&amp;amp;= \sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) &amp;amp;= 0, \text{ by the invertibility of $\Sigma_j^{-1}$} \\
\mu_k &amp;amp;= \frac{\sum_{i=1}^M \alpha^{(i)}_k x^{(i)}}{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \Sigma_k} &amp;amp;= \frac{\partial \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\partial \Sigma_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k ((x^{(i)} - \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k) + \log|\Sigma_k|)}{\partial \Sigma_k} \\
&amp;amp;= -\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
-\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T + I) &amp;amp;= 0 \\
\Sigma_k &amp;amp;= \frac{\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_{i=1}^M \alpha^{(i)}_k} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-k-means&#34;&gt;Soft K-means&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; in GMM, which measures how likely the sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is generated from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian, can be viewed as a contribution of sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to the Cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. That is, a sample can contribute different portions to different clusters and these portions add up to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In cases where &lt;span class=&#34;math inline&#34;&gt;\(\pi_j = \frac{1}{K}, \Sigma_k = I\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p(x^{(i)}|z^{(i)} = j) = \mathcal N(x^{(i)};\mu_j, I) = \frac{1}{\sqrt{(2\pi)^N}}\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2) \\
\alpha^{(i)}_j = p(z^{(i)} = j | x^{(i)}) = \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{p(x^{(i)})} = \frac{\mathcal N(x^{(i)};\mu_j, I) \frac{1}{K}}{\frac{1}{K} \sum_{k=1}^K \mathcal N(x;\mu_k, I)} = \frac{\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2)}{\sum_{k=1}^K \exp(-\frac{1}{2}||x^{(k)}-\mu_j||^2_2)}
\end{gather}
\]&lt;/span&gt; The “Soft K-means” comes from the softmax of the Euclidean distance.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Angle Regression</title>
      <link>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</link>
      <pubDate>Sun, 19 Dec 2021 15:09:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</guid>
      <description>

&lt;h3 id=&#34;forward-selection&#34;&gt;Forward Selection&lt;/h3&gt;
&lt;p&gt;In cases where we are to solve &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Y = XW\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Y \in \R^M, X \in \R^{M \times N}, W \in \R^N\)&lt;/span&gt;, we can do it in an iterative and greedy way.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right, do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \frac{\mathrm x^{(k)} \cdot Y}{||\mathrm x ^{(k)}||_2} \mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or we have used all columns in &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently this algorithm is efficient. However, it does not guarantee an exact solution, even when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; lies in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;forward-stagewise&#34;&gt;Forward Stagewise&lt;/h3&gt;
&lt;p&gt;Like Forward Selection, Forward Stagewise selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Unlike Forward Selection, Forward Selection does not subtract the whole projection from &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Instead, it proceeds along &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; with a small step.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\},\eta\text{ is a small constant}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right, do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \eta \frac{\mathrm x^{(k)} \cdot Y}{||\mathrm x ^{(k)}||_2} \mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;del&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/del&gt; Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently Forward Stagewise gives an exact solution when &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is small enough. But it is more time-consuming.&lt;/p&gt;
&lt;h3 id=&#34;least-angle-regression&#34;&gt;Least Angle Regression&lt;/h3&gt;
&lt;p&gt;LARS a is compromise of Forward Selection and Forward Stagewise. Likewise, LARS selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. LARS proceeds stagewise like Forward Stagewise, however with its own methodology to determine the step size.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}, \mathrm d = \arg\max_{\mathrm x \in C}\frac{Y \cdot \mathrm x }{||\mathrm x||_2}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then do the following: &lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \eta \frac{\mathrm d \cdot Y}{||\mathrm d||_2} \mathrm d \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is determined such that there is some &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)} \in C\)&lt;/span&gt; onto which the projection of &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt;. Or in other words, &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; lies on the bisector hyper-plane between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt; be along this bisector.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


</description>
    </item>
    
    <item>
      <title>三大分布与正态总体的抽样分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</link>
      <pubDate>Sat, 09 Jul 2022 22:21:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布都从正态分布中衍生出来，之前介绍的常用统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。&lt;/p&gt;
&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;gamma函数&#34;&gt;Gamma函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x) = \int_0^{+\infty} e^{-t} t^{x-1} dt \quad (x &amp;gt; 0)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt;函数具有&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x + 1) = x \Gamma(x)\)&lt;/span&gt;的性质：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = \int_0^{+\infty} e^{-t} t^{x} dt = [-e^{-t} t^x] \bigg|^{+\infty}_{t=0} - \int_0^{+\infty} -e^{-t} xt^{x-1} dt
\]&lt;/span&gt; 根据洛必达法则，&lt;span class=&#34;math inline&#34;&gt;\(\lim_{t \to +\infty} = \frac{-t^x}{e^t} = \lim_{t \to +\infty} \frac{x!}{e^t} = 0\)&lt;/span&gt;，故 &lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = 0 + x\int_0^{+\infty} e^{-t} t^{x-1} dt = x \Gamma(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1) = \int_0^{+\infty} e^{-t} dt = 1\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;为正整数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x) = x!\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2) = \sqrt \pi\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x = 2k + 1\)&lt;/span&gt;为正奇数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\frac{x}{2}) = \sqrt \pi \prod_{i=0}^{k-1} \frac{2 * i + 1}{2}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\Gamma (\frac{1}{2}) = \int_0^{+\infty} e^{-t} t^{-\frac{1}{2}} dt 
\stackrel{u = t^\frac{1}{2}}{\Longrightarrow} \int_0^{+\infty} e^{-u^2} u^{-1} \;2u du 
= 2 \int_0^{+\infty} e^{-u^2} du 
= \int_{-\infty}^{+\infty} e^{-u^2} du  \\
\notag \\
\begin{aligned}
\Gamma^2(\frac{1}{2}) &amp;amp;= (\int_{-\infty}^{+\infty} e^{-u^2} du)^2 \\
&amp;amp;= (\int_{-\infty}^{+\infty} e^{-u^2} du)(\int_{-\infty}^{+\infty} e^{-v^2} dv) \\
&amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(u^2+v^2)}dudv \\
&amp;amp;\downarrow_{u = r\sin\theta, v = r\cos\theta} \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} e^{-r^2}\; rdrd\theta \\
&amp;amp;= \int_{0}^{2\pi} [-\frac{1}{2}e^{-r^2}] \bigg|_{r=0}^{+\infty} d\theta \\
&amp;amp;= \int_{0}^{2\pi} \frac{1}{2} d\theta \\
&amp;amp;= \pi \\
\Gamma (\frac{1}{2}) &amp;amp;= \sqrt \pi
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinehw.math.ksu.edu/math340book/chap3/gamma.php&#34;&gt;The Gamma Function&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=XAoe4th0F1k&#34;&gt;The derivation of &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2)\)&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;chi2分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为相互独立的标准正态分布随机变量，即&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim N(0,1)\)&lt;/span&gt;则称&lt;span class=&#34;math inline&#34;&gt;\(Y = X_1^2 + \dots + X_n^2\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布，记作&lt;span class=&#34;math inline&#34;&gt;\(Y \sim \chi^2(n)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P_{X_i^2}(x) = P_{X_i}(X_i^2 \le x) = 2 P_{X_i}(\sqrt x) - 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时，容易得到&lt;span class=&#34;math inline&#34;&gt;\(\forall y \le 0, P_Y(y) = 0, p_Y(y)= 0\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[t]
\forall y &amp;gt; 0, P_Y(y) &amp;amp;= 2P_X(\sqrt y) - 1 \\ 
p_Y(y) &amp;amp;= 2P_X&amp;#39;(\sqrt y) \frac 1 {2 \sqrt y} \\
&amp;amp;= \frac 1 {\sqrt {2\pi y}}  e^{-\frac 1 2 y} \\
&amp;amp;= \frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y 2}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\chi^2(1)\)&lt;/span&gt;分布的密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Y(y) =
\begin{cases}
\frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y 2}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=k\)&lt;/span&gt;时，令&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_k\)&lt;/span&gt;表示一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维空间中的点， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_Y(y) = P_Y(Y \le y) &amp;amp;= \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1 \dots \d x_k \\
&amp;amp;= \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
\end{aligned}
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k x_i^2 \le y\)&lt;/span&gt;的积分区域。可以看出，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;对应一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维球体，且其半径&lt;span class=&#34;math inline&#34;&gt;\(R = \sqrt y\)&lt;/span&gt;。对此，作[[Spherical Coordinates|高维球坐标变换]]： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Y(Y \le y) &amp;amp;= \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1 \dots \d x_k = \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \int_0^\sqrt{y} \\
&amp;amp;\quad\quad\quad\frac{e^{-\frac 1 2 (r^2\cos^2 \varphi_1 +
r^2\sin^2 \varphi_1 \cos^2 \varphi_2 + \dots + 
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \cos^2 \varphi_{k-1} + 
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \sin^2 \varphi_{k-1})} } {(2\pi)^{k / 2}}\\
&amp;amp;\quad\quad\quad r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \int_0^\sqrt{y} \frac{e^{-\frac 1 2 r^2}} {(2\pi)^{k / 2}}
r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^\sqrt{y} \underbrace{ \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \frac{1} {(2\pi)^{k / 2}} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})\ \d \varphi_1 \dots \d \varphi_k}_{c_k} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;= c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;是和&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;相关的常数项，并且由于&lt;span class=&#34;math inline&#34;&gt;\(P_Y(Y \le \infty) = 1\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
1 &amp;amp;= c_k \int_0^\infty e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;\Downarrow_{r = \sqrt{2t}} \\
1 &amp;amp;= c_k \int_0^\infty e^{-t} \sqrt{2t}^{k-1} \frac{1}{\sqrt{2t}} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \int_0^\infty e^{-t} t^{(k-2)/2} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \Gamma(\frac{k}{2}) \\
c_k &amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故可得密度函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
p_Y(y) &amp;amp;= \frac{\d P_Y(Y \le y)}{\d y} \\
&amp;amp;= \frac{\d [c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r]}{\d y} \\
&amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})} e^{-\frac y 2} y^\frac{k-1}{2} \frac{1}{2\sqrt y} \\
&amp;amp;= \frac{1} {2^{\frac k 2} \Gamma(\frac{k-2}{2})} e^{-\frac y 2} y^{\frac{k}{2} - 1}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最终可得密度函数如下： &lt;span class=&#34;math display&#34;&gt;\[
p_Y(y) =
\begin{cases}
\frac 1 {2^\frac{k}{2} \Gamma(\frac k 2)} e^{-\frac y 2} y^{\frac k 2 - 1}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}
\]&lt;/span&gt; 另外，该密度函数也可以通过数学归纳法验证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statlect.com/probability-distributions/chi-square-distribution&#34;&gt;Chi Squared Distribution&lt;/a&gt;|&lt;a href=&#34;https://vtechworks.lib.vt.edu/bitstream/handle/10919/34329/10Apxb.pdf?sequence=12&#34;&gt;Generating Function of Chi Squared Distribution&lt;/a&gt;|&lt;a href=&#34;https://www.zhihu.com/question/30020592&#34;&gt;正向推导&lt;/a&gt;|&lt;a href=&#34;https://www.bilibili.com/video/BV1e54y1v7e5&#34;&gt;数学归纳法&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;t分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim N(0,1), Y \sim \chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(T = \frac{X}{Y/n}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(T \sim t(n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_T(t) = \frac{\Gamma((n+1)/2)} {\sqrt{n\pi} \Gamma(n/2)} \big( 1 + \frac{t^2} n \big)^{-(n+1)/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;f分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim \chi^2(m), Y \sim \chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X/m}{Y/n}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z \sim F(m,n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \begin{cases}
\frac{\Gamma((m+n)/2} {\Gamma(m/2) \Gamma(n/2)} {m \choose n}^{\frac m 2} y^{\frac m 2 - 1} (1 + \frac m n y)^{-\frac{m+n}{2}}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp;\text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;正态总体的抽样分布&#34;&gt;正态总体的抽样分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu, \sigma^2)\)&lt;/span&gt;的一个样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac 1 n \sum_{i=1}^n X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2\)&lt;/span&gt;，则存在如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X \sim N(\mu, \frac{\sigma^2}{n}) \\
\frac{\sum_{i=1}^n (X_i - \bar X)^2}{\sigma^2} \sim \chi^2(n - 1),\text 即 \frac{(n-1) S^2}{\sigma^2} = \frac{n S_n^2}{\sigma^2} \sim \chi^2(n-1) \\
\text{$\bar X$与$S^2$相互独立，$\bar X$与$S_n^2$相互独立}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Dimension Reduction</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/dimension-reduction/</link>
      <pubDate>Fri, 07 Jan 2022 12:39:56 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/dimension-reduction/</guid>
      <description>

&lt;h2 id=&#34;unsupervised-dimension-reduction&#34;&gt;Unsupervised Dimension Reduction&lt;/h2&gt;
&lt;p&gt;Dimension Reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduces computational cost&lt;/li&gt;
&lt;li&gt;de-noise by projecting onto lower-dimensional space and back to original space&lt;/li&gt;
&lt;li&gt;make results easier to understand&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;vs. Feature selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal of feature selection is to remove features that are not informative with respect to the class label. This obviously reduces the dimensionality of the feature space.&lt;/li&gt;
&lt;li&gt;Dimensionality reduction can be used to find a meaningful lower-dim feature space even when there is information in each feature dimension so that none can be discarded&lt;/li&gt;
&lt;li&gt;Dimension Reduction is unsupervised while Feature Selection is supervised&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;vs. Data Compression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dimensionality reduction can be seen as a simplistic form of data compression, it is not equivalent to it, as the goal of data compression is to reduce the entropy of the representation not only the dimensionality&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-dimension-reduction&#34;&gt;Linear Dimension Reduction&lt;/h3&gt;
&lt;p&gt;Linear Dimension Reduction projects data onto lower-dimensional space by representing the data with a new basis consisting of some major components. Mathematically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x^{(i)} = \sum_{k=1}^Kz^{(i)}_kb^{(k)}, \text{ where $z^{(i)} \in \R^K$ is the weight, $b^{(k)} \in \R^N$ is the basis vector.} \\
X = BZ, \text{ where $X = [x^{(i)}, \dots, x^{(M)}], B = [b^{(1)}, \dots, b^{(K)}], Z = [z^{(1)}, \dots, z^{(M)}]$}
\end{gather}
\]&lt;/span&gt; The objective can be set to minimize the error when recovering from &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;alternating-least-squares&#34;&gt;Alternating Least Squares&lt;/h4&gt;
&lt;p&gt;By leveraging the idea of Coordinate Descent and OLS solution to Linear Regression, we can optimize &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; alternatively, until convergence.&lt;/p&gt;
&lt;p&gt;Fix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
2(X - BZ)(-Z^T) = 0 \\
BZZ^T = XZ^T \\
B = XZ^T(ZZ^T)^{-1}
\end{gather}
\]&lt;/span&gt; Fix &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(Z^T\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial||X - BZ||^2_F}{\partial Z^T} = \frac{\partial||X^T - Z^TB^T||^2_F}{\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\
2(X^T - Z^TB^T)(-B) = 0 \\
Z^TB^TB = X^TB \\
Z^T = X^TB(B^TB)^{-1} \\
Z = (B^TB)^{-1}B^TX
\end{gather}
\]&lt;/span&gt; Assume we have run the ALS to convergence and obtain the global optimal parameters &lt;span class=&#34;math display&#34;&gt;\[
B^\star, Z^\star = \arg \min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt; Let any invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(R \in \R^{K \times K}\)&lt;/span&gt;. We can construct a pair of &lt;span class=&#34;math inline&#34;&gt;\(\tilde B, \tilde Z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde B = B^\star R, \tilde Z = R^{-1}Z^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||X - \tilde B \tilde Z||^2_F = ||X - B^\star RR^{-1} Z^\star||^2_F = ||X - B^\star Z^\star||^2_F
\]&lt;/span&gt; Thus the global optima is not unique. We may force regularization on &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; and obtain the unique optima.&lt;/p&gt;
&lt;h4 id=&#34;singular-value-decomposition&#34;&gt;[[Singular Value Decomposition]]&lt;/h4&gt;
&lt;p&gt;Suppose the Singular Value Decomposition for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $N \times N$, $\Sigma$ is diagonal, $V$ is $M \times M$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{N \times M}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = \min\{N,M\}
\end{gather}
\]&lt;/span&gt; By only preserving the first &lt;span class=&#34;math inline&#34;&gt;\(K \le \rank(\Sigma)\)&lt;/span&gt; most prominent singular values, &lt;span class=&#34;math inline&#34;&gt;\(X \approx U_K\Sigma_KV^T_K\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U_K \in \R^{N \times K}$ is first $K$ columns of $U$} \\
\text{$V_K \in \R^{M \times K}$ is first $K$ columns of $V$} \\
\Sigma_K \in \R^{K \times K} = diag(\sigma_1, \sigma_2, ..., \sigma_K) \\
\end{gather}
\]&lt;/span&gt; [[Eckart-Young-Mirsky Theorem]] will tell that &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt; is the best approximation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; among &lt;span class=&#34;math inline&#34;&gt;\(N \times M\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in terms of Frobenius norm.&lt;/p&gt;
&lt;p&gt;We can form the &lt;span class=&#34;math inline&#34;&gt;\(B^\star, Z^\star\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
B^\star = U_K \Sigma_K^{\frac{1}{2}}, Z^\star = \Sigma_K^{\frac{1}{2}} V^T_K
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;principal-component-analysislinear-principal-component-analysisprincipal-component-analysis&#34;&gt;[[Principal Component Analysis#Linear Principal Component Analysis|Principal Component Analysis]]&lt;/h4&gt;
&lt;p&gt;As shown in ALS, the optimal solution of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is not unique. ALS and SVD give their solutions, however without much interpretability. The solution given by Principal Component Analysis explicitly chooses the directions of the basis it uses.&lt;/p&gt;
&lt;p&gt;The goal of PCA is to identify the directions along which the data exhibits the maximum variance. And then PCA projects the data onto the space formed by these directions.&lt;/p&gt;
&lt;p&gt;The solution of PCA is &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; eigenvectors permuted according to their corresponding to eigenvalues, which is exactly the &lt;span class=&#34;math inline&#34;&gt;\(U_K\)&lt;/span&gt; in [[Singular Value Decomposition|SVD]]. Thus, the solution of PCA can be constructed from SVD. Let &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;. If we choose &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; directions with largest directional variance as basis. Then &lt;span class=&#34;math inline&#34;&gt;\(B^\star = U_K\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;random-projection&#34;&gt;Random Projection&lt;/h4&gt;
&lt;p&gt;The above methods all minimize the Frobenius norm during reconstruction. This objective may become time-consuming when input dimension becomes large. We may use some randomly-generated vectors as basis to do the projection. This greatly saves time, at the expense of losing accuracy. We can measure such projection by checking whether the structure of the data can be preserved, e.g. the distance between points.&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss Lemma tells that a random function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \frac{1}{\sqrt{K}}Ax\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K \times N\)&lt;/span&gt; matrix with i.i.d. entries sampled from a standard Gaussian, can preserve the distance between any two points within error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
(1 - \epsilon)||x^{(i)} - x^{(j)}||^2_2 \le ||f(x^{(i)}) - f(x^{j})||^2_2 \le (1 + \epsilon)||x^{(i)} - x^{(j)}||^2_2
\]&lt;/span&gt; with the probability at least &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt; as long as &lt;span class=&#34;math display&#34;&gt;\[
K \ge \frac{8\log M}{\epsilon^2}
\]&lt;/span&gt; And usually this requirement is quite conservative.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-dimension-reduction&#34;&gt;Non-linear Dimension Reduction&lt;/h3&gt;
&lt;p&gt;We can introduce the feature mapping to handle the non-linearity problem in Dimension Reduction. Similar to non-linear regression, we can introduce the kernel trick in this case. [[Principal Component Analysis#Kernel PCA]] is just such an example.&lt;/p&gt;
&lt;h2 id=&#34;supervised-dimension-reduction&#34;&gt;Supervised Dimension Reduction&lt;/h2&gt;
&lt;p&gt;There is no guarantee that the unsupervised Dimension Reduction can throw insight into the classification problem. It may or may not help. Otherwise supervised Dimension Reduction such as [[Fisher’s Linear Discriminant]] finds a lower-dimensional space so as to minimize the class overlap.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>大数定律和中心极限定理</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link>
      <pubDate>Fri, 20 May 2022 09:27:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)\)&lt;/span&gt;及方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X)\)&lt;/span&gt;存在，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
P(|X-\E(X)| \ge \epsilon) \le \frac{\Var(X)}{\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为连续型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(|X-\E(X)| \ge \epsilon) &amp;amp;= \mathop \int_{|x - \E(x)| \ge \epsilon} p(x)dx \\
&amp;amp;\le \mathop \int_{|x - \E(x)| \ge \epsilon} \bigg( \frac{X - \E(x)}{\epsilon} \bigg)^2 p(x)dx \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \int_{|x - \E(x)| \ge \epsilon} \big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \int_{x \in X} \big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为离散型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(|X-\E(X)| \ge \epsilon) &amp;amp;= \mathop \sum_{|x - \E(x)| \ge \epsilon} P(x) \\
&amp;amp;\le \mathop \sum_{|x - \E(x)| \ge \epsilon} \bigg( \frac{x - \E(x)}{\epsilon} \bigg)^2 P(x) \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \sum_{|x - \E(x)| \ge \epsilon} \big( x - \E(x) \big)^2 P(x) \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \sum_{x \in X} \big( x - \E(x) \big)^2 P(x) \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是证明其它大数定理的基础工具。&lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD ### 依概率收敛&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;p&gt;随机变量是事件的映射，当试验次数足够多时，事件的频率会&lt;strong&gt;依概率收敛&lt;/strong&gt;到该事件的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) = 1\)&lt;/span&gt;，则称该随机变量序列依概率收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{P}{\to} c\)&lt;/span&gt;。或者，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) = 0\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;======= &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; raw ======= &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; notes ## 弱大数定律（Weak Law of large numbers）&lt;/p&gt;
&lt;h3 id=&#34;chebyshev大数定律&#34;&gt;Chebyshev大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,\dots\)&lt;/span&gt;两两不相关，若存在常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\Var(X_i) \le c \ne +\infty, i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| &amp;lt; \epsilon) = 1
\]&lt;/span&gt; 亦即&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n \E(X_i)\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;p&gt;由于该随机序列两两不相关，故根据期望及方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\E(\frac{1}{n} \sum_{i=1}^n X_i) =  \frac{1}{n} \sum_{i=1}^n \E(X_i),\quad \Var(\frac{1}{n} \sum_{i=1}^N X_i) = \frac{1}{n^2} \sum_{i=1}^n \Var(X_i) \le \frac{c}{n}
\]&lt;/span&gt; 根据切比雪夫不等式， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 \le P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) &amp;lt; \frac{\Var(\frac{1}{n} \sum_{i=1}^N X_i)}{\epsilon^2} \le \frac{c}{n \epsilon} \\
\underbrace{\lim_{n \to \infty} 0}_0 \le \lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) \le \underbrace{\lim_{n \to \infty} \frac{c}{n \epsilon}}_0 \Rightarrow\\
\lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) = 0
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;辛钦大数定律&#34;&gt;辛钦大数定律&lt;/h3&gt;
&lt;h4 id=&#34;相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) = \mu, \Var(X_i) = \sigma^2 \ne \infty, i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家辛钦证明该定律依然成立，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E_{X_i} = \mu\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - p| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。&lt;/p&gt;
&lt;h2 id=&#34;中心极限定理&#34;&gt;中心极限定理&lt;/h2&gt;
&lt;h3 id=&#34;lindburg-levy中心极限定理&#34;&gt;Lindburg-Levy中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) = \mu, \Var(X_i) = \sigma^2 \ne \infty, i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - n \mu}{\sqrt n \sigma} \le x) = N(0,1,x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;de-moivre-laplace中心极限定理&#34;&gt;de Moivre-Laplace中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，且&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}} \le x) = N(0,1,x)
\]&lt;/span&gt; 显然de Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。&lt;/p&gt;
&lt;p&gt;前面的Bernoulli大数定律告诉我们可以用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n} \sum_{i=1}^n X_i\)&lt;/span&gt;近似&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;，而至于近似程度如何，却不得而知。de Moivre-Laplace中心极限定理则告诉我们近似程度如何： &lt;span class=&#34;math display&#34;&gt;\[
P(|\frac{1}{n}\sum_{i=1}^n X_i - p| &amp;lt; \epsilon) = P(|\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}}| &amp;lt; \frac{\sqrt n \epsilon}{\sqrt{p(1-p)}}) = 2N(0,1,\frac{\sqrt n \epsilon}{\sqrt{p(1-p)}}) - 1
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</link>
      <pubDate>Tue, 11 Jan 2022 16:10:51 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</guid>
      <description>

&lt;p&gt;Given a data matrix &lt;span class=&#34;math inline&#34;&gt;\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \in \mathbb R^{N \times M}\)&lt;/span&gt;, the goal of PCA is to identify the directions of maximum variance contained in the data (compared with directional derivative, this is directional variance).&lt;/p&gt;
&lt;h3 id=&#34;linear-principal-component-analysis&#34;&gt;Linear Principal Component Analysis&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(v \in \mathbb R^{N \times 1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt;, the variance in the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2, \text{where }\mu = \frac{1}{M}\sum_{i=1}^Mv^Tx^{(i)} = v^T\bar x
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{M}\sum_{i=1}^Mx^{(i)} = 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2 &amp;amp;= \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - v^T\bar x)^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(x^{(i)} - \bar x)(x^{(i)} - \bar x)^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(x^{(i)} - \bar x)(x^{(i)} - \bar x)^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\
&amp;amp;= \frac{1}{M}v^TXX^Tv \text{ (by column-row expansion)} \\
\end{aligned}
\]&lt;/span&gt; Suppose we want to identify the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; of the maximum variance, we can formulate the problem as &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v \quad \frac{1}{M}v^TXX^Tv \\
s.t.\quad v^Tv = 1
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = \frac{1}{M}XX^T\)&lt;/span&gt;, which is real symmetric, we can form the Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
L(v, \lambda) = v^T\Sigma v + \lambda (1 - v^Tv)
\]&lt;/span&gt; We represent the constraint as &lt;span class=&#34;math inline&#34;&gt;\(1 - v^Tv = 0\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(v^Tv - 1 = 0\)&lt;/span&gt;. The reason is if we take derivative w.r.t &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial v} = 2\Sigma v - 2\lambda v
\]&lt;/span&gt; which is more intuitive than &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial v} = 2\Sigma v + 2\lambda v\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let the derivative be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(v^tv = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v \ne 0\)&lt;/span&gt;, which means the optimal solution &lt;span class=&#34;math inline&#34;&gt;\((\lambda^*, v^*)\)&lt;/span&gt; must be a pair of nontrivial eigen of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
v^* = v_i, \lambda^* = \lambda_i
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; is rescaled to &lt;span class=&#34;math inline&#34;&gt;\(v_i^Tv_i = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Substituting the result back to the objective to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}v^TXX^Tv &amp;amp;= (v_i)^T\Sigma v_i \\
&amp;amp;= (v_i)^T\lambda_iv_i \\
&amp;amp;= \lambda_i(v_i)^Tv_i \\
&amp;amp;= \lambda_i
\end{aligned}
\]&lt;/span&gt; So the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; largest directions of variance will be the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_N\)&lt;/span&gt;, corresponding to the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 &amp;gt; \lambda_2 &amp;gt; ... &amp;gt; \lambda_N\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;another-view-on-pca&#34;&gt;Another view on PCA&lt;/h4&gt;
&lt;p&gt;Finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\max_{v} \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)})^2 \\
\]&lt;/span&gt; is equivalent to finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\min_{v} \frac{1}{M}\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2
\]&lt;/span&gt; both subject to &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt; and inter-orthogonality. This second notation is essentially the projection of &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(||v|| = 1\)&lt;/span&gt; and thus the objective indicates minimizing the overall approximation error.&lt;/p&gt;
&lt;h3 id=&#34;kernel-pca&#34;&gt;Kernel PCA&lt;/h3&gt;
&lt;p&gt;We can map the data to a higher-dimension space: &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \to \phi(x^{(i)})\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X} = [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})]\)&lt;/span&gt;. Kernel tricks allow us not to explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, but to only focus on the inner product of mapped data: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{i})^T\phi(x^{j})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, the variance along direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(v^Tv=1\)&lt;/span&gt;, is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2, \text{where }\mu = \frac{1}{M}\sum_{i=1}^Mv^T\phi (x^{(i)}) = v^T\bar\phi(x)
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x) = \frac{1}{M}\sum_{i=1}^M\phi(x^{(i)}) = 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2 &amp;amp;= \frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - v^T\bar\phi(x))^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(\phi(x^{(i)}) - \bar{\phi(x)})(\phi (x^{(i)}) - \bar\phi(x))^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(\phi(x^{(i)}) - \bar\phi(x))(\phi (x^{(i)}) - \bar\phi(x))^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M\phi(x^{(i)})(\phi(x^{(i)})^T)v \\
&amp;amp;= \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \text{ (by column-row expansion)} \\
\end{aligned}
\]&lt;/span&gt; Then comes the standard form of linear PCA. Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = \frac{1}{M}\mathcal{X}\mathcal{X}^T\)&lt;/span&gt;, we would like to solve &lt;span class=&#34;math display&#34;&gt;\[
\max_v\quad \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
s.t.\quad v^Tv=1
\]&lt;/span&gt; This is equivalent to solving &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors. Unfortunately, this cannot be directly solved like linear PCA since we don’t have &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. However note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma v &amp;amp;= \lambda v \\
v &amp;amp;= \frac{1}{\lambda}\Sigma v \\
&amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M\phi(x^{(i)})[(\phi(x^{(i)})^Tv] \\
&amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M[(\phi(x^{(i)})^Tv]\phi(x^{(i)})
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some linear combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; and can be written as &lt;span class=&#34;math display&#34;&gt;\[
v = \mathcal{X}\alpha, \text{ where $\alpha$ is $N \times 1$}
\]&lt;/span&gt; Substitute back to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= \lambda\mathcal{X}\alpha \\
\frac{1}{M}\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= \lambda\mathcal{X}^T\mathcal{X}\alpha \\
\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= M\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(K = \mathcal{X}^T\mathcal{X}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is invertible, therefore, &lt;span class=&#34;math display&#34;&gt;\[
KK\alpha = M\lambda K\alpha \\
K\alpha = M\lambda\alpha
\]&lt;/span&gt; We can solve &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvector. Kernel trick can be applied since that &lt;span class=&#34;math inline&#34;&gt;\(K_{ij} = (\mathcal{X}^T\mathcal{X})_{ij} = \phi(x^{(i)})^T\phi(x^{(j)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\lambda\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvalue, which is proportional to &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvalue and thus the objective. So &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s largest &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvalues correspond to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; largest objective respectively. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \alpha_2, ..., \alpha_L\)&lt;/span&gt; has to be solved with constraint that &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i^T\alpha_i = \frac{1}{M\lambda}\)&lt;/span&gt; because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^Tv &amp;amp;= (\mathcal{X}\alpha)^T\mathcal{X}\alpha \\
&amp;amp;= \alpha^T(\mathcal{X}^T\mathcal{X})\alpha \\
&amp;amp;= \alpha^TK\alpha \\
&amp;amp;= M\lambda\alpha^T\alpha \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a sample &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we first transform it with &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and dot-product with &lt;span class=&#34;math inline&#34;&gt;\(v_1,\dots,v_L\)&lt;/span&gt; to get the new coordinates. To get its &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th coordinate, &lt;span class=&#34;math display&#34;&gt;\[
\phi(x^*)^Tv_j = \phi(x^*)^T\mathcal X\alpha_j = [\mathcal K(x^*, x^{(1)}), \mathcal K(x^*, x^{(2)}), \dots, \mathcal K(x^*, x^{(M)})]\alpha_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;centralizing-in-kernel-trick&#34;&gt;Centralizing in Kernel Trick&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde\phi(x^{(i)}) = \phi(x^{(i)}) - \bar\phi(x), \tilde{\mathcal{X}} = [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\)&lt;/span&gt;. We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; is pre-centered by subtracting &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;. However, as said, we didn’t explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, so there is no way to calculate &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt;, let alone &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From another perspective, the objective of pre-centering is to get &lt;span class=&#34;math inline&#34;&gt;\(\tilde K = \tilde{\mathcal{X}}\tilde{\mathcal{X}}^T = \sum_{i=1}^M\tilde\phi(x^{(i)})\tilde\phi(x^{(i)})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{M \times 1}\)&lt;/span&gt; represent a &lt;span class=&#34;math inline&#34;&gt;\(M \times 1\)&lt;/span&gt; column vector with all elements being &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\bar\phi(x) = \frac{1}{M}(\phi(x^{(1)}) + \phi(x^{(2)}) + ... + \phi(x^{(M)})) = \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_M\)&lt;/span&gt; represents a &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt; matrix with all elements being &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt;. Pre-center all data to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde{\mathcal{X}} &amp;amp;= [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\\
&amp;amp;= [\phi(x_1) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, \phi(x^{(2)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, ..., \phi(x^{(M)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}] \\
&amp;amp;= [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})] - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \mathcal{X}\mathbb{1}_M \\
\tilde K &amp;amp;= \tilde{\mathcal{X}}^T\tilde{\mathcal{X}} \\
&amp;amp;= (\mathcal{X} - \mathcal{X}\mathbb{1}_M)^T(\mathcal{X} - \mathcal{X}\mathbb{1}_M) \\
&amp;amp;= (\mathcal{X}(I - \mathbb{1}_M))^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)^T\mathcal{X}^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)K(I - \mathbb{1}_M) \\
\tilde K_{ij} &amp;amp;= (\tilde{\mathcal{X}}^T\tilde{\mathcal{X}})_{ij} \\
&amp;amp;= \tilde\phi(x^{(i)})^T\tilde\phi(x^{(j)})\\
&amp;amp;= (\phi(x^{(i)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1})^T(\phi(x^{(j)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;explained-variance&#34;&gt;Explained Variance&lt;/h3&gt;
&lt;p&gt;It is a question in real application that how many principal components to choose to represent the original data. Explained variance can be a good index on this. We can choose a number of principal components such that they “explains” a certain percentage &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; of the original data.&lt;/p&gt;
&lt;p&gt;Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}v_i^TXX^Tv_i\)&lt;/span&gt; is the directional variance explained along &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;, or the variance of the first dimension of transformed data samples. We may choose the first &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; principal components such that &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^k v_i^TXX^Tv_i \ge \tau \sum_{i=1}^N \sigma_i^2
\]&lt;/span&gt; &lt;a href=&#34;%5B数据降维:%20核主成分分析(Kernel%20PCA)原理解析%20-%20知乎%20(zhihu.com)%5D(https://zhuanlan.zhihu.com/p/59775730)&#34;&gt;Kernel PCA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained&#34;&gt;Explained variance&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eckart-Young-Mirsky Theorem</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</link>
      <pubDate>Fri, 07 Jan 2022 12:40:50 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</guid>
      <description>
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(R \le \min\{M,N\}\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = diag(\sigma_1, \sigma_2, ..., \sigma_R)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star = U\Sigma_K V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_K = diag(\sigma_1, \sigma_2, ..., \sigma_K)\)&lt;/span&gt;, when distance between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as [[Frobenius Normalization|Frobenius norm]]: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||UA||_F = ||AU||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= tr((UA)^TUA) \\
&amp;amp;= tr(A^TUUA) \\
&amp;amp;= tr(A^TA) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||AU||_F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||U\Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^TU\Sigma V^TV - U^TYV||_F^2 \\
&amp;amp;= ||\Sigma - U^TYV||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^TYV\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is also of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||\Sigma - Z||_F^2 \\
&amp;amp;= \sum_{ij}(\Sigma_{ij} - Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^R(\sigma_{i} - Z_{ii})^2 + \sum_{i&amp;gt;R}^{\min\{M,N\}}Z_{ii}^2 + \sum_{i \ne j}Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \sigma_{i}, i = 1, 2, \dots, K \\
Z_{ii} = \sigma_{i}, i = K, K + 1, \dots, R - 1 \\
Z_{ii} = 0, i = R, R + 1, \dots, \min\{M,N\} \\
Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \dots, N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U\Sigma_KV^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Eckart-Young-Mirsky Theorem also holds for [[Spectral Normalization|Spectral norm]] .&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Independent Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</link>
      <pubDate>Sun, 19 Dec 2021 10:16:43 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</guid>
      <description>

&lt;h2 id=&#34;assumption-and-derivation&#34;&gt;Assumption and Derivation&lt;/h2&gt;
&lt;p&gt;Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{x^{(i)}\}_{i=1}^M, x^{(i)} \in R^{N \times 1}\)&lt;/span&gt;, find the &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; mixing matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X = AS\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are the propagation weights from a signal source to observers. Therefore they are considered independent (and are thus called independent components).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is called mixing matrix and &lt;span class=&#34;math inline&#34;&gt;\(W = A^{-1}\)&lt;/span&gt; is called unmixing matrix because &lt;span class=&#34;math inline&#34;&gt;\(S = WX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = U\Sigma V^T\)&lt;/span&gt; by [[Singular Value Decomposition|SVD]]. Assume observations are pre-centered:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^Mx^{(i)} = 0
\]&lt;/span&gt; Assume that signals are independent and the variance of each signal is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (taking the advantage of scale/sign ambiguity): &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I
\]&lt;/span&gt; Then the covariance of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
C &amp;amp;= \frac{1}{M}x^{(i)}(x^{(i)})^T \\
&amp;amp;= \frac{1}{M}As^{(i)}(As^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(U\Sigma V^Ts^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(s^{(i)})^TV\Sigma^T U^T \\
&amp;amp;= U\Sigma V^TIV\Sigma^T U^T \\
&amp;amp;= U\Sigma^2U^T \text{ (by property of SVD, note that $\Sigma$ is $n \times n$ diagonal)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If our assumptions are correct, then the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the stacked eigenvectors of the matrix &lt;span class=&#34;math inline&#34;&gt;\(CC^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; is the diagonal matrix consisting of corresponding eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(C^TC\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(CC^T = C^TC = C^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In other words, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be solved by eigen decomposition. Define &lt;span class=&#34;math inline&#34;&gt;\(\hat x^{(i)} = \Sigma^{-1}U^Tx^{(i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat C &amp;amp;= \frac{1}{M}\hat x^{(i)}(\hat x^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(\Sigma^{-1}U^Tx^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^T(\frac{1}{M}x^{(i)}(x^{(i)})^T)U\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^TU\Sigma^2U^TU\Sigma^{-1} \\
&amp;amp;= I \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = WX = A^{-1}X = V\Sigma^{-1}U^TX = V\hat X
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The remaining job is to find the &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. We resort to multi-information, a generalization of mutual-information from Information Theory to judge how close a distribution is to statistical independence for multiple variables. &lt;span class=&#34;math display&#34;&gt;\[
I(s) = \sum_{s \in \mathcal{S}}p(s)log\frac{p(s)}{\prod_jp(s_j)}
\]&lt;/span&gt; It is a non-negative quantity that reaches the minimum if and only if all variables are statistically independent.&lt;/p&gt;
&lt;p&gt;The multi-information can be written as a function of entropy, which is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(s) = -\sum_{s \in \mathcal{S}}p(s)logp(s)
\]&lt;/span&gt; The multi-information can be written as the difference between the sum of entropies of marginal distribution and the entropy of joint distribution &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(s) &amp;amp;= \sum_jH(s_j) - H(s) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(V\hat x) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log|V|) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log1) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(\hat x) \\
\end{aligned}
\]&lt;/span&gt; Because we are assuming signal are independent from each other, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^\star &amp;amp;= \arg \min_V\sum_jH((V\hat x)_j) - H(\hat x) \\
&amp;amp;= \arg \min_V\sum_jH((V\hat x)_j)
\end{aligned}
\]&lt;/span&gt; Calculating entropy and then taking derivative is no easy task and therefore ICA algorithms focus on approximations or equivalences to the above equation. For example, it can be approximated by finding the rotation that maximizes the expected log-likelihood of the observed data by assuming that the source distributions are known.&lt;/p&gt;
&lt;h3 id=&#34;non-gaussian-and-principal-component-analysispca&#34;&gt;Non-Gaussian and [[Principal Component Analysis|PCA]]&lt;/h3&gt;
&lt;p&gt;The key assumption of ICA is that signals are non-Gaussian. We are trying to recover the &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (by finding &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;) to be as non-Gaussian as possible. &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; consists of independent components because its covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\hat C\)&lt;/span&gt; is shown to be an identity matrix (and thus diagonal). &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is reconstructed by the linear combination of independent components. Thus it will be the most non-Gaussian when each variable is formed by exactly one component of &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; by Central Limit Theorem, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;’s components are independent.&lt;/p&gt;
&lt;p&gt;Consider the case when &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; consists of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent Gaussian, which invalidate the above analysis. If we forcibly calculate &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;, due to the property of multi-variate Gaussian that its isodensity maps are spherical, then any &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix will be a solution to Equation (9), including the left singular matrix of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. the stacked eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, which is exactly the projection basis obtained in PCA. If any &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix can be a solution, there are infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and thus ICA will just fail.&lt;/p&gt;
&lt;p&gt;The conclusion drawn above is based on the ideal case, i.e. many enough observations. Even in the real case where signals are Gaussian, we may get a unique solution to &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html&#34;&gt;ICA&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>RANSAC</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/ransac/</link>
      <pubDate>Wed, 20 Apr 2022 16:38:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/ransac/</guid>
      <description>
&lt;p&gt;Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. &lt;strong&gt;RAN&lt;/strong&gt;dom &lt;strong&gt;SA&lt;/strong&gt;mple &lt;strong&gt;C&lt;/strong&gt;onsensus is a methodology to robustly fit the model in the presence of outliers.&lt;/p&gt;
&lt;p&gt;RANSAC does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Randomly sample a subset of data of an fairly enough amount for training.&lt;/li&gt;
&lt;li&gt;Fit a model to the this subset.&lt;/li&gt;
&lt;li&gt;Determine data points in the whole data set as inliers or outliers by comparing the residuals (prediction errors) to a threshold. The set of inliers is called a consensus set.&lt;/li&gt;
&lt;li&gt;Repeat above for some iterations and retrain the final model with the largest consensus set (since inliers should be the majority).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters of RANSAC include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; - number of points to fit the model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; - threshold of the residual&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; - proportion the outliers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; - probability of success (at least one iteration is finished with no outlier)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; - number of iterations to be determined&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has no outliers)} = (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has at least one outlier)} = 1 - (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T &amp;lt; 1 - \delta \\
T &amp;gt; \log\frac{1 - \delta}{1 - (1 - e)^s}
\end{gather}
\]&lt;/span&gt; The threshold &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is usually set as the median absolute deviation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/xingshansi/p/6763668.html&#34;&gt;随机抽样一致算法（Random sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</link>
      <pubDate>Fri, 07 Jan 2022 13:50:38 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</guid>
      <description>

&lt;h3 id=&#34;two-class&#34;&gt;Two-class&lt;/h3&gt;
&lt;p&gt;Assume we have a set of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-dimensional samples &lt;span class=&#34;math inline&#34;&gt;\(D = \{x^{(i)}\}^M_{i=1}\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; of which belong to Class &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt; to Class &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(M_1 + M_2 = M\)&lt;/span&gt;). We seek to obtain a scalar &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; by projecting &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto a unit vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)} = v^Tx^{(i)}\)&lt;/span&gt;. Of all the possibilities we would like to select the one that maximizes the separability of the scalars between classes.&lt;/p&gt;
&lt;p&gt;The mean of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is &lt;span class=&#34;math display&#34;&gt;\[
\mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} \\
\tilde \mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]v^T x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} = v^T\mu_c\\
\]&lt;/span&gt; The scatter of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is defined as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_c &amp;amp;= \sum_{i=1}^M \mathbb I[y^{(i)} = c](x^{(i)} - \mu_c)(x^{(i)} - \mu_c)^T \\
\tilde s^2_c &amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](z^{(i)} - \tilde u_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)} - u_c) \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} - u_c)^Tv \\
&amp;amp;= v^TS_cv
\end{aligned}
\]&lt;/span&gt; FLD suggests in &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space maximizing the distance among the means of classes (inter-class scatter) and minimizing the variance over each class (within-class scatter), i.e. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_v J(v) &amp;amp;\coloneq \frac{(\tilde \mu_1 - \tilde \mu_2)^2}{\tilde s^2_1 + \tilde s^2_2} \\
&amp;amp;= \frac{(v^T\mu_1 - v^T\mu_2)^2}{v^TS_1v + v^TS_2v} \\
&amp;amp;= \frac{v^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv}{v^T(S_1 + S_2)v} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_W = S_1 + S_2\)&lt;/span&gt; is called within-class scatter, &lt;span class=&#34;math inline&#34;&gt;\(S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T\)&lt;/span&gt; is called inter-class scatter. Then, &lt;span class=&#34;math display&#34;&gt;\[
J(v) = \frac{v^TS_Bv}{v^TS_Wv} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial v} &amp;amp;= v^TS_Wv \frac{\partial v^TS_Bv}{\partial v} - v^TS_Bv \frac{\partial v^TS_Wv}{\partial v}&amp;amp; \\
&amp;amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv  \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv &amp;amp;= 0 \\
S_Bv - \frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv &amp;amp;= 0 \\
S_Bv &amp;amp;= J(v) S_Wv
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_W\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(S^{-1}_WS_B\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_Bv = J(v) v
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_Bv = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv = \alpha(\mu_1 - \mu_2)\)&lt;/span&gt; always points to the same direction as &lt;span class=&#34;math inline&#34;&gt;\((\mu_1 - \mu_2)\)&lt;/span&gt;. Thus we can immediately give a &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and verify: &lt;span class=&#34;math display&#34;&gt;\[
v = S^{-1}_W(\mu_1 - \mu_2)
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_B\underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v = S^{-1}_W\alpha(\mu_1 - \mu_2) = \underbrace{\alpha}_{J(v)} \underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-class&#34;&gt;Multi-class&lt;/h3&gt;


</description>
    </item>
    
    <item>
      <title>Bias-variance Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</guid>
      <description>
&lt;p&gt;The notation used is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Symbol&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Notation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the observation of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;, affected by noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the real value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the mean of the real values&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the model learned with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the expectation of prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l(f(x), y_\mathcal D)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the loss function, chosen to be squared error&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By assuming that the observation errors averages to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, the expectation of the error will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E_{x \sim \mathcal D}&amp;amp;[l(f(x), y_\mathcal D)] \\
&amp;amp;= E[(f(x) - y_\mathcal D)^2] = E\{[(f(x) - \bar f(x) + (\bar f(x) - y_\mathcal D)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y_\mathcal D)^2] + 2E[(f(x) - \bar f(x))(\bar f(x) - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E\{[(\bar f(x) - y) + (y - y_\mathcal D)]^2\} \\
&amp;amp;\quad + 2\underbrace{E[f(x) - \bar f(x)]}_0 E[\bar f(x) - y_\mathcal D] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y)^2] + E[(y - y_\mathcal D)^2] + 2E[(\bar f(x) - y)(y - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;\quad + 2E[\bar f(x) - y] \underbrace{E[y - y_\mathcal D]}_0 \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x) - \bar y)^2] + E[(\bar y - y)^2] + 2E[(\bar f(x) - \bar y)(\bar y - y)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x) - \bar y)^2] + E[(\bar y - y)^2] \\
&amp;amp;\quad + 2E[\bar f(x) - \bar y]\underbrace{E[\bar y - y]}_0 \\
&amp;amp;= \underbrace{E[(f(x) - \bar f(x))^2]}_{variance} + \underbrace{E[(\bar f(x) - \bar y)^2]}_{bias^2} + \underbrace{E[(y - y_\mathcal D)^2]}_{noise} + \underbrace{E[(\bar y - y)^2]}_{scatter} \\
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://medium.com/analytics-vidhya/5-ways-to-achieve-right-balance-of-bias-and-variance-in-ml-model-f734fea6116&#34;&gt;5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
  </channel>
</rss>
