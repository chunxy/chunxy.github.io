<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/</link>
      <atom:link href="https://chunxy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Chunxy&#39; Website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 08 Nov 2022 17:12:01 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Chunxy&#39; Website</title>
      <link>https://chunxy.github.io/</link>
    </image>
    
    <item>
      <title>总览</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</link>
      <pubDate>Tue, 08 Nov 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</guid>
      <description>

&lt;h2 id=&#34;概率论与统计总览&#34;&gt;概率论与统计：总览&lt;/h2&gt;
&lt;p&gt;随机变量其实是一个从事件（概率空间的子集）到数字的映射，是一个抽象事件数字化的过程。概率论和统计的一个共同的主要话题就是随机变量，可以说它们像是随机变量的一体两面。&lt;/p&gt;
&lt;p&gt;概率论更加注重随机变量取值集合相对应事件的概率。为此，概率论需要讨论事件所有可能的试验结果、事件的运算、事件的独立性、随机变量的取值范围、随机变量的概率分布等话题。&lt;/p&gt;
&lt;p&gt;统计中的随机变量来自于对总体的随机抽样，这个过程中，我们会得到一组样本，每个样本在被观测之前，都是服从总体分布的随机变量；观测之后，它们便有了一个具体的观测值。我们可以将观测行为类比概率论中的试验，而这种观测行为将会导致一个随机变量坍缩成为一个具体的观测值。&lt;/p&gt;
&lt;p&gt;可以这样理解概率论与统计：概率论是根据事件总体的自身属性，正向推导所有事件对应概率分布的分布函数；统计是根据某个概率分布（即总体分布）采样得到的结果，反向推导该分布的分布函数。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;概率论&lt;/th&gt;
&lt;th&gt;统计&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;概率空间（事件总体）&lt;/td&gt;
&lt;td&gt;样本空间（总体）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;试验&lt;/td&gt;
&lt;td&gt;样本&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;随机变量的数字特征&lt;/td&gt;
&lt;td&gt;样本的数字特征（统计量）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;渐进理论&lt;/td&gt;
&lt;td&gt;统计推段&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


</description>
    </item>
    
    <item>
      <title>常见分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</link>
      <pubDate>Thu, 11 Aug 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;h2 id=&#34;离散型&#34;&gt;离散型&lt;/h2&gt;
&lt;h3 id=&#34;二项分布binomial-distribution&#34;&gt;二项分布（binomial distribution）&lt;/h3&gt;
&lt;p&gt;如果离散型随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从二项分布，一般记作&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n, p)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(x;n,p) = {n \choose x} p^x (1-p)^{n-x}, x = 0,1,\dots \\
\E[X] = np \\
\Var[X] = np(1-p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;二项分布可以帮助纠正一个生活中很常见的谬误，比如说身高高于两米的人占人类总体的&lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;，那么是否说明随机选取的100个人中一定至少有1个人高于两米呢？记&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为100个人中身高高于两米的人数，显然&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(100, 0.01)\)&lt;/span&gt;，经计算可得&lt;span class=&#34;math inline&#34;&gt;\(P(X=0) \approx 0.366\)&lt;/span&gt;。其实也就意味着，100个人中，能至少看到1个身高高于两米的人的概率其实大约是&lt;span class=&#34;math inline&#34;&gt;\(1-0.366 = 63.4\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;泊松分布poisson-distribution&#34;&gt;泊松分布（Poisson distribution）&lt;/h3&gt;
&lt;p&gt;泊松分布可以看作是某种形式的二项分布取极限而得到： &lt;span class=&#34;math display&#34;&gt;\[
P(X = i; \lambda) = \lim_{n \to \infty} = {n \choose i} (\frac{\lambda}{n})^i (1 - \frac{\lambda}{n})^{n-i}
\]&lt;/span&gt; 将&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} {n \choose i} / n^i = 1 / i!\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} (1 - \frac{\lambda}{n})^{n-i} = e^{-\lambda}\)&lt;/span&gt;代入即可得到泊松分布的分布律。&lt;/p&gt;
&lt;p&gt;一般如果&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n,p)\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;较大、&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;较小、&lt;span class=&#34;math inline&#34;&gt;\(np = \lambda\)&lt;/span&gt;不太大时，&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布接近于泊松分布&lt;span class=&#34;math inline&#34;&gt;\(P(\lambda)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(x;\lambda) = e^{-\lambda} \frac{\lambda^x}{x!}, x = 0,1,\dots \\
\E[X] = \lambda \\
\Var[X] = \lambda
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;伯努利分布bernoulli-distribution&#34;&gt;伯努利分布（Bernoulli distribution）&lt;/h3&gt;
&lt;p&gt;伯努利分布&lt;span class=&#34;math inline&#34;&gt;\(B(1, p)\)&lt;/span&gt;实际上是二项分布中&lt;span class=&#34;math inline&#34;&gt;\(n = 1\)&lt;/span&gt;的一个特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(1;1,p) = p, B(0;1,p) = 1 - p \\
\E[X] = p \\
\Var[X] = p(1 - p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;多项分布multinomial-distribution&#34;&gt;多项分布（multinomial distribution）&lt;/h3&gt;
&lt;p&gt;多项分布其实就是二项分布的推广，不像二项分布，多项分布的取值的是多值的而不是二值的（binary）。假设有&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果，且这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果互相对立、完备穷举（mutually exclusive and collectively exhaustive），此时它们的概率之和为&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，即&lt;span class=&#34;math inline&#34;&gt;\(p_1 + \dots + p_k = 1\)&lt;/span&gt;，多项分布计算的则是这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果分别发生&lt;span class=&#34;math inline&#34;&gt;\(n_1, \dots, n_k\)&lt;/span&gt;次时的概率。令&lt;span class=&#34;math inline&#34;&gt;\(N = n_1 + \dots + n_k, \vec p = [p_1, \dots, p_k], \vec n = [n_1, \dots, n_k]\)&lt;/span&gt;，则： &lt;span class=&#34;math display&#34;&gt;\[
P(\vec n; \vec p, N) = \frac{N!}{n_1! \dots n_k!} p_1^{n_1} \dots p_k^{n_k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;分类分布categorical-distribution&#34;&gt;分类分布（categorical distribution）&lt;/h3&gt;
&lt;p&gt;类似伯努利分布是二项分布&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时的特例，分类分布则是多项分布&lt;span class=&#34;math inline&#34;&gt;\(N=1\)&lt;/span&gt;时的特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(\vec n; \vec p, 1) = \prod_{i=1}^k p_i ^{n_i} \\
\E[X] = \vec p \\
\Var[X] = \vec p (1 - \vec p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;连续型&#34;&gt;连续型&lt;/h2&gt;
&lt;h3 id=&#34;指数分布exponential-distribution&#34;&gt;指数分布（exponential distribution）&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
p(x;\lambda) = \begin{cases}
\lambda e^{-\lambda x}, &amp;amp; x &amp;gt; 0 \\
0, &amp;amp; x \le 0
\end{cases} \\
\E[X] = \lambda^{-1} \\
\Var[X] = \lambda^{-2}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;正态分布normal-distribution&#34;&gt;正态分布（normal distribution）&lt;/h3&gt;
&lt;p&gt;正态分布也叫作高斯分布（Gaussian distribution），一维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}
\]&lt;/span&gt; 二维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p \Big( (x,y); \mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho_{XY} \Big) = \frac{1}{2\pi \sqrt{(\sigma_X^2 \sigma_Y^2 - \rho_{XY}^2)}} e^{-\frac 1 {2(1 - \rho_{XY}^2)} \left(\frac{(x-\mu_X)^2} {\sigma_X^2} - \frac{2\rho_{XY}(x - \mu_X)(y - \mu_Y)} {\sigma_X \sigma_Y} + \frac{(y-\mu_Y)^2} {\sigma_Y^2} \right)}
\]&lt;/span&gt; 高维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(\x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_X|}} e^{-\frac{1}{2} (\x-\mu)^T \Sigma_X^{-1} (\x-\mu)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>椭圆曲线加密算法</title>
      <link>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</link>
      <pubDate>Thu, 23 Jun 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;本文为&lt;a href=&#34;https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/&#34;&gt;Elliptic Curve Cryptography&lt;/a&gt;系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#加密算法分支&#34;&gt;加密算法分支&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线&#34;&gt;椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#群group&#34;&gt;群（Group）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积scalar-multiplication&#34;&gt;标量积（Scalar Multiplication）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看群&#34;&gt;再看群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积与子群&#34;&gt;标量积与子群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve Diffie-Hellman&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve Digital Signature Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看离散对数运算&#34;&gt;再看离散对数运算&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ecc与rsa&#34;&gt;ECC与RSA&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;加密算法分支&#34;&gt;加密算法分支&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于椭圆曲线&lt;/p&gt;
&lt;p&gt;基于椭圆曲线的加密算法包括ECC（Elliptic Curve Cryptography）、ECDH和ECDSA。ECDH与ECDSA是基于ECC发展而来。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于模余运算&lt;/p&gt;
&lt;p&gt;基于模余运算的加密算法包括RSA、DSA、DH以及其他衍生算法。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/h2&gt;
&lt;h3 id=&#34;椭圆曲线&#34;&gt;椭圆曲线&lt;/h3&gt;
&lt;p&gt;一条椭圆曲线就是一组满足&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax + b\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne 0\)&lt;/span&gt;的二维平面点集。&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne 0\)&lt;/span&gt;的条件是为了保证曲线不存在&lt;strong&gt;奇点（singularity）&lt;/strong&gt;；&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax + b\)&lt;/span&gt;又被称作椭圆曲线的&lt;strong&gt;Weierstrass normal form&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;除了这条曲线上的点，我们还需要一个无穷远处的点，我们用&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;这个特殊的符号来表示这个点，所以椭圆曲线更准确的表达式为 &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 椭圆曲线的一条显而易见的性质是，它是关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴对称的。&lt;/p&gt;
&lt;h3 id=&#34;群group&#34;&gt;群（Group）&lt;/h3&gt;
&lt;p&gt;一个集合&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;加上一个二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，若满足以下条件，就构成了数学上的一个&lt;strong&gt;群&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;封闭性（closure）：&lt;span class=&#34;math inline&#34;&gt;\(a \in G, b \in G \to a \oplus b \in G\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;结合律（associativity）：&lt;span class=&#34;math inline&#34;&gt;\((a + b) + c = a + (b + c)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;存在一个单位元（identity element）&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(a + 0 = 0 + a = a\)&lt;/span&gt;，即单位元与任何元素进行运算，不改变该元素的值；&lt;/li&gt;
&lt;li&gt;每个数都存在一个逆元（inverse）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若该群进一步满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;交换律（commutativity）：&lt;span class=&#34;math inline&#34;&gt;\(a + b = b + a\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则称该群为&lt;strong&gt;阿贝尔群（Abelian group）&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/h3&gt;
&lt;p&gt;对于我们定义的椭圆曲线集合，我们&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义无穷远处的&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;为单位元；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义逆元为该点关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴另一侧的对称点；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;如下：&lt;/p&gt;
&lt;p&gt;若一条直线与椭圆曲线的三个交点分别为&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(P \oplus Q \oplus R = 0\)&lt;/span&gt;，我们称这三个点是&lt;strong&gt;对齐的（aligned）&lt;/strong&gt;。在此处我们没有规定三个点之间的顺序，即三个点之间可以任意交换位置，也就是说我们的定义的二元运算是满足交换律的，我们定义的群是一个阿贝尔群。&lt;/p&gt;
&lt;p&gt;给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q = (x_q, y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R = P \oplus Q\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= m^2 - x_P - x_Q \\
y_R &amp;amp;= y_P + m(x_R - x_P) \\
&amp;amp;= y_Q + m(x_R - x_Q)
\end{align}
\]&lt;/span&gt; 其中： &lt;span class=&#34;math display&#34;&gt;\[
m = \begin{cases}
\frac{y_P - y_Q}{x_P - xQ}, &amp;amp; P \ne Q \\
\frac{3x_P^2 + a}{2y_P}, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;标量积scalar-multiplication&#34;&gt;标量积（Scalar Multiplication）&lt;/h3&gt;
&lt;p&gt;给定之前的二元加法运算，我们可以定义出相应的群中元素与标量之间的乘法运算： &lt;span class=&#34;math display&#34;&gt;\[
n P  = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 这样的乘法运算可以在&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log n)\)&lt;/span&gt;时间内完成。&lt;/p&gt;
&lt;h3 id=&#34;对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/h3&gt;
&lt;p&gt;给定&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们可以很高效地完成标量积运算&lt;span class=&#34;math inline&#34;&gt;\(Q = nP\)&lt;/span&gt;；但如果给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们如何计算出对数运算（虽然这里是除法，但是为了和密码学中的标记保持一致，这里使用了对数）&lt;span class=&#34;math inline&#34;&gt;\(n = Q \div P\)&lt;/span&gt;呢？&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/h2&gt;
&lt;h3 id=&#34;有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/h3&gt;
&lt;p&gt;有限域首先是一系列元素的集合，比如说由整数模余某个质数&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;得到的集合（通常表示为&lt;span class=&#34;math inline&#34;&gt;\(\Z/p\)&lt;/span&gt;或&lt;span class=&#34;math inline&#34;&gt;\(\newcommand{F}{\mathbb F} \F_p\)&lt;/span&gt;）；有限域还定义了两种二元运算：加法和乘法，且这两种运算应该满足如下条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在有限域上都是封闭的、满足结合律以及交换律的；&lt;/li&gt;
&lt;li&gt;存在单位元；&lt;/li&gt;
&lt;li&gt;每个元素都存在相应的逆元。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，乘法运算还应该满足分配律（distributive）：&lt;span class=&#34;math inline&#34;&gt;\(x \cdot (y + z) = x \cdot y + x \cdot z\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;包含了从&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(p-1\)&lt;/span&gt;的所有整数，而加法、乘法操作之后要追加模余（除数为&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;）操作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(a + b = 0 \pmod p\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;加法逆元（additive inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=-b, b=-a\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(ab = 1 \pmod o\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;乘法逆元（multiplicative inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=b^{-1},b=a^{-1}\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(xy^{-1}\)&lt;/span&gt;有时也表示为&lt;span class=&#34;math inline&#34;&gt;\(x/y\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元可以通过Extended Euclidean Algorithm，其时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log n)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以证明，&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;也是一个阿贝尔群。&lt;/p&gt;
&lt;h3 id=&#34;有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/h3&gt;
&lt;p&gt;椭圆曲线本身的定义为： &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 加上有限域的限制之后，变为 &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \F^2 | y^2 = x^3 + ax + b \pmod p, 4a^3 + 27b^2 \ne 0 \pmod p, a, b \in \F_p \} \cup \{0\}
\]&lt;/span&gt; 由于有限域的限制，此时所有的点全部出现第一象限。该图像关于&lt;span class=&#34;math inline&#34;&gt;\(y = p / 2\)&lt;/span&gt;对称，因为若&lt;span class=&#34;math inline&#34;&gt;\(y_1 + y_2 = p\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_1^2 &amp;amp;= (p - y_2)^2 \\
&amp;amp;= p^2 - 2py_2 + y_2^2 \\
&amp;amp;= y_2^2 \pmod p
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看群&#34;&gt;再看群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于一个点&lt;span class=&#34;math inline&#34;&gt;\(Q = (x_Q, y_Q)\)&lt;/span&gt;，其逆元&lt;span class=&#34;math inline&#34;&gt;\(-Q\)&lt;/span&gt;定义为&lt;span class=&#34;math inline&#34;&gt;\(-Q = (x_Q, -y_Q \mod p)\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们这样定义有限域上椭圆曲线上的点之间的二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，同之前一样，三个对齐的点（aligned points）&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;满足 &lt;span class=&#34;math display&#34;&gt;\[
P \oplus Q \oplus R = 0
\]&lt;/span&gt; 只不过这里“对齐”的含义与之前有所不同，之前的对齐指的是几何上的共线，即三个点满足&lt;span class=&#34;math inline&#34;&gt;\(ax + by + c = 0\)&lt;/span&gt;；而这里的对齐指的是： &lt;span class=&#34;math display&#34;&gt;\[
ax + by + c = 0 \pmod p
\]&lt;/span&gt; 有趣的是，计算加法的公式和之前没有发生太大变化（&lt;a href=&#34;https://arxiv.org/pdf/1710.00214&#34;&gt;证明&lt;/a&gt;）。给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q = (x_q, y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R = P \oplus Q\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= (m^2 - x_P - x_Q) \mod p \\
y_R &amp;amp;= (y_P + m(x_R - x_P)) \mod p \\
&amp;amp;= (y_Q + m(x_R - x_Q)) \mod p
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math display&#34;&gt;\[
m =
\begin{cases}
(y_P - y_R)(x_P - x_R)^{-1} \mod p, &amp;amp; P \ne Q \\
(3x_P^2 + a)(2y_P)^{-1} \mod p, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;群中元素的个数叫做群的&lt;strong&gt;秩（order）&lt;/strong&gt;，可以通过&lt;a href=&#34;https://en.wikipedia.org/wiki/Schoof%27s_algorithm&#34;&gt;Schoof’s algorithm&lt;/a&gt;计算求得。&lt;/p&gt;
&lt;h3 id=&#34;标量积与子群&#34;&gt;标量积与子群&lt;/h3&gt;
&lt;p&gt;标量积依旧遵循之前的定义，给定正整数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和群中的点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
nP = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 标量积其实就是对某个点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;不断做加法，其中一个有趣的性质是，&lt;span class=&#34;math inline&#34;&gt;\(0P, 1P, 2P, \dots\)&lt;/span&gt;的结果会以某个最小正周期周期&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;循环（&lt;a href=&#34;https://en.wikipedia.org/wiki/Subgroup#Basic_properties_of_subgroups&#34;&gt;证明&lt;/a&gt;）。这也就意味着，群中对加法&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;的倍数是关于加法封闭的（closed under addition），它们又构成了一个循环子群（cyclic subgroup），&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;又称作这个循环子群的&lt;strong&gt;基点（base point/generator）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;是这个&lt;strong&gt;循环子群的秩（subgroup order）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrange%27s_theorem_(group_theory)&#34;&gt;Lagrange’s theorem&lt;/a&gt;，子群的秩是其父群的秩的约数。&lt;/p&gt;
&lt;h4 id=&#34;寻找基点&#34;&gt;寻找基点&lt;/h4&gt;
&lt;p&gt;在ECC算法中，我们一般会先计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，找出它一个比较大的约数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，让&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;作为子群的秩，&lt;span class=&#34;math inline&#34;&gt;\(h = N / n\)&lt;/span&gt;称作这个子群的余因子（cofactor），再根据这个子群的秩去找这个子群的基点。一般来说，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;会从&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子中选取，基本算法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，从大到小排列进行试验：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;计算余因子&lt;span class=&#34;math inline&#34;&gt;\(h = N / n\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;随机选择椭圆曲线上的一点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(G = hP\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，则重新选择&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;进行试验；否则这意味着&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;就是秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的子群的基点。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，ECC算法能够运行的前提是，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;必须是&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子。&lt;/p&gt;
&lt;h3 id=&#34;离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete Logarithm）&lt;/h3&gt;
&lt;p&gt;现在我们解答之前提出的对数运算问题，给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，目前没有算法能够在多项式时间之内求解满足&lt;span class=&#34;math inline&#34;&gt;\(Q = kP\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。这个问题有点类似于给定整数&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，如何求解满足&lt;span class=&#34;math inline&#34;&gt;\(b = a^k \pmod p\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;？这两个问题目前都没有算法能在多项式时间之内求解，这也是ECC算法安全的根本。&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/h2&gt;
&lt;p&gt;寻找到之前秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的子群后，我们就可以生成私钥和公钥了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;私钥是从&lt;span class=&#34;math inline&#34;&gt;\(\{1,\dots,n-1\}\)&lt;/span&gt;中随机抽取的数字&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;公钥是点&lt;span class=&#34;math inline&#34;&gt;\(H = dG\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面介绍两个基于ECC的公钥加密算法。&lt;/p&gt;
&lt;h3 id=&#34;elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve Diffie-Hellman&lt;/h3&gt;
&lt;p&gt;ECDH是DH算法在椭圆曲线中的变体，它实际上是一种密钥交换算法，而不是加密算法。它的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice和Bob各自随机生成私钥和公钥：&lt;span class=&#34;math inline&#34;&gt;\(H_A = d_A G, H_B = d_B G\)&lt;/span&gt;，注意，Alice和Bob使用了相同的基点；&lt;/li&gt;
&lt;li&gt;Alice和Bob在非安全信道上交换各自的公钥，即使中间人拦截到了&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，如果他不能求解出对数运算问题，他也不会知道Alice和Bob的私钥；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_A H_B\)&lt;/span&gt;，Bob计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_B H_A\)&lt;/span&gt;，根据子群对加法的封闭性，二者应该得到相同的结果；&lt;/li&gt;
&lt;li&gt;中间人即使知道&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，也无法得到密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，Alice和Bob便可以通过密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;加密内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve Digital Signature Algorithm&lt;/h3&gt;
&lt;p&gt;ECDSA是一种公钥加密算法，可以用于数字签名。ECDSA的作用对象是消息的哈希值，而不是消息本身，所以在使用ECDSA时，也要选取一个安全的哈希函数。消息的哈希值在签名过程中会被截断，使得该剩余哈希值的比特位数等于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的比特位数，我们用&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;来表示剩余哈希值所代表的整数。ECDSA的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice从&lt;span class=&#34;math inline&#34;&gt;\(\{1, \dots, n\}\)&lt;/span&gt;中随机抽取数字&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(P = kG\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod n\)&lt;/span&gt;，如果&lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + rd_A) \mod n\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(d_A\)&lt;/span&gt;是Alice的私钥，&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;关于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元（我们前面选取&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;为质因子的目的就在于，保证这里的&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;一定存在），如果&lt;span class=&#34;math inline&#34;&gt;\(s=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;元组&lt;span class=&#34;math inline&#34;&gt;\((r,s)\)&lt;/span&gt;就是Alice对应的签名。Bob拿到这样的签名之后，作以下验证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_1 = s^{-1}z \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_2 = s^{-1}r \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算点&lt;span class=&#34;math inline&#34;&gt;\(P = u_1G + u_2H_A\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;仅当&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod n\)&lt;/span&gt;时，Bob可以验证这确实是Alice的签名。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;验证过程的正确性证明如下， &lt;span class=&#34;math display&#34;&gt;\[
\label{P} \begin{aligned}
P &amp;amp;= u_1 G + u_2 H_A \\
&amp;amp;= u_1 G + u_2 d_A G \\
&amp;amp;= (s^{-1} z + s^{-1} r d_A) G \\
&amp;amp;= s^{-1}(z + r d_A) G
\end{aligned}
\]&lt;/span&gt; 之前我们定义&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + r d_A) \mod n\)&lt;/span&gt;，将两边同乘&lt;span class=&#34;math inline&#34;&gt;\(ks^{-1}\)&lt;/span&gt;，我们可以得到&lt;span class=&#34;math inline&#34;&gt;\(k = s^{-1}(z + r d_A) \mod n\)&lt;/span&gt;，将此式代入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{P}\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
P = kG
\]&lt;/span&gt; 这也就是Alice签名过程中得到的&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，证毕。&lt;/p&gt;
&lt;h4 id=&#34;k的选取&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的选取&lt;/h4&gt;
&lt;p&gt;在使用ECDSA时，我们必须注意不能使用相同的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密多份消息，也不能暴露我们选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的方式（比如说随机数生成方式），否则就会有很大的私钥泄露风险。比如说我们用同一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密两份消息，Bob就可以通过这两次签名过程得到&lt;span class=&#34;math inline&#34;&gt;\((r, s_1), (r, s_2)\)&lt;/span&gt;，如果Bob还有额外途径获取两次消息的哈希&lt;span class=&#34;math inline&#34;&gt;\(z_1, z_2\)&lt;/span&gt;，那么： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
s_1 = k^{-1}(z_1 + r d_A), s_2 = k^{-1}(z_2 + r d_A) \to \\
(s_1 - s_2) = k^{-1}(z_1 - z_2) \mod n \to \\
k = (z_1 - z_2)(s_1 - s_2)^{-1}
\end{gather}
\]&lt;/span&gt; 再根据&lt;span class=&#34;math inline&#34;&gt;\(s_1 = k^{-1}(z_1 + r d_A) \mod n\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_A &amp;amp;= r^{-1}(s_1k - z_1) \mod n \\
&amp;amp;= r^{-1}(s_1(z_1 - z_2)(s_1 - s_2)^{-1} - z_1) \mod n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看离散对数运算&#34;&gt;再看离散对数运算&lt;/h3&gt;
&lt;p&gt;给定秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的椭圆曲线子群，以及该子群上的两点&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，离散对数运算求解的是满足&lt;span class=&#34;math inline&#34;&gt;\(Q = xP\)&lt;/span&gt;的整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。我们接下来了解两个求解离散对数运算的算法。&lt;/p&gt;
&lt;h4 id=&#34;baby-step-giant-step&#34;&gt;Baby-step-giant-step&lt;/h4&gt;
&lt;p&gt;首先任意一个整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，都可以写成&lt;span class=&#34;math inline&#34;&gt;\(x = am + b\)&lt;/span&gt;，由&lt;span class=&#34;math inline&#34;&gt;\(a,m,b\)&lt;/span&gt;这三个满足关系的任意整数表示，那么，我们就可以考虑这样解决离散对数运算问题： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q &amp;amp;= xP \\
Q &amp;amp;= (am + b)P \\
Q - amP &amp;amp;= bP
\end{aligned}
\]&lt;/span&gt; Baby-step-giant-step算法采取了从两边夹逼的方式解决问题，过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(m = \lceil \sqrt n \rceil\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots, m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，计算&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，并将&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;存储到哈希表中；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots, m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(Q - amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;检查哈希表中是否存在某个&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(Q - amP = bP\)&lt;/span&gt;，如果存在，就意味着我们找到了一个解。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;的计算对应着baby-step，&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;的计算对应着giant-step，该算法的合理性在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=0\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(0, P, \dots, mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(mP, P + mp, \dots, mP + mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=2\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(2mP, P + 2mp, \dots, mP + 2mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=m-1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\((m-1)mP, P + (m-1)mp, \dots, mP + (m-1)mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，我们检查了&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(m^2P=nP\)&lt;/span&gt;之间的所有点，也就是所有可能的点。而检查的过程我们并不需要做实际的加法运算，只需要检查哈希表中有没有对应的差值。在baby-step中，我们需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加法，在giant-step中，由于哈希表查询速度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;，并且至多需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加减法，所以整体上该算法的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt n)\)&lt;/span&gt;，而哈希表带来的空间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt n)\)&lt;/span&gt;。尽管看上去这个多项式时间的算法还不错，但是由于一般&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;非常大，这个算法实际需要的时间成本以及存储成本远远超出当前计算机的水平。&lt;/p&gt;
&lt;h4 id=&#34;pollards-rho&#34;&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法的时间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt n)\)&lt;/span&gt;，但是它的空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;。和Baby-step-giant-step算法一样，我们实际解决的问题与原问题稍微有所不同，在Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中，给定&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，我们想要找到整数&lt;span class=&#34;math inline&#34;&gt;\(a,b,A,B\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
aP + bQ = AP + BQ
\]&lt;/span&gt; 找到这四个整数之后，我们代入&lt;span class=&#34;math inline&#34;&gt;\(Q = xP\)&lt;/span&gt;来求解&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
aP + bxP &amp;amp;= AP + BxP \\
(a-A)P &amp;amp;= (b-B)xP \\
&amp;amp;\Downarrow \\
(a-A) &amp;amp;= (b-B)x \pmod n \\
x &amp;amp;= (a-A)(b-B)^{-1} \mod n
\end{aligned}
\]&lt;/span&gt; Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法思路是这样的：我们生成一系列伪随机点&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(X_i = a_iP + b_iQ\)&lt;/span&gt;。这样的序列可以由一个伪随机函数&lt;span class=&#34;math inline&#34;&gt;\(f(X_i) = (a_{i+1}, b_{i+1})\)&lt;/span&gt;生成，也就是说下一点是由当前点决定的，而&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部如何工作并不重要。通过这样的&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;产生序列，我们的序列迟早会出现一个回环，也就是说&lt;span class=&#34;math inline&#34;&gt;\(X_j = X_i\)&lt;/span&gt;，而这时我们也就能够找到相应的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。出现回环的原因也很好理解：我们点的个数是有限的，问题其实在于如何找到回环入口。&lt;/p&gt;
&lt;h5 id=&#34;龟兔赛跑&#34;&gt;龟兔赛跑&lt;/h5&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中的回环入口查找，其实类似单向链表中的回环入口查找：在链表开头设置一快一慢两个指针，我们让快指针每次前进两步，慢指针每次前进一步；二者相遇时，从相遇点和起点再设置两个新的慢指针，这两个新的慢指针相遇之处即为环的入口。&lt;/p&gt;
&lt;h4 id=&#34;量子计算shors-algorithm&#34;&gt;量子计算：Shor’s Algorithm&lt;/h4&gt;
&lt;p&gt;理论上，Shor’s Algorithm的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron((\log n)^3)\)&lt;/span&gt;，空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log n)\)&lt;/span&gt;，但是目前的量子计算机还不能进行像Shor’s Algorithm这样复杂的运算。&lt;/p&gt;
&lt;h3 id=&#34;ecc与rsa&#34;&gt;ECC与RSA&lt;/h3&gt;
&lt;p&gt;RSA的密钥长度在数量级上大于ECC的密钥长度，这不仅意味着更多的内存占用，还意味着更慢的计算速度。这其中的原因在于，RSA算法的离散对数运算是快于ECC算法的离散对数运算（参考&lt;a href=&#34;https://en.wikipedia.org/wiki/General_number_field_sieve&#34;&gt;General number field sieve&lt;/a&gt;），这也就意味着RSA算法不得不采用更长的密钥来加大破解难度。更少的内存占用，更快的计算速度，这就是在已经有了成熟的RSA算法的情况下，ECC仍被提出的原因。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Gaussian Distribution</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/</link>
      <pubDate>Sun, 08 May 2022 19:09:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/</guid>
      <description>

&lt;h2 id=&#34;gaussian-distribution&#34;&gt;Gaussian Distribution&lt;/h2&gt;
&lt;h3 id=&#34;one-dimensional&#34;&gt;One-dimensional&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;-d random variable &lt;span class=&#34;math inline&#34;&gt;\(x \sim N(\mu, \sigma^2)\)&lt;/span&gt;, then its density function is &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To verify that it integrates to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} p(x) \d x &amp;amp;= \sqrt{(\int_{-\infty}^{+\infty} p(x) \d x) \cdot (\int_{-\infty}^{+\infty} p(y) \d y)} \\
&amp;amp;= \sqrt {\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x)p(y) \d x \d y} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I^2 = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} p(x)p(y) \d x \d y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-\infty}^{+\infty} p(x) \d x = \sqrt{I^2} \\
I^2 = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x - \mu}{\sigma})^2} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{y - \mu}{\sigma})^2} \d x \d y
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = \frac{x-\mu}{\sigma}, v = \frac{y-\mu}{\sigma}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}u^2}\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}v^2} \d u \d v \\
&amp;amp;= \frac{1}{2\pi} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-\frac{1}{2}(u^2 + v^2)} \d u \d v
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = r \sin \theta, v = r \cos \theta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-\frac{1}{2}(u^2+v^2)} \d u \d v &amp;amp;= 
\int_{0}^{2\pi} \int_{0}^{+\infty} e^{-\frac{1}{2} (r^2 \sin^2\theta + r^2 \cos^2\theta)} r \d r \d\theta \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} -e^{-\frac{1}{2} r^2} \d(-\frac{1}{2} r^2) \d\theta \\
&amp;amp;= \int_{0}^{2\pi}-e^{t}\Big|_{t=0}^{t=-\infty}d\theta \\
&amp;amp;= \int_{0}^{2\pi}d\theta \\
&amp;amp;= 2\pi
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I^2 = \frac{1}{2\pi}2\pi = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\int_{-\infty}^{+\infty}p(x)dx = \sqrt{I^2} = 1\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independent-standard-n-dimensional&#34;&gt;Independent standard &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;, suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i (i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution, we can derive the joint distribution density function for random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; to be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\newcommand{z}{\mathrm{z}}
p(\z) &amp;amp;= p(z_1, z_2, ..., z_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} z_i^2} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2}\z^T\z} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;first-order-correlated-n-dimensional&#34;&gt;First-order correlated &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;We have given the joint distribution function of independent &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional standard Gaussian distribution. What if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; dimensions are not standard, are not independent with each other, but are correlated only in first order?&lt;/p&gt;
&lt;p&gt;Formally, &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are not independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i (i=1,...,n)\)&lt;/span&gt; observes &lt;span class=&#34;math inline&#34;&gt;\(N(\mu_i, \sigma_i^2)\)&lt;/span&gt;. We would like to linearly transform &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X_i, X_j (i,j=1,...,n \and i \ne j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(X_i (i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution.&lt;/p&gt;
&lt;p&gt;Denote &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;’s covariance matrix as &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Z\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Z\)&lt;/span&gt; is &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/&#34;&gt;real-symmetric&lt;/a&gt; and thus can be diagonalized into &lt;span class=&#34;math inline&#34;&gt;\(U\Lambda U^T\)&lt;/span&gt;. There is an invertible transformation matrix &lt;span class=&#34;math inline&#34;&gt;\(B^{-1} = \Lambda^{-\frac{1}{2}} U^T\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
X = B^{-1}(Z - \mu) \sim \mathcal{N}(0, I)
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_X(\x) &amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \x^T\x} 
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is to take on values in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Z}\)&lt;/span&gt;, which is a subset of &lt;span class=&#34;math inline&#34;&gt;\(\R^{n}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{z}{\mathrm{z}} P_Z(Z \in \mathcal{Z}) = \int_\mathcal{Z} p_Z(\z) \d \z
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(Z = f(X) = BX + \mu\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is invertible, the mapping &lt;span class=&#34;math inline&#34;&gt;\(X \to Z\)&lt;/span&gt; is one-to-one, therefore the multivariate &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian transformation&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
J(X \to Z) = B^{-1} \\
\]&lt;/span&gt; with its determinant &lt;span class=&#34;math inline&#34;&gt;\(J = |J(X \to Z)| = |B^{-1}| = |B|^{-1}\)&lt;/span&gt;. Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J| &amp;amp;= \sqrt{|B|^{-1}|B|^{-1}} \\
&amp;amp;= \sqrt{|B|^{-1}|B^T|^{-1}} \\
&amp;amp;= \sqrt{|BB^T|^{-1}} \\
&amp;amp;= |BB^T|^{-\frac{1}{2}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Z (Z \in \mathcal{Z}) &amp;amp;= P_X (X \in f^{-1}(\mathcal{Z})) \\
&amp;amp;= \int_{f^{-1}(S)} p_X(\x) \d \x \\
&amp;amp;= [\int_\mathcal{Z} p_X (f^{-1}(\z)) |J| \d \z]_{\x = f^{-1}(\z)} \\
&amp;amp;= \int_\mathcal{Z} p_X (f^{-1}(\z)) |J| \d \z \\
\int_\mathcal{Z} p_Z (\z) \d \z &amp;amp;= \int_\mathcal{Z} p_X (f^{-1}(\z)) |J| \d \z \\
p_Z (\z) &amp;amp;= p_X (f^{-1}(\z))|J| = p_X (B^{-1} (\z - \mu) |J| \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T(B^{-1})^T B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T (B^T)^{-1} B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T (BB^T)^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}|BB^T|^\frac{1}{2}} e^{-\frac{1}{2} (\z-\mu)^T (BB^T)^{-1} (\z-\mu)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= E[(\z-\mu) (\z-\mu)^T] \\
&amp;amp;= E[B X X^T B^T] \\
&amp;amp;= B E[X X^T] B^T \\
&amp;amp;= B I B^T \\
&amp;amp;= B B^T
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, &lt;span class=&#34;math display&#34;&gt;\[
p_Z(\z) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_Z|}} e^{-\frac{1}{2} (\z-\mu)^T \Sigma_Z^{-1} (\z-\mu)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/40225646&#34;&gt;Integration&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/58987388&#34;&gt;Multi-variate&lt;/a&gt; || &lt;a href=&#34;https://en.wikipedia.org/wiki/Integration_by_substitution&#34;&gt;Change of variable&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/entropy/</link>
      <pubDate>Thu, 28 Apr 2022 22:47:16 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/entropy/</guid>
      <description>

&lt;p&gt;The Entropy of discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (probability mass function) is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p) = -\mathrm{E}_{x \sim p}\log p(x)
\]&lt;/span&gt; The Entropy of continuous distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; (probability density function) is usually similarly defined as &lt;span class=&#34;math display&#34;&gt;\[
H(q) = -\E_{x\sim q}\log q(x) = -\int q(x)\log q(x)\d x
\]&lt;/span&gt; This is actually the &lt;strong&gt;differential entropy&lt;/strong&gt; introduced by Shannon. In fact, it is not a good continuous analog of discrete entropy and it was not rigorously derived. For example, this formula can be negative. Therefore, in the case of entropy, the random variable had better be discrete, despite the wide usage of differential entropy.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;The entropy of a &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(x) = \frac{e^{-\frac 1 2 (x-\mu)^T \Sigma^{-1}(x-\mu)}}{\sqrt{|2\pi\Sigma|}}\)&lt;/span&gt; can be derived as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
H(p) &amp;amp;\triangleq -\int p(x) \log p(x) \d x = -\int p(x) [-\frac 1 2 (x-\mu)^T \Sigma^{-1} (x-\mu) - \frac 1 2 \log |2\pi\Sigma|] \d x \\
&amp;amp;= \frac 1 2 \int p(x) (x-\mu)^T \Sigma^{-1}(x-\mu) \d x + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 \int p(x) x^T \Sigma^{-1} x \d x + \frac 1 2 \int p(x) \mu^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad - \frac 1 2 \int p(x) \mu^T \Sigma^{-1} x \d x - \frac 1 2 \int p(x) x^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= [\frac 1 2 \tr(\Sigma^{-1} \Sigma) + \frac 1 2 \mu^T \Sigma^{-1} \mu] + \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad - \frac 1 2 \mu^T \Sigma^{-1} \mu - \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 n + \frac 1 2 \log |2\pi\Sigma|
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier</title>
      <link>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</link>
      <pubDate>Fri, 07 Jan 2022 13:00:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</guid>
      <description>

&lt;h2 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h2&gt;
&lt;p&gt;The standard form of the Lagrange Multiplier optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\inf f_0(x) \\
s.t.\quad &amp;amp; f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp; h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Denote the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; that satisfies all the requirements in original problem as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X \subseteq \R^n\)&lt;/span&gt;. Then the Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(L:\R^n \times \R^r \times \R^s \mapsto \R\)&lt;/span&gt; (there is an implicit constraint that the variables must reside in the domain of the functions):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^r\lambda_if_i(x) + \sum_{i=1}^s\mu_ih_i(x)
\]&lt;/span&gt; Define the function &lt;span class=&#34;math inline&#34;&gt;\(P: \mathcal X \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu)
\]&lt;/span&gt; The primal problem is &lt;span class=&#34;math display&#34;&gt;\[
\inf_{x \in \mathcal X} P(x) \label{primal}
\]&lt;/span&gt; It is easy to derive &lt;span class=&#34;math inline&#34;&gt;\(P(x) =f_0(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt;. Thus the primal problem is equivalent to the original problem. Denote primal problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;’s optimal value as &lt;span class=&#34;math inline&#34;&gt;\(p^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define the Lagrangian dual function &lt;span class=&#34;math inline&#34;&gt;\(D: \R^r \times \R^s \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
D(\lambda, \mu) = \inf_{x \in \mathcal \R^n}L(x, \lambda, \mu)
\]&lt;/span&gt; The Lagrangian dual problem is &lt;span class=&#34;math display&#34;&gt;\[
\sup_{\lambda,\mu;\lambda_i \ge 0} D(\lambda, \mu) \label{dual}
\]&lt;/span&gt; Denote dual problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;’s optimal value as &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt;. We always have &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;For any feasible &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathcal X, (\lambda, \mu) \in {\R^r}^+ \times \R^s\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu) \ge L(x,\lambda,\mu) \ge \inf_{x&amp;#39; \in \R^n}L(x&amp;#39;, \lambda, \mu) =D (\lambda, \mu)
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt; is called weak duality because it always holds. &lt;span class=&#34;math inline&#34;&gt;\(p^\star = d^\star\)&lt;/span&gt; is called strong duality because it does not hold in general. Assume, though, a strong duality holds, let &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; be the primal optima, &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star, \mu^\star)\)&lt;/span&gt; be the dual optima, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
f_0(x^\star) = D(\lambda^\star, \mu^\star) = \inf_{x \in \mathcal X}L(x,\lambda^\star,\mu^\star) \le L(x^\star,\lambda^\star,\mu^\star) \\
f_0(x^\star) = P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x^\star,\lambda,\mu) \ge L(x^\star,\lambda^\star,\mu^\star) \\
L(x^\star,\lambda^\star,\mu^\star) \le f_0(x^\star) \le L(x^\star,\lambda^\star,\mu^\star)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) = L(x^\star,\lambda^\star,\mu^\star)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the strong duality holds and &lt;span class=&#34;math inline&#34;&gt;\((x,\lambda,\mu)\)&lt;/span&gt; is the optima, they must satisfy the KKT conditions, and we can leverage the KKT conditions to solve the optima and optimal value:&lt;/p&gt;
&lt;h4 id=&#34;karush-kuhn-tucker-conditions&#34;&gt;Karush-Kuhn-Tucker Conditions&lt;/h4&gt;
&lt;p&gt;In standard optimization problem, KKT conditions refer to the below four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;primal constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(f_1(x) \dots,f_r(x) \le 0, h_1(x),\dots,h_s(x) = 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;dual constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\dots,\lambda_r \ge 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;complementary slackness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1f_1(x),\dots,\lambda_rf_r(x) = 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;vanishing gradient&lt;/strong&gt; of Lagrangian w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; : &lt;span class=&#34;math display&#34;&gt;\[
\nabla f_0(x) + \sum_{i=1}^r\lambda_i\nabla f_i(x) + \sum_{i=1}^s\mu_i\nabla h_i(x) = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that solutions satisfying KKT conditions do not imply a strong duality or an optimal point. For a better discussion between strong duality and KKT conditions, please go to &lt;a href=&#34;https://math.stackexchange.com/questions/3616646/question-about-kkt-conditions-and-strong-duality&#34;&gt;this question&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;when-to-apply&#34;&gt;When-to-apply&lt;/h3&gt;
&lt;h4 id=&#34;slater-condition&#34;&gt;Slater Condition&lt;/h4&gt;
&lt;p&gt;Strong duality does not hold generally. But it does hold in &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/convex-optimization/#Standard Form of the Convex Optimization Problem&#34;&gt;standard convex optimization problem&lt;/a&gt;. In such case, KKT conditions are also sufficient for strong duality provided that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is an interior point of the feasible region.&lt;/p&gt;
&lt;h4 id=&#34;general-case&#34;&gt;General Case&lt;/h4&gt;
&lt;p&gt;In cases where we cannot tell strong duality directly, we may still try to apply Lagrangian multiplier to convert the primal problem to the less-constrained dual problem (&lt;span class=&#34;math inline&#34;&gt;\(\lambda \ge 0\)&lt;/span&gt; is much looser than the constraints in the original problem). That is, we solve &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt; first, and then check if &lt;span class=&#34;math inline&#34;&gt;\(d^\star = p^\star\)&lt;/span&gt;. We do so with the following process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;firstly take derivative of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; w.r.t. the unconstrained &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and make it zero (vanishing gradient) to obtain the closed-form expression of &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;find &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star \ge 0\)&lt;/span&gt; (dual constraint) and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt; that maximizes the &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt; and solve &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;finally verify that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; satisfies the constraints (primal constraint and the implied complementary slackness) in the original problem and &lt;span class=&#34;math inline&#34;&gt;\(f_0(x^\star) = d^\star\)&lt;/span&gt; (strong duality).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we can successfully go through the above process, we can still solve the problem with Lagrangian multiplier.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Squares</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</guid>
      <description>
&lt;p&gt;Suppose we are solving the &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt; problem. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; does not always lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. However, we can try to find within &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s column space a vector &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\hat x\)&lt;/span&gt; best approximates &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. By best approximation we mean to minimize the &lt;span class=&#34;math inline&#34;&gt;\(||Ax - b||\)&lt;/span&gt; over all &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The best approximation can be achieved when &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b = \mathop{proj}_{Col(A)}b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of finding a orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, computing &lt;span class=&#34;math inline&#34;&gt;\(\hat b\)&lt;/span&gt; and then solving &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b\)&lt;/span&gt;, we can derive &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in this way: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
(b - \hat b) \perp Col(A) \iff (b - \hat b) \in Nul(A^T)\iff A^T(b - \hat b) = 0 \\
A^T(b - Ax) = 0 \\
A^TAx = A^Tb
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will show that if columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, then the least-square solution &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; is uniquely given by &lt;span class=&#34;math inline&#34;&gt;\((A^TA)^{-1}A^Tb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Firstly, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = Nul(A^TA)\)&lt;/span&gt;. This is because &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = 0 \iff A^TAx = A^T0 = 0 \\
A^TAx = 0 \iff x^TA^TAx = 0 \iff (Ax)^TAx = 0 \iff Ax = 0
\end{gather}
\]&lt;/span&gt; When columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt;, which indicates that equation (1) has the unique solution. Conversely, when &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt;, which indicates that columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eigenvectors and Eigenvalues</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;An eigenvector of a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;strong&gt;nonzero&lt;/strong&gt; vector &lt;span class=&#34;math inline&#34;&gt;\(\rm x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\rm x = \lambda \rm x\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the corresponding eigenvalue. Note that by definition, &lt;span class=&#34;math inline&#34;&gt;\((0, \x)\)&lt;/span&gt; is a pair of eigenvalue and eigenvector of any square matrix for any nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; is just too trivial an eigenvalue that people exclude it from the eigen discussion.&lt;/p&gt;
&lt;h3 id=&#34;similarity&#34;&gt;Similarity&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrices, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is similar to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; if there is an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(PAP^{-1} = B\)&lt;/span&gt;, or equivalently &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}BP = A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are similar, then they have the same characteristic polynomial and hence then the same eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
B - \lambda I = PAP^{-1} - \lambda PP^{-1} = P(A - \lambda I)P^{-1} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(B - \lambda I) = \det(P(A - \lambda I)P^{-1}) \\
&amp;amp;= \det(P) \cdot \det(A - \lambda I) \cdot \det(P^{-1}) \\
&amp;amp;= \det(A - \lambda I)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independence-between-eigenvectors&#34;&gt;Independence between Eigenvectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are the eigenvectors corresponding to the distinct eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, ..., \lambda_r\)&lt;/span&gt; of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_r\)&lt;/span&gt; are linearly independent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;Suppose instead these vectors are dependent. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the least index such that &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is a linear combination of previous vectors. Then there exists scalars &lt;span class=&#34;math inline&#34;&gt;\(c_1, c_2, ..., c_p\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
  c_1v_1 + c_2v_2 + \cdots + c_pv_p = v_{p+1} \label{lincom} 
  \]&lt;/span&gt; Multiply both sides with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to obtain &lt;span class=&#34;math display&#34;&gt;\[
  c_1\lambda_1v_1 + c_2\lambda_2v_2 + \cdots + c_p\lambda_pv_p = \lambda_{p+1}v_{p+1} \label{eq1}
  \]&lt;/span&gt; Multiply both sides of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{p+1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  c_1\lambda_{p+1}v_1 + c_2\lambda_{p+1}v_2 + \cdots + c_p\lambda_{p+1}v_p = \lambda_{p+1}v_{p+1} \label{eq2}
  \]&lt;/span&gt; Subtract &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  c_1(\lambda_1 - \lambda_{p+1})v_1 + c_2(\lambda_2 - \lambda_{p+1})v_2 + \cdots + c_p(\lambda_p - \lambda_{p+1})v_p = 0 \label{diff} 
  \]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_p\)&lt;/span&gt; are independent, the weights in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; are all zeros. None of &lt;span class=&#34;math inline&#34;&gt;\((\lambda_i - \lambda_{p+1})\)&lt;/span&gt; is zero, so &lt;span class=&#34;math inline&#34;&gt;\(c_i = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; says &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which is impossible. Q.E.D.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, as the eigenvector, is nonzero so that the conclusion can hold even for &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagonalization&#34;&gt;Diagonalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has n independent eigenvectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(AP = PD\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\p}{\mathrm{p}} P = [\p_1, \p_2, ..., \p_n]\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix, &lt;span class=&#34;math display&#34;&gt;\[
AP = [A\p_1, A\p_2, ..., A\p_n] = [D_{11}\ p_1, D_{22}\p_2, ..., D_{nn}\p_n]
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(\p_i\)&lt;/span&gt;’s are independent, which indicates that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;’s are &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors.&lt;/p&gt;
&lt;p&gt;From this, we could also see that if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, then &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; must be the matrix stacked with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; must be the diagonal matrix filled with corresponding eigenvalues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Easy to show.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;linear-transformation&#34;&gt;Linear Transformation&lt;/h4&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then the transformation &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x \mapsto A\mathrm x\)&lt;/span&gt; in coordinate system formed by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s columns can be viewed as &lt;span class=&#34;math inline&#34;&gt;\([\mathrm x]_{\cal B} \mapsto D[\mathrm x]_\cal B\)&lt;/span&gt; in coordinate system &lt;span class=&#34;math inline&#34;&gt;\(\cal B\)&lt;/span&gt;, which is formed by &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;’s columns.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-rank-trace-and-determinant&#34;&gt;Eigenvalues: Rank, Trace and Determinant&lt;/h3&gt;
&lt;p&gt;Since the characteristic equation of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix involves a polynomial of degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the equation always has exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; roots, counting multiplicities, including complex roots. There are some relations between eigenvalues and matrix’s rank, trace and determinant: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\rank = \text{the number of (non-zero) real eigenvalues, including multiplicities} \\
\tr = \text{sum of the eigenvalues} \\
\det = \text{product of the eigenvalues}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Eigen%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Eigen Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Machine Learning Bullet Points</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</link>
      <pubDate>Sun, 19 Dec 2021 13:59:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</guid>
      <description>

&lt;p&gt;This post lists out various topics under the machine learning subject.&lt;/p&gt;
&lt;h3 id=&#34;datafeature&#34;&gt;Data/Feature&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;data speciation
&lt;ul&gt;
&lt;li&gt;numbers&lt;/li&gt;
&lt;li&gt;texts&lt;/li&gt;
&lt;li&gt;images&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;audios&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data sampling
&lt;ul&gt;
&lt;li&gt;bootstrap aggregation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data cleaning&lt;/li&gt;
&lt;li&gt;imbalanced data&lt;/li&gt;
&lt;li&gt;data augmentation&lt;/li&gt;
&lt;li&gt;data splitting&lt;/li&gt;
&lt;li&gt;feature extraction/engineering
&lt;ul&gt;
&lt;li&gt;domain expertise&lt;/li&gt;
&lt;li&gt;statistics&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;feature normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;supervised vs. unsupervised
&lt;ul&gt;
&lt;li&gt;unsupervised learning
&lt;ul&gt;
&lt;li&gt;discovers inherent properties (latent variables) in the data&lt;/li&gt;
&lt;li&gt;includes:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/clustering/&#34;&gt;clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/dimension-reduction/&#34;&gt;dimension reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;manifold embedding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;discriminative vs. generative&lt;/li&gt;
&lt;li&gt;classification vs. regression
&lt;ul&gt;
&lt;li&gt;from classification model to regression model&lt;/li&gt;
&lt;li&gt;from binary classification to multi-class classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;linear vs. non-linear&lt;/li&gt;
&lt;li&gt;parametric vs. non-parametric&lt;/li&gt;
&lt;li&gt;boosting method
&lt;ul&gt;
&lt;li&gt;ensemble method&lt;/li&gt;
&lt;li&gt;Adaboost&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regularization and overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;training criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean squared error&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;cross entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;impurity
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/entropy/&#34;&gt;entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;testing criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;confusion matrix&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Real/Pred&lt;/th&gt;
&lt;th&gt;Positive&lt;/th&gt;
&lt;th&gt;Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Positive&lt;/td&gt;
&lt;td&gt;TP&lt;/td&gt;
&lt;td&gt;FN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Negative&lt;/td&gt;
&lt;td&gt;FP&lt;/td&gt;
&lt;td&gt;TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;F-score&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>单源最短路径问题</title>
      <link>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 21 Jun 2021 09:33:12 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。&lt;/p&gt;
&lt;p&gt;当然，减少严谨的符号运算，并不意味着完全不出现符号，因为算法本身就是对问题的抽象，剥掉这层抽象，就没办法进行架构在抽象之上的信息传递了。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#问题描述及定义&#34;&gt;问题描述及定义&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决思路&#34;&gt;解决思路&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#正文之前&#34;&gt;正文之前&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#负权边&#34;&gt;负权边&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#负权环路&#34;&gt;负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#非负权环路&#34;&gt;非负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#放缩操作&#34;&gt;放缩操作&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决方法&#34;&gt;解决方法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#dijkstra算法&#34;&gt;Dijkstra算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对比&#34;&gt;对比&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;&lt;img src=&#34;sssp.png&#34;/&gt;&lt;/p&gt;
&lt;h2 id=&#34;问题描述及定义&#34;&gt;问题描述及定义&lt;/h2&gt;
&lt;p&gt;单源最短路径问题，旨在求解&lt;strong&gt;带权有向图&lt;/strong&gt;（weighted directed graph）中&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;，从某个&lt;strong&gt;点&lt;/strong&gt;（vertex）出发，到图中任意一&lt;strong&gt;点&lt;/strong&gt;的最短距离，某些情况下，还需要找出这一条最短距离的&lt;strong&gt;路径&lt;/strong&gt;，称之为&lt;strong&gt;最短路径&lt;/strong&gt;，若无特殊指明且不致歧义，以下最短路径问题均指代单源最短路径问题。&lt;/p&gt;
&lt;p&gt;更严格一些，设&lt;span class=&#34;math inline&#34;&gt;\(G(V, E)\)&lt;/span&gt;表示带权有向图，&lt;span class=&#34;math inline&#34;&gt;\(w : E \to \mathbb{R}\)&lt;/span&gt;表示&lt;strong&gt;权重&lt;/strong&gt;，&lt;strong&gt;路径&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(p = \left &amp;lt;v_0, v_1, ... v_k\right &amp;gt;\)&lt;/span&gt;的&lt;strong&gt;距离&lt;/strong&gt;定义为： &lt;span class=&#34;math display&#34;&gt;\[
W(p) = \sum\limits_{i = 1}^k w(v_{k-1}, v_k)
\]&lt;/span&gt; 其中， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\forall i \in [0, k], v_i \in V \\
\forall i \in [1, k], (v_{i-1}, v_i) \in E
\end{gather*}
\]&lt;/span&gt; 我们从某一点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;（叫作&lt;strong&gt;起点&lt;/strong&gt;，或者源点）出发，记其到图中任意一&lt;strong&gt;点&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径距离为&lt;span class=&#34;math inline&#34;&gt;\(\delta(v)\)&lt;/span&gt;，&lt;strong&gt;单源最短路径&lt;/strong&gt;求解的就是任意一条从&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(W(p) = \delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;为了方便，我们为每一个点&lt;span class=&#34;math inline&#34;&gt;\(v \in V\)&lt;/span&gt;设立一个中间变量&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;，用&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;来表示求解过程中的最短距离的&lt;strong&gt;可行上界&lt;/strong&gt;，也就是说始终有&lt;span class=&#34;math inline&#34;&gt;\(\delta(v) \leq v.d\)&lt;/span&gt;，算法初始化时，&lt;span class=&#34;math inline&#34;&gt;\(v.d = +\infty\)&lt;/span&gt;，算法运行过程中，我们通过寻找路径使&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;这个上界不断减小，直到&lt;span class=&#34;math inline&#34;&gt;\(v.d = \delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;解决思路&#34;&gt;解决思路&lt;/h2&gt;
&lt;p&gt;最短路径问题（包括多源最短路径问题）都隐含着一个最优子结构（optimal substructure），即：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;是一条连接两个点的最短路径，那么&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的任意一条&lt;strong&gt;子路径&lt;/strong&gt;，一定也是连接其两个端点的最短路径。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这条性质可由反证法轻松得到，也将是后续寻找最短路径所需要理解的一个重要概念。&lt;/p&gt;
&lt;h2 id=&#34;正文之前&#34;&gt;正文之前&lt;/h2&gt;
&lt;h3 id=&#34;负权边&#34;&gt;负权边&lt;/h3&gt;
&lt;p&gt;负权边指的是图中某些边的权重为负。虽然负权边不会对最短路径的最优子结构性质产生任何影响，但是后面我们会看到，负权边会导致Dijkstra算法失效。&lt;/p&gt;
&lt;h3 id=&#34;负权环路&#34;&gt;负权环路&lt;/h3&gt;
&lt;p&gt;负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;（loop），并且这条环路上的所有边的权重相加结果为负。&lt;/p&gt;
&lt;p&gt;一旦从起点可以到达这个环路上的点，那么最短路径问题就变得没有意义了：我们可以不断重复地走这条环路，然后 “拐出” 环路，到达目标点，使得到达目标点的路径的权重变得任意小（arbitrarily short），所以也就不存在什么 “最短路径” 了。&lt;/p&gt;
&lt;p&gt;一个成熟的算法应当能够检测出图中是否有可以由&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到达的负权环路，如果没有，则算法照常进行；如果有，应予以通报。&lt;/p&gt;
&lt;h3 id=&#34;非负权环路&#34;&gt;非负权环路&lt;/h3&gt;
&lt;p&gt;非负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;，并且这条环路上的所有边的权重相加的结果大于等于0。&lt;/p&gt;
&lt;p&gt;负权环路会使最短路径问题没有意义，那么非负权环路呢？或者说，最短路径是否包含非负权环路呢？&lt;/p&gt;
&lt;p&gt;答案是否，如果一条最短路径包含了非负权环路，我们大可将这段环路从路径中 “拿掉”，得到的路径和原路径可以达到同样的终点，并且新路径的权重不大于原路径的权重。&lt;/p&gt;
&lt;h3 id=&#34;放缩操作&#34;&gt;放缩操作&lt;/h3&gt;
&lt;p&gt;放缩操作的对象是边，对于边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;，放缩操作将检测能否优化点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界：&lt;/p&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;RELAX(u, v, w):
if v.d &amp;gt; u.d + w(u, v):
    v.d = u.d + w(u, v)
    v.predecessor = u&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即如果路径&lt;span class=&#34;math inline&#34;&gt;\(s \sim u \to v\)&lt;/span&gt;的长度小于当前&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，我们便可以借此优化&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，并同时通过将的&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;前继设为&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;来记录这一次优化。&lt;/p&gt;
&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;
&lt;h3 id=&#34;bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/h3&gt;
&lt;p&gt;Bellman-Ford算法是最短路径问题中最为robust的一种了，能处理负权边、能检测负权环路、不要求当前图为有向无环图（directed acyclic graph）。Bellman-Ford算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原图中存在可由起点抵达的负权环路，返回 false，用以告知存在负权环路，最短路径问题无意义&lt;/li&gt;
&lt;li&gt;原图中不存在可由起点抵达的负权环路，返回 true，用以告知最短路径问题已解决，并将结果蕴含在相应的数据结构中&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;// 算法主体
for i = 1 to |V| - 1
    for each edge (u, v) in E
        RELAX(u, v, w)

// 检测是否存在负权回路
for each edge (u, v) in E
    if v.d &amp;gt; u.d + w(u, v)
        return false

return true&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;算法主体&lt;/p&gt;
&lt;p&gt;我们不妨先假设原图中不存在负权环路，先思考Bellman-Ford在解决最短路径问题时的正确性。&lt;/p&gt;
&lt;p&gt;根据以上的讨论，任意一点的最短路径中不存在环，故任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径最多由&lt;span class=&#34;math inline&#34;&gt;\(|V|-1\)&lt;/span&gt;条边，&lt;span class=&#34;math inline&#34;&gt;\(|V|\)&lt;/span&gt;个点构成。设： &lt;span class=&#34;math display&#34;&gt;\[
p=\left &amp;lt;v_0,v_1, ... v_k\right &amp;gt;，其中v_0 = s，v_k = t
\]&lt;/span&gt; 在寻找&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径时，任一&lt;span class=&#34;math inline&#34;&gt;\(v \in \{u | (u,t) \in E, u.d = \delta(s, u)\}\)&lt;/span&gt;（即此时&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径已找到，且点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;有一条连向点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的边） ，都是&lt;span class=&#34;math inline&#34;&gt;\(v_{k-1}\)&lt;/span&gt;（也就是&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在其最短路径中的前继）的一个候选，我们需要证明的是，&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的实际前继能够在Bellman-Ford算法运行之下被发现，从而被真正地选为&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的前继。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bellman.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;根据前面的讨论，路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的前缀&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right &amp;gt;, \left &amp;lt; v_0, v_1, v_2\right &amp;gt;...\)&lt;/span&gt;分别是&lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ...\)&lt;/span&gt;的最短路径，在外侧第一轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_0, v_1)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_1.d = \delta(v_1)\)&lt;/span&gt;且将不再变化（因为这已经是最小）；在外侧第二轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_1, v_2)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1, v_2\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_2.d = \delta(v_2)\)&lt;/span&gt;且将不再变化（因为这已经是最小）&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ... v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;负权回路检测&lt;/p&gt;
&lt;p&gt;至于负权回路检测部分的正确性，则不得不引入一些公式，但其实并不复杂。&lt;/p&gt;
&lt;p&gt;假设原图存在可由到达的负权回路&lt;span class=&#34;math inline&#34;&gt;\(c = \left &amp;lt;v_0, v_1, ... v_k\right &amp;gt;, v_0 = v_k\)&lt;/span&gt;，其中，&lt;span class=&#34;math inline&#34;&gt;\(W(c) = \sum_{i=0}^{k-1}w(v_i, v_{i+1}) &amp;lt; 0\)&lt;/span&gt;。运用反证法，即假设最终不存在点&lt;span class=&#34;math inline&#34;&gt;\(u,v\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(v.d &amp;gt; u.d + w(u, v)\)&lt;/span&gt;，则有： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_i.d &amp;amp;\leq v_{i-1}.d + w(v_{i-1}, v_i), \forall i \in [1, k] \Rightarrow  \\
\sum_{i=1}^k v_i.d &amp;amp;\leq \sum_{i=1}^k (v_{i-1}.d + w(v_{i-1},v_i)) \\
v_k.d + \sum_{i=1}^{k-1} v_i.d &amp;amp;\leq v_0.d + \sum_{i=1}^{k-1} v_{i}.d + \sum_{i=1}^k w(v_{i-1},v_i) \\
0 &amp;amp;\leq \sum_{i=1}^k w(v_{i-1},v_i) = W(c)
\end{aligned}
\]&lt;/span&gt; 与&lt;span class=&#34;math inline&#34;&gt;\(W(c) &amp;lt; 0\)&lt;/span&gt;矛盾，故得证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dijkstra算法&#34;&gt;Dijkstra算法&lt;/h3&gt;
&lt;p&gt;Dijkstra算法相对于Bellman-Ford算法来说，可以在时间复杂度上有所优化，但是能够处理的情形也就少了一些：Dijkstra算法不能处理负权边（所以更不用提负权环路了）。Dijkstra算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;维持一个点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;由最短路径已确定的点构成；&lt;/li&gt;
&lt;li&gt;不断向中加入能够确定最短路径的点，直到所有中的点都被加入。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;S = {}
Q = G.V
while Q is not empty
    u = EXTRACT-MIN(Q)
    add u to S
    for each v in G.adj[u]
        RELAX(u, v, w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然，难点在于如何根据&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;找出能够确定最短路径的点。寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dijkstra.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in S\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\((u, t) \in E\)&lt;/span&gt;，对&lt;span class=&#34;math inline&#34;&gt;\((u, t)\)&lt;/span&gt;进行放缩后得到的值&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;，都是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的一个备选，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中每加入一个点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;（非&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的点），若边&lt;span class=&#34;math inline&#34;&gt;\((v,t)\)&lt;/span&gt;存在，对该边进行放缩后，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选（也就是放缩后的&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;）就会多一个，而&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;自然是这些备选中最小的那个。而当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径确定后，便可以将&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;不断扩展，直至最终包含整个点集&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;，也就是所有点的最短路径都被找到。&lt;/p&gt;
&lt;p&gt;之前提到过，Dijkstra算法不能处理负权边的情况，但上述 Dijkstra算法的讨论中似乎也没有涉及到负权边，为什么它就不能处理了呢？并且，我们只知道放缩后&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选，那么对&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的放缩要进行到什么时候，才能确认&lt;span class=&#34;math inline&#34;&gt;\(t.d=\delta(t)\)&lt;/span&gt;呢？Dijkstra算法告诉我们，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(t.d \leq u.d\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t) = t.d\)&lt;/span&gt;，也就是当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界小于所有待确认点&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;的上界时，我们就能确定&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径，也就能够将点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中。&lt;/p&gt;
&lt;p&gt;为什么？如果没有负权边，我们可以会发现，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，其上界&lt;span class=&#34;math inline&#34;&gt;\(u.d\)&lt;/span&gt;总是由放缩操作得到的，所以在算法运行过程中，它必然是单调递减的，而且它代表了一条具体的到达&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的路径。但为什么&lt;span class=&#34;math inline&#34;&gt;\(V - S\)&lt;/span&gt;中的所有点的上界的最小值，却能够成为某个特定点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径呢？&lt;/p&gt;
&lt;p&gt;我们来看看&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为最小上界的之前之后都发生了什么，换言之，在此之前，或者在此之后，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;有没有可能更小？之前是不会更小了，因为我们说过，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;是单调递减的；那么之后呢？如果在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;中的最小上界、从而被加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;之后，我们在新的某一轮中选取另外一个点&lt;span class=&#34;math inline&#34;&gt;\(u \in V - S\)&lt;/span&gt;，作为此轮加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的点，在随后的操作中&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;会不会因为某个由&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;出发的放缩操作继续减小呢？&lt;/p&gt;
&lt;p&gt;不会的，对于某个新加入的点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\delta(u)\)&lt;/span&gt;必然不小于任何一个&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点的最短距离。运用数学归纳法，假设某一时刻&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;按加入顺序排列为&lt;span class=&#34;math inline&#34;&gt;\(\{ u_0, \dots, u_k \}\)&lt;/span&gt;，且有&lt;span class=&#34;math inline&#34;&gt;\(u_0.d \le \dots \le u_k.d\)&lt;/span&gt;。若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;“引荐”进来（也就是说进行了&lt;span class=&#34;math inline&#34;&gt;\(RELAX(u_k, u, w)\)&lt;/span&gt;操作）的，则必有&lt;span class=&#34;math inline&#34;&gt;\(u.d = u_k.d + w(u_k, u) \ge u_k.d\)&lt;/span&gt;；而若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由非&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;的某个&lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;引荐而来，则也必然应该有&lt;span class=&#34;math inline&#34;&gt;\(u.d \ge u_k.d\)&lt;/span&gt;，运用反证法：如果&lt;span class=&#34;math inline&#34;&gt;\(u.d &amp;lt; u_k.d\)&lt;/span&gt;，则在我们的算法中，&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;至少应该在选择&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;加入的一轮中，因优于&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;被加入，和&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的当前情况矛盾，从而得证。&lt;/p&gt;
&lt;p&gt;而如果有负权边，则不能保证在后续加入的点的最短距离单调递增，故不能用以上论证来证明Dijkstra算法的正确性了。何况，这种情况下，强行使用Dijkstra算法，很轻易地就能举出反例来证明结果的错误，比如对下图以&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;为起点应用Dijkstra算法，就会得到错误结果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;counterexample.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;当然我们也得证明，每一轮加入新点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(t.d = \delta(t)\)&lt;/span&gt;，因为此时虽然&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;达到整个算法流程中的最小，但这个最小值尚未被证明等于&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;。但正如前面所说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们可以使用数学归纳法，来证明&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;始终是一个最短路径已经确定的点的集合，也就是说，假设此时&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;是一个最短路径已经确定的点的集合，加入&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;后，&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;依然保持它的性质。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;前继的候选无非就是图中所有点，我们已经证明，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;不可能再由于&lt;span class=&#34;math inline&#34;&gt;\(V-S\)&lt;/span&gt;中的点放缩而变小了，所以&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在最短路径中的前继只可能来自于&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正是由&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点放缩而来，故&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正式所有可能中最小的那一个，证毕。&lt;/p&gt;
&lt;h3 id=&#34;有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/h3&gt;
&lt;p&gt;有向无环图，从定义上就排除了负权环路存在的可能，故所有点的最短路径必然存在，问题就在于如何寻找到这些最短路径。&lt;/p&gt;
&lt;p&gt;我们当不能对其直接应用Dijkstra算法，因为有向无环图并不排除负权边存在的可能——那就直接用Bellman-Ford算法咯？&lt;/p&gt;
&lt;p&gt;也不尽然。有向无环图显然只是Bellman-Ford算法能够处理的情况中的一小部分，并且这一小部分具有一些特殊的性质：无环。设任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径为&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1, ... v_k\right &amp;gt;\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(v_0 = s，v_k = t\)&lt;/span&gt;，既然不存在环路，从起点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;出发，只要沿着边走，一步一步放缩，必然是先放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right &amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_1.d = \delta(v_1)\)&lt;/span&gt;；然后放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_1, v_2\right &amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_2.d = \delta(v_2)\)&lt;/span&gt;……如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便按顺序寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ... v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;
&lt;p&gt;“沿着边走” 有一个专业名词，叫做按&lt;strong&gt;拓扑顺序&lt;/strong&gt;遍历。事实上，我们放缩过边&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1}, v_i)\)&lt;/span&gt;之后，并不一定要马上放缩边&lt;span class=&#34;math inline&#34;&gt;\((v_i, v_{i+1})\)&lt;/span&gt;，只要我们能够保证&lt;span class=&#34;math inline&#34;&gt;\((v_i, v_{i+1})\)&lt;/span&gt;一定在&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1}, v_i)\)&lt;/span&gt;之后放缩即可，至于中间是否穿插其他边的放缩操作，都无所谓。而拓扑顺序正是满足上述性质的一组顺序，为了得到一组拓扑顺序，我们需要对原图进行&lt;strong&gt;拓扑排序&lt;/strong&gt;，然后按照得到的拓扑顺序进行放缩。&lt;/p&gt;
&lt;p&gt;如何进行拓扑排序呢？实际很简单，首先对原图进行&lt;strong&gt;深度优先遍历&lt;/strong&gt;（还有一种基于&lt;strong&gt;入度&lt;/strong&gt;的拓扑排序，此处不表），将完成遍历的点依次插入队列的首部，便可得到按照拓扑顺序排列的一个队列，拓扑顺序的实际意义是，如果边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;存在，那么对点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的访问必须先于对点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的访问。我们每次从队列中取出一个点，并对从其出发的所有边进行放缩操作即可。虽说拓扑顺序是对点的一个排序，但从该点出发的边和这个点是关联的，所以，先访问点，也就相当于先访问从这个点出发的边了，我们先&lt;span class=&#34;math inline&#34;&gt;\(v_{i}\)&lt;/span&gt;访问，也就必然先于&lt;span class=&#34;math inline&#34;&gt;\((v_i,v_{i+1})\)&lt;/span&gt;放缩&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},v_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;对比&#34;&gt;对比&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;条目&lt;/th&gt;
&lt;th&gt;Bellman-Ford&lt;/th&gt;
&lt;th&gt;Dijkstra&lt;/th&gt;
&lt;th&gt;DAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;复杂度&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|E| \log |V|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| + |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;条件&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;无负权边&lt;/td&gt;
&lt;td&gt;有向无环&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;无向图可以很便捷的转换为带权有向图。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>Limit Computing Tricks</title>
      <link>https://chunxy.github.io/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/limit-computing-tricks/</link>
      <pubDate>Thu, 14 Jul 2022 13:42:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/limit-computing-tricks/</guid>
      <description>

&lt;h2 id=&#34;composited-exponential-functions&#34;&gt;Composited Exponential Functions&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x) = A, \lim_{x \to x_0}g(x) = B\)&lt;/span&gt;, then suppose &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x)^{g(x)}\)&lt;/span&gt; exists, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln(\lim_{x \to x_0}f(x)^{g(x)}) &amp;amp;= \lim_{x \to x_0}\ln(f(x)^{g(x)}) \\
&amp;amp;= \lim_{x \to x_0}g(x)\ln(f(x)) \\
&amp;amp;= \lim_{x \to x_0}g(x) \cdot \lim_{x \to x_0}\ln(f(x)) \\
&amp;amp;= B\ln\lim_{x \to x_0}f(x) \\
&amp;amp;= B\ln A \\
\lim_{x \to x_0}f(x)^{g(x)} &amp;amp;= e^{B\ln A} \\
&amp;amp;= (e^{\ln A})^B \\
&amp;amp;= A^B
\end{aligned}
\]&lt;/span&gt; Above deduction is not rigid, but if you want the result only, it is enough.&lt;/p&gt;
&lt;h2 id=&#34;composite-fractional-functions&#34;&gt;Composite Fractional Functions&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0}f(x) = A, \lim_{x \to x_0}g(x) = B\)&lt;/span&gt;, then suppose &lt;span class=&#34;math inline&#34;&gt;\(\lim_{x \to x_0} \frac {f(x)}{g(x)}\)&lt;/span&gt; exists, then, &lt;span class=&#34;math display&#34;&gt;\[
\lim_{x \to x_0} \frac {f(x)}{g(x)} = \frac A B
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;beta-function&#34;&gt;Beta-function&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y^{p-1} (1-y)^{q-1} = y^{p-1} \sum_{i=0}^{q-1} {q-1 \choose i} 1 ^{i} (-y)^{q-1-i} \\
= \sum_{i=0}^{q-1} {q-1 \choose i} 1 ^{i} (-y)^{q-1-i} \\
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Source Coding Theory</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</guid>
      <description>

&lt;h2 id=&#34;notations-and-concepts&#34;&gt;Notations and Concepts&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is a triple &lt;span class=&#34;math inline&#34;&gt;\((X, \newcommand{A}{\mathcal A} \newcommand{P}{\mathcal P} \A_X, \P_X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the outcome of a random variable, which takes on values from &lt;span class=&#34;math inline&#34;&gt;\(\A_X = \{a_1, a_1, \dots \}\)&lt;/span&gt;, that has probability &lt;span class=&#34;math inline&#34;&gt;\(\P_X = \{p_1, p_2, \dots \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The raw bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_0(X) = \log |\mathcal A_X|
\]&lt;/span&gt; &lt;strong&gt;The smallest &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_\delta\)&lt;/span&gt; is the smallest subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal A_X\)&lt;/span&gt; satisfying &lt;span class=&#34;math display&#34;&gt;\[
P(X \in S_\delta) \ge 1 - \delta
\]&lt;/span&gt; &lt;strong&gt;The essential bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X) = \log |S_\delta|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;shannons-source-coding-theorem&#34;&gt;Shannon’s source coding theorem&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be an ensemble with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; bits, and &lt;span class=&#34;math inline&#34;&gt;\(X^N\)&lt;/span&gt; be the ensemble composed of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. such random variables. Given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \delta &amp;lt; 1\)&lt;/span&gt;, there exists a positive integer &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; such that for &lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
|\frac{1}{N} H_\delta (X^N) - H| &amp;lt; \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or put it in a verbal way,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. random variables each with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; can be compressed into more than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits with negligible information loss as &lt;span class=&#34;math inline&#34;&gt;\(N \to \infty\)&lt;/span&gt;; conversely if they are compressed fewer than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits, it is virtually certain that there is information loss.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The random variable &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; is defined for the ensemble &lt;span class=&#34;math inline&#34;&gt;\(Y = X^N\)&lt;/span&gt; which composes of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. &lt;span class=&#34;math inline&#34;&gt;\(X_1 \dots X_N\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; can be re-written as the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; information contents &lt;span class=&#34;math inline&#34;&gt;\(h_i = \log \frac 1 {P(X_i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots P(X_N)} = \frac 1 N (\log \frac{1}{P(X_1)} + \dots + \log \frac{1}{P(X_N)})
\]&lt;/span&gt; Each of these information contents is in turn a random variable with mean &lt;span class=&#34;math inline&#34;&gt;\(\bar h_i = H(X) = H\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{h_i}^2 = \sigma^2\)&lt;/span&gt;. The &lt;strong&gt;typical set&lt;/strong&gt; with parameters &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
T_{N \beta} = \{y \in \mathcal A_X^N: [\frac 1 N \log \frac{1}{P(y)} - H]^2 &amp;lt; \beta^2 \}
\]&lt;/span&gt; By the &lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;Weak Law of Large Numbers&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P((\frac 1 N \log \frac{1}{P(y)} - H)^2 \ge \beta^2) = \frac{\sigma^2}{\beta^2 N}\)&lt;/span&gt;, and thus &lt;span class=&#34;math display&#34;&gt;\[
P(y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; As &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, the probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; falls in &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; draws near to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. We need to relate this to the theorem that for any given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta\)&lt;/span&gt;, there is a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \simeq NH\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;lt; N(H + \epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; is not the best sufficient subset for compression. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\log |T_{N \beta}|\)&lt;/span&gt; upper-bounds the &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N)\)&lt;/span&gt;. On the other hand, for all &lt;span class=&#34;math inline&#34;&gt;\(y \in T_{N \beta}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)} &amp;lt; P(y) &amp;lt; 2^{-N(H + \beta)}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;amp;&amp;lt; 1 \\
|T_{N \beta}| &amp;amp;&amp;lt; 2^{N(H + \beta)} \\
\end{align*}
\]&lt;/span&gt; If we set &lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N = N_0\)&lt;/span&gt; in a way such that &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{\epsilon^2 N_0} \le \delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(P(y \in T_{N_0 \epsilon}) \ge 1 - \delta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_{N_0 \epsilon}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset. Then, &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \le \log |T_{N_0 \epsilon}| \le N_0(H + \epsilon)\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(N \ge N_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H - \epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This part is reached by contradiction. Suppose instead there exists a &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;#39;\)&lt;/span&gt; such that there exists a sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; which results in &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^{N_0}) \le N_0(H - \epsilon)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon/2\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]&lt;/span&gt; Denote the associated subset by &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;. We are to disprove &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; can achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) = P(y \in S&amp;#39; \cap T_{N \beta}) + P(y \in S&amp;#39; \cap \overline{T_{N \beta}})
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39; \cap T_{N \beta}| \le |S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt;. The maximum value of the first term is obtained when &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap T_{N \beta}\)&lt;/span&gt; contains &lt;span class=&#34;math inline&#34;&gt;\(2^{N(H - 2\beta)}\)&lt;/span&gt; outcomes all with probability &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap \overline{T_{N \beta}} \subseteq \overline{T_{N \beta}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39; \cap \overline{T_{N \beta}}) \le P(y \in \overline{T_{N \beta}}) &amp;lt; \frac{\sigma^2}{\beta^2 N}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} + \frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; For arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt;, we can have &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \le 1 - \delta\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt;. We shall conclude that &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; cannot achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 - \delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H - \epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;source-coding-theorem-for-symbol-codes&#34;&gt;Source coding theorem for symbol codes&lt;/h2&gt;
&lt;h3 id=&#34;kraft-mcmillan-inequality&#34;&gt;Kraft-McMillan inequality&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. If &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt; there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes’ lengths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The proof is done by construction. The number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; should be less than &lt;span class=&#34;math inline&#34;&gt;\(2^{l+1}\)&lt;/span&gt;, or else the above condition will be violated. Therefore we can loosely arrange all the codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; to be unique. Then the uniqueness condition is checked.&lt;/p&gt;
&lt;p&gt;Denote the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt;. For any two consecutive lengths &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;amp;\le 1 \\
C_{l+1} &amp;amp;\le 2^{l+1} - 2 C_l \\
C_{l+1} &amp;amp;\le 2(2^l - C_l) \\
\end{aligned}
\]&lt;/span&gt; This means we can append these unused &lt;span class=&#34;math inline&#34;&gt;\(2^l - C_l\)&lt;/span&gt; codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to meet the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;. Construction completes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we have a set of uniquely-decodable prefix coding. Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. Then, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sum_{i=1}^I 2^{-l_i} \le 1\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S^N &amp;amp;= (\sum_{i=1}^I 2^{-l_i})^N \\
  &amp;amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots + l_{i_N})}
  \end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\((l_{i_1} + \dots + l_{i_N})\)&lt;/span&gt; term can be treated as the length of encoding of &lt;span class=&#34;math inline&#34;&gt;\(a_{i_1} \dots a_{i_N}\)&lt;/span&gt; of arbitrary length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(l_\min = \min_i l_i, l_\max = \max_i l_i\)&lt;/span&gt;, the above can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
  S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt; represents the number of symbol codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. Since the coding is uniquely-decodable, &lt;span class=&#34;math inline&#34;&gt;\(C_l \le 2^l\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(S &amp;gt; 1\)&lt;/span&gt;, the above cannot hold for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(S \le 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-source-coding-theorem&#34;&gt;The source coding theorem&lt;/h3&gt;
&lt;p&gt;For an ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, there exists a prefix code &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; with expected length satisfying &lt;span class=&#34;math display&#34;&gt;\[
H(X) \le L(C,X) &amp;lt; H(X) + 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We define the implicit probabilities &lt;span class=&#34;math inline&#34;&gt;\(q_i = 2^{-l_i} / z\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z = \sum_i 2^{-l_i}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) = \sum_i p_i l_i &amp;amp;= -\sum_i [p_i \log (q_iz)] \\
&amp;amp;=\sum_i [p_i \log 1/q_i] - \log z \\
&amp;amp;\ge H(X)
\end{aligned}
\]&lt;/span&gt; The equality holds when &lt;span class=&#34;math inline&#34;&gt;\(z = 1\)&lt;/span&gt; (the code is complete) and &lt;span class=&#34;math inline&#34;&gt;\(q = p\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(l_i = \log 1/p_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;From another perspective, suppose the coding is complete but not optimal, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;L(C,X) = -\sum_i [p_i \log (q_iz)] = -\sum_i [p_i \log (q_i)] \\
&amp;amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i \log (q_i)] \\
&amp;amp;= H(X) + D_{KL}(p || q)
\end{aligned}
\]&lt;/span&gt; where the cost is the extra &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p || q)\)&lt;/span&gt; bits, which is brought by instead treating &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as the real distribution. &lt;span class=&#34;math inline&#34;&gt;\(D_{KL(p||q)}\)&lt;/span&gt; is termed as relative entropy or the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</link>
      <pubDate>Thu, 12 May 2022 11:26:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</guid>
      <description>

&lt;h2 id=&#34;three-perspectives-on-nce&#34;&gt;Three Perspectives on NCE&lt;/h2&gt;
&lt;h3 id=&#34;non-parametric-estimation&#34;&gt;Non-parametric estimation&lt;/h3&gt;
&lt;p&gt;The traditional log-likelihood function will be &lt;span class=&#34;math inline&#34;&gt;\(\ell = \sum_x \ln p_\theta(x)\)&lt;/span&gt;. In NCE, we learn &lt;span class=&#34;math display&#34;&gt;\[
p_\theta(1|x) = \sigma(G(x;\theta) - \gamma) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}}
\]&lt;/span&gt; And corresponding loss function becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal {L} &amp;amp;= -\E_{x \sim \tilde p(x)} \ln p_\theta(1|x) - \E_{x \sim q(x)} \ln p_\theta(0|x) \\
&amp;amp;= -\int \tilde p(x) \ln p_\theta(1|x) dx - \int q(x) \ln p_\theta(0|x) dx \\
&amp;amp;= - \int [\tilde p(x) + q(x)] [\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln p_\theta(1|x) + \frac{q(x)}{\tilde p(x) + q(x)} \ln p_\theta(0|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P(y|x) = \begin{cases}\frac{\tilde p(x)}{\tilde p(x) + q(x)}, y=1 \\\frac{q(x)}{\tilde p(x) + q(x)},y=0\end{cases}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\label{loss} \begin{aligned} 
\arg \min_{\theta, \gamma} \mathcal{L} &amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln p_\theta(1|x) + \int \frac{q(x)}{\tilde p(x) + q(x)} \ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][P(1|x) \ln p_\theta(1|x) + P(0|x)\ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)][P(1|x) \ln \frac{1}{p_\theta(1|x)} + P(0|x)\ln \frac{1}{p_\theta(0|x)}]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)] H[P(y|x)||p_\theta(y|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since cross entropy &lt;span class=&#34;math inline&#34;&gt;\(H(p||q) \ge H(p)\)&lt;/span&gt; and the minimum is reached only when &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;, the global minimum for equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is reached when &lt;span class=&#34;math inline&#34;&gt;\(p_\theta(y|x) = P(y|x)\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_\theta(1|x) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}} &amp;amp;= \frac{\tilde p(x)}{\tilde p(x) + q(x)} = P(1|x) \\
\frac{q(x)}{\tilde p(x)} &amp;amp;= e^{-G(x;\theta) + \gamma} \\
\tilde p(x) &amp;amp;= \frac{q(x) e^{G(x;\theta)}}{e^\gamma}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; are learnt so that &lt;span class=&#34;math inline&#34;&gt;\(q(x) e^{G(x;\theta) - \gamma}\)&lt;/span&gt; fit the real distribution. It becomes more intuitive when &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a uniform distribution and &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(e^\gamma\)&lt;/span&gt; will be the learnt normalizing factor.&lt;/p&gt;
&lt;p&gt;https://kexue.fm/archives/5617/comment-page-1&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation-the-original-papers-view&#34;&gt;Maximum likelihood estimation (the original paper’s view)&lt;/h3&gt;
&lt;p&gt;The model is defined as &lt;span class=&#34;math inline&#34;&gt;\(\ln p_\theta(x) = \ln p^0_\alpha(x) + c\)&lt;/span&gt;. The MLE will maximize the objective function &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J_T(\theta) &amp;amp;= \frac{1}{T_d} [\sum_{i=1}^{T_d \\} \ln h(x_i;\theta) + \sum_{i=1}^{T_n} \ln (1 - h(y_i;\theta)) \text{, ($x_i$&amp;#39;s are samples, $y_i$&amp;#39;s are noises, $T_n = \nu T_d$)} \\
&amp;amp;\stackrel{P}\to J(\theta) \triangleq \E_{x \sim \tilde p} \ln h(x;\theta) + \nu \E_{x \sim q} \ln (1 - h(x;\theta)) \\
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
r_\nu(u) = \frac{1}{1 + \nu e^{-u}} \\
G(x; \theta) = \ln p_\theta(x) - \ln q(x) \\
h(x;\theta) = r_\nu(G(x;\theta)) = \frac{1}{1 + \nu e^{-G(x;\theta)}} \\
\end{gather}
\]&lt;/span&gt; Denote by &lt;span class=&#34;math inline&#34;&gt;\(\tilde J\)&lt;/span&gt; the objective function &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; seen as a function of &lt;span class=&#34;math inline&#34;&gt;\(f(.) = \ln p_\theta(.)\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f) = \E_{x \sim \tilde p} \ln r_\nu(f(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) - q(x)])
\]&lt;/span&gt; For &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; a perturbation of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f + \epsilon \phi) = \E_{x \sim \tilde p} \ln r_\nu(f(x) + \epsilon \phi(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) + \epsilon \phi - q(x)])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By Taylor’s expansion,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln r_\nu(u + \epsilon u_1 + \epsilon^2 u_2) &amp;amp;\approx \ln r_\nu (u) + r_{\frac{1}{\nu}}(-u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_\nu (u)r_\frac{1}{\nu}(-u)}{2}(\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln r_\nu (u) + \epsilon u_1r_{\frac{1}{\nu}}(-u) + \epsilon^2(u_2r_{\frac{1}{\nu}}(-u) - \frac{1}{2}u_1^2 r_{\frac{1}{\nu}}(-u)r_\nu (u)) + \Omicron \big(\epsilon^3 \big) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln \big(1 - r_v(u + \epsilon u_1 + \epsilon^2 u_2) \big) &amp;amp;\approx \ln \big(1 - r_v(u) \big) - r_v(u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_{\frac{1}{\nu}}(-u) r_\nu(u)}{2} (\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln(1 - r_v(u)) - \epsilon u_1 r_v(u) - \epsilon^2 \big( u_2 r_v(u) + \frac{1}{2} u_1^2 r_{\frac{1}{\nu}}(-u) r_\nu(u) \big) + \Omicron(\epsilon^3)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde J(f + \epsilon \phi) \approx &amp;amp;\E_{x \sim \tilde p} \ln r_\nu \big(f(x) + \epsilon \phi(x) - \ln q(x) \big) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) + \epsilon \phi - q(x)]) \\
= &amp;amp;\E_{x \sim \tilde p} \{\ln r_\nu \big(f(x) - \ln q(x) \big) + \epsilon \phi(x) r_{\frac{1}{\nu}} \big(\ln q(x) - f(x) \big) \} \\ 
&amp;amp;+\nu \E_{x \sim q} \{ \ln \big(1 - r_\nu[f(x) -\ln q(x)] \big) - \epsilon \phi(x) r_\nu \big( f(x) - \ln q(x) \big) \} + \Omicron(\epsilon^2) \\
= &amp;amp;\tilde J(f) + \epsilon \int \phi(x) \big(r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) - \nu r_\nu[f(x) - \ln q(x)] q(x) \big) + \Omicron(\epsilon^2)
\end{aligned}
\]&lt;/span&gt; The above equation attains the local maximum at &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; only if the term of order &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) &amp;amp;= \nu r_\nu[f(x) - \ln q(x)] q(x) \\
\frac{\nu \tilde p(x)}{\nu + e^{f(x) - \ln q(x)}} &amp;amp;= \frac{\nu q(x)}{1 + \nu e^{\ln q(x) - f(x)}} \\
\frac{\tilde p(x)q(x)}{\nu q(x) + p_\theta(x)} &amp;amp;= \frac{q(x) p_\theta(x)}{p_\theta(x) + \nu q(x)} \\
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is nonzero, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\tilde p(x)}{\nu q(x) + p_\theta(x)} = \frac{p_\theta(x)}{p_\theta(x) + \nu q(x)} \iff p_\theta(x) = \tilde p(x)\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;https://leimao.github.io/article/Noise-Contrastive-Estimation/&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Stonesjtu/Pytorch-NCE&#34;&gt;PyTorch-NCE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;other-reference&#34;&gt;Other Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/334772391&#34;&gt;NCE与语言模型&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jacobian Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</guid>
      <description>
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \to \R^m\)&lt;/span&gt;, with input &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt; and output &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f = 
\begin{cases}
y_1 = f_1(x_1, x_2, ..., x_n) \\
y_2 = f_2(x_1, x_2, ..., x_n) \\
... \\
y_m = f_m(x_1, x_2, ..., x_n) \\
\end{cases}
\]&lt;/span&gt; Then Jacobian matrix is &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J &amp;amp;= 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} &amp;amp; \frac{\partial f}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f}{\partial x_n} \\
\end{bmatrix} \\
&amp;amp;= 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp;amp; \frac{\partial f_m}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \\
\end{bmatrix}
\end{aligned}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear transformation, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y = Tx\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
J = T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear transformation and &lt;span class=&#34;math inline&#34;&gt;\(n = m\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; square matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
dy_1 \\
dy_2 \\
\vdots \\
dy_n \\
\end{bmatrix} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}
\begin{bmatrix}
dx_1 \\
dx_2 \\
\vdots \\
dx_n \\
\end{bmatrix}
\]&lt;/span&gt; That is, &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{
\begin{bmatrix}
dy_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dy_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dy_n
\end{bmatrix}}_A
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix} =
\underbrace{
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}}_J
\underbrace{
\begin{bmatrix}
dx_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dx_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dx_n
\end{bmatrix}}_B
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(JB\)&lt;/span&gt; are both diagonal. From above equation we can find that &lt;span class=&#34;math inline&#34;&gt;\(A = JB\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|A| &amp;amp;= |JB| \\
dy_1 \dots dy_n &amp;amp;= |J|dx_1 \dots dx_n \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spherical Coordinates</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</link>
      <pubDate>Mon, 09 May 2022 20:26:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</guid>
      <description>
&lt;p&gt;The conversion between the 2-d Cartesian coordinate system and the 2-d polar coordinate system can be extended to a higher dimension, say &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d. In &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d case, their conversion can be described as below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cartesian-to-spherical &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
r &amp;amp;= \sqrt{x_1^2 + \dots + x_k^2} &amp;amp;&amp;amp; \\
\varphi_1 &amp;amp;= \arccot \frac{x_1} {\sqrt{x_k^2 + \dots + x_2^2}} &amp;amp;&amp;amp;=  \arccos \frac{x_1} {\sqrt{x_k^2 + \dots + x_1^2}} \\
\varphi_2 &amp;amp;= \arccot \frac{x_2} {\sqrt{x_k^2 + \dots + x_3^2}} &amp;amp;&amp;amp;=  \arccos \frac{x_2} {\sqrt{x_k^2 + \dots + x_2^2}} \\
&amp;amp; \vdots &amp;amp;&amp;amp;\vdots\\
\varphi_{k-2} &amp;amp;= \arccot \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}} &amp;amp;&amp;amp;= \arccos \frac{x_{k-2}} {\sqrt{x_k^2 + x_{k-1}^2 + x_{k-2}^2}} \\
\varphi_{k-1} &amp;amp;= 2 \arccot \frac{x_{k-1} + \sqrt{x_k^2 + x_{k-1}^2}}{x_k} &amp;amp;&amp;amp;= 
\begin{cases}
\arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n \ge 0 \\
2\pi - \arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n &amp;gt; 0\\
\end{cases}
\end{alignat}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Spherical-to-Cartesian &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_1 &amp;amp;= r \cos(\varphi_1) \\
x_2 &amp;amp;= r \sin(\varphi_1) \cos(\varphi_2) \\
\notag &amp;amp;\vdots \\
x_{k-1} &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) \\
x_k &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\end{align}
\]&lt;/span&gt; The corresponding &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian Matrix&lt;/a&gt; is &lt;span class=&#34;math display&#34;&gt;\[
J^{(k)} = \left[ \begin{array}{ccccc|c}
\cos (\varphi_1) &amp;amp; -r \sin(\varphi_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\sin(\varphi_1) \cos(\varphi_2) &amp;amp; r \cos(\varphi_1) \cos(\varphi_2) &amp;amp; -r \sin(\varphi_1) \sin(\varphi_2) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ &amp;amp; \ddots &amp;amp; \vdots \\
\hline
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; -r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1})
\end{array} \right]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|J^{(2)}|\)&lt;/span&gt; can be easily derived as &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k)}|\)&lt;/span&gt; can be constructed from &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k-1)}|\)&lt;/span&gt;. Comparing &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt;,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and an extra row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\cos(\varphi_{k-1})\)&lt;/span&gt; term in each element than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\sin(\varphi_{k-1})\)&lt;/span&gt; term in each element than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; are totally the same on the region delimited by row &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, row &lt;span class=&#34;math inline&#34;&gt;\(k-2\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apply the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/&#34;&gt;Laplace Expansion&lt;/a&gt; along column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and combine the property of determinant to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J^{(k)}| =\ &amp;amp; \underbrace{0 + \dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \sin(\varphi_{k-1}) \big( \sin(\varphi_{k-1}) |J^{(k-1)}| \big)] + \\
&amp;amp; (-1)^{n+n} [r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) \big( \cos(\varphi_{k-1}) |J^{(k-1)}| \big)] \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \big( \sin_{\varphi_{k-1}}^2 + \cos{\varphi_{k-1}}^2 \big) |J^{(k-1)}| \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) |J^{(k-1)}| \\
\end{aligned}
\]&lt;/span&gt; By induction, &lt;span class=&#34;math display&#34;&gt;\[
|J^{(k)}| = r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\]&lt;/span&gt; Therefore when changing basis from orthogonal coordinate system to polar coordinate system, &lt;span class=&#34;math display&#34;&gt;\[
\d x_1 \dots \d x_k = r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2}) \d r \d \varphi_1 \dots \d \varphi_{k-1}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates&#34;&gt;Wiki&lt;/a&gt;|&lt;a href=&#34;https://wuli.wiki//online/SphCar.html&#34;&gt;3d Case Blog 1&lt;/a&gt;|&lt;a href=&#34;https://www.cnblogs.com/hans_gis/archive/2012/11/21/2755126.html&#34;&gt;3d Case Blog 2&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</guid>
      <description>

&lt;h2 id=&#34;continuous-time-fourier-transform&#34;&gt;Continuous-time Fourier Transform&lt;/h2&gt;
&lt;h3 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h3&gt;
&lt;p&gt;In Euclidean space, we usually represent a vector by a set of independent and orthogonal base vectors (basis). Orthogonality means the inner product between two basis is zero. Inner product can also be defined on some common interval between two functions, and thus the orthogonality.&lt;/p&gt;
&lt;h4 id=&#34;frequency-domain&#34;&gt;Frequency Domain&lt;/h4&gt;
&lt;p&gt;It is intuitive to model after the inner product between vectors. Function (signal) on its domain can be viewed as an “infinite-dimension” vector. We represent this infinity in the definition of function inner product by integration. In particular, given two functions &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, an interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[
\int\limits_{x \in I}s(x)g(x)dx
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are orthogonal on interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; if their inner product &lt;span class=&#34;math inline&#34;&gt;\(\int_{x \in I}s(x)g(x)dx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set of basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt; Euclidean space contains &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent orthogonal basis. For an “infinite-dimension” function space, there are an infinite number of basis, among which a group of sine and cosine functions satisfy. For integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and positive integers &lt;span class=&#34;math inline&#34;&gt;\(m, n\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = \pi, m = n, m, n \ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = 0, m \ne n, m, n \ge 1 \\
\end{array}
\right. \\
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = \pi, m = n, m, n \ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = 0, m \ne n, m, n \ge 1 \\
\end{array}
\right. \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx = 0, m, n \ge 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m = n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 - \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 + \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(nx) \cos(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin(2nx)}{2}dx \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m \ne n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) + \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} + \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) - \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} - \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin((n+m)x) + \sin((n-m)x)}{2}dx \\
&amp;amp;= -\frac{\frac{\cos((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi} + \frac{\cos((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi + 2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, we can use the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(x), \sin(x), \cos(2x), \sin(2x), \dots\)&lt;/span&gt; to fit any &lt;strong&gt;continuous function&lt;/strong&gt; on interval &lt;span class=&#34;math inline&#34;&gt;\([-\pi, \pi]\)&lt;/span&gt;. Or use &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(2\pi fx), \sin(2\pi fx), \cos(2\pi f2x), \sin(2\pi f2x), \dots\)&lt;/span&gt; to fit any function on interval &lt;span class=&#34;math inline&#34;&gt;\([\frac{-1}{2f} + \frac{k}{f}, \frac{1}{2f} + \frac{k}{f}]\)&lt;/span&gt;, which can be any interval depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (frequency) and the integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; approximated with such series up to level &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; can be written as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{split}
s_N(x) &amp;amp;= a_0 + \sum_{i=1}^N \big( \underbrace{a_n}_{A_n\sin(\varphi_n)} \cos(2\pi fnx) + \underbrace{b_n}_{A_n\cos(\varphi_n)} \sin(2\pi fnx) \big) \\
&amp;amp;= a_0 + \sum_{n=1}^N \bigg( A_n\sin(2\pi fnx + \varphi_n) \bigg) \text{, where} \\
\end{split} \\
\notag &amp;amp;A_n = \sqrt{a_n^2 + b_n^2}, \sin(\varphi_n) = \frac{a_n}{\sqrt{a_n^2 + b_n^2}}, \cos(\varphi_n) = \frac{b_n}{\sqrt{a_n^2 + b_n^2}}
\end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A_n\)&lt;/span&gt; can be interpreted as the amplitude, &lt;span class=&#34;math inline&#34;&gt;\(\varphi_n\)&lt;/span&gt; as the phase, &lt;span class=&#34;math inline&#34;&gt;\(nf\)&lt;/span&gt; as the frequency.&lt;/p&gt;
&lt;h4 id=&#34;complex-frequency-domain&#34;&gt;Complex Frequency Domain&lt;/h4&gt;
&lt;p&gt;By Euler’s Formula we have &lt;span class=&#34;math display&#34;&gt;\[
e^{ix} = \cos x + i\sin x
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(s_N(x)\)&lt;/span&gt; can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
s_N(x) &amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( \underbrace{a_n}_{A_n\cos \phi_n} \cos(2\pi fnx) + \underbrace{b_n}_{A_n\sin \phi_n} \sin(2\pi fnx) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( A_n\cos(2\pi fnx - \phi_n) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx - \phi_n) + i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx - \phi_n) - i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;\Downarrow_\text{by multiplication rule between complex numbers in polar form}  \\
&amp;amp;= &amp;amp;&amp;amp; \sum_{n=-N}^N c_ne^{i 2\pi fnx} \\
\end{alignat}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
c_n &amp;amp;= 
\begin{cases}
\frac{A_n}{2}(\cos \phi_n - i\sin \phi_n) = \frac{1}{2}(a_n - ib_n), &amp;amp;n &amp;gt; 0 \\
\overline{c_{|n|}} =\frac{A_n}{2}(\cos \phi_n + i\sin \phi_n), &amp;amp;n &amp;lt; 0 \\
a_0, &amp;amp;n = 0
\end{cases}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(N \to +\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; can be reconstructed as the Fourier Series: &lt;span class=&#34;math display&#34;&gt;\[
s(x) = \lim_{N \to +\infty} s_N(x) = a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg) \\
\]&lt;/span&gt; The problem comes how &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; can be computed. By the orthogonality mentioned before, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \d x = \frac{1}{f} a_0 \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \cos(2\pi fkx) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \cos (2\pi fkx) \d x = \frac{1}{2f} a_k \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \sin(2\pi fkx) \d x =\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \sin (2\pi fkx) \d x = \frac{1}{2f} b_k \\
\end{gather}
\]&lt;/span&gt; The computation of &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; can be combined together by the Euler’s Formula: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) e^{-i 2\pi fkx} \d x &amp;amp; = \int_{-1/2f+f/k}^{1/2f+f/k} (a_k \cos(2\pi fkx) + b_k \sin(2\pi fkx)) (\cos(2\pi fkx) - i \sin(2\pi fkx)) \d x \\
&amp;amp;= \frac{1}{2f} (a_k - i b_k) = \frac{1}{f} c_k
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;fourier-transform&#34;&gt;Fourier Transform&lt;/h3&gt;
&lt;p&gt;We have been through representing a continuous function on a certain interval using the Fourier Series. This can be quite useful for periodic functions. As long as we figure out the representation on its repeating interval, we obtain the representation on its whole domain. The problem is more of computing the factor for each sine and cosine function.&lt;/p&gt;
&lt;p&gt;The process of finding out factors for an arbitrary continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is called Fourier Transform. It transforms the function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; from the time domain to the frequency domain. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is periodic, it can be easily represented by the Fourier Series as discussed in previous section. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is not periodic, we can treat the periodic interval as &lt;span class=&#34;math inline&#34;&gt;\([-\infty, +\infty]\)&lt;/span&gt;. Its Fourier Transform and the inverse will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\hat s \stackrel{\mathcal F}\Longleftrightarrow s \\
\hat s(f) = \int_{-\infty}^{+\infty} s(t) e^{-i 2\pi f x} \d t \\
s(x) = \int_{-\infty}^{+\infty} \hat s(f) e^{i 2\pi f x} \d f
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;discrete-time-fourier-transform&#34;&gt;Discrete-time Fourier Transform&lt;/h2&gt;
&lt;p&gt;The domain (time axis) of the function (signal) is continuous in our discussion by now. When the time axis is discrete (and usually takes on a series of integers), we are facing the Discrete-time Fourier Transform. We will be using the term &lt;strong&gt;signal&lt;/strong&gt; instead of the function from now on.&lt;/p&gt;
&lt;p&gt;For a discrete signal &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, its Fourier Transform is &lt;span class=&#34;math display&#34;&gt;\[
\hat s(\omega) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i\omega k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the angular speed. &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is in the unit of radian/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(\omega)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; between two signal samples. &lt;span class=&#34;math display&#34;&gt;\[
\hat s(f) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i 2\pi f k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the “frequency”. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the unit of cycles/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(f)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; cycles between two signal samples.&lt;/p&gt;
&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Laplace Transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;-transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Fast Fourier Transform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://charlesliuyx.github.io/2018/02/18/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E8%AE%A9%E4%BD%A0%E6%B0%B8%E8%BF%9C%E5%BF%98%E4%B8%8D%E4%BA%86%E7%9A%84%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E8%A7%A3%E6%9E%90/&#34;&gt;傅立叶变换与群&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/1119070975&#34;&gt;如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/334686351&#34;&gt;如何理解傅里叶变换公式？ - 马同学的回答 - 知乎&lt;/a&gt; || &lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_transform&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Unconscious Statistics</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/unconscious-statistics/</link>
      <pubDate>Tue, 03 May 2022 10:50:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/unconscious-statistics/</guid>
      <description>

&lt;h2 id=&#34;law-of-the-unconscious-statistician&#34;&gt;Law of the Unconscious Statistician&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, the &lt;strong&gt;law of the unconscious statistician&lt;/strong&gt; (LOTUS), is a theorem used to calculate the expected value of a function &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt; of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; when one knows the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; but one does not know the distribution of &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the probability mass function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \sum_x g(x) p(x)
\]&lt;/span&gt; If the probability density function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)p(x)\ \d x
\]&lt;/span&gt; If the cumulative distribution function is known, it is &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)\ \d F(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;marginal-expectation&#34;&gt;Marginal Expectation&lt;/h2&gt;
&lt;p&gt;If the joint distribution of two random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is known, then the expectation of one component can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\E[X] = \int_{-\infty}^{+\infty} x p_X(x)\; \d x = \int_{-\infty}^{+\infty} x \int_{-\infty}^{+\infty} p(x,y)\; \d y\; \d x = = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xp(x,y)\ \d y\ \d x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\E [X] = \E{y \sim p_Y} [\E_{x \sim p(X|Y=y)}]  = \int_{-\infty}^{+\infty} p(y) \bigg( \int_{-\infty}^{+\infty} x p(x|y)\ \d x \bigg) \d y
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/questions/185729/expected-value-of-a-marginal-distribution-when-the-joint-distribution-is-given&#34;&gt;StackExchange Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-non-negative-random-variables&#34;&gt;Expectation of Non-negative Random Variables&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable whose value is non-negative, and &lt;strong&gt;its expectation exists&lt;/strong&gt;, and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is continuous, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[b]
\E (X) &amp;amp;= \int_{0}^{+\infty} x p(x)\ \d x = \int_{0}^{+\infty} x\ \d \big( P(x) - 1 \big) \\
&amp;amp;= [x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty} - \int_0^{+\infty} \big( P(x) - 1 \big)\ \d x
\end{aligned}
\]&lt;/span&gt; because the expectation exists, the above expression and especially the &lt;span class=&#34;math inline&#34;&gt;\([x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty}\)&lt;/span&gt; term must converge: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
[x \big( P(x) - 1 \big)]\bigg|_{x=0} = 0 \\
[P(x) - 1]\bigg|_{x \to +\infty} = 0 \Rightarrow [x \big( P(x) - 1 \big)]\bigg|_{x \to +\infty} = 0
\end{gather}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \int_{0}^{+\infty} \big (1 - P(x) \big)\ \d x
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is discrete and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; only takes on integer values, supposing the max value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E(X) &amp;amp;= \sum_{k=0}^{N} [k P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [(\sum_{j=0}^{k-1} 1) P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [\sum_{j=0}^{k-1} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} [\sum_{k=j+1}^{N} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} P(X &amp;gt; j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/305031/expectation-when-cumulative-distribution-function-is-given&#34;&gt;StackExchange Discussion&lt;/a&gt;|&lt;a href=&#34;https://en.wikipedia.org/wiki/Summation_by_parts&#34;&gt;Summation by Parts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-and-quantile-function&#34;&gt;Expectation and Quantile Function&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be the PDF and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; be the CDF of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Q = F^{-1}\)&lt;/span&gt; be the inverse of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is also called the &lt;strong&gt;quantile function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\int_0^1 Q(p)\ \d p \stackrel{p=F(x)}{\Longrightarrow} = \int_{-\infty}^{+\infty} x f(x)\ \d x = \E(X) 
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/a/18439&#34;&gt;StackExchange Answer&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Contrastive Predictive Coding</title>
      <link>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</link>
      <pubDate>Fri, 29 Apr 2022 11:32:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</guid>
      <description>

&lt;h2 id=&#34;rationale&#34;&gt;Rationale&lt;/h2&gt;
&lt;p&gt;CPC was initially proposed in autoregressive models. It enhances the autoencoder by lifting the lower bound of mutual information between the encoded representation and the original data. The original data can either be the data before the encoding, or the future data after various steps.&lt;/p&gt;
&lt;p&gt;CPC learns the representation by minimizing the following loss function: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\c}{\mathrm{c}} \label{loss}
\mathcal L_N = -\E_{t \sim \Phi} \log \frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} = -\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(t = (\x_1, \dots, \x_{N+1}, \c^*; \ell)\)&lt;/span&gt; is a tuple of random variables and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the distribution from which &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is drawn. &lt;span class=&#34;math inline&#34;&gt;\((\x:\c)_{1:N+1}\)&lt;/span&gt; are drawn from the joint distribution &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(\x,\c)\)&lt;/span&gt;. All &lt;span class=&#34;math inline&#34;&gt;\(\c_i\)&lt;/span&gt;’s but one randomly-chosen &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; are trimmed from the original samples. &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; is known but it is unknown which sample it is associated with. &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; denotes the index of this unique sample we are trying to predict. In essence, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is parameterizing the representation &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. The formal score function &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt; is simply a deterministic cosine similarity between &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\x_{1:N+1}\)&lt;/span&gt; consists of one positive sample &lt;span class=&#34;math inline&#34;&gt;\(\x^*\)&lt;/span&gt; that is matched with &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; and more other independent negative (noise) samples &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt;’s that are not matched with &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed ratio of the number of negative samples to the number of positive samples. Let &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i|\x_{1:N+1},\c^*)\)&lt;/span&gt; represent the probability that &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt; is the positive sample given &lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots x_{N+1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\ell=i, \x_{1:N+1},\c^*)}{\sum_{j=1}^{N+1} P(\ell=j, \x_{1:N+1},\c^*)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i,\x_{1:N+1},\c^*) = P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)}{\sum_{j=1}^{N+1} [P(\x_j,\c^*) \prod_{k=1,k \ne j}^{N+1} P(\x_k)]} \\
&amp;amp;= \frac{\tilde p(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} \tilde p_X(\x_j)} {\sum_{j=1}^{N+1} [\tilde p(\x_j,\c^*) \prod_{k=1,k \ne j}^{N+1} \tilde p_X(\x_j)]} \\
&amp;amp;= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} \\
\end{aligned}
\]&lt;/span&gt; The loss function &lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is in fact the expectation (the outer &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of categorical cross entropy (the inner &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of correctly identifying the sample as positive. The minimum of loss function is thus reached when the two categorical distributions are identical. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_\theta(l = i|x_{1:N+1},\c^*) = \frac{f_\theta(\x_i,\c)}{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} = \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} = P(\ell=i|\x_{1:N+1},\c^*) \\
f_\theta(\x_i,\c) = \frac{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}{\sum_{\x^\prime \in X} \frac{\tilde p(\x^\prime|\c)}{\tilde p_X(\x^\prime)}} \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} \\
f_\theta(\x_i,\c) \propto \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;bounding-the-mutual-information&#34;&gt;Bounding the Mutual Information&lt;/h2&gt;
&lt;p&gt;CPC helps estimate the lower bound of the mutual information when optimizing the InfoNCE loss. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal L_N^{\text{opt}} &amp;amp;= -\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} {\sum_{\x&amp;#39; \in X} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)} + \sum_{\x&amp;#39; \in X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}} {\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} \sum_{\x&amp;#39; \in X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)} \big) \\
&amp;amp;\approx \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \E_{\tilde p_X(\x&amp;#39;)} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)} \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;\ge \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} [\log \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)}] + \log (N - 1) \\
&amp;amp;= -I(\x;\c^*) + \log (N - 1)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(\x;\c^\star) \ge \log(N-1) - \mathcal L^{\mathrm{opt}}_{N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://anilkeshwani.github.io/CPC/&#34;&gt;Paper Review&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=zNKMHj1eLa0&#34;&gt;CPC Formulation&lt;/a&gt; || &lt;a href=&#34;https://jxmo.io/posts/nce&#34;&gt;NCE and InfoNCE&lt;/a&gt; || &lt;a href=&#34;https://colab.research.google.com/github/google-research/google-research/blob/master/vbmi/vbmi_demo.ipynb#scrollTo=DUd0hFZ5Js8k&#34;&gt;Demo of Bounding Mutual Information&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Conditional Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</link>
      <pubDate>Wed, 13 Apr 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</guid>
      <description>
&lt;p&gt;The Conditional Entropy measures the the amount of information needed to describe the outcome of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that the value of another random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is known. The entropy of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(Y|X) = -\sum_{(x,y) \in X:Y} p_{(X,Y)}(x,y) \log \frac{p_{(X,Y)}(x,y)}{p_X(x)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lipschitz Continuity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</link>
      <pubDate>Mon, 31 Jan 2022 00:02:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</guid>
      <description>

&lt;p&gt;For a continuous mapping &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, it is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous if there exists a number &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in dom(f)\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
||f(x) - f(y)|| \le K||x - y||
\]&lt;/span&gt; If the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous, we further say &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-smooth.&lt;/p&gt;
&lt;h3 id=&#34;lipschitz-constant&#34;&gt;Lipschitz Constant&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the minimum number to make the above condition hold, then &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is called the &lt;strong&gt;Lipschitz constant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Lipschitz constant for a general differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will be the maximum spectral norm of its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt; over its domain. &lt;span class=&#34;math display&#34;&gt;\[
||f||_{Lip} = \sup_x \sigma[\nabla f(x)] = \sup_x \sup_{||v||=1} \nabla f(x) \cdot v
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma[\nabla f(x)]\)&lt;/span&gt; denotes the spectral norm of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s gradient at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Lipschitz constant for a matrix transformation will be the matrix’s &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The Lipschitz constant for a &lt;span class=&#34;math inline&#34;&gt;\(\R \mapsto \R\)&lt;/span&gt; function will be its largest &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;subgradient&lt;/a&gt; over its domain&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;composition-of-functions&#34;&gt;Composition of Functions&lt;/h3&gt;
&lt;p&gt;Suppose two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are Lipschitz continuous respectively. Then, &lt;span class=&#34;math display&#34;&gt;\[
\nabla (f \circ g)(x) = \nabla f [g(x)] \nabla g(x)
\]&lt;/span&gt; by the chain rule of derivatives. &lt;span class=&#34;math inline&#34;&gt;\(f \circ g\)&lt;/span&gt;’s Lipschitz constant will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sigma(\nabla (f \circ g)(x)) &amp;amp;= \sup_{||v|| = 1} ||\{\nabla f[g(x)] \nabla g(x)\} v|| \\
&amp;amp;=  \sup_{||v|| = 1} \{||\nabla g(x) v||\} \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;\le  \sup_{||v|| = 1} \sigma[\nabla g(x)] \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \sup_{||v|| = 1} \{||\nabla f[g(x)] \frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \cdot \sigma\{\nabla f[g(x)]\}
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(||f \circ g||_{Lip} = ||f||_{Lip} \cdot ||g||_{Lip}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/&#34;&gt;Lipschitz Continuity, convexity, subgradients – Marco Tulio Ribeiro – (washington.edu)&lt;/a&gt; &lt;a href=&#34;https://xingyuzhou.org/blog/notes/Lipschitz-gradient&#34;&gt;Lipschitz continuous gradient · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Real Symmetric Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</link>
      <pubDate>Sun, 30 Jan 2022 13:46:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</guid>
      <description>

&lt;p&gt;Assume &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real-valued symmetric matrix, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;its eigenvalues and thus eigenvectors are real-valued:&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has some imaginary eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and the corresponding imaginary eigenvector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Ax &amp;amp;= \lambda x \\
A(x_{real} + x_{img}) &amp;amp;= (\lambda_{real} + \lambda_{img})(x_{real} + x_{img}) \\
Ax_{real} + Ax_{img} &amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) + (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
\end{aligned}
\]&lt;/span&gt; Denoting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s complex conjugate by &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; respectively, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\left.
\begin{aligned}
&amp;amp;A\bar x = Ax_{real} - Ax_{img} \\
&amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) \\
&amp;amp;\quad- (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
&amp;amp;= \bar \lambda \bar x 
\end{aligned}
\right\} \Rightarrow
\begin{aligned}
(A\bar x)^T &amp;amp;= (\bar \lambda \bar x)^T \\
\bar x^TA^T &amp;amp;= \bar \lambda \bar x^T \\
\bar x^TA &amp;amp;= \bar \lambda \bar x^T
\end{aligned}
\end{gather}
\]&lt;/span&gt; Left-multiply &lt;span class=&#34;math inline&#34;&gt;\(\bar x^T\)&lt;/span&gt; on both sides of &lt;span class=&#34;math inline&#34;&gt;\(Ax = \lambda x\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar x^T \lambda x = \lambda \bar x^T x
\]&lt;/span&gt; Right-multiply &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on both side of &lt;span class=&#34;math inline&#34;&gt;\(\bar x^TA = \bar \lambda \bar x^T\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar \lambda \bar x^T x
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda \bar x^T x = \lambda \bar x^T x\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\bar x^Tx\)&lt;/span&gt; is real-value, &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda = \lambda\)&lt;/span&gt;. In other words, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is real-valued.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;its eigenvectors corresponding to different eigenvalues are orthogonal:&lt;/p&gt;
&lt;p&gt;Arbitrarily taking &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’ s two eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2\)&lt;/span&gt; and their eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, \lambda_1 \ne \lambda_2\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{align} 
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= v_1^TA^Tv_2 \\
&amp;amp;= v_1^TAv_2 \\
&amp;amp;= v_1^T \lambda_2v_2 \\
&amp;amp;= \lambda_2v_1^Tv_2 \\
\end{aligned} \\
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= \lambda_1v_1^Tv_2
\end{aligned}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda_1v_1^Tv_2 &amp;amp;= \lambda_2v_1^Tv_2 \\
(\lambda_1 - \lambda_2)v_1^Tv_2 &amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ne \lambda_2\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(v_1^Tv_2 = 0\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; are orthogonal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors and thus &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this case, suppose &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^{-1}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is not invertible, by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Eigenvalues: Rank, Trace and Determinant&#34;&gt;the relation between the matrix rank and the eigenvalues&lt;/a&gt;, some of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;’s entries on the diagonal are zero. By adding &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^\prime &amp;amp;= P\Lambda P^{-1} + \lambda PIP^{-1} \\
&amp;amp;= P(\Lambda + \lambda I)P^{-1}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt; complements all the zero entries on the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(A^\prime\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; non-zero eigenvalues and is invertible. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; becomes invertible by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;its diagonalization can be in the form of &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^T\)&lt;/span&gt; by properly selecting the eigenvectors, drawing from its orthogonal eigenvectors;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is positive semi-definite &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; its eigenvalues are non-negative.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zybuluo.com/fsfzp888/note/1112344&#34;&gt;real-valued eigenvalues and orthogonal eigenvectors&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spectral Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/spectral-normalization/</link>
      <pubDate>Sat, 29 Jan 2022 21:23:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/spectral-normalization/</guid>
      <description>

&lt;p&gt;Spectral Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
||A||_2 = \max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} = \sqrt{\lambda_{\max}(A^TA)} = \sigma_{\max}(A)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\rm z \in \R^N\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\max}(A^TA)\)&lt;/span&gt; is the maximum eigenvalue of matrix &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\max}(A)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s largest singular value.&lt;/p&gt;
&lt;p&gt;The equation on the right can be proved in this way: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} \iff \max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; We may force a constraint on &lt;span class=&#34;math inline&#34;&gt;\(\mathrm z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(||\mathrm z||^2_2 = 1\)&lt;/span&gt;. This is because &lt;span class=&#34;math display&#34;&gt;\[
\frac{||Ac\mathrm z||^2_2}{||c\mathrm z||^2_2} = \frac{c^2||A\mathrm z||^2_2}{c^2||\mathrm z||^2_2} = \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2} = ||A\mathrm z||^2_2 = \mathrm z^TA^TA\mathrm z \\
s.t. ||\mathrm z||^2_2 = 1
\end{gather}
\]&lt;/span&gt; This can be solved by Lagrange Multiplier, where the Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
L(\mathrm z, \lambda) = \mathrm z^TA^TA\mathrm z + \lambda(||\mathrm z||^2_2 - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extrapolation of the Spectral Normalization will be related to Rayleigh Quotient.&lt;/p&gt;
&lt;h3 id=&#34;power-iteration&#34;&gt;Power Iteration&lt;/h3&gt;
&lt;p&gt;We can apply a SVD to obtain a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s Spectral Norm, i.e. the square root of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s largest eigenvalue. There is also an iterative method to do so.&lt;/p&gt;
&lt;p&gt;Begin with an arbitrary vector &lt;span class=&#34;math inline&#34;&gt;\(v_0 = v \in \R^N\)&lt;/span&gt;, the iteration rule will be &lt;span class=&#34;math display&#34;&gt;\[
v_{t+1} = A^TAv_t
\]&lt;/span&gt; Unroll &lt;span class=&#34;math inline&#34;&gt;\(v_{t}\)&lt;/span&gt; to get &lt;span class=&#34;math inline&#34;&gt;\(v_t = (A^TA)^tv\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is real symmetric, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be written as a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s orthonormal eigenvectors: &lt;span class=&#34;math display&#34;&gt;\[
v = \sum_{i=1}^N \alpha_i u_i
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt; be arranged by corresponding eigenvector from large to small, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_t &amp;amp;= (A^TA)^t \sum_{i=1}^N \alpha_i u_i \\
&amp;amp;= \sum_{i=1}^N \alpha_i \lambda_i^t u_i \\
&amp;amp;= \alpha_1 \lambda_1^t \sum_{i=1}^N \frac{\alpha_i}{\alpha_1} (\frac{\lambda_i}{\lambda_1})^t u_i \\
&amp;amp;\to \alpha_1 \lambda_1^t u_1
\end{aligned}
\]&lt;/span&gt; Thus &lt;span class=&#34;math display&#34;&gt;\[
||A^TA \frac{v_t}{||v_t||}|| = ||A^TAu_1|| = ||\lambda_1 u_1|| = \lambda_1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://math.stackexchange.com/questions/2723629/why-is-the-maximum-rayleigh-quotient-equal-to-the-maximum-eigenvalue&#34;&gt;matrices - Why is the maximum Rayleigh quotient equal to the maximum eigenvalue? - Mathematics Stack Exchange&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Convex Optimization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</link>
      <pubDate>Fri, 07 Jan 2022 12:56:57 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</guid>
      <description>

&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convex Set&lt;/p&gt;
&lt;p&gt;A set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt; is called a convex set if &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, x^{(2)} \in \mathcal X\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall \alpha \in [0,1], \alpha x^{(1)} + (1 - \alpha)x^{(2)} \in \mathcal X\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convex Function&lt;/p&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is convex if &lt;span class=&#34;math inline&#34;&gt;\(dom(f)\)&lt;/span&gt; is a convex set and &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x^{(1)} + (1 - \alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1 - \alpha)f(x^{(2)}), \forall x^{(1)}, x^{(2)} \in dom(f), \alpha \in [0,1]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is concave if &lt;span class=&#34;math inline&#34;&gt;\(-f\)&lt;/span&gt; is convex.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with a convex domain is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + \nabla^T f(x)(y - x), \forall x, y \in dom(f)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is twice differentiable if &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2f(x)_{ij} = \frac{\partial^2f(x)}{\partial x_ix_j}\)&lt;/span&gt; exists at each &lt;span class=&#34;math inline&#34;&gt;\(x \in dom(f)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla^2f(x) \succeq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any local minima of a convex function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto R\)&lt;/span&gt; is also a global minima.&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a local minima. Then there exists a number &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x \in dom(f) \land ||x-y||_2 \le r \Rightarrow f(x) \ge f(y)\)&lt;/span&gt;. Suppose instead there exists a global minima &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Then it must hold that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||y-z||_2 &amp;gt; r \\
0 &amp;lt; \frac{r}{||y-z||_2} &amp;lt; 1
\end{gather}
\]&lt;/span&gt; There exists some &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \epsilon &amp;lt; r\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta = 1 - \frac{r - \epsilon}{||y-z||_2} &amp;lt; 1\)&lt;/span&gt;. Then the distance from &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to point &lt;span class=&#34;math inline&#34;&gt;\((\theta y + (1 - \theta)z)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
||y - (\theta y + (1 - \theta)z)||_2 = (1 - \theta)||y-z||_2 = \frac{r - \epsilon}{||y-z||_2}||y-z||_2 &amp;lt; r
\]&lt;/span&gt; That is to say &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \ge f(y)
\]&lt;/span&gt; However, since &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta &amp;lt; 1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the global minima, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z) &amp;lt; \theta f(y) + (1 - \theta)f(y) = f(y)
\]&lt;/span&gt; which contradicts that &lt;span class=&#34;math inline&#34;&gt;\(f(\theta y + (1 - \theta)z) \ge f(y)\)&lt;/span&gt; derived. In other words, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a global minima.&lt;/p&gt;
&lt;h3 id=&#34;standard-form-of-the-convex-optimization-problem&#34;&gt;Standard Form of the Convex Optimization Problem&lt;/h3&gt;
&lt;p&gt;Standard form of the optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Standard form of the convex optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;a_i^Tx = b_i (Ax = b), i = 1,\dots,s \\
&amp;amp;\text{$f_0,\dots,f_r$ are convex, and $a_0,\dots,a_s \in \R^N$}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 12:55:32 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</guid>
      <description>

&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be decomposed into &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $M \times M$, $\Sigma$ is diagonal, $V$ is $N \times N$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{M \times N}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = min(M, N)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;This is shown by construction.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Constructing &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real symmetric matrix, so it can be diagonalized by an orthogonal matrix. Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be this matrix: &lt;span class=&#34;math display&#34;&gt;\[
V^{-1}(A^TA)V = \Lambda \text{, where $\Lambda$ is the diagonal matrix consisting of $A^TA$&amp;#39;s eigenvalues}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; consists of orthonormal basis, we have &lt;span class=&#34;math inline&#34;&gt;\(V^TV = I\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
V^{T}(A^TA)V = \Lambda
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvalues are non-negative. Let &lt;span class=&#34;math inline&#34;&gt;\((\lambda, x)\)&lt;/span&gt; be one pair of its eigen, &lt;span class=&#34;math display&#34;&gt;\[
||Ax||^2 = (Ax)^TAx = x^T(A^TA)x = x^T\lambda x = \lambda||x||^2 \ge 0
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;’s columns be permuted such that their corresponding eigenvalues are ordered in &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_N \ge 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = \sqrt{\lambda_i}, i = 1, 2, ..., N\)&lt;/span&gt; (which are defined as &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s the singular values).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^TA) = r\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the number of its positive eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\lambda_1, \lambda_2, ..., \lambda_r \ge 0, \lambda_r, \lambda_{r+1}, ..., \lambda_{N} = 0 \\
\sigma_1, \sigma_2, ..., \sigma_r \ge 0, \sigma_r, \sigma_{r+1}, ..., \sigma_{N} = 0
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V_1 = [v_1, v_2, ..., v_r], V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\)&lt;/span&gt;. Let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_1 = 
\begin{bmatrix}
\sigma_1 \\
&amp;amp; \sigma_2 \\
&amp;amp; &amp;amp; \ddots \\
&amp;amp; &amp;amp; &amp;amp; \sigma_r
\end{bmatrix}
\]&lt;/span&gt; Complement it with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to get the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma =
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; consists of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvectors whose eigenvalues are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
A^TAV_2 = 0
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^TA) = r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(N - r\)&lt;/span&gt; independent columns. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; spans the &lt;span class=&#34;math inline&#34;&gt;\(N(A^TA) = N(A)\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1V_1^T + V_2V_2^T \\
A = AI = AV_1V_1^T + AV_2V_2^T = AV_1V_1^T
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Constructing &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
u_i = \frac{1}{\sigma_i}Av_i, i = 1, 2, ..., r
\end{equation}
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
U_1 = [u_1, u_2, ..., u_r]
\]&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(AV_1 = U_1\Sigma_1\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
u_i^Tu_j &amp;amp;= (\frac{1}{\sigma_i}Av_i)^T(\frac{1}{\sigma_j}Av_j) \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j}v_i^TA^TAv_j \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j}v_i^T\lambda_jv_j \\
&amp;amp;= \frac{\sigma_j}{\sigma_i}v_i^Tv_j \\
&amp;amp;= 
\begin{cases}
0&amp;amp; i \ne j \\
1&amp;amp; i = j
\end{cases}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(u_i, i = 1, 2, ..., r\)&lt;/span&gt; is from &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt; and they are orthogonal as shown, and &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, they form the &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Col(A)^\perp\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(Col(A)\)&lt;/span&gt;’s complement. we have &lt;span class=&#34;math inline&#34;&gt;\(Col(A)^\perp = N(A^T)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_{r+1}, u_{r+2}, ..., u_{M}\}\)&lt;/span&gt; be an orthonormal basis of &lt;span class=&#34;math inline&#34;&gt;\(N(A^T)\)&lt;/span&gt;. They will be orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s column vectors.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
U_2 = [u_{r+1}, u_{r+2}, ..., u_{M}] \\
U = [U_1, U_2]
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prove that &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T = A\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
U\Sigma V^T &amp;amp;= [U_1, U_2] 
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= [U_1\Sigma_1, 0]
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= U_1\Sigma_1V_1^T \\
&amp;amp;= AV_1V_1^T \\
&amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;p&gt;There is another view on &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
AA^TU = [AA^TU_1, AA^TU_2]
\]&lt;/span&gt; Breaking it into two parts, &lt;span class=&#34;math inline&#34;&gt;\(\forall i = 1, 2, ..., r\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A A^T u_i &amp;amp;= A A^T \frac{1}{\sigma_i} A v_i \\
&amp;amp;= \frac{1}{\sigma_i}A (A^T A) v_i \\
&amp;amp;= \frac{1}{\sigma_i}A \lambda_i v_i \\
&amp;amp;= \lambda_i(\frac{1}{\sigma_i} A v_i) \\
&amp;amp;= \lambda_i u_i
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;’s stacked eigenvectors, ordered by corresponding eigenvalues. &lt;span class=&#34;math display&#34;&gt;\[
AA^TU_2 = 0 = 0U_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is the corresponding eigenvectors with eigenvalues being &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In all, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the stacked orthonormal eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;. It doesn’t matter how &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is permuted.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6251584.html&#34;&gt;奇异值分解(SVD)原理与在降维中的应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/22237507&#34;&gt;奇异值的物理意义是什么？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Singular%20Value%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Singular Value Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</link>
      <pubDate>Fri, 07 Jan 2022 12:42:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</guid>
      <description>
&lt;p&gt;Linear Discriminant Analysis is another approximation to the Bayes optimal classifier. Instead of assuming independence between each pair of input dimensions given certain label, LDA assumes a single common shared covariance matrix among the input dimensions, no matter the label is. Take the Gaussian distribution as an example: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(x|y=C_j) &amp;amp;= \mathcal{N}(x;\mu_j, \Sigma) \\
&amp;amp;= \frac{1}{\sqrt{(2\pi)^n|\Sigma|}}e^{-\frac{1}{2}(x-\mu_j)^T\Sigma^{-1}(x-\mu_j)}
\end{aligned}
\]&lt;/span&gt; Then the classification function will be &lt;span class=&#34;math inline&#34;&gt;\(f_{LDA} = \arg \max\limits_{j}p(x|y=C_j)p(y=C_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;LDA’s parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\mu, \Sigma, \varphi)\)&lt;/span&gt; is also learned with Maximum Likelihood Estimation: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\theta) &amp;amp;= \prod_{i=1}^mp(x^{(i)}, y^{(i)};\theta) \\
&amp;amp;= \prod_{i=1}^mp(x^{(i)}|y^{(i)};\theta)p(y^{(i)})
\end{aligned}
\]&lt;/span&gt; The log-likelihood function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log L(\theta) \\
&amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) + \sum_{i=1}^m\log p(y^{(i)}, \varphi) \\
\end{aligned}
\]&lt;/span&gt; We can maximize above two summations separately.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_1(\mu,\Sigma) &amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) \\
&amp;amp;= -\frac{1}{2}\sum_{i=1}^m(x^{(i)}-\mu_{j|y^{(i)}})^T\Sigma^{-1}(x^{(i)}-\mu_{j|y^{(i)}}) + \log|\Sigma| + n\log2\pi\\
\end{aligned}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \mu_j} = \sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) \\
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) &amp;amp;= 0 \\
\Sigma^{-1}\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) &amp;amp;= 0 \\
\end{aligned}
\]&lt;/span&gt; Since the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is real-symmetric and invertible and thus its nullspace is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) = 0 \\
\mu_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \Sigma} = -\frac{1}{2}\sum_{i=1}^m(\Sigma^{-1} -\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) \\
\sum_{i=1}^m(\Sigma^{-1} -\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) = 0 \\
\Sigma^{-1}\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^mI = \Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\sum_{i=1}^m\Sigma =  \Sigma\Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_2(\varphi) &amp;amp;= \sum_{i=1}^m\log p(y^{(i)}; \varphi) \\
&amp;amp;= \sum_{i=1}^m\log\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} \\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(p(y^{(i)};\varphi) = \prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} = \sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\varphi_j\)&lt;/span&gt;. We chose the product notation because it could be easily “logged”.&lt;/p&gt;
&lt;p&gt;There is also a constraint in maximization of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;. We can form the corresponding Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
J(\varphi, \lambda) = \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j + \lambda(-1 + \sum_{j=1}^c\varphi_j)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\varphi_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\varphi_j} + \lambda = 0 \\
\varphi_j = -\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda}
\end{gather}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-\frac{\sum_{j=1}^c\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda} = 1 \\
\lambda = -M
\end{gather}
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
\varphi_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{M}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Matrix Identity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</link>
      <pubDate>Wed, 22 Dec 2021 15:51:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</guid>
      <description>
&lt;p&gt;A useful matrix identity &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} &amp;amp;= PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1} \\
\end{aligned}
\]&lt;/span&gt; Its reduced form &lt;span class=&#34;math display&#34;&gt;\[
(I_N+AB)^{-1}A = A(I_M+BA)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(I_N+AB)^{-1}A &amp;amp;= A(I_M+BA)^{-1} \\
\iff A &amp;amp;= (I_N + AB)A(I_M + BA)^{-1} \\
\iff A &amp;amp;= (A + ABA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A(I_M + BA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Frobenius Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/frobenius-normalization/</link>
      <pubDate>Mon, 20 Dec 2021 15:43:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/frobenius-normalization/</guid>
      <description>
&lt;p&gt;Frobenius Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
||A||_F = \sqrt{\sum_{ij}A_{ij}^2}
\]&lt;/span&gt; It can be found that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||A||_F^2 &amp;amp;= \sum_{ij}A_{ij}^2 \\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^nA_{ij}A_{ij} = \sum_{i=1}^n\sum_{j=1}^mA_{ji}A_{ji} \\
&amp;amp;= \sum_{i=1}^m(\sum_{j=1}^nA_{ij}A_{ji}^T) = \sum_{i=1}^n(\sum_{j=1}^mA_{ij}^TA_{ji}) \\
&amp;amp;= \sum_{i=1}^m(A_{i:}A_{:i}^T) = \sum_{i=1}^n(A_{i:}^TA_{:i})\\
&amp;amp;= \sum_{i=1}^m(AA^T)_{ii} = \sum_{i=1}^n(A^TA)_{ii}\\
&amp;amp;= tr(AA^T) = tr(A^TA)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Orthogonality and Projection</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</link>
      <pubDate>Mon, 20 Dec 2021 10:19:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</guid>
      <description>
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; are orthogonal to each other, then they are independent with each other.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; be an orthogonal basis for a subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, the weights in the linear combination &lt;span class=&#34;math display&#34;&gt;\[
y = c_1u_1 + c_2u_2 + ... + c_ku_k
\]&lt;/span&gt; are given by &lt;span class=&#34;math display&#34;&gt;\[
c_1 = \frac{y_1 \cdot u_1}{u_1 \cdot u_1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; and another vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, we wish to decompose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \alpha u\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is some vector orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
z &amp;amp;= y - \hat y \\
z \cdot u &amp;amp;= (y - \alpha u) \cdot u \\
y \cdot u - \alpha u \cdot u &amp;amp;= 0 \\
\alpha &amp;amp;= \frac{y \cdot u}{u \cdot u}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \frac{y \cdot u}{u \cdot u}u\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z = y - \hat y\)&lt;/span&gt; is called the component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(cu\)&lt;/span&gt; for any scalar &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is the projection onto the subspace &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; spanned by &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. In this sense, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{proj}_Ly\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; be a subspace of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, then each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; can be uniquely written in the form: &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W^\perp\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{proj}_Wy\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This projection can be found by finding an arbitrary orthogonal basis of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and then computing weights using equation (2).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also the closest point in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, in the sense that: &lt;span class=&#34;math display&#34;&gt;\[
||y - \hat y|| &amp;lt; ||y - v||, \forall v \in W \text{ and } v \ne \hat y
\]&lt;/span&gt; In this case, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is called the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by elements of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Above can be seen by &lt;span class=&#34;math display&#34;&gt;\[
y - v = (y - \hat y) + (\hat y - v)
\]&lt;/span&gt; which gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||y - v||^2 &amp;amp;= ||(y - \hat y) + (\hat y - v)|| \\
&amp;amp;= ||(y - \hat y)||^2 + ||(\hat y - v)||^2 + 2(y - \hat y) \cdot (\hat y - v) \\
&amp;amp;= ||(y - \hat y)||^2 + ||(\hat y - v)||^2 \text{($y - \hat y$ is in $W^\perp$, $\hat y - v$ is in $W$)} \\
&amp;amp;&amp;gt; ||(y - \hat y)||^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; has orthonormal columns if and only if &lt;span class=&#34;math inline&#34;&gt;\(U^TU = I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An orthogonal matrix is a square invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(U^{-1} = U^T\)&lt;/span&gt;. By its definition, it has orthonormal columns and orthonormal rows.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate System and Change of Basis</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</link>
      <pubDate>Sat, 18 Dec 2021 20:47:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</guid>
      <description>
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \{b_1, b_2, ..., b_n\}\)&lt;/span&gt; be a basis for a vector space &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Then for &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1, x_2, ..., x_n]^T\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, there exists a unique set of scalars &lt;span class=&#34;math inline&#34;&gt;\(q_1, q_2, ..., p_n\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
x = q_1b_1 + q_2b_2 + ... + q_nb_n
\]&lt;/span&gt; These scalars are called the coordinates of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{B} = 
\begin{bmatrix}
q_1 \\
\vdots \\
q_n
\end{bmatrix}
\]&lt;/span&gt; is the coordinate vectors of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. The mapping &lt;span class=&#34;math inline&#34;&gt;\(x \mapsto [x]_\mathcal{B}\)&lt;/span&gt; is called the coordinate mapping determined by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{B} = [b_1, b_2, ..., b_n]\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x = P_\mathcal{B}[x]_\mathcal{B}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt; both be an basis for an n-dimensional vector space &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then there is a unique &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow B}\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{C} = \mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B}
\]&lt;/span&gt; The columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow B}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;-coordinate vectors of the vectors in the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\mathop{P}\limits_\mathcal{C \leftarrow B} = [[b_1]_\mathcal{C}, [b_2]_\mathcal{C}, ..., [b_n]_\mathcal{C}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} &amp;amp;=  P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow B}
\begin{bmatrix}
q_1 \\
\vdots \\
q_n
\end{bmatrix} \\
&amp;amp;= P_\mathcal{C}(q_1[b_1]_\mathcal{C} + q_2[b_2]_\mathcal{C} + ... + q_n[b_n]_\mathcal{C}) \\
&amp;amp;= q_1P_\mathcal{C}[b_1]_\mathcal{C} + q_2P_\mathcal{C}[b_2]_\mathcal{C} + ... + q_nP_\mathcal{C}[b_n]_\mathcal{C} \\
&amp;amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\
&amp;amp;= x
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \mathcal{I}\)&lt;/span&gt; is the standard basis, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= [[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= P_\mathcal{C}[[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
&amp;amp;= [e_1, e_2, ..., e_n] \\
&amp;amp;= I
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{C}\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow I} = P_\mathcal{C}^{-1}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\([x]_\mathcal{B} = P_\mathcal{B}^{-1}x\)&lt;/span&gt; in equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \\
[x]_\mathcal{C} &amp;amp;= \mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
[x]_\mathcal{B} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C}  \\
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{B \leftarrow C} = (\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}, \mathop{P}\limits_\mathcal{B \leftarrow C}\mathop{P}\limits_\mathcal{C \leftarrow B} = I\)&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Chebyshev Distance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/chebyshev-distance/</link>
      <pubDate>Sat, 18 Dec 2021 20:28:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/chebyshev-distance/</guid>
      <description>

&lt;h3 id=&#34;discrete-form&#34;&gt;Discrete form&lt;/h3&gt;
&lt;p&gt;Chebyshev distance is a specific form of Minkowski norm (&lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norm): &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= ||x - x^\prime||_p \\
&amp;amp;= (\sum_{i=1}^n|x_i - x^\prime_i|^p)^{1/p} \text{, where $p \to \infty$ }
\end{aligned}
\]&lt;/span&gt; Chebyshev distance is in effect &lt;span class=&#34;math inline&#34;&gt;\(\max\limits_i (|x_i - x^\prime_i|)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = |x_i - x^\prime_i|\)&lt;/span&gt; and without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(a_1 = \max\limits_ia_i = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^n|x_i - x^\prime_i|^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^na_i^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(a_1^p\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
&amp;amp;= \lim_{p \to \infty}(a_1^p)^{1/p} \cdot \lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
&amp;amp;= a_1 \cdot \lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\ 
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\forall i, a_1 &amp;gt; a_i\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\frac{a_i}{a_1} \le 1\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 \le &amp;amp;\sum_{i=1}^n(\frac{a_i}{a_1})^p \le n \\
1^{1/p} \le &amp;amp;(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le n^{1/p} \text{, where $p &amp;gt; 1$} \\
\lim_{p \to \infty}1^{1/p} \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le \lim_{p \to \infty}n^{1/p} \\
1 \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le 1 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(d_p(x, x^\prime) = a_1 \cdot 1 = a_1 = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;continuous-form&#34;&gt;Continuous form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; be continuous and bounded on interval &lt;span class=&#34;math inline&#34;&gt;\((a, b)\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = \sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;proof-1&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sup\limits_{x \in (a,b)}|f(x)|\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall\varepsilon &amp;gt; 0, \exists x_0 \in (a,b), |f(x_0)| &amp;gt; S - \varepsilon\)&lt;/span&gt;. By continuity, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{x \to x_0}|f(x)| &amp;gt; S - \varepsilon\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\exists\delta &amp;gt; 0, \forall x \in U(x_0, \delta), ||f(x)| - \lim\limits_{x \to x_0}|f(x)|| &amp;lt; \varepsilon \rightarrow |f(x)| &amp;gt; S - 2\varepsilon\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
&amp;amp;= \lim\limits_{p \to +\infty}(2\delta(S - 2\varepsilon)^p)^{1/p} \\
&amp;amp;= S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(U(x_0, \delta)\)&lt;/span&gt; in within the interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|f(x)|^p \ge 0\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} \\
&amp;amp;\ge S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is arbitrarily and positively valued, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \ge S\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \le \lim\limits_{p \to +\infty}(\int_a^b S^pdx)^{1/p} = \lim\limits_{p \to +\infty}((b - a)S^p)^{1/p} = S
\]&lt;/span&gt; then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = S = \sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/267575473&#34;&gt;p范数的极限（无穷范数）为什么是极大值范数？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的收敛</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</link>
      <pubDate>Wed, 13 Jul 2022 14:41:05 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</guid>
      <description>

&lt;h2 id=&#34;依概率收敛convergence-in-probability&#34;&gt;依概率收敛（convergence in probability）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成的序列。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能总和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) = 1\)&lt;/span&gt;，抑或是，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) = 0\)&lt;/span&gt;），则称该随机变量序列&lt;strong&gt;依概率收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{P}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt; 0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; 0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有 &lt;span class=&#34;math display&#34;&gt;\[
1 - \delta &amp;lt; P(|X_n - c| &amp;lt; \epsilon) \le 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;依概率收敛的一个例子便是&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/a&gt;，即当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。&lt;/p&gt;
&lt;h2 id=&#34;几乎必然收敛almost-sure-convergence&#34;&gt;几乎必然收敛（almost-sure convergence）&lt;/h2&gt;
&lt;p&gt;在某些情况下，若随机变量序列能够和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;几乎接近，我们说它几乎必然收敛。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(P(\lim_{n \to \infty} X_n = c) = 1\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;几乎必然收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{a.s.}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; 0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有 &lt;span class=&#34;math display&#34;&gt;\[
P(|X_n - c| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，几乎必然收敛和依概率收敛是不等价的，因为&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} f(x_n)\)&lt;/span&gt;中的极限符号不总是能够交换到函数&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部，举个简单的例子： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\{ x_n \} = -\frac{1}{n}, \
f(x) = \begin{cases}
x^2 - 1, &amp;amp; -1 \le x &amp;lt; 0 \\
x, &amp;amp; x \ge 0
\end{cases} \\
\lim_{n \to \infty} f(x_n) = \lim_{n \to \infty} n = -1 \ne f(\lim_{n \to \infty} x_n) = f(0) = 0
\end{gathered}
\]&lt;/span&gt; 注意&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是右连续的，这也意味着，我们可以找到类似的右连续的分布函数&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，使得极限符号不能被移至&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;内部。也就是说，几乎必然收敛和依概率收敛是不等价的，而显然，几乎必然收敛是强于依概率收敛的。&lt;/p&gt;
&lt;h2 id=&#34;l_p收敛convergence-in-l_p&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛（convergence in &lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;是一个随机变量序列，对于某个&lt;span class=&#34;math inline&#34;&gt;\(p &amp;gt; 0\)&lt;/span&gt;，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} \E(|| X_n - c||_p^p) = 0\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{L_p}{\to} c\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;均方收敛&#34;&gt;均方收敛&lt;/h3&gt;
&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛又称作均方收敛。根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/a&gt;， &lt;span class=&#34;math display&#34;&gt;\[
P(|X_n-\E(X_n)| \ge \epsilon) \le \frac{\Var(X_n)}{\epsilon^2} = \frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2}
\]&lt;/span&gt; 在两边取&lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|X_n-\E(X_n)| \ge \epsilon) \le \lim_{n \to \infty} \frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2} = 0
\]&lt;/span&gt; 即均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也强于依概率收敛；但它和几乎必然收敛之间并没有推导关系。&lt;/p&gt;
&lt;h2 id=&#34;依分布收敛convergence-in-distribution&#34;&gt;依分布收敛（convergence in distribution）&lt;/h2&gt;
&lt;p&gt;前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;是一个随机变量序列，让&lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;的分布函数，如果存在一个分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} F_n(x) = F(x)\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;依分布收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{d}{\to} F\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B&#34;&gt;随机变量的收敛&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</link>
      <pubDate>Fri, 17 Jun 2022 20:32:59 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</guid>
      <description>

&lt;h2 id=&#34;two-class-logistic-regression&#34;&gt;Two-class Logistic Regression&lt;/h2&gt;
&lt;p&gt;A linear classifier is a classifier with a linear separation hyperplane. That is to say, suppose the feature space is &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt;, then we will process the feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with a feature extractor function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = w^Tx + b\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R\)&lt;/span&gt;, and then operate on &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, without directly depending on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; anymore.&lt;/p&gt;
&lt;p&gt;Logistic Regression is a binary linear classifier, which takes a probabilistic approach. It maps the &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to a probability value between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; by applying a sigmoid function &lt;span class=&#34;math inline&#34;&gt;\(\sigma(z) = \frac{1}{1 + e^{-z}}\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(y = +1|x) &amp;amp;= \sigma(f(x)) \\
p(y = -1|x) &amp;amp;= 1 - \sigma(f(x)) = \sigma(-f(x)) \\
\end{aligned}
\]&lt;/span&gt; Or equivalently, &lt;span class=&#34;math inline&#34;&gt;\(p(y|x) = \sigma(yf(x))\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f_{LR}(x) = \arg \max\limits_{y \in \{-1, 1\}} \sigma(y(f(x)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Logistic Regression is a discriminative classifier because we are directly modelling &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;, with no intermediate step on &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = {x^{(i)}, y^{(i)}, i=1, \dots, M}\)&lt;/span&gt;, Logistic Regression is learning by maximizing the &lt;strong&gt;conditional&lt;/strong&gt; log-likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
l(w, b) = \log L(w, b) = \log \prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) = \sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(w^\star, b^\star) &amp;amp;= \arg \max\limits_{w, b}l(w, b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log \frac{1}{1 + e^{-y^{(i)}(w^Tx^{(i)}+b)}} \\
&amp;amp;= \arg \max\limits_{w, b}--\sum_{i=1}^M\log \frac{1}{1 + e^{-y^{(i)}(w^Tx^{(i)}+b)}} \\
&amp;amp;= \arg \min\limits_{w, b}-\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b)) \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(J(w, b) = -\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b))\)&lt;/span&gt; be the target function. There is no closed-form solution to this optimization problem. Rather, it is to be solved by iterative algorithm, e.g. gradient descent. For each iteration, parameters are updated by &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
w \leftarrow w - \eta\frac{\partial J}{\partial w} \\
b \leftarrow b - \eta\frac{\partial J}{\partial b}
\end{gather}
\]&lt;/span&gt; Specifically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial w} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))} \frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))} \frac{\partial (y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))} \sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial b} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log \sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))} \frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))} \frac{\partial (y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))} \sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The posterior obtained by binary &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt; has the form of &lt;span class=&#34;math display&#34;&gt;\[
p(y|x) = \frac{1}{1 + e^{-(wx + b)}}
\]&lt;/span&gt; This is not to say LDA and LR are the same in binary case. Note that LDA is a generative model which learns &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;, LR is a discriminative model which only learns &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;. One can discriminate with a generative model, but not reversely.&lt;/p&gt;
&lt;h2 id=&#34;multi-class-logistic-regression&#34;&gt;Multi-class Logistic Regression&lt;/h2&gt;
&lt;p&gt;One way to train use Logistic Regression in multi-class classification in to for each class &lt;span class=&#34;math inline&#34;&gt;\(i = {1, \dots, C}\)&lt;/span&gt;, assign a weight vector &lt;span class=&#34;math inline&#34;&gt;\(w^{(i)}\)&lt;/span&gt;, the probability is define by the softmax function: &lt;span class=&#34;math display&#34;&gt;\[
p(y = c|x) =\frac{exp(w^{(c)}\cdot x + b^{(c)})}{\sum_{i=1}^Cexp(w^{(i)} \cdot x + b^{(i)})}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(f_{LR_{multi-class}}(x) = \arg \max\limits_{y \in \{1,\dots,C\}}p(y|x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To prevent overfitting, a prior distribution is usually added to &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose each dimension of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is independently sampled from a Gaussian distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0, \frac{C}{2})\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
p(w) \propto \exp(-\frac{1}{C}w^Tw)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bounding Mutual Information</title>
      <link>https://chunxy.github.io/notes/papers/bounding-mutual-information/</link>
      <pubDate>Thu, 02 Jun 2022 14:01:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/bounding-mutual-information/</guid>
      <description>

&lt;h2 id=&#34;i_textba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;The very basic bound on the Mutual Information is based on the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/differentiation/#KL-divergence Entropy and Conditional Entropy&#34;&gt;non-negativity&lt;/a&gt; of KL-divergence. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) = &amp;amp;\E_{p(x,y)} \log \frac{p(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} + \\
&amp;amp;\underbrace{\E_{p(x,y)} \log \frac{p(x|y)}{q(x|y)}}_{\E_{p(y)} D_{KL}(p(x|y) || q(x|y)} \\
\ge &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log q(x|y) + H(X) \triangleq I_\text{BA}
\end{aligned}
\]&lt;/span&gt; This bound is not usually tractable since &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H(X)\)&lt;/span&gt; has no closed-form expression.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) = p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;i_textuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; is replaced with an unnormalized model, that is &lt;span class=&#34;math display&#34;&gt;\[
q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)}, \text{where } Z(y) = \E_{p(x)} e^{f(x,y)}
\]&lt;/span&gt; Substituting this back to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} \log \frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \triangleq I_\text{UBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that by scaling &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; term is canceled.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{f(x,y)} &amp;amp;= \frac{p(x, y)}{p(x)p(y)} Z(y) \\
e^{f(x,y)} &amp;amp;= \frac{Z(y)}{p(y)} p(y|x) \\
f(x,y) &amp;amp;= \ln p(y|x) + \underbrace{\ln \frac{Z(y)}{p(y)}}_{c(y)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textdv&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By further applying Jensen’s inequality to the &lt;span class=&#34;math inline&#34;&gt;\(\E_{p(y)} Z(y)\)&lt;/span&gt; term in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log\E_{p(y)} [Z(y)] \triangleq I_\text{DV}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The original &lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt; bound is in effect derived from &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/#Variational Lower-bound&#34;&gt;the variational lower bound of KL-divergence&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_\text{KL}(p_{(x,y)} || p(x) \otimes p(y)) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log [\E_{p(x) \otimes p(y)} f(x,y)] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log [\E_{p(y)} \E_{p(x)} e^{f(x,y)}] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log \E_{p(y)} Z(y) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_texttuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt; function also has the following property: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\forall x,a&amp;gt;0,\log(x) &amp;amp;\le \frac{x}{a} + \log(a) - 1 &amp;amp;\iff \\
a + a\log(x) &amp;amp;\le x + a\log(a) &amp;amp;\iff \\
a\log(x) - x &amp;amp;\le a\log(a) - a \\
\end{aligned}
\]&lt;/span&gt; Therefore, we can insert the inequality &lt;span class=&#34;math inline&#34;&gt;\(\log Z(y) \le \frac{Z(y)}{a(y)} + \log a(y) - 1\)&lt;/span&gt; into the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; to get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) - 1 \big)] \triangleq I_\text{TUBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This bound is tight when&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(f(x,y) = \log p(y|x) + \underbrace{\log \frac{Z(y)}{p(y)}}_{c(y)}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(a(y) = Z(y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;i_textnwj&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By setting &lt;span class=&#34;math inline&#34;&gt;\(a(y) = e\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) - 1 \big)] \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - e^{-1}\E_{p(y)} Z(y)\triangleq I_\text{NWJ}
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt; is a special case of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, its bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(Z(y)\)&lt;/span&gt; self-normalizes to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. In this case &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x,y) &amp;amp;= \log p(y|x) + \log \frac{e}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(x|y)}{p(x)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textnce&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate the &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; given one sample from &lt;span class=&#34;math inline&#34;&gt;\(p(x_1)p(y|x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; additional independent samples &lt;span class=&#34;math inline&#34;&gt;\(x_{2:K} \sim p^{K-1}(x_{2:K})\)&lt;/span&gt;. For any random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is independent from &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_1,Z;Y) &amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\
&amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\
&amp;amp;= I(X_1;Y)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y) = I(X_1,X_{2:K};Y)\)&lt;/span&gt;. Bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; becomes bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1,X_{2:K};Y)\)&lt;/span&gt;, which can be estimated using any of the preceding methods.&lt;/p&gt;
&lt;p&gt;Set the critic to &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + \overbrace{\log \frac{e^{g(x,y)}} {a(y;x_{1:K})}}^{h(x_{1:K},y)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sample among &lt;span class=&#34;math inline&#34;&gt;\(x_{1:K}\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Usually &lt;span class=&#34;math inline&#34;&gt;\((x,y)_{1:K}\)&lt;/span&gt; are sampled from the same marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(x,y)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is then uniformly drawn among &lt;span class=&#34;math inline&#34;&gt;\(y_{1:K}\)&lt;/span&gt;. Substitute these to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\label{infonce} \begin{aligned} 
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\E_{p(y)} Z(y) \\
&amp;amp;= \E_{p(x_{1:K},y)} [1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [\E_{p(x_{1:K})} e^{1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [e\E_{p(x_{1:K})} \frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} \\
\end{aligned}
\]&lt;/span&gt; Further set &lt;span class=&#34;math inline&#34;&gt;\(a(y;x_{1:K}) = \frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}\)&lt;/span&gt;. Substitute this into the last term in equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{infonce}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} &amp;amp;= \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}}{\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} } \\
&amp;amp;\stackrel{P}{\to} \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}} {\E_{p(x_{1:K})} e^{g(x,y)} } \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K})p(y|x_{1:K})} [\log \frac{e^{g(x,y)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} }] \\
&amp;amp;\approx \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log \frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)} }] \triangleq I_\text{NCE}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt; is tightly bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log K\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log \frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)}} ] \\
&amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K (\log \frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} )] + \log K \\
&amp;amp;\le \E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{\sum_{i=1}^K e^{g(x_i, y_j)}} {e^{g(x_j,y_j)}} ]} + \log K
\end{aligned}
\]&lt;/span&gt; The equality is reached when &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + {h(x_{1:K},y)} = 1 + \log \frac{p(x|y)}{p(x)}\)&lt;/span&gt; as in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;. In this case, it can be derived that &lt;span class=&#34;math inline&#34;&gt;\(g(x,y) = g^\star(x,y) = \frac{p(y|x)}{p(y)}\)&lt;/span&gt;. And then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{\frac{p(y_j|x_j)}{p(y_j)} + \sum_{i=1,i \ne j}^K \frac{p(y_j|x_i)}{p(y_j)}} {\frac{p(y_j|x_j)}{p(y_j)}} ]} + \log K \\
&amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K (1 + \frac{p(y_j)}{p(y_j|x_j)} \sum_{i=1,i \ne j}^K \frac{p(y_j|x_i)}{p(y_j)}) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(1 + \frac{p(y_j)}{p(y_j|x_j)} (K-1) \E_{p(y)} \frac{p(y|x_i)}{p(y)} \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[1 + \frac{1}{K} \sum_{j=1}^K \big( \frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big( \frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \frac{p(y_j)}{p(y_j|x_j)} ]} - \log(K-1) + \log K \\
&amp;amp;= I(X_1;Y) - \log(K-1) + \log K
\end{aligned}
\]&lt;/span&gt; This derivation is much looser than &lt;a href=&#34;https://chunxy.github.io/notes/papers/contrastive-predictive-coding/#Bounding the Mutual Information&#34;&gt;that in the InfoNCE’s original paper&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;with-i_textuba&#34;&gt;With &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;There is another approach to the derivation of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;, that stems from an estimate of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; (which I won’t call empirical estimate) and that may be more intuitive: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;I(X;Y) \ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;= \E_{p^N(x,y)} f(x_1,y_1) - \E_{p^N(x,y)} \log Z(y_1) \\
&amp;amp;= \E_{p^N(x,y)} \log e^{f(x_1,y_1)} \\
&amp;amp;\quad\;- \E_{p^N(x,y)} \log \E_{p(x&amp;#39;)} e^{f(x&amp;#39;,y_1)} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}}{\E_{p(x&amp;#39;)} e^{f(x&amp;#39;,y)}} \\
&amp;amp;\simeq \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\frac{1}{N} \sum_i e^{f(x_i,y_1)}} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\sum_i e^{f(x_i,y_1)}} + \log N \\
&amp;amp;\triangleq I_\text{NCE} \le \log N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Cross Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:46 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</guid>
      <description>
&lt;p&gt;The Cross Entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, instead of the true distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Cross Entropy of distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; relative to a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) = -\mathrm{E}_{x \sim p} \log q(x)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的函数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 31 Mar 2022 10:08:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</guid>
      <description>
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一一维随机变量，&lt;span class=&#34;math inline&#34;&gt;\(f: \R \to \R\)&lt;/span&gt;为一函数，那么&lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;的期望以及分布情况会是什么样的呢？&lt;/p&gt;
&lt;p&gt;我们这里只讨论&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是单调函数的情况，令&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X)\)&lt;/span&gt;，那么 &lt;span class=&#34;math display&#34;&gt;\[
P_Y(y) = P_Y(Y \le y) = P_X(f(X) \le y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递增， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \le f^{-1}(y)) = P_X(f^{-1}(y)) \\ 
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial P_X(f^{-1}(y))}{\partial y} \\
&amp;amp;= p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递减， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \ge f^{-1}(y)) = 1 - P_X(X \le f^{-1}(y)) = 1 - P_X(f^{-1}(y)) \\ 
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial [1 - P_X(f^{-1}(y))]}{\partial y} \\
&amp;amp;= -p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之，&lt;span class=&#34;math inline&#34;&gt;\(p_Y(y) = p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)|\)&lt;/span&gt;。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</link>
      <pubDate>Tue, 11 Jan 2022 20:26:40 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</guid>
      <description>

&lt;p&gt;If a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable and satisfies certain “regularity conditions”, we can get a nice guarantee that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will converge by gradient descent.&lt;/p&gt;
&lt;h3 id=&#34;l-smoothness&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness&lt;/h3&gt;
&lt;p&gt;Qualitatively, smoothness means that the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; changes in a controlled, bounded manner. Quantitatively, smoothness assumes that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has a Lipschitz gradient. That means there exists an &lt;span class=&#34;math inline&#34;&gt;\(L &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
||\nabla f(x) - \nabla f(y)||_2 \le L||x - y||_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; We will say that such function is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth or strongly smooth. &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Proof&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\frac{L}{2}||x||^2_2 - f(x)\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With the smoothness, or the Lipschitz gradient, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from above.&lt;/p&gt;
&lt;p&gt;We will assume &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness in all cases below. Meanwhile, the local minima (in convex case, the global minima) is denoted as &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-convexity&#34;&gt;Convergence with Convexity&lt;/h3&gt;
&lt;p&gt;With convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with linear approximation: &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;Now consider a &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We adopt a fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt; so that the update in each iteration will be &lt;span class=&#34;math display&#34;&gt;\[
x_{k+1} = x_k - \frac{1}{L}\nabla f(x)
\]&lt;/span&gt; By the definition of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness, &lt;span class=&#34;math display&#34;&gt;\[
\label{diff} \begin{aligned} 
 f(x_{k+1}) &amp;amp;\le f(x_k) + (-\frac{1}{L}\nabla f(x_k))^T \nabla f(x_k) + \frac{L}{2}||-\frac{1}{L}\nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_k)\)&lt;/span&gt; holds in each iteration.&lt;/p&gt;
&lt;p&gt;By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
f(x^\star) \ge f(x_k) + (x^\star - x_k)^T \nabla f(x_k) \\
\label{convexity} f(x_k) \le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k)
\end{gather}
\]&lt;/span&gt; Substituting the result back to the right-hand side of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k) - \frac{1}{2M}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; (x_k - x^\star)^T L(x_k - x_{k+1}) - \frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; \frac{L}{2}(2(x_k - x^\star)^T(x_k - x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\
\end{aligned}
\]&lt;/span&gt; By using the fact that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\
2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2
\end{aligned}
\]&lt;/span&gt; We have &lt;span class=&#34;math display&#34;&gt;\[
\label{closer} f(x_{k+1}) - f(x^\star) \le \frac{L}{2}(||x_k - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This indicates that &lt;span class=&#34;math inline&#34;&gt;\(x_{k+1}\)&lt;/span&gt; is closer to the global minima &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k [f(x_{i+1}) - f(x^\star)] &amp;amp;\le \sum_{i=0}^k \frac{L}{2}(||x_i - x^\star||^2_2 - ||x_{i+1} - x^\star||^2_2) \\
&amp;amp;= \frac{L}{2}(||x_0 - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2) \\
&amp;amp;\le \frac{L}{2}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(f(x_{i+1}) \le f(x_i), \forall i=0,\dots,k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_i), \forall i=0,\dots,k\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k(f(x_{k+1}) - f(x^\star)) \le \frac{L}{2}||x_0 - x^\star||^2_2 \\
f(x_{k+1}) - f(x^\star) \le \frac{L}{2k}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;variant-step-size&#34;&gt;Variant Step Size&lt;/h5&gt;
&lt;p&gt;In real application scenario, it may be difficult to know &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; exactly. However, as long as we choose &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k \le \frac{1}{L}\)&lt;/span&gt; in each iteration, the convergence and the rate still holds. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\alpha_k \nabla f(x_k))^T \nabla f(x_k) + \frac{L}{2}||-\alpha_k \nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} = f(x_{k+1}) - f(x^\star) \ge 0\)&lt;/span&gt;, the above becomes &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} \le \eta_{k} - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2\)&lt;/span&gt;. From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{convexity}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\eta_k = f(x_k) - f(x^\star) \le (x_k - x^\star)^T \nabla f(x_k) \le ||x_k - x^\star||_2||\nabla f(x_k)||_2 \\
0 \le \frac{\eta_k}{||x_k - x^\star||_2} \le ||\nabla f(x_k)||_2 \\
-||\nabla f(x_k)||^2_2 \le -\frac{\eta^2_k}{||x_k - x^\star||^2_2} \\
\end{gather}
\]&lt;/span&gt; Substituting it back to give &lt;span class=&#34;math display&#34;&gt;\[
\eta_{k+1} \le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k) \eta^2_k}{||x_k - x^\star||^2_2}
\]&lt;/span&gt; From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{closer}\)&lt;/span&gt; we already have &lt;span class=&#34;math display&#34;&gt;\[
||x_{k+1} - x^\star||^2_2 \le ||x_k - x^\star||^2_2 \le \dots \le ||x_0 - x^\star||^2_2
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\eta_{k+1} &amp;amp;\le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k) \eta^2_k}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_k} &amp;amp;\le \frac{1}{\eta_{k+1}} - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot \frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot \frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \text{, by } \eta_k \ge \eta_{k+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k (\frac{1}{\eta_{i+1}} - \frac{1}{\eta_i}) &amp;amp;\ge \sum_{i=0}^k \frac{(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_0} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
f(x_{k+1} - f(x^\star)) &amp;amp;\le \frac{||x_0 - x^\star||^2_2}{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = \infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha^2_i &amp;lt; \infty\)&lt;/span&gt;, the Variant Step Size can be shown to converge. Particularly when &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \frac{1}{k}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
f(x_{k+1} - f(x^\star)) \le \Theta(\frac{1}{\log k})||x_0 - x^\star||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-strong-convexity&#34;&gt;Convergence with Strong Convexity&lt;/h3&gt;
&lt;h4 id=&#34;strong-convexity&#34;&gt;Strong Convexity&lt;/h4&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly convex if there exists a &lt;span class=&#34;math inline&#34;&gt;\(l &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Further, strong convexity is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(f(x) - \frac{l}{2}||x||^2_2\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With strong convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with a convex quadratic approximation.&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate-1&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size-1&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth above. That is &lt;span class=&#34;math display&#34;&gt;\[
\label{sm} f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; In this section we further assume that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth as well as strong convex: &lt;span class=&#34;math display&#34;&gt;\[
\label{sc} f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2, \forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(F(y) = f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2\)&lt;/span&gt;. Take derivative of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla f(x) + l(y - x) = 0 \\
y_0 - x = -\frac{\nabla f(x)}{l}
\end{aligned}
\]&lt;/span&gt; Because the second derivative of &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(l \succeq 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; reaches global minimum at the local minima &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt;. Substitute &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; back to &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge F(y) \ge F(y_0) = f(x) - \frac{1}{2l}||\nabla f(x)||^2_2
\]&lt;/span&gt; This holds when &lt;span class=&#34;math inline&#34;&gt;\(y = x^\star\)&lt;/span&gt;. That is, &lt;span class=&#34;math display&#34;&gt;\[
f(x^\star) \ge f(x) - \frac{1}{2l}||\nabla f(x)||^2_2 \\
||\nabla f(x)||^2_2 \ge 2l(f(x) - f(x^\star))
\]&lt;/span&gt; The above inequality is referred to as Polyak-Lojasiewicz Inequality. By combining it with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, where we implicitly assume a fixed step-size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) + \frac{1}{2L}(-||\nabla f(x_k)||^2_2) \\ 
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) + \frac{1}{2L}(-2l(f(x) - f(x^\star))) \\
&amp;amp;= (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le (1 - \frac{l}{L}) (f(x_k) - f(x^\star)) \\
&amp;amp;\le (1 - \frac{l}{L})(1 - \frac{l}{L}) (f(x_{k-1}) - f(x^\star)) \\
&amp;amp;\le \dots \\
&amp;amp;\le (1 - \frac{l}{L})^{k+1} (f(x_0) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sm}\)&lt;/span&gt; and equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sc}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2 \le f&amp;amp;(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2 \\
l &amp;amp;\le L \\
\end{aligned}
\]&lt;/span&gt; Usually it would be the case that &lt;span class=&#34;math inline&#34;&gt;\(l &amp;lt; L\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; 1 - \frac{l}{L} &amp;lt; 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;non-convex-case&#34;&gt;Non-convex Case&lt;/h3&gt;
&lt;p&gt;Without convexity condition, we can still exploit the property of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness. Sum up on both sides of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(k = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} 
\sum_{i=0}^k f(x_{k+1}) &amp;amp;\le \sum_{i=0}^k f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k f(x_{k+1}) - f(x_k) &amp;amp;\le -\frac{1}{2L} \sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is a local minima, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) \le f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
f(x^\star) - f(x_0) &amp;amp;\le - \frac{1}{2L}\sum_{i=0}^k ||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k ||\nabla f(x_k)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
\end{aligned}
\]&lt;/span&gt; In this case, the error is measured by the first derivative. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\tilde x = \arg \min_i ||\nabla f(x_i)||^2_2\)&lt;/span&gt;, where the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the closest to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k||\nabla f(\tilde x)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
||\nabla f(\tilde x)||^2_2 &amp;amp;\le \frac{2L(f(x_0) - f(x^\star))}{k} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Convergence%20of%20Gradient%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Convergence of Gradient Descent.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rkganti.wordpress.com/2015/08/21/convergence-rate-of-gradient-descent-algorithm/&#34;&gt;Convergence rate of gradient descent algorithm | bps/Hz (wordpress.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/strong-convexity&#34;&gt;Strong convexity · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mutual Information</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</link>
      <pubDate>Thu, 19 May 2022 12:20:04 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</guid>
      <description>

&lt;p&gt;Mutual Information of &lt;strong&gt;two random variables&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]&lt;/span&gt; By its definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I(X;Y) = I(Y;X) \\
I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \otimes p_Y) \\
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;To better illustrate the formula of mutual information between two &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/&#34;&gt;Gaussian&lt;/a&gt;-distributed random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. We can concatenate them to form, say an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, which is also Gaussian-distributed. Then the mutual information between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be computed as: &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\]&lt;/span&gt; The key to the derivation is that mutual information is the KL-divergence between the joint distribution and the product of the marginal distributions.&lt;/p&gt;
&lt;p&gt;The joint can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X:Y} = N(\underbrace{\mu_X:\mu_Y}_\mu,
\underbrace{
\begin{bmatrix} \
\Sigma_{X} &amp;amp; \Cov_{XY} \\
\Cov_{YX} &amp;amp; \Sigma_{Y} \\
\end{bmatrix}
}_\Sigma
)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The product of marginals can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X} \times p_{Y} = N(\mu_x:\mu_y,
\begin{bmatrix} \
\Sigma_{xx} &amp;amp; 0 \\
0 &amp;amp; \Sigma_{yy} \\
\end{bmatrix}
)
\]&lt;/span&gt; The probability density function of an &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;-dimensional Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(p(x&amp;#39;) = \frac{1}{\sqrt{|2\pi \Sigma&amp;#39;|}}e^{-\frac{1}{2}(x&amp;#39;-\mu&amp;#39;)^T\Sigma&amp;#39;^{-1}(x&amp;#39;-\mu&amp;#39;)}\)&lt;/span&gt;. The entropy of this Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 n&amp;#39; + \frac 1 2 \log |2\pi\Sigma&amp;#39;|\)&lt;/span&gt;. In view of above, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \times p_Y)
= \int p_{X:Y}(\underbrace{x:y}_z) \log \frac{p_{X:Y}(x:y)} {p_X(x) p_{Y}(y)} \d z \\
&amp;amp;= \int p_{X:Y}(\underbrace{x:y}_z) \log p_{X:Y}(x:y) \d z -
    \int p_{X:Y}(\underbrace{x:y}_z) \log p_X(x) \d z \\
&amp;amp;\quad\quad\quad -\int p_{X:Y}(\underbrace{x:y}_z) \log p_Y(y) \d z \\
&amp;amp;= \int p_{Z}(z) \log p_{Z}(z) \d z -
    \int p_{X}(x) \log p_X(x) \d x - 
    \int p_{Y}(y) \log p_Y(y) \d y \\
&amp;amp;= -(\log \sqrt{\det(2\pi \Sigma)} + \frac n 2) +
    (\log \sqrt{\det(2\pi \Sigma_{X}}) + \frac {n_X} 2) \\
&amp;amp;\quad\quad\quad +(\log \sqrt{\det(2\pi \Sigma_{Y}}) + \frac {n_Y} 2) \\    
&amp;amp;= \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;kronecker-gaussian&#34;&gt;Kronecker Gaussian&lt;/h4&gt;
&lt;p&gt;Consider the multivariate Gaussian distribution random vector &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; of the same length &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. Suppose they are both independent internally and they have the component-wise correlation &lt;span class=&#34;math inline&#34;&gt;\(corr(X_i, Y_j) = \delta_{ij} \rho\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\rho \in (-1, 1)\)&lt;/span&gt; (open to ensure the covariance matrix is invertible), &lt;span class=&#34;math inline&#34;&gt;\(1 \le i, j \le k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\delta_{ij}\)&lt;/span&gt; is the Kronecker’s delta: &lt;span class=&#34;math display&#34;&gt;\[
\delta_{ij} = 
\begin{cases}
0, &amp;amp; i \ne j \\
1, &amp;amp; i = j
\end{cases}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z_k\)&lt;/span&gt; be the vector concatenated by &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;. It is easy to draw its covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{Z_k}\)&lt;/span&gt; like &lt;span class=&#34;math display&#34;&gt;\[
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho \\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\
\end{pmatrix}
\]&lt;/span&gt; The mutual information between the &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_{X_k} \det \Sigma_{Y_k}}{\det \Sigma_{Z_k}} = -\frac 1 2 \log \det \Sigma_{Z_k}
\]&lt;/span&gt; The problem remains as how to compute &lt;span class=&#34;math inline&#34;&gt;\(\det \Sigma_{Z_{k}}\)&lt;/span&gt;. Applying the Laplacian expansion along the first column, we find we have to deal with the determinants of following two matrices (dashed lines rule out the row/column to be deleted): &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
A_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho \\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\
\end{array} \right)
}_\text{$2k$ columns},
B_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho \\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\
\end{array} \right)
}_\text{$2k$ columns}, \\
\det Z_k =
\left.
\begin{cases}
\det A_k - \rho \det B_k, &amp;amp; \text{$k$ is odd} \\
\det A_k + \rho \det B_k, &amp;amp; \text{$k$ is even} \\
\end{cases}
\right\}
\Rightarrow \det Z_k = \det A_k + (-1)^k \rho \det B_k
\end{gather}
\]&lt;/span&gt; Applying the Laplacian expansion along the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th row of &lt;span class=&#34;math inline&#34;&gt;\(A_k\)&lt;/span&gt;, we find &lt;span class=&#34;math display&#34;&gt;\[
\det A_k =
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho \\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\
\end{array} \right|
}_\text{$2k$ columns}
= \det Z_{k-1}
\]&lt;/span&gt; Applying the Laplacian expansion along the first row of &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt;, we find &lt;span class=&#34;math display&#34;&gt;\[
\det B_k = (-1)^k \rho
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho \\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\
\end{array} \right|
}_\text{$2k$ columns} 
= (-1)^k \rho \det Z_{k-1}
\]&lt;/span&gt; In all, &lt;span class=&#34;math inline&#34;&gt;\(\det Z_k = \det Z_{k-1} - (-1)^{2k} \rho^2 \det Z_{k-1} = (1 - \rho^2) Z_{k-1}\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(\det Z_1 = 1 - \rho^2\)&lt;/span&gt;, we finally have &lt;span class=&#34;math display&#34;&gt;\[
\det Z_k = (1 - \rho^2)^k
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = -\frac 1 2 \log \det \Sigma_{Z_k} = -\frac k 2 \log (1 - \rho^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/438607/mutual-information-between-subsets-of-variables-in-the-multivariate-normal-distr&#34;&gt;Mutual information between subsets of variables in the multivariate normal distribution - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.nyu.edu/~kleeman/infolect7.pdf&#34;&gt;Information Theory and Predictability Lecture 7: Gaussian Case&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>特征函数</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 09 May 2022 11:51:08 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid>
      <description>

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;h3 id=&#34;感性认知&#34;&gt;感性认知&lt;/h3&gt;
&lt;p&gt;根据泰勒级数我们可以得知，两个函数&lt;span class=&#34;math inline&#34;&gt;\(f(x),g(x)\)&lt;/span&gt;，如果它们各阶导数相等的越多，它们就越相似，换言之 &lt;span class=&#34;math display&#34;&gt;\[
\text{各阶导数都相同} \Rightarrow f(x) = g(x)
\]&lt;/span&gt; 可以说，函数的各阶导数即是它们的特征。&lt;/p&gt;
&lt;p&gt;对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即 &lt;span class=&#34;math display&#34;&gt;\[
\text{各阶矩都相同} \Rightarrow \text{随机变量对应的分布相同}
\]&lt;/span&gt; 对于随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，其特征函数定义为 &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{itX}\)&lt;/span&gt;的泰勒级数为 &lt;span class=&#34;math display&#34;&gt;\[
e^{itX} = 1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots + \frac{(itX)^n}{n!}
\]&lt;/span&gt; 代入特征函数可得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\varphi(t) &amp;amp;= \E[1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots + \frac{(itX)^n}{n!}] \\
&amp;amp;= \E[1] + \E[\frac{itX}{1!}] - \E[\frac{t^2X^2}{2!}] + \dots + \E[\frac{(itX)^n}{n!}] \\
&amp;amp;= 1 + \frac{it \overbrace{\E[X]}^\text{一阶矩} }{1!} - \frac{t^2 \overbrace{\E[X^2]}^\text{二阶矩} }{2!} + \dots + \frac{(it)^n \overbrace{\E[X^n]}^\text{n阶矩} }{n!} \\
\end{aligned}
\]&lt;/span&gt; 可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。&lt;/p&gt;
&lt;h3 id=&#34;理性认知&#34;&gt;理性认知&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}] = \int_{-\infty}^{+\infty} e^{itx} p(x)\; dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而对&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;进行逆傅里叶变换可得 &lt;span class=&#34;math display&#34;&gt;\[
F(t) = \int_{-\infty}^{+\infty} p(x) e^{-itx} dx
\]&lt;/span&gt; 可见二者互为共轭关系： &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \overline{F(t)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;应用&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;通过求&lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt;时的各阶导数，可以快速求得各阶矩： &lt;span class=&#34;math display&#34;&gt;\[
\varphi^{(k)}(0) = i^k \E[X^k]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/23686709&#34;&gt;特征函数的理解&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</link>
      <pubDate>Fri, 07 Jan 2022 12:52:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</guid>
      <description>

&lt;p&gt;Support Vector Machine is used in binary classification task. It aims to find a linear hyperplane that separates the data with different labels with the maximum margin. By symmetry, there should be at least one margin point on both side of the decision boundary. These points are called &lt;strong&gt;support vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hard-margin-svm&#34;&gt;Hard Margin SVM&lt;/h3&gt;
&lt;p&gt;Suppose the data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}), i=1, \dots, M\}\)&lt;/span&gt;, and is linearly separable. The separation hyperplane will be in the form of &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is above the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;gt; 0\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = -1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is below the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;lt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The distance (margin) from a data point to the separation hyperplane will be &lt;span class=&#34;math display&#34;&gt;\[
d^{(i)} = \frac{|w^Tx^{(i)} + b|}{||w||_2} = \frac{y^{(i)}(w^Tx^{(i)} + b)}{||w||_2}\\
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d^{(i)}\)&lt;/span&gt; is called the geometric distance, while &lt;span class=&#34;math inline&#34;&gt;\(|w^Tx^{(i)} + b|\)&lt;/span&gt; is called the functional distance.&lt;/p&gt;
&lt;p&gt;The margin of the hyperplane &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[
d = \min\limits_{i \in \{1,\dots,M\}} d^{(i)} = \min\limits_{i \in \{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; The maximum margin solution is found by solving &lt;span class=&#34;math display&#34;&gt;\[
\max\limits_{w,b} \min\limits_{i \in \{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; Suppose &lt;span class=&#34;math inline&#34;&gt;\((w^\prime, b^\prime)\)&lt;/span&gt; is one solution to the above. Then &lt;span class=&#34;math inline&#34;&gt;\((\lambda w^\prime, \lambda b^\prime)\)&lt;/span&gt; is also a solution, because &lt;span class=&#34;math display&#34;&gt;\[
\frac{y^{(i)}(\lambda (w^{\prime})^Tx^{(i)} + \lambda b^\prime)}{||\lambda w^\prime||_2} = \frac{\lambda y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{\lambda ||w^\prime||_2} = \frac{y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{||w^\prime||_2}
\]&lt;/span&gt; There is an extra degree of freedom in this problem. Therefore, we may impose &lt;span class=&#34;math display&#34;&gt;\[
\min\limits_{i \in \{1,\dots,M\}} y^{(i)}(w^Tx^{(i)}+b) = 1
\]&lt;/span&gt; to consume this freedom. Consequently, the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max \frac{1}{||w||_2} \iff \min \frac{1}{2}||w||^2_2 \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1, i = 1,\dots,M
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-margin-svm&#34;&gt;Soft Margin SVM&lt;/h3&gt;
&lt;p&gt;It may not happen that the real data are linearly-separable, or there may exist noisy samples that disrupt this linear separability. In such case, we may allow some samples to violate the margin. We define some slack variables &lt;span class=&#34;math inline&#34;&gt;\(\xi_i \ge 0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;gt; 0\)&lt;/span&gt; means that the sample is inside the margin (or even this sample will be misclassified), &lt;span class=&#34;math inline&#34;&gt;\(\xi_i = 0\)&lt;/span&gt; means that the sample is outside the margin.&lt;/p&gt;
&lt;p&gt;Of course, these slack variables should be as small as possible. Then the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\min \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, \xi_i \ge 0, i=1,\dots,M
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is a penalty factor on violating the margin. To some extent, it avoid overfitting.&lt;/p&gt;
&lt;p&gt;To solve this, first convert the problem into the standard form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \frac{1}{2}||w||^2_2 &amp;amp;+ C\sum_{i=1}^M\xi_i \\
s.t.\quad 1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b) &amp;amp;\le 0, i=1,\dots,M \\
-\xi_i &amp;amp;\le 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; This problem gives a strong duality. The solution will satisfy the KKT conditions. We first write down the Lagrangian: &lt;span class=&#34;math display&#34;&gt;\[
L(w,b,\xi,\lambda,\mu) = \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b)) - \sum_{i=1}^M\mu_i\xi_i
\]&lt;/span&gt; Then we form the dual problem: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\lambda,\mu;\lambda_i,\mu_i \ge 0} \min_{w,b,\xi}L(w,b,\xi,\lambda,\mu)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial L}{\partial w} = w - \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)}, \frac{\partial^2 L}{\partial w\partial w} = I \succeq 0 \\
\frac{\partial L}{\partial b} = -\sum_{i=1}^M\lambda_iy^{(i)}, \frac{\partial^2 L}{\partial b\partial b} = 0 \succeq 0 \\
\frac{\partial L}{\partial \xi} = C - \lambda - \mu, \frac{\partial^2 L}{\partial \xi\partial \xi} = 0 \succeq 0
\end{gather}
\]&lt;/span&gt; The Hessian matrices of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; are positive semi-definite. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min\limits_{w,b,\xi}L(w,\xi,\lambda,\mu)\)&lt;/span&gt; is obtained at its local minimum, i.e. where its first-order derivative meets &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
w = \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)} \\
\sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda + \mu = C
\]&lt;/span&gt; Substitute above back to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; to transform the dual problem into &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu} \frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + C\sum_{i=1}^M\xi_i - \sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}b) + \sum_{i=1}^M(\lambda_i - C)\xi_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda_i,\mu_i \ge 0, i=1,\dots,M \\
\Downarrow \\
\max_{\lambda,\mu}D(\lambda,\mu) = -\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; Since we know the optimal solution exists, instead of taking derivative of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to solve it, we assume the solution to above problem is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} \\
\mu^\star = C - \lambda^\star
\]&lt;/span&gt; Complementary constraints will give &lt;span class=&#34;math display&#34;&gt;\[
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star)) = 0 \\
\mu_i\xi_i = 0
\]&lt;/span&gt; There is at least one &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j &amp;gt; 0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} = 0\)&lt;/span&gt;, which means the primal constraints &lt;span class=&#34;math display&#34;&gt;\[
1 - \xi_i - y^{(i)}((w^\star)^Tx^{(i)} + b^\star) =  1 - \xi_i - y^{(i)}b^\star\le 0
\]&lt;/span&gt; won’t hold no matter what value &lt;span class=&#34;math inline&#34;&gt;\(b^\star\)&lt;/span&gt; takes. This specific &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j\)&lt;/span&gt;’s slackness complementary will give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
0 &amp;amp;= (1 - \xi^\star_j - y^{(j)}((w^\star)^Tx^{(j)} + b^\star)) \\
b^\star &amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - (w^\star)^Tx^{(j)} \\
&amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)}\cdot x^{(j)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
According to the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt; (not know yet), added with primal constraints and the complementary constraints, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0 \\
\xi_i \ge 0 \\
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star)) = 0 \\
\mu^\star_i\xi^\star_i = 0 \\
\lambda^\star_i + \mu^\star_i = C
\end{gather}
\]&lt;/span&gt; we can make interpretations on the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt;: $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\lambda^\star_i = 0 &amp;amp;\Rightarrow 
\begin{cases}
\mu^\star_i = C \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0
\end{cases}
 \Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \ge 1 \\
 0 &amp;lt; \lambda^\star_i &amp;lt; C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i &amp;gt; 0 \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 1 \\
\lambda^\star_i = C &amp;amp;\Rightarrow 
\begin{cases}
\mu^\star_i = 0 \Rightarrow \xi^\star_i \ge 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 1 \\

\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ They corresponds to cases where sample &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is outside margin, is on the margin, violates the margin, respectively.&lt;/p&gt;
&lt;p&gt;The key to the above derivation is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;, which is to be solved in Solving SVM section.&lt;/p&gt;
&lt;p&gt;Soft Margin SVM is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\min \frac{1}{C}||w||^2_2 + \sum_{i=1}^M\max(0, 1 - y^{(i)}(w^Tx^{(i)} + b))
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l_h(z) = \max(0, 1 - z)\)&lt;/span&gt; is called Hinge loss.&lt;/p&gt;
&lt;h3 id=&#34;kernel-svm&#34;&gt;Kernel SVM&lt;/h3&gt;
&lt;p&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \cdot x^{(j)}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda,\mu)\)&lt;/span&gt; is the inner product between two samples. We can make a table storing these inner products. This naturally introduces the kernel trick, which means we can manipulate the entry in this table by remapping the &lt;span class=&#34;math inline&#34;&gt;\((x^{(i)}, x^{(j)})\)&lt;/span&gt; so that the entry represents the inner product of some higher-dimensional features. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{(i)}), \phi(x^{(j)})
\]&lt;/span&gt; This saves us from explicitly defining the mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a higher-dimensional feature. This also saves the time of the computation of the inner products of these higher-dimensional features, than that of transform-then-inner-product method.&lt;/p&gt;
&lt;p&gt;SVM is a linear classifier, which means its decision boundary is a linear hyperplane in input feature space. By applying kernel trick, we implicitly map the input feature to a higher dimensional one. Therefore the decision boundary would become a linear hyperplane in this higher-dimensional space: &lt;span class=&#34;math display&#34;&gt;\[
w_\phi\phi(x) + b_\phi = 0
\]&lt;/span&gt; And thus this hyperplane is not linear in original feature space.&lt;/p&gt;
&lt;h4 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input feature &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1,\dots,x_N] \in \R^N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order polynomial: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x, x^\prime) = (x^Tx^\prime)^p = (\sum_{i=1}^Nx_ix^\prime_i)^p
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
K(x,x^\prime) = (\sum_{i=1}^Nx_ix^\prime_i)^2 = (\sum_{i=1}^N\sum_{j=1}^Nx_ix_jx^\prime_ix^\prime_j)^2 = \phi(x)\phi(x^\prime)
\]&lt;/span&gt; The transformation applied on original input feature will be &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = [x_1x_1,x_1x_2,\dots,x_1x_N,x_2x_1,\dots x_2x_N,\dots,x_Nx_1,\dots,x_Nx_N] \in \R^{N^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is similar to a Gaussian &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = e^{-\gamma||x - x^\prime||^2_2}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a hyper-parameter to be determined.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This kernel is the case where it is hard to define the transformation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; explicitly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-svm&#34;&gt;Solving SVM&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; is the key to the final solution and by far it is still remained unsolved: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}D(\lambda,\mu) = -\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; We attack it by Sequential Minimal Optimization.&lt;/p&gt;
&lt;h3 id=&#34;multi-class-svm&#34;&gt;Multi-class SVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 v.s. 1&lt;/p&gt;
&lt;p&gt;Train 1-vs-1 SVM for all pairs of classes. Then decide by majority voting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1 v.s. rest&lt;/p&gt;
&lt;p&gt;Train 1-vs-rest SVM for all classes (&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; belongs to the “1”). Choose the class with the largest geometric margin.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</link>
      <pubDate>Mon, 03 Jan 2022 14:50:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</guid>
      <description>

&lt;h3 id=&#34;convex-and-differentiable-function&#34;&gt;Convex and Differentiable Function&lt;/h3&gt;
&lt;p&gt;Given a convex, differentiable &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;, if we are at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is minimized along each coordinate axis, have we found a global minima? That is, does &lt;span class=&#34;math display&#34;&gt;\[
\forall d, i,f(x + d \cdot e_i) \ge f(x) \Rightarrow f(x) = \min_zf(z), \text{ where $e_i$ is the $i$-th standard basis ?}
\]&lt;/span&gt; The answer is yes. This is because &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(x) = \left[
\begin{array} \\
\frac{\partial f}{\partial x_1},
\cdots,
\frac{\partial f}{\partial x_n}
\end{array}
\right]
= 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convex-and-non-differentiable-function-but-non-smooth-separable&#34;&gt;Convex and Non-differentiable Function, but Non-smooth Separable&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is only convex but not differentiable, the above will not necessarily hold. However, if &lt;span class=&#34;math display&#34;&gt;\[
f(x) = g(x) + \sum_{i=1}^nh_i(x_i)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; is convex and differentiable, and each &lt;span class=&#34;math inline&#34;&gt;\(h_i(x_i)\)&lt;/span&gt; is convex, the global minima still holds. For any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;= g(y) - g(x) + \sum_{i=1}^nh_i(y_i) - \sum_{i=1}^nh_i(x_i) \\
&amp;amp;\ge \nabla g(x)^T(y - x) + \sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\
&amp;amp;= \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)]
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; obtains minimum along each axes, for any &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x + d \cdot e_k) &amp;amp;\ge f(x) \\
g(x + d \cdot e_k) + \sum_{i=1,i\ne k}^nh_i(x_i) + h_k(x_k + d) &amp;amp;\ge g(x) + \sum_{i=1}^nh_i(x_i) \\
g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) &amp;amp;\ge 0 \\
\end{aligned}
\]&lt;/span&gt; By &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;the linearity of subgradient&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial(g_k + h_k) = \nabla_k g + \partial h_k \Rightarrow -\nabla_k g \in \partial h_k\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
h_k(y_k) - h_k(x_k) \ge -\nabla_k g(x)(y_k - x_k)
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
f(y) - f(x) = \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)] \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Coordinate%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Coordinate Descent.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zeyuan.wordpress.com/2016/06/13/coordinate-descent/&#34;&gt;Coordinate Descent in One Line, or Three if Accelerated | A Butterfly Valley (wordpress.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>FlatNCE</title>
      <link>https://chunxy.github.io/notes/papers/flatnce/</link>
      <pubDate>Sat, 27 Aug 2022 21:42:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/flatnce/</guid>
      <description>

&lt;p&gt;FlatNCE provides a way to compute the gradient of InfoNCE without introducing the rounding error when subtracting between two similar numbers.&lt;/p&gt;
&lt;p&gt;Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(g^\ominus_{ij}\)&lt;/span&gt; is the affinity score between reference sample &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and negative (noise) sample &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; is the affinity score between positive sample and itself/its transformation &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the batch index. Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{InfoNCE}\)&lt;/span&gt; the batch estimate of the loss from InfoNCE: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{detach}{\mathop{\text{detach}}}
\newcommand{logsumexp}{\mathop{\text{logsumexp}}}
\begin{gather}
\hat l_\text{InfoNCE} = \logsumexp_j g^\ominus_{ij} - g^\oplus_{ii} = \log (\sum_{j \ne i} \exp g^\ominus_{ij} ) -g^\oplus_{ii} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually, the above is calculated as &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} } \exp(g^\ominus_{ij} - \max_k g^\ominus_{ik}) ] + \max_k g^\ominus_{ik} - g^\oplus_{ii}
\]&lt;/span&gt; When the learning saturates, &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{InfoNCE}\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which means &lt;span class=&#34;math inline&#34;&gt;\(\max_k g^\ominus_{ik}\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; and thus &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} } \exp(g^\ominus_{ij} - g^\oplus_{ii}) ] + \underbrace{g^\oplus_{ii} - g^\oplus_{ii} }_{\text{error-prone}}
\]&lt;/span&gt; A rounding error will very likely happen when &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_of_significance&#34;&gt;subtracting two near numbers&lt;/a&gt;. Such error will accumulate and fail the InfoNCE. As said, FlatNCE provides a way to circumvent this rounding error.&lt;/p&gt;
&lt;h2 id=&#34;gradient-perspective&#34;&gt;Gradient Perspective&lt;/h2&gt;
&lt;p&gt;Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{FlatNCE}\)&lt;/span&gt; the batch estimate of the negative loss from FlatNCE: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat l_\text{FlatNCE} &amp;amp;= \exp [ \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii} ) - \detach \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii} ) ] \\
&amp;amp;= \frac{\exp \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii} ) } {\detach [ \exp \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii} ) ] } \\
&amp;amp;= \frac{\exp \log \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ]} {\detach \{\exp \log \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ]} {\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] \} }
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By putting the positive sample into the contrasting samples, &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{FlatNCE}^\oplus =  \frac{1 + \sum_j \exp \big( g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) \big)} {1 + \text{detach}[\sum_j \exp \big( g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) \big)]}
\]&lt;/span&gt; where the &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; comes from adding the positive sample &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; to the set of negative samples (let’s denote this “negative” sample by &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_0\)&lt;/span&gt;). It can be easily found that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_\theta \hat l_\text{FlatNCE}^\oplus (g_\theta) = \nabla_\theta \hat l_\text{InfoNCE} (g_\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may further find that the gradient of FlatNCE is an importance-weighted estimator of the form &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla_\theta \hat l^\oplus_\text{FlatNCE} &amp;amp;= \frac{\sum_{j \ne i} \{ \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] [\nabla_\theta g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i, y_i)] \} } {\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \{ \exp [ g_\theta(x_i, y&amp;#39;_j) ] [\nabla_\theta g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i, y_i)] \} } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] } \\
&amp;amp;= \sum_{k \ne i} \left\{ \underbrace {\frac{ \exp [ g_\theta(x_i, y&amp;#39;_k) ] } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] } }_{w_k} \nabla_\theta g_\theta(x_i, y&amp;#39;_k) \right\} - \nabla_\theta g_\theta(x_i, y_i) \\
\end{aligned}
\]&lt;/span&gt; As the learning progresses, &lt;span class=&#34;math inline&#34;&gt;\(w_k\)&lt;/span&gt;’s other than &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, which will cause the gradient to vanish.&lt;/p&gt;
&lt;h2 id=&#34;lower-bound-perspective&#34;&gt;Lower-bound Perspective&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{InfoNCE}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{FlatNCE}\)&lt;/span&gt; are part of the lower bounds to the mutual information in two methods. Given &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; the positive sample and &lt;span class=&#34;math inline&#34;&gt;\(y_{j&amp;gt;0}\)&lt;/span&gt; are the negative samples,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\label{lemma3.3} \begin{aligned} 
-\hat l^{K, \theta}_\text{InfoNCE} &amp;amp;= -\log \{ \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)] \} \\
&amp;amp;= \sup_v (v \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)] - (-1 - \log (-v)) \\
&amp;amp;\Downarrow_{v = -e^{-u}} \\
&amp;amp;\ge  -e^{-u} \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)] - (-1 - \log (e^{-u}) \\
&amp;amp;= 1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0) - u]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; as the primal critic and &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; as the dual critic. Since arbitrary choice of &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; lower-bounds the mutual information, we can either jointly optimize &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; or more preferably, train in an iterative fashion. Given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, set &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat u(g_\theta) = \log ({\frac 1 K \sum_j \exp[g_\theta(x,y_j) - g_\theta(x, y)]})
\]&lt;/span&gt; Then we fix &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and only update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is fixed, the only gradient comes from &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt;. Plugin &lt;span class=&#34;math inline&#34;&gt;\(\hat u\)&lt;/span&gt; to the right-hand side of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0) - u] \\
&amp;amp;= 1 - \log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0, y_0)]}) \notag \\
&amp;amp;\quad - \frac 1 K \frac{\sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)}{{\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0, y_0)]}} \\
&amp;amp;= -\log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0, y_0)]}) \label{obj} \\
&amp;amp;= -\hat l^K_\text{InfoNCE}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which tightly lower-bounds the &lt;span class=&#34;math inline&#34;&gt;\(-\hat l^K_\text{InfoNCE}\)&lt;/span&gt;. However we update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; will always hold. The objective of the whole FlatNCE is to enlarge &lt;span class=&#34;math inline&#34;&gt;\(\eqref{obj}\)&lt;/span&gt; after substituting &lt;span class=&#34;math inline&#34;&gt;\(u = \hat u(g_\theta&amp;#39;)\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(-\hat l^{K, \theta&amp;#39;}_\text{InfoNCE}\)&lt;/span&gt; can float up.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kexue.fm/archives/8586&#34;&gt;FlatNCE：小批次对比学习效果差的原因竟是浮点误差？ - 科学空间|Scientific Spaces (kexue.fm)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Whitening</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/statistics/whitening/</link>
      <pubDate>Thu, 11 Aug 2022 17:52:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/statistics/whitening/</guid>
      <description>

&lt;h2 id=&#34;whitening&#34;&gt;Whitening&lt;/h2&gt;
&lt;p&gt;Data whitening is the process of converting a random vector &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with only first-order correlation into a new random vector &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; such that the covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is an identity matrix.&lt;/p&gt;
&lt;p&gt;To do so, we shall first apply the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalization&lt;/a&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_X\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma_X \Phi = \Phi \Lambda
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; contains the normalized eigenvectors, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is diagonal and contains the eigenvalues. Now let &lt;span class=&#34;math inline&#34;&gt;\(Y = \Phi X\)&lt;/span&gt;, we can verify that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Y &amp;amp;= \E \{ \Phi^T (\x - \mu_X) [\Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Phi^T (\x - \mu_X) (\x - \mu_X)^T \Phi] \\
&amp;amp;= \Phi^T \E [(\x - \mu_X) (\x - \mu_X)^T] \Phi \\
&amp;amp;= \Phi^T \Sigma_X \Phi = \Phi^T \Phi \Lambda = \Lambda
\end{aligned}
\]&lt;/span&gt; which is diagonal. To further make an identity matrix, we apply &lt;span class=&#34;math inline&#34;&gt;\(Z = \Lambda^{-1/2} Y = \Lambda^{-1/2} \Phi^T X\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= \E \{\Lambda^{-1/2} \Phi^T (\x - \mu_X) [\Lambda^{-1/2} \Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Lambda^{-1/2} \Phi^T (\x - \mu_X)  (\x - \mu_X)^T \Phi \Lambda^{-1/2}] \\
&amp;amp;= \Lambda^{-1/2} \Phi^T \Sigma_X \Phi \Lambda^{-1/2} = I
\end{aligned}
\]&lt;/span&gt; Data whitening in Gaussian case is discussed &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/#First-order correlated n -dimensional&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>协方差与相关系数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Sun, 01 May 2022 10:41:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description>

&lt;p&gt;以下以二维随机变量为例，展示协方差以及相关系数的概念。&lt;/p&gt;
&lt;h2 id=&#34;协方差&#34;&gt;协方差&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\([X, Y]\)&lt;/span&gt;为一组二维随机变量，如果&lt;span class=&#34;math inline&#34;&gt;\(\mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}\)&lt;/span&gt;存在，则称 &lt;span class=&#34;math display&#34;&gt;\[
\notag \mathrm {Cov}(X, Y) \triangleq \mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}
\]&lt;/span&gt; 为随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的协方差。在实际中计算协方差时，更多的是使用以下公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\mathrm {Cov}(X, Y) = \mathrm E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\} \\
&amp;amp;= \mathrm E[XY - X\mathrm E(Y) - \mathrm E(X)Y + \mathrm E(X) \mathrm E(Y)] \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X)\mathrm E(Y) - \mathrm E(X)\mathrm E(Y) + \mathrm E(X) \mathrm E(Y) \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X) \mathrm E(Y)
\end{aligned}
\]&lt;/span&gt; 而二维随机变量[X, Y]对应的协方差矩阵即为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma = \begin{bmatrix} \Cov(X,X) &amp;amp; \Cov(X,Y) \\ \Cov(Y,X) &amp;amp; \Cov(Y,Y) \\ \end{bmatrix}  
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关系数&#34;&gt;相关系数&lt;/h2&gt;
&lt;p&gt;协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们可以首先将随机变量标准化： &lt;span class=&#34;math display&#34;&gt;\[
X^\star = \frac{X - \E(X)}{\sqrt{\Var(X)}}，Y^\star = \frac{Y - \E(Y)}{\sqrt{\Var(Y)}}
\]&lt;/span&gt; 再求协方差&lt;span class=&#34;math inline&#34;&gt;\(\Cov(X^\star, Y^\star)\)&lt;/span&gt;，这便是随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的相关系数： &lt;span class=&#34;math display&#34;&gt;\[
\rho(X, Y) = \mathrm{Cov}(X^\star, Y^\star) = \frac{\Cov(X, Y)}{\sqrt{\Var(X) \Var(Y)}}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>KL-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</guid>
      <description>

&lt;p&gt;KL-divergence, &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|q)\)&lt;/span&gt; is statistical distance, measuring how the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is different from the reference probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, both defined on &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathcal{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In information theory, it measures the &lt;strong&gt;relative entropy&lt;/strong&gt; from &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which is the average number of extra bits required to represent a message with &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In discrete form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\sum_{x \in \mathcal{X}} p(x)\log \frac{q(x)}{p(x)} = \sum_{x \in \mathcal{X}} p(x)\log \frac{p(x)}{q(x)}
\]&lt;/span&gt; In continuous form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\int_{x \in \mathcal{X}} p(x) \log \frac{q(x)}{p(x)}dx = \int_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;variational-lower-bound&#34;&gt;Variational Lower-bound&lt;/h3&gt;
&lt;p&gt;One property of KL-divergence is &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p || q) = \sup_{T: \Omega \to \R} \E_{p} [T] - \log (\E_q[e^T])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proof is as follows. Given a distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; and a function &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, construct the Gibbs distribution &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(g(x) = \frac{q(x)e^{T(x)}}{Z}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z = \E_{q(x)} e^{T(x)}\)&lt;/span&gt;. Then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E&amp;amp;_{p(x)} T(x) - \log Z = \E_{p(x)} [T(x) - \log Z] \\
&amp;amp;= \E_{p(x)} [\log e^{T(x)} - \log \E_{q(x)} e^{T(x)}] \\
&amp;amp;= \E_{p(x)} \log \frac{e^{T(x)}} {\E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{q(x) e^{T(x)}} {q(x) \E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{g(x)} {q(x)} \\
\end{aligned}
\]&lt;/span&gt; Finally KL-divergence minus above gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;D_\text{KL}(p || q) - (\E_{p(x)} T(x) - \log Z) \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {q(x)} - \E_{p(x)} \log \frac{g(x)} {q(x)} \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {g(x)} \triangleq D_\text{KL}(p || g) \ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are random variables, both of some &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt;. Then the KL-divergence between them can be formulated as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_\text{KL}(p_X || p_Y) &amp;amp;= \int p_X(x) \log \frac{p_X(x)} {p_Y(x)} \d x = \int p_X(x) \log [
    \sqrt \frac{|\Sigma_X|}{|\Sigma_Y|} 
    \frac {
        e^{-\frac 1 2 (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X)}
    } {
        e^{-\frac 1 2 (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)}
    }
] 
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - 
\frac 1 2 \int p_X(x) [
    (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) + 
    (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - 
\frac 1 2 \int p_X(x) (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) \d x \\
&amp;amp;\quad -\frac 1 2 \int p_X(x) (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y) \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \int p_X(x) x^T \Sigma_X^{-1} x \d x + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \int p_X(x) x^T \Sigma_Y^{-1} x \d x + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \tr(\Sigma_X^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_X^{-1} \mu_X + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \tr(\Sigma_Y^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_Y^{-1} \mu_X + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y     \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y) + \mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu_X \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y)] \\
&amp;amp;\quad + \frac 1 2 [\mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu^T_X \Sigma_Y^{-1} \mu_X + \mu_X^T \Sigma_Y^{-1} \mu_Y - \mu_Y^T \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1} \Sigma_Y) + (\mu_Y^T - \mu_X^T) \Sigma_Y^{-1} (\mu_Y - \mu_X)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</link>
      <pubDate>Fri, 07 Jan 2022 12:58:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</guid>
      <description>

&lt;p&gt;If a probabilistic model contains only observable variables, Maximum likelihood estimation or Bayesian methods can be adopted to derive the model parameters. However it is also possible that a probabilistic model contains unobservable variables (called latent variables). Latent variables are those that you cannot observe but you know its existence and its corresponding random event in random trial. In such case, &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange multipliers&lt;/a&gt; may be hard to apply because of the existence of the “log of sum” term.&lt;/p&gt;
&lt;p&gt;Given observed samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} = \{\x^{(1)}, \x^{(2)}, ..., \x^{(m)}\}\)&lt;/span&gt; (with unobservable latent variable samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Z}\)&lt;/span&gt;), MLE tries to the find best parameters &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = \arg\max_{\theta}\log(p(\mathrm{X};\theta))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The log-likelihood function is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log p(\mathrm{X};\theta) = \sum_{\x \in \mathrm{X}} \log p(\x; \theta) \\
&amp;amp;= \sum_{\x \in \mathrm{X}} \log \E_{\z \sim q(\z)} \frac{p(\x, \z; \theta)}{q(\z)} \\
&amp;amp;\Downarrow_\text{by Jensen&amp;#39;s Inequality} \\
&amp;amp;\ge \sum_{\x \in \mathrm{X}} \E_{\z \sim q(\z)} \log \frac{p(\x, \z; \theta)}{q(\z)} \\
&amp;amp;\text{where $q(\z)$ is a reference distribution}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we maximize the &lt;span class=&#34;math inline&#34;&gt;\(\E_{\z \sim q(\z)} \log \frac{p(\x, \z; \theta)}{q(\z)}\)&lt;/span&gt;, the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; can be maximized, which gives us a good guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; will increase monotonically. &lt;span class=&#34;math inline&#34;&gt;\(\E_{\z \sim q(\z)} \log \frac{p(\x, \z; \theta)}{q(\z)}\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\((\phi, \theta)\)&lt;/span&gt;, firstly we maximize it w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, and then w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and then back and forth.&lt;/p&gt;
&lt;p&gt;The first step is to fix &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and have &lt;span class=&#34;math inline&#34;&gt;\(\E_{\z \sim q(\z)} \log \frac{p(\x, \z; \theta)}{q(\z)}\)&lt;/span&gt; reach its upper bound when the equality in Jensen’s inequality holds, where &lt;span class=&#34;math inline&#34;&gt;\(\frac{p(\x, \z; \theta)}{q(\z)}\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(\z \in \mathcal{Z}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(\x, \z) &amp;amp;= cq(\z) \\
&amp;amp;\Downarrow \\
\sum_{\z \in \mathcal{Z}} p(\x, \z) &amp;amp;= c\sum_{\z \in \mathcal{Z}} q(\z) \\
p(\x;\theta) &amp;amp;= c \\
\end{aligned}
\]&lt;/span&gt; Thus we know how to choose &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
q(\z) = \frac{p(\x, \z)}{c} = \frac{p(\x, \z)}{p(\x;\theta)} = p(\z|\x;\theta)
\]&lt;/span&gt; The above step is called the &lt;strong&gt;expectation&lt;/strong&gt; step because we are deriving a closed-form expression for &lt;span class=&#34;math inline&#34;&gt;\(\E_{\z \sim q(\z)} \log \frac{p(\x, \z; \theta)}{q(\z)}\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The next step is the &lt;strong&gt;maximization&lt;/strong&gt; step where we fix &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; and optimize w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We do these two steps back and forth, comprising the whole expectation maximization algorithm.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/306016801/answer/624061631&#34;&gt;A good comparison of latent variables&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>大数定律和中心极限定理</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link>
      <pubDate>Fri, 20 May 2022 09:27:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)\)&lt;/span&gt;及方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X)\)&lt;/span&gt;存在，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
P(|X-\E(X)| \ge \epsilon) \le \frac{\Var(X)}{\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为连续型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \int_{|x - \E(x)| \ge \epsilon} p(x)dx \\
&amp;amp;\le \mathop \int_{|x - \E(x)| \ge \epsilon} \bigg( \frac{X - \E(x)}{\epsilon} \bigg)^2 p(x)dx \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \int_{|x - \E(x)| \ge \epsilon} \big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \int_{x \in X} \big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为离散型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \sum_{|x - \E(x)| \ge \epsilon} P(x) \\
&amp;amp;\le \mathop \sum_{|x - \E()| \ge \epsilon} \bigg( \frac{x - \E(x)}{\epsilon} \bigg)^2 P(x) \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \sum_{|x - \E(x)| \ge \epsilon} \big( x - \E(x) \big)^2 P(x) \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \sum_{x \in X} \big( x - \E(x) \big)^2 P(x) \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是作为证明其它大数定理的基础工具。&lt;/p&gt;
&lt;h3 id=&#34;依概率收敛&#34;&gt;依概率收敛&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;p&gt;随机变量是事件的映射，当试验次数足够多时，事件的频率会&lt;strong&gt;依概率收敛&lt;/strong&gt;到该事件的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) = 1\)&lt;/span&gt;，则称该随机变量序列依概率收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n \stackrel{P}{\to} c\)&lt;/span&gt;。或者，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) = 0\)&lt;/span&gt;。 ### Markov不等式&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式其实是Markov不等式的一个特例。令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一非负随机变量、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为一非负实数，Markov不等式描述的是以下关系： &lt;span class=&#34;math display&#34;&gt;\[
P(X \ge \alpha) \le \frac{\E(X)}{\alpha}
\]&lt;/span&gt; 将其中的&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\(|X - \E(X)|\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;即可得到Chebyshev不等式。不过由于Markov不等式有随机变量非负的要求，适用范围就小了一些；而且同样，Markov不等式的这种估计也是很粗糙的。&lt;/p&gt;
&lt;h2 id=&#34;弱大数定律weak-law-of-large-numbers&#34;&gt;弱大数定律（Weak Law of large numbers）&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev大数定律&#34;&gt;Chebyshev大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,\dots\)&lt;/span&gt;两两不相关，若存在常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\Var(X_i) \le c \ne +\infty, i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| &amp;lt; \epsilon) = 1
\]&lt;/span&gt; 亦即&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n \E(X_i)\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;p&gt;由于该随机序列两两不相关，故根据期望及方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\E(\frac{1}{n} \sum_{i=1}^n X_i) =  \frac{1}{n} \sum_{i=1}^n \E(X_i),\quad \Var(\frac{1}{n} \sum_{i=1}^N X_i) = \frac{1}{n^2} \sum_{i=1}^n \Var(X_i) \le \frac{c}{n}
\]&lt;/span&gt; 根据切比雪夫不等式， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 \le P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) &amp;lt; \frac{\Var(\frac{1}{n} \sum_{i=1}^N X_i)}{\epsilon^2} \le \frac{c}{n \epsilon} \\
\underbrace{\lim_{n \to \infty} 0}_0 \le \lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) \le \underbrace{\lim_{n \to \infty} \frac{c}{n \epsilon}}_0 \Rightarrow\\
\lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge \epsilon) = 0
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;khinchin大数定律&#34;&gt;Khinchin大数定律&lt;/h3&gt;
&lt;h4 id=&#34;相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) = \mu, \Var(X_i) = \sigma^2 \ne \infty, i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家辛钦证明该定律依然成立，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E_{X_i} = \mu\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - p| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。&lt;/p&gt;
&lt;h2 id=&#34;中心极限定理&#34;&gt;中心极限定理&lt;/h2&gt;
&lt;p&gt;注意，在本节中，我们用&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;表示标准正态分布的分布函数。&lt;/p&gt;
&lt;h3 id=&#34;lindburg-levy中心极限定理&#34;&gt;Lindburg-Levy中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) = \mu, \Var(X_i) = \sigma^2 \ne \infty, i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - n \mu}{\sqrt n \sigma} \le x) = \Phi(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;de-moivre-laplace中心极限定理&#34;&gt;de Moivre-Laplace中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;相互独立且同分布，且&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}} \le x) = \Phi(x)
\]&lt;/span&gt; 显然de Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。&lt;/p&gt;
&lt;p&gt;前面的Bernoulli大数定律告诉我们可以用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n} \sum_{i=1}^n X_i\)&lt;/span&gt;（频率）近似&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;（概率），而至于近似程度如何，却不得而知。de Moivre-Laplace中心极限定理则告诉我们当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;足够大时，近似程度如何： &lt;span class=&#34;math display&#34;&gt;\[
P(|\frac{1}{n}\sum_{i=1}^n X_i - p| \le \epsilon) = P(|\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}}| \le \frac{\sqrt n \epsilon}{\sqrt{p(1-p)}}) \simeq 2\Phi(\frac{\sqrt n \epsilon}{\sqrt{p(1-p)}}) - 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上式实际是在用正态分布近似二项分布（多个伯努利分布随机变量加和为伯努利分布）。在&lt;a href=&#34;https://www.mathsisfun.com/data/quincunx-explained.html&#34;&gt;Galton Board游戏&lt;/a&gt;我们可以近似应用de Moivre-Laplace中心极限定理，这样或许能够帮助理解该定理。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>f-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence can be treated as the generalization of the KL-divergence. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has to satisfy that &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a convex function. The reason for these two constraints is that we hope &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
D_f (p||q) = 0 \text{ when $p=q$} \\
\forall p, q, D_f (p||q) \ge 0
\end{gather}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x\log x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence becomes KL-divergence.&lt;/p&gt;
&lt;h2 id=&#34;variational-f-divergence&#34;&gt;Variational &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; have no closed-form expression, it is difficult to compute the &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence. Therefore in practice &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence is computed with a variational expression: &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the convex conjugate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. The derivation is as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_f (p||q) &amp;amp;= \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int f^{**}(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int \sup_t [\frac{p(x)}{q(x)} t - f^*(t)] q(x)\ \d x \\
&amp;amp;= \int \sup_t[p(x) t - f^*(t) q(x)]\ \d x \\
&amp;amp;\Downarrow_{T(x) = \arg \sup_t[p(x) t - f^*(t) q(x)]} \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \int [p(x) T(x) - f^*(T(x)) q(x)]\ \d x \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</guid>
      <description>

&lt;p&gt;Given a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}),i=1,\dots,M\}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \in \R^N\)&lt;/span&gt; is the feature vector and &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} \in R\)&lt;/span&gt; is the output, learn a linear function &lt;span class=&#34;math inline&#34;&gt;\(f = w^Tx + b: \R^N \mapsto \R\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N, b \in \R\)&lt;/span&gt;, that best predicts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for any feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The “best” is usually measured by the Mean Square Error: &lt;span class=&#34;math display&#34;&gt;\[
MSE(f,\mathcal D) = \frac{1}{M}\sum_{i=1}^M(y^{(i)} - f(x^{(i)}))^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;ordinary-least-squares&#34;&gt;Ordinary Least Squares&lt;/h3&gt;
&lt;p&gt;OLS selects the linear regression parameters &lt;span class=&#34;math inline&#34;&gt;\(w, b\)&lt;/span&gt; the minimize the MSE: &lt;span class=&#34;math display&#34;&gt;\[
w^\star, b^\star = \arg \min_{w,b}\frac{1}{M}\sum_{i=1}^M(y^{(i)} - w^Tx^{(i)} - b)^2
\]&lt;/span&gt; This is equivalent to the below Least Squares problem. Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
X = 
\left [
\begin{array}{c|c}
(x^{(1)})^T &amp;amp; 1 \\
(x^{(2)})^T &amp;amp; 1 \\
\vdots &amp;amp; \vdots \\
(x^{(M)})^T &amp;amp; 1
\end{array}
\right ], 
W = \begin{bmatrix}
w \\
b \\
\end{bmatrix},
Y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(M)}
\end{bmatrix} \\
\\
XW =  Y
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may not lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Therefore we have to approximate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X\hat W = proj_{Col(X)}Y \Rightarrow Y &amp;amp;- X\hat W \in Nul(X^T) \Rightarrow\\
X^T(Y - X\hat W) &amp;amp;= 0 \\
X^TX\hat W &amp;amp;= X^TY
\end{aligned}
\]&lt;/span&gt; Columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are independent &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is not invertible, there are infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt;. We can get one specific &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; by enforcing regularization on &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; or adding more samples when &lt;span class=&#34;math inline&#34;&gt;\(M &amp;lt; N + 1\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible, there is unique solution that &lt;span class=&#34;math inline&#34;&gt;\(\hat W = (X^TX)^{-1}X^TY\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This gives the same result with minimizing the MSE: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \min_{W}MSE(W) = \frac{1}{M}(Y - XW)^T(Y - XW)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial MSE}{\partial W} &amp;amp;= \frac{\partial(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)}{\partial W} \\
&amp;amp;= \frac{\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\partial W} \\
&amp;amp;= -2X^TY + 2X^TXW \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
0 = -2X^TY + 2X^TXW^\star \\
X^TXW^\star = X^TY 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are chances that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is too large, making equation (7) too computationally expensive. In this case, we can use gradient descent. The update rule will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
W^{(t+1)} &amp;amp;= W^{(t)} - \frac{\eta}{2}\nabla MSE(W^{(t)}) \\
&amp;amp;= W^{(t)} - \eta(X^TXW^{(t)} -X^TY)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;a-probabilistic-view&#34;&gt;A Probabilistic View&lt;/h4&gt;
&lt;h5 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h5&gt;
&lt;p&gt;In classification task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the feature, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the label; in regression task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the ‘label’, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the ‘feature’. From a probabilistic point of view, we would like estimate &lt;span class=&#34;math inline&#34;&gt;\(p(feature|label)\)&lt;/span&gt;. In this case, we treat &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as the ‘feature’ composed of a deterministic function with a noise sampled from an identical and independent Gaussian distribution, i.e., for random variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X, \mathcal Y\)&lt;/span&gt; (to distinguish from matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), &lt;span class=&#34;math display&#34;&gt;\[
\mathcal Y = \mathcal XW + \epsilon, \text{ where }p(\epsilon;\sigma^2) = \mathcal N(\epsilon;0, \sigma^2)
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
p(Y|X;W,\sigma^2) = p(\epsilon=Y - XW;\sigma^2) = \mathcal N(Y - XW;0,\sigma^2I)
\]&lt;/span&gt; Then OLS can be attacked by Maximum Likelihood Estimation. The log-likelihood function will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(W,\sigma^2) &amp;amp;= \log(L(W, \sigma^2)) = \log(p(Y|X;W,\sigma^2)) = \log \mathcal N(Y - XW;0,\sigma^2I) \\
&amp;amp;= \log(\frac{1}{\sqrt{(2\pi)^M|\sigma^2I|}}e^{-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW)}) \\
&amp;amp;= -\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\arg \max_{W,\sigma^2}l(W, \sigma^2) &amp;amp;= \arg \max_{W,\sigma^2}-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi \\
&amp;amp;= \arg \min_{W,\sigma^2}\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) + \frac{M}{2}\log \sigma^2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{1}{\sigma^2}(X^TXW^\star - X^TY) = 0 \\
X^TXW^\star = X^TY
\end{gather}
\]&lt;/span&gt; Substitute the solution &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; back, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{\star2}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{M}{2\sigma^{\star2}} - \frac{(Y - XW^\star)^T(Y - XW^\star)}{2(\sigma^{\star2})^2} = 0 \\
\sigma^{\star2} = \frac{1}{M}(Y - XW^\star)^T(Y - XW^\star)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;maximizing-a-posteriori&#34;&gt;Maximizing a Posteriori&lt;/h5&gt;
&lt;p&gt;If we add a priori to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(W \sim \mathcal N(0,\frac{C}{2}I)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(p(W) \propto \exp(-\frac{1}{C}W^TW)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(W|X, Y) &amp;amp;= \frac{p(W, X, Y)}{p(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(X, W)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)P(X)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)}{P(Y|X)} \\
&amp;amp;\propto p(Y|X,W)p(W)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \max_W p(W|X,Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;By adding a regularization term to OLS objective we obtain the Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \frac{\alpha}{2}||W||^2_2
\]&lt;/span&gt; By regularization, we are “shrink” the amount of &lt;span class=&#34;math inline&#34;&gt;\(||W||_2\)&lt;/span&gt;, whose magnitude is controlled by the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We prefer smaller weights because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smaller weights are more robust to the perturbations of input&lt;/li&gt;
&lt;li&gt;there are better chances to zero out some dimensions of the input feature, which may be uninformative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The constant term before the sum of square errors that before &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not of significance. We choose them to be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; because this gives a nicer closed-form solution to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Take derivative of the objective w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-X^TY + X^TXW^\star + \alpha W^\star = 0 \\
(X^TX + \alpha I)W^\star = X^TY
\end{gather}
\]&lt;/span&gt; The “ridge” comes from the &lt;span class=&#34;math inline&#34;&gt;\(\alpha I\)&lt;/span&gt; term, which is added to the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h3&gt;
&lt;p&gt;In Ridge Regression, there is still chance that some weights are small but not zero, because the regularization term is small so far as the weights are fairly small. Think in this way: &lt;span class=&#34;math inline&#34;&gt;\(0.01^2 = 0.0001 \ll 0.01\)&lt;/span&gt;, though it is not rigid.&lt;/p&gt;
&lt;p&gt;To get better regularization, we can change the regularization term to &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \alpha||W||_1
\]&lt;/span&gt; The above problem can be efficiently solved using Coordinate Descent or &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/least-angle-regression/&#34;&gt;Least Angle Regression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6018889.html&#34;&gt;Lasso回归算法： 坐标轴下降法与最小角回归法小结 - 刘建平Pinard - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/76055830&#34;&gt;LASSO回归求解 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Subgradient</title>
      <link>https://chunxy.github.io/notes/articles/optimization/subgradient/</link>
      <pubDate>Tue, 11 Jan 2022 16:09:23 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/subgradient/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Subgradient of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
\partial f(x) = \{g|f(y) \ge f(x) + g^T(y - x), \forall y \in dom(f) \}
\]&lt;/span&gt; Or put it another way, the subgradient defines a hyper-plane passing through &lt;span class=&#34;math inline&#34;&gt;\((x,f(x))\)&lt;/span&gt;: $$ ^T ( - ) = 0,&lt;/p&gt;
y ^n, t \ &lt;span class=&#34;math display&#34;&gt;\[
And
\]&lt;/span&gt; ^T ( - ) , (y, t) epi(f) &lt;span class=&#34;math display&#34;&gt;\[
This is because by definition,
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
g^T(y - x) - (f(y) - f(x)) &amp;amp;\le 0, \forall y &amp;amp;\Rightarrow \\
g^T(y - x) - (t - f(x)) &amp;amp;\le 0, \forall y,\forall t \ge f(y) &amp;amp;\Rightarrow \\
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
&amp;amp;\le 0, \forall (y, t) \in epi(f)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\left[\begin{array}\\ g \\ -1 \\\end{array}\right]\)&lt;/span&gt; is the normal of the tangent plane of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;The subgradient &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is a set, instead of a single number like the gradient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Subgradient is a monotonic operator: &lt;span class=&#34;math display&#34;&gt;\[
(u - v)^T(y - x) \ge 0, \forall u \in \partial f(y), \forall v \in \partial f(x)
\]&lt;/span&gt; This can be shown by: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left.
\begin{array} \\
f(y) \ge f(x) + v^T(y - x) \\
f(x) \ge f(y) + u^T(x - y) \\
\end{array}
\right\}
&amp;amp;\Rightarrow 
f(y) + f(x) \ge f(x) + f(y) - (u - v)^T(y - x) \\
&amp;amp;\Rightarrow (u - v)^T(y - x) \ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x^\star = \arg \min_x f(x) \iff 0 \in \partial f(x^\star)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \{\nabla f(x)\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x) \and p \in \partial f(x)\)&lt;/span&gt;, then for &lt;span class=&#34;math inline&#34;&gt;\(y = x + r(p - \nabla f(x))\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;\ge p^T(y - x) \\
f(x + r(p - \nabla f(x))) - f(x) &amp;amp;\ge rp^T(p - \nabla f(x)) \\
\frac{f(x + r(p - \nabla f(x))) - f(x)}{r} &amp;amp;\ge p^T(p - \nabla f(x)) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(p - \nabla f(x))^T(p - \nabla f(x)) &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)}{r} - \nabla f(x)^T(p - \nabla f(x)) \\
||p - \nabla f(x)||^2_2 &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take limit on &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; on both sides to give &lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}||p - \nabla f(x)||^2_2 \le \lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big) \\
\]&lt;/span&gt; By the definition of gradient, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f&amp;amp;(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x)) \\
&amp;amp;= \mathcal{O}(||r(p - \nabla f(x))||^2_2) \\
&amp;amp;= ||p - \nabla f(x)||^2\mathcal{O}(r^2) \\
&amp;amp;= \mathcal{O}(r^2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big) = \lim_{r \to 0}\frac{\mathcal{O}(r^2)}{r} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||p - \nabla f(x)||^2_2 \le 0 \\
0 \le ||p - \nabla f(x)||^2_2 \le 0 \\
||p - \nabla f(x)||^2_2 = 0
\end{gather} 
\]&lt;/span&gt; contradicting the assumption that &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x)\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(p = \nabla f(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \alpha_1 f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \alpha_1 \partial f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/139083837&#34;&gt;凸优化笔记16：次梯度 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Non-linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</link>
      <pubDate>Fri, 07 Jan 2022 12:46:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</guid>
      <description>

&lt;p&gt;The basic idea about non-linear regression is to perform the feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, followed by linear regression in the new feature space.&lt;/p&gt;
&lt;h3 id=&#34;polynomial-regression&#34;&gt;Polynomial Regression&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a scalar, the feature mapping in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order Polynomial Regression is &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = 
\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
\]&lt;/span&gt; The regression function and the parameters are then like those in linear regression: &lt;span class=&#34;math display&#34;&gt;\[
f(x) = [w_0,w_1,w_2,\dots,w^p]\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
= w^T\phi(x)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; captures all features in each degree from &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; up to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. In higher-dimensional input feature space, there are many more entries for feature mapping in each degree. Take a 2-D input for example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Degree 1: &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Degree 2: &lt;span class=&#34;math inline&#34;&gt;\([x_1^2, x_1x_2, x_2^2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Degree 3: &lt;span class=&#34;math inline&#34;&gt;\([x_1^3, x_1^2x_2, x_1x_2^2, x_2^3]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-trick-in-non-linear-regression&#34;&gt;Kernel Trick in Non-linear Regression&lt;/h3&gt;
&lt;p&gt;Many Linear Regression algorithms learning algorithms depend on the calculation of inner products between feature vectors, either during training or in prediction, without directly depending on the feature vector. We can transform the feature vector by applying a feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; to do the Non-linear Regression task.&lt;/p&gt;
&lt;p&gt;However, it is not necessary to explicitly define the feature mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; when only the inner products are needed. Instead, we can define a function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K: \R^N \times \R^N \mapsto \R\)&lt;/span&gt; that directly calculate the inner products of pseudo-transformed features: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = \phi_{pseudo}(x)^T\phi_{pseudo}(x^\prime)
\]&lt;/span&gt; This will be more flexible and computationally-efficient.&lt;/p&gt;
&lt;h4 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h4&gt;
&lt;p&gt;Recall Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \min_{W}\frac{1}{2}||Y - X^TW||^2_2 + \frac{\alpha}{2}||W||^2_2 \iff (XX^T + \alpha I_{N+1})W^\star = XY
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\((XX^T + \alpha I_{N+1})\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(W^\star = (XX^T + \alpha I_{N+1})^{-1}XY\)&lt;/span&gt;. By &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/&#34;&gt;matrix identity&lt;/a&gt; &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; we can obtain that &lt;span class=&#34;math inline&#34;&gt;\(W^\star = X(X^TX + \alpha I_M)^{-1}Y\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(X \in \R^{(N+1)\times M}, (X^TX + \alpha I_M)^{-1}Y \in \R^{M}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; can be rewritten as a linear combination of columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; lies in the span of input vectors: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \sum_{i=1}^M\lambda_ix^{(i)}
\]&lt;/span&gt; For a new data &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we make prediction by &lt;span class=&#34;math display&#34;&gt;\[
y^* = (x^*)^TW = (x^*)^TX(X^TX + \alpha I_M)^{-1}Y
\]&lt;/span&gt; which totally depends on the inner products.&lt;/p&gt;
&lt;h4 id=&#34;support-vector-regression&#34;&gt;Support Vector Regression&lt;/h4&gt;
&lt;p&gt;Support Vector Regression learns a hyper-plane that incorporates within its margin as many points as possible. As a comparison, &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/&#34;&gt;Support Vector Machine&lt;/a&gt; learns a hyper-plane that excludes outside its margin as many points as possible. Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b \le \epsilon + \xi_i, i=1,\dots,M \\
y^{(i)} - w^Tx^{(i)} - b \ge \epsilon + \xi^*_i, i=1,\dots,M \\
\xi_i, \xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a hyper-parameter to be determined. Transform the problem into the standard optimization form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i \le 0, i=1,\dots,M \\
w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i \le 0, i=1,\dots,M \\
-\xi_i, -\xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(w,\xi,\xi^*,\lambda,\mu,\nu,\nu^*) = &amp;amp;\frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
&amp;amp;+ \sum_{i=1}^M\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i) \\
&amp;amp;+ \sum_{i=1}^M\mu_i(w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i) \\
&amp;amp;- \sum_{i=1}^M\nu_i\xi_i -\sum_{i=1}^M\nu^*_i\xi^*_i
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Support%20Vector%20Regression.pdf&#34; target=&#34;_blank&#34;&gt;Support Vector Regression.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>统计量</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为取自总体的一组样本，若函数&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;不直接包含总体分布中的任何参数，则称&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;为&lt;strong&gt;统计量&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;样本均值和样本方差&#34;&gt;样本均值和样本方差&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{样本均值：}\bar X = \frac{1}{n} \sum_{i=1}^n X_i \\
\text{样本方差：}S^2 = \frac{1}{n-1} \sum_{i=1}^N (X_i - \bar X)^2 = \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(A_k = \frac{1}{n} \sum_{i=1}^n X_i^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(M_k = \frac{1}{n} \sum_{i=1}^n (X_i - \bar X)^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩&lt;/strong&gt;，当&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq M_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;由于统计量是随机变量的函数，故统计量也是随机变量。设总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) = \sigma^2\)&lt;/span&gt;，关于统计量有如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
&amp;amp; \E(\bar X) = \mu, \Var(\bar X) = \frac{\sigma^2}{n} \\
\notag \\
&amp;amp; \E(S^2) = \sigma^2, \E(S_n^2) = \frac{n-1}{n} \sigma^2 \\
\notag \\
&amp;amp; \bar X \stackrel{P}{\to} \mu, S^2 \stackrel{P}{\to} \sigma^2, S_n^2 \stackrel{P}{\to} \sigma^2
\end{gather}
\]&lt;/span&gt; 有关证明如下： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\E(\bar X) = \E (\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n} \sum_{i=1}^n \E(X_i) = \mu \\
\Var(\bar X) = \Var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2} \sum_{i=1}^n D(X_i) = \frac{\sigma^2}{n}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}[t]
\E(S^2) &amp;amp;= \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n \E (X_i^2 ) - n \E(\bar X^2) \big) \\
&amp;amp;\Downarrow_ {\E(X_i^2) = \Var(X_i) + \E^2(X_i) = \sigma^2 + \mu^2, \E(\bar X^2) = \Var(\bar X) + \E^2(\bar X) = \frac{\sigma^2}{n} + \mu^2} \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n (\sigma^2 + \mu^2) - n (\frac{\sigma^2}{n} + \mu^2) \big) \\
&amp;amp;= \sigma^2
\end{aligned}
\begin{aligned}[t]
\E(S_n^2) &amp;amp;= \E [\frac{n-1}{n} \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;归根结底，样本方差使用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;的原因是，其使用的“均值”为&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，这导致了一个自由度的缺失。而假设&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知，我们定义一个新的统计量&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;^2 = \frac{1}{n} \sum_{i=1}^N (X_i - \mu)^2 = \frac{1}{n} (n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n X_i^2)\)&lt;/span&gt;，我们会发现&lt;span class=&#34;math inline&#34;&gt;\(\E(S&amp;#39;^2) = \sigma^2\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E(S&amp;#39;^2) = \frac{1}{n} \E(n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n X_i^2) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu \E(\bar X) + \sum_{i=1}^n \E (X_i^2)) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu^2 + \sum_{i=1}^n (\sigma^2 + \mu^2)) \\
&amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt; 至于三个统计量的依概率收敛证明，根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/a&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \mu \\
\frac{1}{n} \sum_{i=1}^n X_i^2 \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n \E (X_i^2) = \sigma^2 + \mu^2
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt; 0\)&lt;/span&gt;，存在&lt;span class=&#34;math inline&#34;&gt;\(N_1, N_2 &amp;gt; 0\)&lt;/span&gt;，使得当&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; \max(N_1, N_2)\)&lt;/span&gt;时，始终有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 &amp;lt; P(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 + \mu^2)| \ge \epsilon / 2) &amp;lt; \delta / 2 \\
0 &amp;lt; P(|\mu^2 - \bar X^2| \ge \epsilon / 2) &amp;lt; \delta / 2 \\
\end{gather}
\]&lt;/span&gt; 记事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 + \mu^2)| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\bar X^2 - \mu^2| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2 - \sigma^2| \ge \epsilon / 2\)&lt;/span&gt;。由于&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 + \mu^2)| + |\mu^2 - \bar X^2| \ge |\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2 - \sigma^2|\)&lt;/span&gt;，则事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;发生时，事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;至少发生其中之一，即事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;是事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;并集的子集。故&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
0 &amp;lt; P(\text{事件$C$}) \le P(\text{事件$A$ 或 事件$B$}) \le P(\text{事件$A$}) + P(\text{事件$B$}) &amp;lt; \delta
\]&lt;/span&gt; 即&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; P(|\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2 \ - \sigma^2| \ge \epsilon) &amp;lt; \delta\)&lt;/span&gt;。又由于对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt; 0\)&lt;/span&gt;该结论都成立，故 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\lim_{n \to \infty} P(|\underbrace{\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2}_{S_n^2} - \sigma^2| \ge \epsilon) = 0 \iff \\
S_n^2 \stackrel{P}{\to} \sigma^2
\end{gathered}
\]&lt;/span&gt; 运用类似的&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta\)&lt;/span&gt;语言，我们可以证明&lt;span class=&#34;math inline&#34;&gt;\(S^2 = \frac{n}{n-1} S_n^2 \stackrel{P}{\to} \sigma^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;次序统计量&#34;&gt;次序统计量&lt;/h2&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\((X_{(1)}, \dots, X_{(n)})\)&lt;/span&gt;为样本&lt;span class=&#34;math inline&#34;&gt;\((X_1, \dots, X_n)\)&lt;/span&gt;排序后的结果，则&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)} = \min (X_1, \dots, X_n), X_{(n)} = \max (X_1, \dots, X_n)\)&lt;/span&gt;亦是统计量。&lt;/p&gt;
&lt;p&gt;记&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)}, X_{(n)}\)&lt;/span&gt;的概率密度函数分别为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(1)}}, p_{X_{(n)}}\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p_{X_{(1)}}(u) = n \big( 1 - P_X(u) \big)^{n-1} p_X(u) \\
p_{X_{(n)}}(u) = n \big( P_X(u) \big)^{n-1} p_X(u)
\end{gather}
\]&lt;/span&gt; 记&lt;span class=&#34;math inline&#34;&gt;\(X_{(k)}\)&lt;/span&gt;的概率密度函数为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(k)}}\)&lt;/span&gt;，则…&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Differentiation</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/differentiation/</link>
      <pubDate>Fri, 22 Apr 2022 21:13:30 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/differentiation/</guid>
      <description>

&lt;h2 id=&#34;distribution-or-random-variable&#34;&gt;Distribution or Random Variable?&lt;/h2&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;Cross Entropy&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; describe the relationship between &lt;strong&gt;two distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/&#34;&gt;Conditional Entropy&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/mutual-information/&#34;&gt;Mutual Information&lt;/a&gt; describe the relationship between &lt;strong&gt;two random variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/&#34;&gt;Conditional Entropy&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;Cross Entropy&lt;/a&gt; would better be applied on discrete random variables.&lt;/p&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/mutual-information/&#34;&gt;Mutual Information&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; can be applied on either discrete or continuous random variables&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-entropy-and-conditional-entropy&#34;&gt;KL-divergence, Entropy and Conditional Entropy&lt;/h2&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= -\mathrm{E}_{x \sim p} \log \frac{q(x)}{p(x)} \\
&amp;amp;= -\mathrm{E}_{x \sim p} \log q(x) - (-\mathrm{E}_{x \sim p} \log {p(x)}) \\
&amp;amp;= H(p||q) - H(p)
\end{align}
\]&lt;/span&gt; By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(-\log\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= \mathrm{E}_{x \sim p} [-\log \frac{q(x)}{p(x)}] \\
&amp;amp;\ge -\log(\mathrm{E}_{x \sim p} \frac{q(x)}{p(x)}) \\
&amp;amp;= 0
\end{align}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) \ge H(p)
\]&lt;/span&gt; The equality holds if and only if &lt;span class=&#34;math inline&#34;&gt;\(q = p\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-mutual-information&#34;&gt;KL-divergence and Mutual Information&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;= \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&amp;amp;= \sum_{(x,y) \in X \times Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} p(x) D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{x \in X} D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{y \in Y} D_{KL}[p(x|y), p(x)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/clustering/</link>
      <pubDate>Sat, 09 Apr 2022 14:32:03 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/clustering/</guid>
      <description>

&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)}, y^{(i)}), i=1,\dots,M\}\)&lt;/span&gt;, a clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; points into &lt;span class=&#34;math inline&#34;&gt;\(K (K \le M)\)&lt;/span&gt; clusters is a partition of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; disjoint groups &lt;span class=&#34;math inline&#34;&gt;\(\{C_1,\dots,C_K\}\)&lt;/span&gt;. Suppose we have a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that evaluates the clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; and returns lower score with better clustering. The best clustering will be &lt;span class=&#34;math display&#34;&gt;\[
\arg \min_{\mathcal C} f(\mathcal C)
\]&lt;/span&gt; The number of all possibilities of clustering with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements is called the Bell number, denoted as &lt;span class=&#34;math inline&#34;&gt;\(B_M\)&lt;/span&gt;. The calculation of the Bell number is based on dynamic programming. The number of ways to cluster &lt;span class=&#34;math inline&#34;&gt;\(M+1\)&lt;/span&gt; elements is the sum of number of ways to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; element and cluster it, with the rest belong to a new cluster&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; elements and cluster them, with the rest belong to a new cluster&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements and cluster them, with the rest belong to a new cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_{M+1} = \sum_{i=1}^M\binom{M}{0}B_i \\
B_0 = 1
\end{gather}
\]&lt;/span&gt; The exhaustive method will be computationally intractable. We need either an approximation algorithm or a scoring function with special properties.&lt;/p&gt;
&lt;h3 id=&#34;k-means&#34;&gt;K-means&lt;/h3&gt;
&lt;p&gt;K-means assumes there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; clusters. This greatly eliminates many possibilities described above. Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\min_{c_1,\dots,c_K}\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \text{ where }z^{(i)} = \arg \min_{j \in \{1,\dots,K\}}||x^{(i)}-c_j||^2_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}\)&lt;/span&gt; is the cluster index &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is assigned to. K-means’ objective is to assign each point to its closest cluster center and minimize the total within-cluster square errors. If &lt;span class=&#34;math inline&#34;&gt;\(z=j\)&lt;/span&gt; is known, let &lt;span class=&#34;math inline&#34;&gt;\(C_j = \{x^{(i)}|z^{(i)} = j\}\)&lt;/span&gt; be the set of points assigned to cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, the cluster center of cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
c_j = \frac{1}{|C_j|}\sum_{x^{(i)} \in C_j}x^{(i)}
\]&lt;/span&gt; Initially, however, &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is not known. K-means solves this by randomly pick up initial cluster centers and enter the assign-data-points/update-cluster-centers loop, until the cluster centers converge or become satisfactory.&lt;/p&gt;
&lt;p&gt;Rewrite the objective of K-means as: &lt;span class=&#34;math display&#34;&gt;\[
\min_{z,c}(l(z,c) = \sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2)
\]&lt;/span&gt; K-means is in essence a coordinate descent of the objective &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. The main loop of K-means is to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assign data points to its nearest cluster center, i.e. minimizing over the assignment &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;update cluster centers according to the points assigned to, i.e. minimizing over the centroids &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is monotonically decreasing after each step in the above loop. &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is also lower-bounded by &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and thus K-means will converge finally.&lt;/p&gt;
&lt;p&gt;One problem with K-means is that it assumes that each cluster has a circular shape because of the Euclidean distance it uses.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-mixture-model&#34;&gt;Gaussian Mixture Model&lt;/h3&gt;
&lt;p&gt;A cluster can be modelled by a multi-variate Gaussian with elliptical shape: the elliptical shape is controlled by the covariance matrix; the location is controlled by the mean. Gaussian Mixture Model is a weighted sum of Gaussians: &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^K\pi_j\mathcal N(x;\mu_j, \Sigma_j)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; is the prior that a sample is generated from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x;\mu_j, \Sigma_j)\)&lt;/span&gt; is the probability to generate the sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. Put it together, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; is the total probability of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; over its latent &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(z = j\)&lt;/span&gt; representing the prior condition that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is sampled from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^Kp(x, z = j) = \sum_{j=1}^Kp(x|z = j)p(z = j)
\]&lt;/span&gt; GMM is learned by &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)})\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt; The log-likelihood function form will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(l(\pi,\mu,\Sigma) = \log L(\pi,\mu,\Sigma)\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-lagrange-multiplier&#34;&gt;Attempt with &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange Multiplier&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)}) = \prod_{i=1}^M\sum_{j=1}^Kp(z_j)p(x|z = z_j) = \prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\end{gather}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
J(\pi,\mu,\Sigma,\lambda) = -\sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) + \lambda(\sum_{j=1}^K\pi_j - 1)
\]&lt;/span&gt; This expression is just too hard to zero the derivatives. For example, writing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x^{(i)};\mu_j, \Sigma_j)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}_j\)&lt;/span&gt;, then take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial J}{\partial \pi_j} = -\sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_j, \Sigma_j)} + \lambda \\
\lambda = \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_1, \Sigma_1)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_1, \Sigma_1)} 
= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_2, \Sigma_2)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_2, \Sigma_2} 
= \dots 
= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_K, \Sigma_K)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_K, \Sigma_K}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By far, the whole expression is too complicated for us to continue with.&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-expectation-maximization&#34;&gt;Attempt with &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/expectation-maximization/&#34;&gt;Expectation Maximization&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^M p(x^{(i)}) = \prod_{i=1}^M \sum_{j=1}^K p(x^{(i)}, z^{(i)} = j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)
\end{gather}
\]&lt;/span&gt; By Jensen’s inequality, if &lt;span class=&#34;math inline&#34;&gt;\(\forall j \in \{1,\dots,K\}, \alpha_j \ge 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K\alpha_j = 1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\pi,\mu,\Sigma) &amp;amp;= \sum_{i=1}^M \log \sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j) = \sum_{i=1}^M \log \sum_{j=1}^K \alpha^{(i)}_j \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\alpha^{(i)}_j} \\
&amp;amp;\ge B(\alpha,\pi,\mu,\Sigma) := \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\alpha^{(i)}_j} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. If we can enlarge &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, we can guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is increasing in this process. If we fix &lt;span class=&#34;math inline&#34;&gt;\(\pi,\mu,\Sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; can be easily optimized by the equality condition of Jensen’s Inequality. The equality holds if and only if &lt;span class=&#34;math display&#34;&gt;\[
\frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\alpha^{(i)}_j} = c^{(i)}, i=1,\dots M, j=1,\dots,K
\]&lt;/span&gt; In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\mathcal N(x^{(i)};\mu_1, \Sigma_1) \pi_j + \dots + \mathcal N(x^{(i)};\mu_K, \Sigma_j) \pi_K}{\alpha^{(i)}_1 + \dots + \alpha^{(i)}_K} = c^{(i)} \\
c^{(i)} = \sum_{j=1}^K \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j \\
\begin{aligned}
\alpha^{(i)}_j &amp;amp;= \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j}{\sum_{j=1}^K \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j} \\
&amp;amp;= \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{p(x^{(i)})} \\
&amp;amp;= \frac{p(x^{(i)}, z^{(i)} = j)}{p(x^{(i)})} \\
&amp;amp;= p(z^{(i)} = j|x^{(i)})
\end{aligned}
\end{gather}
\]&lt;/span&gt; If we fix &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B(\alpha,\pi,\mu,\Sigma) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j - \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \alpha^{(i)}_j \\
\end{aligned}
\]&lt;/span&gt; Grouping terms in &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; that are relevant to &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;, we are to &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_\pi \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j \\
s.t.\quad \sum_{i=1}^K \pi_j = 1
\end{gather}
\]&lt;/span&gt; We do this by Lagrangian multipliers: &lt;span class=&#34;math display&#34;&gt;\[
J(\pi, \lambda) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j + \lambda(1 - \sum_{i=1}^K\pi_i )
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial J}{\partial \pi_k} =\sum_{i=1}^M \frac{\alpha^{(i)}_k}{\pi_k} - \lambda \\
\pi_k = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{\lambda}
\]&lt;/span&gt; With the knowledge that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K \pi_j = 1, \sum_{i=1}^M\alpha^{(i)}_j = 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\pi_k = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{M}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \mu_k} &amp;amp;= \frac{\partial \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\partial \mu_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k)}{\partial \mu_k} \\
&amp;amp;= \sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) &amp;amp;= 0, \text{ by the invertibility of $\Sigma_j^{-1}$} \\
\mu_k &amp;amp;= \frac{\sum_{i=1}^M \alpha^{(i)}_k x^{(i)}}{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \Sigma_k} &amp;amp;= \frac{\partial \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j, \Sigma_j)}{\partial \Sigma_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k ((x^{(i)} - \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k) + \log|\Sigma_k|)}{\partial \Sigma_k} \\
&amp;amp;= -\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
-\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T + I) &amp;amp;= 0 \\
\end{aligned} \\
\Downarrow \\
\Sigma_k = \frac{\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_{i=1}^M \alpha^{(i)}_k} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-k-means&#34;&gt;Soft K-means&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; in GMM, which measures how likely the sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is generated from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian, can be viewed as a contribution of sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to the cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. That is, a sample can contribute different portions to different clusters and these portions add up to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the case where &lt;span class=&#34;math inline&#34;&gt;\(\pi_j = \frac{1}{K}, \Sigma_k = I\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p(x^{(i)}|z^{(i)} = j) = \mathcal N(x^{(i)};\mu_j, I) = \frac{1}{\sqrt{(2\pi)^N}}\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2) \\
\alpha^{(i)}_j = p(z^{(i)} = j | x^{(i)}) = \frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{p(x^{(i)})} = \frac{\mathcal N(x^{(i)};\mu_j, I) \frac{1}{K}}{\frac{1}{K} \sum_{k=1}^K \mathcal N(x;\mu_k, I)} = \frac{\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2)}{\sum_{k=1}^K \exp(-\frac{1}{2}||x^{(k)}-\mu_j||^2_2)}
\end{gather}
\]&lt;/span&gt; The “Soft K-means” comes from the softmax of the Euclidean distance.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Angle Regression</title>
      <link>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</link>
      <pubDate>Sun, 19 Dec 2021 15:09:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</guid>
      <description>

&lt;h3 id=&#34;forward-selection&#34;&gt;Forward Selection&lt;/h3&gt;
&lt;p&gt;In cases where we are to solve &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Y = XW\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Y \in \R^M, X \in \R^{M \times N}, W \in \R^N\)&lt;/span&gt;, we can do it in an iterative and greedy way.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right, do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \frac{\mathrm x^{(k)} \cdot Y}{||\mathrm x ^{(k)}||_2} \mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or we have used all columns in &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently this algorithm is efficient. However, it does not guarantee an exact solution, even when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; lies in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;forward-stagewise&#34;&gt;Forward Stagewise&lt;/h3&gt;
&lt;p&gt;Like Forward Selection, Forward Stagewise selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Unlike Forward Selection, Forward Selection does not subtract the whole projection from &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Instead, it proceeds along &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; with a small step.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\},\eta\text{ is a small constant}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right, do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \eta \frac{\mathrm x^{(k)} \cdot Y}{||\mathrm x ^{(k)}||_2} \mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;del&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/del&gt; Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently Forward Stagewise gives an exact solution when &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is small enough. But it is more time-consuming.&lt;/p&gt;
&lt;h3 id=&#34;least-angle-regression&#34;&gt;Least Angle Regression&lt;/h3&gt;
&lt;p&gt;LARS a is compromise of Forward Selection and Forward Stagewise. Likewise, LARS selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. LARS proceeds stagewise like Forward Stagewise, however with its own methodology to determine the step size.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)}, \mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}, \mathrm d = \arg\max_{\mathrm x \in C}\frac{Y \cdot \mathrm x }{||\mathrm x||_2}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then do the following: &lt;span class=&#34;math display&#34;&gt;\[
\hat Y = \eta \frac{\mathrm d \cdot Y}{||\mathrm d||_2} \mathrm d \\
Y_{res} = Y_{res} - \hat Y
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is determined such that there is some &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)} \in C\)&lt;/span&gt; onto which the projection of &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt;. Or in other words, &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; lies on the bisector hyper-plane between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathrm d\)&lt;/span&gt; be along this bisector.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


</description>
    </item>
    
    <item>
      <title>参数估计</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid>
      <description>

&lt;h2 id=&#34;点估计&#34;&gt;点估计&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim p(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;为总体的一个样本，若用一个统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;来估计&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。&lt;/p&gt;
&lt;h3 id=&#34;矩估计&#34;&gt;矩估计&lt;/h3&gt;
&lt;p&gt;矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;span class=&#34;math inline&#34;&gt;\(\mu_k = \E(X^k)\)&lt;/span&gt;，样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩为&lt;span class=&#34;math inline&#34;&gt;\(A_k = \frac 1 n \sum_{i=1}^n X_i^k\)&lt;/span&gt;，如果未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta = \varphi(\mu_1, \dots, \mu_m)\)&lt;/span&gt;，则其估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \varphi(A_1, \dots, A_m)\)&lt;/span&gt;，这种估计总体未知参数的方法叫作矩估计法。&lt;/p&gt;
&lt;p&gt;矩估计往往不唯一，如设&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda)\)&lt;/span&gt;，则由于&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;；又&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\frac 1 n \sum_{i=1}^n X_i^2 - \bar X^2\)&lt;/span&gt;。此时往往采用较低阶的矩来估计未知参数。&lt;/p&gt;
&lt;h3 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;设总体有分布律&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(X=x;\theta)\)&lt;/span&gt;或密度函数&lt;span class=&#34;math inline&#34;&gt;\(X \sim p(x;\theta)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_n\)&lt;/span&gt;为取自总体的一组样本观测值，将样本的联合分布律或联合密度函数看作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的函数： &lt;span class=&#34;math display&#34;&gt;\[
L(\theta) = \prod_{i=1}^n P(X=x_i;\theta)\ \text或 \ L(\theta) = \prod_{i=1}^n p(x_i;\theta)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta)\)&lt;/span&gt;又称作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的似然函数，似然函数满足关系式&lt;span class=&#34;math inline&#34;&gt;\(L(\hat \theta) = \max_{\theta} L(\theta)\)&lt;/span&gt;的解&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计量。&lt;/p&gt;
&lt;h3 id=&#34;优良性评判&#34;&gt;优良性评判&lt;/h3&gt;
&lt;h4 id=&#34;无偏性unbiased&#34;&gt;无偏性（Unbiased）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个无偏估计（量），否则则称作有偏估计（量）。如果有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} \E [\hat \theta(X_1, \dots, X_n)] = \theta, \text{亦即} \hat \theta \stackrel{L_1}{\to} \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个渐进无偏估计（量）。&lt;/p&gt;
&lt;p&gt;估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值也许偏大，取另一些样本时估计量也许偏小，但多次取样本进行估计，平均来讲偏差为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即系统误差。&lt;/p&gt;
&lt;h4 id=&#34;最小方差minimum-variance&#34;&gt;最小方差（Minimum-variance）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1 = \hat \theta_2\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的两个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat \theta_1) \le \Var(\hat \theta_2)\)&lt;/span&gt;，且至少有一个&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;使得该不等式严格成立，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1\)&lt;/span&gt;比&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_2\)&lt;/span&gt;有效。&lt;/p&gt;
&lt;h4 id=&#34;一致性consistent&#34;&gt;一致性（Consistent）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，若对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\hat \theta - \theta| \ge \epsilon) = 0, \text{亦即} \hat \theta \stackrel{P}{\to} \theta
\]&lt;/span&gt; 则称估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;具有一致性。一致性是一个很基本的要求：随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。&lt;/p&gt;
&lt;h3 id=&#34;cramer-rao不等式&#34;&gt;Cramer-Rao不等式&lt;/h3&gt;
&lt;p&gt;实际上，点估计量不仅仅可以估计未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;本身（假设为一元情况），更可以估计未知参数的某个函数&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，即给定总体的一个样本&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;，用统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g = \hat g(X_1, \dots, X_n)\)&lt;/span&gt;估计&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;。估计量最好的效果便是达到最小方差无偏（minimum-variance unbiased &amp;lt;MVU&amp;gt;）估计，Cramer-Rao不等式给出了点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g\)&lt;/span&gt;方差的一个下界。 &lt;span class=&#34;math display&#34;&gt;\[
\label{cr} \Var(\hat g) \ge (g&amp;#39;(\theta))^2 / (nI(\theta))
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(I(\theta) = \int [(\frac{\partial p(x;\theta)}{\partial \theta})^2 / p(x;\theta)] \d x\)&lt;/span&gt;为Fisher Information。当&lt;span class=&#34;math inline&#34;&gt;\(g(\theta) = \theta\)&lt;/span&gt;，即只估计未知参数本身时，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat g) \ge 1 / (nI(\theta))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;成立有一定的条件，其本身就暗含了&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial p(x;\theta)}{\partial \theta}\)&lt;/span&gt;存在及&lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\theta)\)&lt;/span&gt;存在的&lt;strong&gt;条件&lt;/strong&gt;。记 &lt;span class=&#34;math display&#34;&gt;\[
S = S(X_1, \dots, X_n, \theta) = \sum_{i=1}^n \frac{\partial \ln p(X_i;\theta)} {\partial \theta} = \sum_{i=1}^n [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\int p(x;\theta)\ \d x = 1\)&lt;/span&gt;，此式两边同时对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导，并&lt;strong&gt;假定&lt;/strong&gt;此处求导可以移至积分号内部，可得到&lt;span class=&#34;math inline&#34;&gt;\(\int \frac{\partial p(x;\theta)}{\partial \theta} \d x = 0\)&lt;/span&gt;。根据&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/statistics/unconscious-statistics/#Law of the Unconscious Statistician&#34;&gt;LOTUS&lt;/a&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\E [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] 
= \int [\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta)] p(x;\theta)\ \d x
= \int \frac{\partial p(x;\theta)} {\partial \theta}\d x = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;的独立性， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Var(S) &amp;amp;= \sum_{i=1}^n \Var [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \\
&amp;amp;= \sum_{i=1}^n \{ \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] - \E^2 [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \} \\
&amp;amp;= \sum_{i=1}^n \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] \\
&amp;amp;= n \int \big (\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta) \big)^2 p(x;\theta)\ \d x \\
&amp;amp;= n I(\theta)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根据协方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\label{cov_prop} [\Cov(\hat g, S)]^2 \le \Var(\hat g) \Var(S) = \Var(\hat g) n I(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又&lt;span class=&#34;math inline&#34;&gt;\(\E(S) = 0\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) = \E (\hat g S) &amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \sum_{i=1}^n [\frac{\partial p(x_i;\theta)} {\partial \theta} / p(x_i;\theta)] \prod_{i=1}^n p(x_1;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \frac{\partial p(x_1;\theta) \dots p(x_n;\theta)} {\partial \theta}\ \d x_1 \dots \d x_n 
\end{aligned}
\]&lt;/span&gt; &lt;strong&gt;假定&lt;/strong&gt;此处对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导可以移至积分号外部， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) 
&amp;amp;= \frac \partial{\partial \theta} \int \dots \int \hat g(x_1, \dots, x_n) p(x_1;\theta) \dots p(x_n;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \frac \partial{\partial \theta} g(\theta) = g&amp;#39;(\theta)
\end{aligned}
\]&lt;/span&gt; 将上式重新带入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cov_prop}\)&lt;/span&gt;，从而得到&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/56411276/answer/204992057&#34;&gt;对Cramer-Rao不等式的理解&lt;/a&gt; || &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Wiki (see the multi-variate case)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;区间估计&#34;&gt;区间估计&lt;/h2&gt;
&lt;p&gt;点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作&lt;strong&gt;置信区间&lt;/strong&gt;（confidence interval）。&lt;/p&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim f(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;为总体的一个样本，给定一个很小的数&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \alpha &amp;lt; 1\)&lt;/span&gt;，若有统计量&lt;span class=&#34;math inline&#34;&gt;\(\theta_l = \theta_l (X_1, \dots, X_n) \le \theta_r(X_1, \dots, X_n) = \theta_r\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
P(\theta_l \le \theta \le \theta_r) \ge 1 - \alpha
\]&lt;/span&gt; 我们称&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\([\theta_l, \theta_r]\)&lt;/span&gt;的&lt;strong&gt;置信水平&lt;/strong&gt;（confidence level），&lt;span class=&#34;math inline&#34;&gt;\(\theta_l\)&lt;/span&gt;为&lt;strong&gt;置信下限&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\theta_r\)&lt;/span&gt;为&lt;strong&gt;置信上限&lt;/strong&gt;。一般来说置信水平不唯一，因为若&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;是某个区间的置信水平，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\alpha &amp;lt; \tilde \alpha &amp;lt; 1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(1 - \tilde \alpha\)&lt;/span&gt;亦是该区间的置信水平。故一般的“置信水平”是这一系列置信水平中的最大者。&lt;/p&gt;
&lt;h3 id=&#34;枢轴变量法&#34;&gt;枢轴变量法&lt;/h3&gt;
&lt;p&gt;区间估计一般采用枢轴变量法，枢轴变量法的一般步骤为：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;（如&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;的一个函数&lt;span class=&#34;math inline&#34;&gt;\(G = G(\theta, \hat \theta)\)&lt;/span&gt;（称作枢轴（pivot）函数），且&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;完全已知，且其分布与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关，&lt;/li&gt;
&lt;li&gt;对任何常数&lt;span class=&#34;math inline&#34;&gt;\(a &amp;lt; b\)&lt;/span&gt;，不等式&lt;span class=&#34;math inline&#34;&gt;\(a \le G(\theta, \hat \theta) \le b\)&lt;/span&gt;能够改写成等价的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;仅与&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta,a,b\)&lt;/span&gt;有关，与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;取&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;的上&lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha / 2}) = 1 - \alpha/2\)&lt;/span&gt;）及上&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{1 - \alpha / 2}) = \alpha / 2\)&lt;/span&gt;），此时有&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha/2}) - F(w_{1 - \alpha / 2}) = 1 - \alpha\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
P(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}) = 1 - \alpha
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}\)&lt;/span&gt;可改写为对应的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;的形式，且&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;仅与估计量和两个分位点有关，&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;就构成了&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在构造枢轴函数时，一般会使用一些现有结论，比如&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#中心极限定理&#34;&gt;中心极限定理&lt;/a&gt;的近似、&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/&#34;&gt;三大分布与正态总体的抽样分布&lt;/a&gt;等等。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Dimension Reduction</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/dimension-reduction/</link>
      <pubDate>Fri, 07 Jan 2022 12:39:56 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/dimension-reduction/</guid>
      <description>

&lt;h2 id=&#34;unsupervised-dimension-reduction&#34;&gt;Unsupervised Dimension Reduction&lt;/h2&gt;
&lt;p&gt;Dimension reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduces computational cost,&lt;/li&gt;
&lt;li&gt;de-noises by projecting onto lower-dimensional space and back to original space,&lt;/li&gt;
&lt;li&gt;makes results easier to understand.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;vs. feature selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal of feature selection is to remove features that are not informative with respect to the class label. This obviously reduces the dimensionality of the feature space.&lt;/li&gt;
&lt;li&gt;Dimension reduction can be used to find a meaningful lower-dim feature space even when there is information in each feature dimension so that none can be discarded.&lt;/li&gt;
&lt;li&gt;Dimension reduction is unsupervised while feature selection is supervised.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;vs. data compression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dimension reduction can be seen as a simplistic form of data compression. But they are not equivalent, as the goal of data compression is to reduce the entropy of the representation, which is not limited to the dimension reduction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-dimension-reduction&#34;&gt;Linear Dimension Reduction&lt;/h3&gt;
&lt;p&gt;Linear Dimension Reduction projects data onto lower-dimensional space by representing the data with a new basis consisting of some major components. Mathematically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x^{(i)} = \sum_{k=1}^Kz^{(i)}_kb^{(k)}, \text{ where $z^{(i)} \in \R^K$ is the weight, $b^{(k)} \in \R^N$ is the basis vector.} \\
X = BZ, \text{ where $X = [x^{(i)}, \dots, x^{(M)}], B = [b^{(1)}, \dots, b^{(K)}], Z = [z^{(1)}, \dots, z^{(M)}]$}
\end{gather}
\]&lt;/span&gt; The objective can be set to minimize the error when recovering from &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;alternating-least-squares&#34;&gt;Alternating Least Squares&lt;/h4&gt;
&lt;p&gt;By leveraging the idea of Coordinate Descent and OLS solution to Linear Regression, we can optimize &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; alternatively, until convergence.&lt;/p&gt;
&lt;p&gt;Fix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
2(X - BZ)(-Z^T) = 0 \\
BZZ^T = XZ^T \\
B = XZ^T(ZZ^T)^{-1}
\end{gather}
\]&lt;/span&gt; Fix &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(Z^T\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial||X - BZ||^2_F}{\partial Z^T} = \frac{\partial||X^T - Z^TB^T||^2_F}{\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\
2(X^T - Z^TB^T)(-B) = 0 \\
Z^TB^TB = X^TB \\
Z^T = X^TB(B^TB)^{-1} \\
Z = (B^TB)^{-1}B^TX
\end{gather}
\]&lt;/span&gt; Assume we have run the ALS to convergence and obtain the global optimal parameters &lt;span class=&#34;math display&#34;&gt;\[
B^\star, Z^\star = \arg \min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt; Let any invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(R \in \R^{K \times K}\)&lt;/span&gt;. We can construct a pair of &lt;span class=&#34;math inline&#34;&gt;\(\tilde B, \tilde Z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde B = B^\star R, \tilde Z = R^{-1}Z^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||X - \tilde B \tilde Z||^2_F = ||X - B^\star RR^{-1} Z^\star||^2_F = ||X - B^\star Z^\star||^2_F
\]&lt;/span&gt; Thus the global optima is not unique. We may force regularization on &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; and obtain the unique optima.&lt;/p&gt;
&lt;h4 id=&#34;singular-value-decomposition&#34;&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;Singular Value Decomposition&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Suppose the Singular Value Decomposition for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $N \times N$, $\Sigma$ is diagonal, $V$ is $M \times M$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{N \times M}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = \min\{N,M\}
\end{gather}
\]&lt;/span&gt; By only preserving the first &lt;span class=&#34;math inline&#34;&gt;\(K \le \rank(\Sigma)\)&lt;/span&gt; most prominent singular values, &lt;span class=&#34;math inline&#34;&gt;\(X \approx U_K\Sigma_KV^T_K\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U_K \in \R^{N \times K}$ is first $K$ columns of $U$} \\
\text{$V_K \in \R^{M \times K}$ is first $K$ columns of $V$} \\
\Sigma_K \in \R^{K \times K} = diag(\sigma_1, \sigma_2, ..., \sigma_K) \\
\end{gather}
\]&lt;/span&gt; &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/&#34;&gt;Eckart-Young-Mirsky Theorem&lt;/a&gt; will tell that &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt; is the best approximation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; among &lt;span class=&#34;math inline&#34;&gt;\(N \times M\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in terms of Frobenius norm.&lt;/p&gt;
&lt;p&gt;We can form the &lt;span class=&#34;math inline&#34;&gt;\(B^\star, Z^\star\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
B^\star = U_K \Sigma_K^{\frac{1}{2}}, Z^\star = \Sigma_K^{\frac{1}{2}} V^T_K
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;principal-component-analysis&#34;&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/#Linear Principal Component Analysis&#34;&gt;Principal Component Analysis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;As shown in ALS, the optimal solution of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is not unique. ALS and SVD give their solutions, however without much interpretability. The solution given by Principal Component Analysis explicitly chooses the directions of the basis it uses.&lt;/p&gt;
&lt;p&gt;The goal of PCA is to identify the directions along which the data exhibits the maximum variance. And then PCA projects the data onto the space formed by these directions.&lt;/p&gt;
&lt;p&gt;The solution of PCA is &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; eigenvectors permuted according to their corresponding to eigenvalues, which is exactly the &lt;span class=&#34;math inline&#34;&gt;\(U_K\)&lt;/span&gt; in &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. Thus, the solution of PCA can be constructed from SVD. Let &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;. If we choose &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; directions with largest directional variance as basis. Then &lt;span class=&#34;math inline&#34;&gt;\(B^\star = U_K\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;random-projection&#34;&gt;Random Projection&lt;/h4&gt;
&lt;p&gt;The above methods all minimize the Frobenius norm during reconstruction. This objective may become time-consuming when input dimension becomes large. We may use some randomly-generated vectors as basis to do the projection. This greatly saves time, at the expense of losing accuracy. We can measure such projection by checking whether the structure of the data can be preserved, e.g. the distance between points.&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss Lemma tells that a random function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \frac{1}{\sqrt{K}}Ax\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K \times N\)&lt;/span&gt; matrix with i.i.d. entries sampled from a standard Gaussian, can preserve the distance between any two points within error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
(1 - \epsilon)||x^{(i)} - x^{(j)}||^2_2 \le ||f(x^{(i)}) - f(x^{j})||^2_2 \le (1 + \epsilon)||x^{(i)} - x^{(j)}||^2_2
\]&lt;/span&gt; with the probability at least &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt; as long as &lt;span class=&#34;math display&#34;&gt;\[
K \ge \frac{8\log M}{\epsilon^2}
\]&lt;/span&gt; And usually this requirement is quite conservative.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-dimension-reduction&#34;&gt;Non-linear Dimension Reduction&lt;/h3&gt;
&lt;p&gt;We can introduce the feature mapping to handle the non-linearity problem in Dimension Reduction. Similar to non-linear regression, we can introduce the kernel trick in this case. &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/#Kernel PCA&#34;&gt;Principal Component Analysis &amp;gt; Kernel PCA&lt;/a&gt; is just such an example.&lt;/p&gt;
&lt;h2 id=&#34;supervised-dimension-reduction&#34;&gt;Supervised Dimension Reduction&lt;/h2&gt;
&lt;p&gt;There is no guarantee that the unsupervised Dimension Reduction can throw insight into the classification problem. It may or may not help. Otherwise supervised Dimension Reduction such as &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/&#34;&gt;Fisher’s Linear Discriminant&lt;/a&gt; finds a lower-dimensional space so as to minimize the class overlap.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>三大分布与正态总体的抽样分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</link>
      <pubDate>Sat, 09 Jul 2022 22:21:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布都从正态分布中衍生出来，常用统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。&lt;/p&gt;
&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;gamma函数&#34;&gt;Gamma函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x) = \int_0^{+\infty} e^{-t} t^{x-1} dt \quad (x &amp;gt; 0)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt;函数具有&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x + 1) = x \Gamma(x)\)&lt;/span&gt;的性质：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = \int_0^{+\infty} e^{-t} t^{x} dt = [-e^{-t} t^x] \bigg|^{+\infty}_{t=0} - \int_0^{+\infty} -e^{-t} xt^{x-1} dt
\]&lt;/span&gt; 根据洛必达法则，&lt;span class=&#34;math inline&#34;&gt;\(\lim_{t \to +\infty} = \frac{-t^x}{e^t} = \lim_{t \to +\infty} \frac{x!}{e^t} = 0\)&lt;/span&gt;，故 &lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = 0 + x\int_0^{+\infty} e^{-t} t^{x-1} dt = x \Gamma(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1) = \int_0^{+\infty} e^{-t} dt = 1\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;为正整数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x) = x!\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2) = \sqrt \pi\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x = 2k + 1\)&lt;/span&gt;为正奇数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\frac{x}{2}) = \sqrt \pi \prod_{i=0}^{k-1} \frac{2 * i + 1}{2}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\Gamma (\frac{1}{2}) = \int_0^{+\infty} e^{-t} t^{-\frac{1}{2}} dt 
\stackrel{u = t^\frac{1}{2}}{\Longrightarrow} \int_0^{+\infty} e^{-u^2} u^{-1} \;2u du 
= 2 \int_0^{+\infty} e^{-u^2} du 
= \int_{-\infty}^{+\infty} e^{-u^2} du  \\
\notag \\
\begin{aligned}
&amp;amp;\Gamma^2(\frac{1}{2}) = (\int_{-\infty}^{+\infty} e^{-u^2} du)^2 \\
&amp;amp;= (\int_{-\infty}^{+\infty} e^{-u^2} du)(\int_{-\infty}^{+\infty} e^{-v^2} dv) \\
&amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(u^2+v^2)}dudv \\
&amp;amp;\downarrow_{u = r\sin\theta, v = r\cos\theta} \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} e^{-r^2}\; rdrd\theta \\
&amp;amp;= \int_{0}^{2\pi} [-\frac{1}{2}e^{-r^2}] \bigg|_{r=0}^{+\infty} d\theta \\
&amp;amp;= \int_{0}^{2\pi} \frac{1}{2} d\theta \\
&amp;amp;= \pi \\
&amp;amp;\Gamma (\frac{1}{2}) = \sqrt \pi
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinehw.math.ksu.edu/math340book/chap3/gamma.php&#34;&gt;The Gamma Function&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=XAoe4th0F1k&#34;&gt;The derivation of &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2)\)&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;chi2分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为相互独立的标准正态分布随机变量，即&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim N(0,1)\)&lt;/span&gt;则称&lt;span class=&#34;math inline&#34;&gt;\(Y = X_1^2 + \dots + X_n^2\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布，记作&lt;span class=&#34;math inline&#34;&gt;\(Y \sim \chi^2(n)\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时，容易得到&lt;span class=&#34;math inline&#34;&gt;\(\forall y \le 0, P_Y(y) = 0, p_Y(y)= 0\)&lt;/span&gt;， $$ &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\begin{split}
\forall y &amp;gt; 0, P_Y(y) &amp;amp;= 2P_X(\sqrt y) - 1 \\ 
\end{split} \\

\begin{split}
&amp;amp;p_Y(y) = 2P_X&amp;#39;(\sqrt y) \frac 1 {2 \sqrt y} \\
&amp;amp;= \frac 1 {\sqrt {2\pi y}}  e^{-\frac 1 2 y} \\
&amp;amp;= \frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y 2}
\end{split}
\end{align}\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
$\chi^2(1)$分布的密度函数为：
\]&lt;/span&gt; p_Y(y) =&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
\frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y 2}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=k\)&lt;/span&gt;时，令&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_k\)&lt;/span&gt;表示一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维空间中的点， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_Y(y) = P_Y(Y \le y) &amp;amp;= \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1 \dots \d x_k \\
&amp;amp;= \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
\end{aligned}
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k x_i^2 \le y\)&lt;/span&gt;的积分区域。可以看出，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;对应一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维球体，且其半径&lt;span class=&#34;math inline&#34;&gt;\(R = \sqrt y\)&lt;/span&gt;。对此，作&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/&#34;&gt;高维球坐标变换&lt;/a&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P_Y(Y \le y) = \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1 \dots \d x_k = \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \int_0^\sqrt{y} \\
&amp;amp;\quad\quad\quad\frac{e^{-\frac 1 2 (r^2\cos^2 \varphi_1 +
r^2\sin^2 \varphi_1 \cos^2 \varphi_2 + \dots + 
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \cos^2 \varphi_{k-1} + 
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \sin^2 \varphi_{k-1})} } {(2\pi)^{k / 2}}\\
&amp;amp;\quad\quad\quad r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \int_0^\sqrt{y} \frac{e^{-\frac 1 2 r^2}} {(2\pi)^{k / 2}}
r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^\sqrt{y} \underbrace{ \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2} \frac{1} {(2\pi)^{k / 2}} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})\ \d \varphi_1 \dots \d \varphi_k}_{c_k} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;= c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;是和&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;相关的常数项，并且由于&lt;span class=&#34;math inline&#34;&gt;\(P_Y(Y \le \infty) = 1\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 &amp;amp;= c_k \int_0^\infty e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;\Downarrow_{r = \sqrt{2t}} \\
1 &amp;amp;= c_k \int_0^\infty e^{-t} \sqrt{2t}^{k-1} \frac{1}{\sqrt{2t}} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \int_0^\infty e^{-t} t^{(k-2)/2} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \Gamma(\frac{k}{2}) \\
c_k &amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故可得密度函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p_Y(y) = \frac{\d P_Y(Y \le y)}{\d y} \\
&amp;amp;= \frac{\d [c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r]}{\d y} \\
&amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})} e^{-\frac y 2} y^\frac{k-1}{2} \frac{1}{2\sqrt y} \\
&amp;amp;= \frac{1} {2^{\frac k 2} \Gamma(\frac{k-2}{2})} e^{-\frac y 2} y^{\frac{k}{2} - 1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最终可得密度函数如下： &lt;span class=&#34;math display&#34;&gt;\[
p_Y(y) =
\begin{cases}
\frac 1 {2^\frac{k}{2} \Gamma(\frac k 2)} e^{-\frac y 2} y^{\frac k 2 - 1}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}
\]&lt;/span&gt; 另外，该密度函数也可以通过数学归纳法验证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statlect.com/probability-distributions/chi-square-distribution&#34;&gt;Chi Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://vtechworks.lib.vt.edu/bitstream/handle/10919/34329/10Apxb.pdf?sequence=12&#34;&gt;Generating Function of Chi Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/30020592&#34;&gt;正向推导&lt;/a&gt; || &lt;a href=&#34;https://www.bilibili.com/video/BV1e54y1v7e5&#34;&gt;数学归纳法&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;t分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim N(0,1), Y \sim \chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X}{\sqrt{Y/n}}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z \sim t(n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \frac{\Gamma((n+1)/2)} {\sqrt{n\pi} \Gamma(n/2)} \big( 1 + \frac{z^2} n \big)^{-(n+1)/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;f分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim \chi^2(m), Y \sim \chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X/m}{Y/n}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z \sim F(m,n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \begin{cases}
\frac{\Gamma((m+n)/2} {\Gamma(m/2) \Gamma(n/2)} {m \choose n}^{\frac m 2} z^{\frac m 2 - 1} (1 + \frac m n z)^{-\frac{m+n}{2}}, &amp;amp;z &amp;gt; 0 \\
0, &amp;amp;\text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;正态总体的抽样分布&#34;&gt;正态总体的抽样分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu, \sigma^2)\)&lt;/span&gt;的一个样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac 1 n \sum_{i=1}^n X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2\)&lt;/span&gt;，则存在如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X \sim N(\mu, \frac{\sigma^2}{n}) \\
\frac{\sum_{i=1}^n (X_i - \bar X)^2}{\sigma^2} \sim \chi^2(n - 1),\text 即 \frac{(n-1) S^2}{\sigma^2} = \frac{n S_n^2}{\sigma^2} \sim \chi^2(n-1) \\
\text{$\bar X$与$S^2$相互独立，$\bar X$与$S_n^2$相互独立}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;分布计算器&#34;&gt;分布计算器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html&#34;&gt;Normal Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/chisq.html&#34;&gt;Chi-Square Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/t.html&#34;&gt;Student’s t-Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/f.html&#34;&gt;F-Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</link>
      <pubDate>Tue, 11 Jan 2022 16:10:51 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</guid>
      <description>

&lt;p&gt;Given a data matrix &lt;span class=&#34;math inline&#34;&gt;\(X = [x^{(1)}, x^{(2)}, ..., x^{(M)}] \in \mathbb R^{N \times M}\)&lt;/span&gt;, the goal of PCA is to identify the directions of maximum variance contained in the data (compared with directional derivative, this is directional variance).&lt;/p&gt;
&lt;h3 id=&#34;linear-pca&#34;&gt;Linear PCA&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(v \in \mathbb R^{N \times 1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt;, the variance in the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2, \text{where }\mu = \frac{1}{M}\sum_{i=1}^Mv^Tx^{(i)} = v^T\bar x
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{M}\sum_{i=1}^Mx^{(i)} = 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2 = \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - v^T\bar x)^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(x^{(i)} - \bar x)(x^{(i)} - \bar x)^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(x^{(i)} - \bar x)(x^{(i)} - \bar x)^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^TXX^Tv \\
\end{aligned}
\]&lt;/span&gt; Suppose we want to identify the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; of the maximum variance, we can formulate the problem as &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v \quad \frac{1}{M}v^TXX^Tv \\
s.t.\quad v^Tv = 1
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = \frac{1}{M}XX^T\)&lt;/span&gt;, which is real symmetric, we can form the Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
L(v, \lambda) = v^T\Sigma v + \lambda (1 - v^Tv)
\]&lt;/span&gt; We represent the constraint as &lt;span class=&#34;math inline&#34;&gt;\(1 - v^Tv = 0\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(v^Tv - 1 = 0\)&lt;/span&gt;. The reason is if we take derivative w.r.t &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial v} = 2\Sigma v - 2\lambda v
\]&lt;/span&gt; which is more intuitive than &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial v} = 2\Sigma v + 2\lambda v\)&lt;/span&gt;. Let the derivative be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(v^tv = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v \ne 0\)&lt;/span&gt;, this means the optimal solution &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star, v^\star)\)&lt;/span&gt; must be a pair of eigenvalue of eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v^\star = v_i, \lambda^\star = \lambda_i
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; is rescaled such that &lt;span class=&#34;math inline&#34;&gt;\(v_i^Tv_i = 1\)&lt;/span&gt;. Substitute the result back to the objective to give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}v^TXX^Tv &amp;amp;= (v_i)^T\Sigma v_i \\
&amp;amp;= (v_i)^T\lambda_iv_i \\
&amp;amp;= \lambda_i(v_i)^Tv_i \\
&amp;amp;= \lambda_i
\end{aligned}
\]&lt;/span&gt; So the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; largest directions of variance will be the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_N\)&lt;/span&gt;, corresponding to the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 &amp;gt; \lambda_2 &amp;gt; ... &amp;gt; \lambda_N\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;another-view-on-pca&#34;&gt;Another view on PCA&lt;/h4&gt;
&lt;p&gt;Finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\max_{v} \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)})^2 \\
\]&lt;/span&gt; is equivalent to finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\min_{v} \frac{1}{M}\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2
\]&lt;/span&gt; both subject to &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt; and inter-orthogonality. This second notation is essentially the projection of &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(||v|| = 1\)&lt;/span&gt; and thus the objective indicates minimizing the overall approximation error.&lt;/p&gt;
&lt;h3 id=&#34;kernel-pca&#34;&gt;Kernel PCA&lt;/h3&gt;
&lt;p&gt;We can map the data to a higher-dimension space: &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \to \phi(x^{(i)})\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X} = [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})]\)&lt;/span&gt;. Kernel tricks allow us not to explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, but to only focus on the inner product of mapped data: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{i})^T\phi(x^{j})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, the variance along direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(v^Tv=1\)&lt;/span&gt;, is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2, \text{where }\mu = \frac{1}{M}\sum_{i=1}^Mv^T\phi (x^{(i)}) = v^T\bar\phi(x)
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x) = \frac{1}{M}\sum_{i=1}^M\phi(x^{(i)}) = 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2 = \frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - v^T\bar\phi(x))^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(\phi(x^{(i)}) - \bar{\phi(x)})(\phi (x^{(i)}) - \bar\phi(x))^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(\phi(x^{(i)}) - \bar\phi(x))(\phi (x^{(i)}) - \bar\phi(x))^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M\phi(x^{(i)})(\phi(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
\end{aligned}
\]&lt;/span&gt; Then comes the standard form of linear PCA. Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = \frac{1}{M}\mathcal{X}\mathcal{X}^T\)&lt;/span&gt;, we would like to solve &lt;span class=&#34;math display&#34;&gt;\[
\max_v\quad \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
s.t.\quad v^Tv=1
\]&lt;/span&gt; This is equivalent to solving &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors. Unfortunately, this cannot be directly solved like in linear PCA since we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. However note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
\Sigma v &amp;amp;= \lambda v \\
v &amp;amp;= \frac{1}{\lambda}\Sigma v
\end{aligned} \\
\begin{aligned}
v &amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M\phi(x^{(i)})[(\phi(x^{(i)})^Tv] \\
&amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M[(\phi(x^{(i)})^Tv]\phi(x^{(i)})
\end{aligned}
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some linear combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; and can be written as &lt;span class=&#34;math display&#34;&gt;\[
v = \mathcal{X}\alpha, \alpha \in \R^N
\]&lt;/span&gt; Substitute this back to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= \lambda\mathcal{X}\alpha \\
\frac{1}{M}\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= \lambda\mathcal{X}^T\mathcal{X}\alpha \\
\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;= M\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(K = \mathcal{X}^T\mathcal{X}\)&lt;/span&gt; which is invertible. Then, &lt;span class=&#34;math display&#34;&gt;\[
KK\alpha = M\lambda K\alpha \\
K\alpha = M\lambda\alpha
\]&lt;/span&gt; We can solve &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvector. Kernel trick can be applied since that &lt;span class=&#34;math inline&#34;&gt;\(K_{ij} = (\mathcal{X}^T\mathcal{X})_{ij} = \phi(x^{(i)})^T\phi(x^{(j)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\lambda\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvalue, which is proportional to &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvalue and thus the objective. So &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s largest &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvalues correspond to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; largest objective respectively. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \alpha_2, ..., \alpha_L\)&lt;/span&gt; has to be solved with constraint that &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i^T\alpha_i = \frac{1}{M\lambda}\)&lt;/span&gt; because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^Tv &amp;amp;= (\mathcal{X}\alpha)^T\mathcal{X}\alpha \\
&amp;amp;= \alpha^T(\mathcal{X}^T\mathcal{X})\alpha \\
&amp;amp;= \alpha^TK\alpha \\
&amp;amp;= M\lambda\alpha^T\alpha \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a sample &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we first transform it with &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and dot-product with &lt;span class=&#34;math inline&#34;&gt;\(v_1,\dots,v_L\)&lt;/span&gt; to get the new coordinates. To get its &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th coordinate, &lt;span class=&#34;math display&#34;&gt;\[
\phi(x^*)^Tv_j = \phi(x^*)^T\mathcal X\alpha_j = [\mathcal K(x^*, x^{(1)}), \mathcal K(x^*, x^{(2)}), \dots, \mathcal K(x^*, x^{(M)})]\alpha_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;centralizing-in-kernel-trick&#34;&gt;Centralizing in Kernel Trick&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde\phi(x^{(i)}) = \phi(x^{(i)}) - \bar\phi(x), \tilde{\mathcal{X}} = [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\)&lt;/span&gt;. We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; is pre-centered by subtracting &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;. However, as said, we didn’t explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, so there is no way to calculate &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt;, let alone &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From another perspective, the objective of pre-centering is to get &lt;span class=&#34;math inline&#34;&gt;\(\tilde K = \tilde{\mathcal{X}}\tilde{\mathcal{X}}^T = \sum_{i=1}^M\tilde\phi(x^{(i)})\tilde\phi(x^{(i)})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{M \times 1}\)&lt;/span&gt; represent a &lt;span class=&#34;math inline&#34;&gt;\(M \times 1\)&lt;/span&gt; column vector with all elements being &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\bar\phi(x) = \frac{1}{M}(\phi(x^{(1)}) + \phi(x^{(2)}) + ... + \phi(x^{(M)})) = \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_M\)&lt;/span&gt; represents a &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt; matrix with all elements being &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt;. Pre-center all data to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde{\mathcal{X}} &amp;amp;= [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\\
&amp;amp;= [\phi(x_1) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, \phi(x^{(2)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, ..., \phi(x^{(M)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}] \\
&amp;amp;= [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})] - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \mathcal{X}\mathbb{1}_M \\
\tilde K &amp;amp;= \tilde{\mathcal{X}}^T\tilde{\mathcal{X}} \\
&amp;amp;= (\mathcal{X} - \mathcal{X}\mathbb{1}_M)^T(\mathcal{X} - \mathcal{X}\mathbb{1}_M) \\
&amp;amp;= (\mathcal{X}(I - \mathbb{1}_M))^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)^T\mathcal{X}^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)K(I - \mathbb{1}_M) \\
\tilde K_{ij} &amp;amp;= (\tilde{\mathcal{X}}^T\tilde{\mathcal{X}})_{ij} \\
&amp;amp;= \tilde\phi(x^{(i)})^T\tilde\phi(x^{(j)})\\
&amp;amp;= (\phi(x^{(i)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1})^T(\phi(x^{(j)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;explained-variance&#34;&gt;Explained Variance&lt;/h3&gt;
&lt;p&gt;It is a question in real application that how many principal components to choose to represent the original data. Explained variance can be a good measure on this. We can choose a number of principal components such that they “explains” a certain percentage &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; of the original data.&lt;/p&gt;
&lt;p&gt;Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}v_i^TXX^Tv_i\)&lt;/span&gt; is the directional variance explained along &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;, or the variance of the first dimension of transformed data samples. We may choose the first &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; principal components such that &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^k v_i^TXX^Tv_i \ge \tau \sum_{i=1}^N \sigma_i^2
\]&lt;/span&gt; ## External Materials&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/59775730&#34;&gt;数据降维: 核主成分分析(Kernel PCA)原理解析&lt;/a&gt; || &lt;a href=&#34;https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained&#34;&gt;Explained variance&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eckart-Young-Mirsky Theorem</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</link>
      <pubDate>Fri, 07 Jan 2022 12:40:50 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</guid>
      <description>
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(R \le \min\{M,N\}\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = diag(\sigma_1, \sigma_2, ..., \sigma_R)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star = U\Sigma_K V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_K = diag(\sigma_1, \sigma_2, ..., \sigma_K)\)&lt;/span&gt;, when distance between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||UA||_F = ||AU||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= tr((UA)^TUA) \\
&amp;amp;= tr(A^TUUA) \\
&amp;amp;= tr(A^TA) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||AU||_F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||U\Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^TU\Sigma V^TV - U^TYV||_F^2 \\
&amp;amp;= ||\Sigma - U^TYV||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^TYV\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is also of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||\Sigma - Z||_F^2 \\
&amp;amp;= \sum_{ij}(\Sigma_{ij} - Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^R(\sigma_{i} - Z_{ii})^2 + \sum_{i&amp;gt;R}^{\min\{M,N\}}Z_{ii}^2 + \sum_{i \ne j}Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \sigma_{i}, i = 1, 2, \dots, K \\
Z_{ii} = \sigma_{i}, i = K, K + 1, \dots, R - 1 \\
Z_{ii} = 0, i = R, R + 1, \dots, \min\{M,N\} \\
Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \dots, N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U\Sigma_KV^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Eckart-Young-Mirsky Theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/measurements/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt; .&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Independent Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</link>
      <pubDate>Sun, 19 Dec 2021 10:16:43 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</guid>
      <description>

&lt;h2 id=&#34;assumption-and-derivation&#34;&gt;Assumption and Derivation&lt;/h2&gt;
&lt;p&gt;Suppose observations are the linear combination of signals. Also suppose that the number of signal sources is equal to the number of linearly mixed channel. Given observations (along the time axis &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{x^{(i)}\}_{i=1}^M, x^{(i)} \in R^{N \times 1}\)&lt;/span&gt;, find the &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; mixing matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X = AS\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are the propagation weights from a signal source to observers. Therefore they are considered independent (and are thus called independent components).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is called mixing matrix and &lt;span class=&#34;math inline&#34;&gt;\(W = A^{-1}\)&lt;/span&gt; is called unmixing matrix because &lt;span class=&#34;math inline&#34;&gt;\(S = WX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = U\Sigma V^T\)&lt;/span&gt; by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. Assume observations are pre-centered:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^Mx^{(i)} = 0
\]&lt;/span&gt; Assume that signals are independent and the variance of each signal is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (taking the advantage of scale/sign ambiguity): &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I
\]&lt;/span&gt; Then the covariance of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
C &amp;amp;= \frac{1}{M}x^{(i)}(x^{(i)})^T \\
&amp;amp;= \frac{1}{M}As^{(i)}(As^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(U\Sigma V^Ts^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(s^{(i)})^TV\Sigma^T U^T \\
&amp;amp;= U\Sigma V^TIV\Sigma^T U^T \\
&amp;amp;= U\Sigma^2U^T \text{ (by property of SVD, note that $\Sigma$ is $n \times n$ diagonal)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If our assumptions are correct, then the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the stacked eigenvectors of the matrix &lt;span class=&#34;math inline&#34;&gt;\(CC^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; is the diagonal matrix consisting of corresponding eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(C^TC\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(CC^T = C^TC = C^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In other words, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be solved by eigen decomposition. Define &lt;span class=&#34;math inline&#34;&gt;\(\hat x^{(i)} = \Sigma^{-1}U^Tx^{(i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat C &amp;amp;= \frac{1}{M}\hat x^{(i)}(\hat x^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(\Sigma^{-1}U^Tx^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^T(\frac{1}{M}x^{(i)}(x^{(i)})^T)U\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^TU\Sigma^2U^TU\Sigma^{-1} \\
&amp;amp;= I \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = WX = A^{-1}X = V\Sigma^{-1}U^TX = V\hat X
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The remaining job is to find the &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. We resort to multi-information, a generalization of mutual-information from Information Theory to judge how close a distribution is to statistical independence for multiple variables. &lt;span class=&#34;math display&#34;&gt;\[
I(s) = \sum_{s \in \mathcal{S}}p(s)log\frac{p(s)}{\prod_jp(s_j)}
\]&lt;/span&gt; It is a non-negative quantity that reaches the minimum if and only if all variables are statistically independent.&lt;/p&gt;
&lt;p&gt;The multi-information can be written as a function of entropy, which is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(s) = -\sum_{s \in \mathcal{S}}p(s)logp(s)
\]&lt;/span&gt; The multi-information can be written as the difference between the sum of entropies of marginal distribution and the entropy of joint distribution &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(s) &amp;amp;= \sum_jH(s_j) - H(s) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(V\hat x) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log|V|) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log1) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(\hat x) \\
\end{aligned}
\]&lt;/span&gt; Because we are assuming signal are independent from each other, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^\star &amp;amp;= \arg \min_V\sum_jH((V\hat x)_j) - H(\hat x) \\
&amp;amp;= \arg \min_V\sum_jH((V\hat x)_j)
\end{aligned}
\]&lt;/span&gt; Calculating entropy and then taking derivative is no easy task and therefore ICA algorithms focus on approximations or equivalences to the above equation. For example, it can be approximated by finding the rotation that maximizes the expected log-likelihood of the observed data by assuming that the source distributions are known.&lt;/p&gt;
&lt;h3 id=&#34;non-gaussian-and-pca&#34;&gt;Non-Gaussian and &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The key assumption of ICA is that signals are non-Gaussian. We are trying to recover the &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (by finding &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;) to be as non-Gaussian as possible. &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; consists of independent components because its covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\hat C\)&lt;/span&gt; is shown to be an identity matrix (and thus diagonal). &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is reconstructed by the linear combination of independent components. Thus it will be the most non-Gaussian when each variable is formed by exactly one component of &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; by Central Limit Theorem, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;’s components are independent.&lt;/p&gt;
&lt;p&gt;Consider the case when &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; consists of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent Gaussian, which invalidate the above analysis. If we forcibly calculate &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;, due to the property of multi-variate Gaussian that its iso-density maps are spherical, then any &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix will be a solution to Equation (9), including the left singular matrix of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. the stacked eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, which is exactly the projection basis obtained in PCA. If any &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix can be a solution, there are infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and thus ICA will just fail.&lt;/p&gt;
&lt;p&gt;The conclusion drawn above is based on the ideal case, i.e. many enough observations. Even in the real case where signals are Gaussian, we may get a unique solution to &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html&#34;&gt;ICA&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>RANSAC</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/ransac/</link>
      <pubDate>Wed, 20 Apr 2022 16:38:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/ransac/</guid>
      <description>
&lt;p&gt;Outliers (noises) in the data can diverge the regression model to reduce prediction errors for them, instead of the majority real data points. &lt;strong&gt;RAN&lt;/strong&gt;dom &lt;strong&gt;SA&lt;/strong&gt;mple &lt;strong&gt;C&lt;/strong&gt;onsensus is a methodology to robustly fit the model in the presence of outliers.&lt;/p&gt;
&lt;p&gt;RANSAC does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Randomly sample a subset of data of an fairly enough amount for training.&lt;/li&gt;
&lt;li&gt;Fit a model to the this subset.&lt;/li&gt;
&lt;li&gt;Determine data points in the whole data set as inliers or outliers by comparing the residuals (prediction errors) to a threshold. The set of inliers is called a consensus set.&lt;/li&gt;
&lt;li&gt;Repeat above for some iterations and retrain the final model with the largest consensus set (since inliers should be the majority).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters of RANSAC include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; - number of points to fit the model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; - threshold of the residual&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; - proportion the outliers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; - probability of success (at least one iteration is finished with no outlier)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; - number of iterations to be determined&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has no outliers)} = (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has at least one outlier)} = 1 - (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T &amp;lt; 1 - \delta \\
T &amp;gt; \log\frac{1 - \delta}{1 - (1 - e)^s}
\end{gather}
\]&lt;/span&gt; The threshold &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is usually set as the median absolute deviation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/xingshansi/p/6763668.html&#34;&gt;随机抽样一致算法（Random sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</link>
      <pubDate>Fri, 07 Jan 2022 13:50:38 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</guid>
      <description>

&lt;h3 id=&#34;two-class&#34;&gt;Two-class&lt;/h3&gt;
&lt;p&gt;Assume we have a set of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-dimensional samples &lt;span class=&#34;math inline&#34;&gt;\(D = \{x^{(i)}\}^M_{i=1}\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; of which belong to Class &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt; to Class &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(M_1 + M_2 = M\)&lt;/span&gt;). We seek to obtain a scalar &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; by projecting &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto a unit vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)} = v^Tx^{(i)}\)&lt;/span&gt;. Of all the possibilities we would like to select the one that maximizes the separability of the scalars between classes.&lt;/p&gt;
&lt;p&gt;The mean of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is &lt;span class=&#34;math display&#34;&gt;\[
\mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} \\
\tilde \mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]v^T x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} = v^T\mu_c\\
\]&lt;/span&gt; The scatter of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is defined as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_c &amp;amp;= \sum_{i=1}^M \mathbb I[y^{(i)} = c](x^{(i)} - \mu_c)(x^{(i)} - \mu_c)^T \\
\tilde s^2_c &amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](z^{(i)} - \tilde u_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)} - u_c) \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} - u_c)^Tv \\
&amp;amp;= v^TS_cv
\end{aligned}
\]&lt;/span&gt; FLD suggests in &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space maximizing the distance among the means of classes (inter-class scatter) and minimizing the variance over each class (within-class scatter), i.e. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_v J(v) &amp;amp;\coloneq \frac{(\tilde \mu_1 - \tilde \mu_2)^2}{\tilde s^2_1 + \tilde s^2_2} \\
&amp;amp;= \frac{(v^T\mu_1 - v^T\mu_2)^2}{v^TS_1v + v^TS_2v} \\
&amp;amp;= \frac{v^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv}{v^T(S_1 + S_2)v} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_W = S_1 + S_2\)&lt;/span&gt; is called within-class scatter, &lt;span class=&#34;math inline&#34;&gt;\(S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T\)&lt;/span&gt; is called inter-class scatter. Then, &lt;span class=&#34;math display&#34;&gt;\[
J(v) = \frac{v^TS_Bv}{v^TS_Wv} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial v} &amp;amp;= v^TS_Wv \frac{\partial v^TS_Bv}{\partial v} - v^TS_Bv \frac{\partial v^TS_Wv}{\partial v}&amp;amp; \\
&amp;amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv  \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv &amp;amp;= 0 \\
S_Bv - \frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv &amp;amp;= 0 \\
S_Bv &amp;amp;= J(v) S_Wv
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_W\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(S^{-1}_WS_B\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_Bv = J(v) v
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_Bv = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv = \alpha(\mu_1 - \mu_2)\)&lt;/span&gt; always points to the same direction as &lt;span class=&#34;math inline&#34;&gt;\((\mu_1 - \mu_2)\)&lt;/span&gt;. Thus we can immediately give a &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and verify: &lt;span class=&#34;math display&#34;&gt;\[
v = S^{-1}_W(\mu_1 - \mu_2)
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_B\underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v = S^{-1}_W\alpha(\mu_1 - \mu_2) = \underbrace{\alpha}_{J(v)} \underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-class&#34;&gt;Multi-class&lt;/h3&gt;


</description>
    </item>
    
    <item>
      <title>Bias-variance Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</guid>
      <description>
&lt;p&gt;The notation used is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Symbol&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Notation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the observation of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;, affected by noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the real value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the mean of the real values&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the model learned with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the expectation of prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l(f(x), y_\mathcal D)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the loss function, chosen to be squared error&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By assuming that the observation errors averages to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, the expectation of the error will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E_{x \sim \mathcal D}&amp;amp;[l(f(x), y_\mathcal D)] \\
&amp;amp;= E[(f(x) - y_\mathcal D)^2] = E\{[(f(x) - \bar f(x) + (\bar f(x) - y_\mathcal D)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y_\mathcal D)^2] + 2E[(f(x) - \bar f(x))(\bar f(x) - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E\{[(\bar f(x) - y) + (y - y_\mathcal D)]^2\} \\
&amp;amp;\quad + 2\underbrace{E[f(x) - \bar f(x)]}_0 E[\bar f(x) - y_\mathcal D] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y)^2] + E[(y - y_\mathcal D)^2] + 2E[(\bar f(x) - y)(y - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;\quad + 2E[\bar f(x) - y] \underbrace{E[y - y_\mathcal D]}_0 \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x) - \bar y)^2] + E[(\bar y - y)^2] + 2E[(\bar f(x) - \bar y)(\bar y - y)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x) - \bar y)^2] + E[(\bar y - y)^2] \\
&amp;amp;\quad + 2E[\bar f(x) - \bar y]\underbrace{E[\bar y - y]}_0 \\
&amp;amp;= \underbrace{E[(f(x) - \bar f(x))^2]}_{variance} + \underbrace{E[(\bar f(x) - \bar y)^2]}_{bias^2} + \underbrace{E[(y - y_\mathcal D)^2]}_{noise} + \underbrace{E[(\bar y - y)^2]}_{scatter} \\
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://medium.com/analytics-vidhya/5-ways-to-achieve-right-balance-of-bias-and-variance-in-ml-model-f734fea6116&#34;&gt;5 ways to achieve right balance of Bias and Variance in ML model | by Niwratti Kasture | Analytics Vidhya | Medium&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/laplace-expansion/</guid>
      <description>&lt;h2 id=&#34;laplace-expansion&#34;&gt;Laplace Expansion&lt;/h2&gt;
&lt;p&gt;Also known as Cofactor Expansion, Laplace Expansion is an expression of an $n \times n$ matrix as the weighted sum of determinants of some $(n-1) \times (n-1)$ sub-matrices.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</guid>
      <description>&lt;h2 id=&#34;convex-conjugate&#34;&gt;Convex Conjugate&lt;/h2&gt;
&lt;p&gt;For a convex function $f$, its &lt;strong&gt;convex conjugate&lt;/strong&gt; $f^&lt;em&gt;$ is defined as
$$
f^&lt;/em&gt;(t) = \sup_x [x^T \cdot t - f(x)]
$$
By definition, a Convex Conjugate pair $(f,f^&lt;em&gt;)$ has the following property:
$$
f(x) + f^&lt;/em&gt;(t) \ge x^T \cdot t
$$
As a conjugate, $f^{&lt;strong&gt;} = f$:
$$
\begin{aligned}
f^{&lt;/strong&gt;}(t) &amp;amp;= \sup_x [x^T \cdot t - f^*(x)] \
&amp;amp;= \sup_x [x^T \cdot t - \sup_y [y^T \cdot x - f(y)]] \
&amp;amp;= \sup_x [x^T \cdot t + \inf_y [f(y) - y^T \cdot x]] \
&amp;amp;= \sup_x \inf_y [x^T (t-y) + f(y)]	\
&amp;amp;\Downarrow_\text{Mini-max Theorem} \
&amp;amp;= \inf_y \sup_x [x^T (t-y) + f(y)]
\end{aligned}
$$
The above reaches the infimum only if $y=t$. Otherwise, $\sup_x [x^T (t-y) + f(y)]$ can make it to infinity. Therefore,
$$
f^{**}(t) = \inf_y \sup_x [f(t)] = f(t)
$$
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_conjugate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wiki&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/188702379&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;凸优化-凸共轭&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid>
      <description>&lt;h2 id=&#34;假设检验&#34;&gt;假设检验&lt;/h2&gt;
&lt;p&gt;样本中包含了总体分布的信息，而**假设检验（hypothesis test）**的含义就是指能否通过样本去判断某个命题（比如关于总体分布的命题）是否成立。在统计上，如果判断其成立，一般称作“接受（accept）该假设”；如果判断其不成立，一般称作“否定（reject）该假设”。这里的“假设”要避免和平常常用的带有“假定”含义的“假设”混淆。&lt;/p&gt;
&lt;p&gt;在假设检验中，有时会将需要判断的命题称作&lt;strong&gt;原假设&lt;/strong&gt;或者&lt;strong&gt;零假设&lt;/strong&gt;，原假设的对立面就称作&lt;strong&gt;对立假设&lt;/strong&gt;。在假设检验中使用的统计量称作&lt;strong&gt;检验统计量&lt;/strong&gt;，使原假设得到接收的那些样本所在的区域称为该检验的&lt;strong&gt;接受域&lt;/strong&gt;，使原假设被否定的那些样本所在的区域称为该检验的&lt;strong&gt;否定域&lt;/strong&gt;。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
