<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/</link>
      <atom:link href="https://chunxy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Chunxy&#39; Website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 16 May 2023 11:42:12 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Chunxy&#39; Website</title>
      <link>https://chunxy.github.io/</link>
    </image>
    
    <item>
      <title>Determinant</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</link>
      <pubDate>Tue, 16 May 2023 11:42:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</guid>
      <description>

&lt;h2 id=&#34;derivation-of-determinant&#34;&gt;Derivation of Determinant&lt;/h2&gt;
&lt;p&gt;Determinant may be the most infamous concept in linear algebra, in
terms of its odd definition and computation. Sometimes, we may wonder
why there has to be a determinant.&lt;/p&gt;
&lt;p&gt;So instead of giving its definition directly, we first lay down some
properties we expect the determinant to have. Then we try to construct
the determinant from the ground up and prove its existence and
uniqueness.&lt;/p&gt;
&lt;p&gt;Determinant in essence captures the volume of the parallelepiped
formed by vectors of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt;
matrix. When &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, it is the length;
when &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, it is the area of
parallelogram; when &lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt;, it is the
volume of parallelepiped. What about when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; goes larger?&lt;/p&gt;
&lt;p&gt;As said, there are some basic properties that we expect the
determinant (the volume) to have, and that indeed hold for cases &lt;span class=&#34;math inline&#34;&gt;\(n = 1,2,3\)&lt;/span&gt;. In the following, let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix and denote &lt;span class=&#34;math inline&#34;&gt;\(\det(\v_1, \dots, \v_n)\)&lt;/span&gt; as the
determinant of the matrix formed by a system of vectors &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;basic-properties&#34;&gt;Basic properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Linearity in each argument: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\alpha \v_k + \beta \u_k}_k, \dots, \v_n)
= \alpha \det(\v_1, \dots, \underset{k}{\v_k}, \dots, \v_n) + \beta
\det(\v_1, \dots, \underset{k}{\u_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anti-symmetry: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underset{j}{\v_j}, \dots, \underset{k}{\v_k}, \dots,
\v_n) = -\det(\v_1, \dots, \underset{j}{\v_k}, \dots,
\underset{k}{\v_j}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normalization: &lt;span class=&#34;math display&#34;&gt;\[
\det(I) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these basic properties, we can derive some advanced properties
for determinant.&lt;/p&gt;
&lt;h3 id=&#34;derived-properties&#34;&gt;Derived properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Preservation under column replacement: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\v_j + \alpha \v_k}_{j}, \dots,
\underset{k}{\v_k}, \dots, \v_n) = \det(\v_1, \dots, \underset{j}{\v_j},
\dots, \underset{k}{\v_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;zero-determinants&#34;&gt;Zero determinants&lt;/h4&gt;
&lt;ol start=&#34;2&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has a zero column, then
&lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has two equal columns,
then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If one column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the
multiple of another, then &lt;span class=&#34;math inline&#34;&gt;\(\det A =
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are linearly
dependent, then &lt;span class=&#34;math inline&#34;&gt;\(\det A =
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;diagonal-matrices-and-triangular-matrices&#34;&gt;Diagonal matrices and
triangular matrices&lt;/h4&gt;
&lt;ol start=&#34;6&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;diagonal matrix&lt;/strong&gt; equal the product
of the diagonal entries.&lt;/li&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;triangular matrix&lt;/strong&gt; equal the
product of the diagonal entries.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;transpose-and-product&#34;&gt;Transpose and product&lt;/h4&gt;
&lt;ol start=&#34;8&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det A^T = \det A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This property implies that, all the statements above about columns,
can be applied to rows. Thus, to compute the determinant of a matrix, we
can apply row operations to transform it to reduced row echelon form
first, and then obtain the result by computing the product of the
diagonal entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det (AB) = (\det A)(\det
B)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;construction&#34;&gt;Construction&lt;/h3&gt;
&lt;p&gt;Now with these properties on hand, how can we find the definition of
the determinant and how can we know that the definition is unique over
these properties?&lt;/p&gt;
&lt;p&gt;Consider an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix
&lt;span class=&#34;math inline&#34;&gt;\(A = \{ a_{jk} \}_{j,k=1}^n\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt; be its columns
and &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\e}{\mathrm{e}} \e_1, \dots,
\e_n\)&lt;/span&gt; be the unit vectors. We have &lt;span class=&#34;math display&#34;&gt;\[
\v_k = a_{1, k} \e_1 + a_{2, k} \e_2 + \dots + a_{n, k} \e_{n} =
\sum_{j=1}^n a_{j, k} \e_j
\]&lt;/span&gt; By the linearity in each argument, expand the first column to
give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \det(\sum_{j=1}^n a_{j, 1} \e_i, \v_2, \dots,
\v_n) = \sum_{j=1}^n a_{j, 1} \det(\e_j, \v_2, \dots, \v_n)
\]&lt;/span&gt; Further expand the second column to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1},
\v_2, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1}, \sum_{j_2=1}^n a_{j_2,
2} e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \sum_{j_2=1}^n a_{j_2, 2}
\det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n \sum_{j_2=1}^n a_{j_1, 1} a_{j_2, 2}
\det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n)
\end{aligned}
\]&lt;/span&gt; Expand the remaining columns to give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n \sum_{j_2=1}^n \dots
\sum_{j_n=1}^n a_{j_1, 1} a_{j_2, 2} \dots a_{j_n, n} \det(\e_{j_1},
e_{j_2}, \dots, \e_{j_n})
\]&lt;/span&gt; This yields &lt;span class=&#34;math inline&#34;&gt;\(n^n\)&lt;/span&gt; terms!
But luckily, many terms are zero, as long as any two of &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots, j_n\)&lt;/span&gt; coincides. To eliminate
zero terms, consider the permutation of &lt;span class=&#34;math inline&#34;&gt;\(\{
1, \dots, n \}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots,
j_n\)&lt;/span&gt; are chosen to be a permutation, &lt;span class=&#34;math inline&#34;&gt;\(\det(\e_{j_1}, e_{j_2}, \dots, \e_{j_n})\)&lt;/span&gt;
is nonzero. We may use a function &lt;span class=&#34;math inline&#34;&gt;\(\sigma: \{
1, \dots, n \} \to \{ 1, \dots, n \}\)&lt;/span&gt; to denote a permutation.
Let the set of all permutations of set &lt;span class=&#34;math inline&#34;&gt;\(\{ 1,
\dots n \}\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Perm}(n)\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{\sigma \in \mathrm{Perm}(n)}
a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n}
\det(e_{\sigma(1)}, e_{\sigma(2)}, \dots, e_{\sigma(n)})
\]&lt;/span&gt; The matrix with columns &lt;span class=&#34;math inline&#34;&gt;\(e_{\sigma(1)}, e_{\sigma(2)}, \dots,
e_{\sigma(n)}\)&lt;/span&gt; can be obtained from identity matrix by finitely
many column exchanges. So &lt;span class=&#34;math inline&#34;&gt;\(\det(e_{\sigma(1)}, e_{\sigma(2)}, \dots,
e_{\sigma(n)})\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or
&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; depending on the number of
column exchanges. We informally define the sign of function &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; if an even number of column exchanges
are needed to permute &lt;span class=&#34;math inline&#34;&gt;\(1, \dots, n\)&lt;/span&gt;
to &lt;span class=&#34;math inline&#34;&gt;\(\sigma(1), \dots, \sigma(n)\)&lt;/span&gt;; and
the signa of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; if the number of exchanges is odd.&lt;/p&gt;
&lt;p&gt;The necessary condition of the definition of determinant requires us
define it like &lt;span class=&#34;math display&#34;&gt;\[
\det A \coloneq \sum_{\sigma \in \mathrm{Perm}(n)} a_{\sigma(1), 1}
a_{\sigma(2), 2} \dots a_{\sigma(n), n} \mathrm{sign}(\sigma)
\]&lt;/span&gt; If we define it in this way, we can verify that it indeed
satisfies the basic properties, concluding the construction of
determinant.&lt;/p&gt;
&lt;h2 id=&#34;cofactor-expansion&#34;&gt;Cofactor Expansion&lt;/h2&gt;
&lt;p&gt;For an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(A_{j,k}\)&lt;/span&gt; denote &lt;span class=&#34;math inline&#34;&gt;\((n-1) \times (n-1)\)&lt;/span&gt; matrix obtained by
crossing out the row &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and column
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The determinant of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be expanded in the row number &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
\det A = a_{j, 1} (-1)^{j+1} \det A_{j, 1} + a_{j, 2} (-1)^{j+2} \det
A_{j, 2} + \dots + a_{j, n} (-1)^{j+n} \det A_{j, n}
\]&lt;/span&gt; The numbers &lt;span class=&#34;math inline&#34;&gt;\(C_{j,k} = (-1)^{j+k}
\det A_{j,k}\)&lt;/span&gt; are called the &lt;strong&gt;cofactors&lt;/strong&gt; of
matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The matrix &lt;span class=&#34;math inline&#34;&gt;\(C = \{ C_{j,k} \}_{j=1,k=1}^n\)&lt;/span&gt; is called
the &lt;strong&gt;cofactor matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an invertible matrix, then &lt;span class=&#34;math display&#34;&gt;\[
A^{-1} = \frac{1}{\det A} C^T
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C^T\)&lt;/span&gt; is sometimes denoted
as &lt;span class=&#34;math inline&#34;&gt;\(A^*\)&lt;/span&gt;, called the &lt;strong&gt;adjugate
matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cramers-rule&#34;&gt;Cramer’s Rule&lt;/h3&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and
an equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt;, there is a
unique solution that &lt;span class=&#34;math display&#34;&gt;\[
x = A^{-1} b = \frac{1}{\det A} C^T b
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; is obtained by
multiplying the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of the
cofactor matrix with &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which is
equivalent to the determinant of the matrix obtained by replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. This property is known as
&lt;strong&gt;Cramer’s rule&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, the
entry &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the solution of the
equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt; is given by the
formula &lt;span class=&#34;math display&#34;&gt;\[
x_k = \frac{\det B_k}{\det A}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt; is obtained by
replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;algebraic-properties&#34;&gt;Algebraic Properties&lt;/h2&gt;
&lt;p&gt;In blockwise matrix multiplication, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det \begin{pmatrix}
A &amp;amp; 0 \\
C &amp;amp; D
\end{pmatrix} &amp;amp;= \det \left( \begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; B \\
C &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; 0 \\
0 &amp;amp; D
\end{pmatrix} \right) \\
\\
&amp;amp;= \det(A) \det(D) \\
\\
\det \begin{pmatrix}
A &amp;amp; B \\
0 &amp;amp; D
\end{pmatrix} &amp;amp;= \det \left( \begin{pmatrix}
I &amp;amp; 0 \\
0 &amp;amp; D
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; B \\
0 &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; I
\end{pmatrix} \right)
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is invertible, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det \begin{pmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{pmatrix}
&amp;amp;= \det \left( \begin{pmatrix}
I &amp;amp; 0 \\
C A^{-1} &amp;amp; I
\end{pmatrix}
\begin{pmatrix}
A &amp;amp; 0 \\
0 &amp;amp; D - C A^{-1} B
\end{pmatrix}
\begin{pmatrix}
I &amp;amp; A^{-1} B \\
0 &amp;amp; I
\end{pmatrix} \right) \\
&amp;amp;= 1 \cdot \det A \det (D - C A^{-1} B) \cdot 1 \\
&amp;amp;= \det A \det (D - C A^{-1} B)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Law of Total Variance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/</link>
      <pubDate>Wed, 05 Apr 2023 09:06:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/law-of-total-variance/</guid>
      <description>

&lt;h2 id=&#34;conditional-expectation&#34;&gt;Conditional Expectation&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be two discrete random variables. The
&lt;strong&gt;conditional probability function of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\Pr(X=x|Y=y) = \frac{\Pr(X=x, Y=y)}{P(Y=y)}
\]&lt;/span&gt; Thus the &lt;strong&gt;conditional expectation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given that &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\E(X|Y=y) \coloneq \sum_x x \Pr(X=x|Y=y)
\]&lt;/span&gt; Clearly the &lt;strong&gt;conditional expectation&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\E(X|Y)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, or put it another way, a random
variable depending on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, instead of
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conditional-variance&#34;&gt;Conditional Variance&lt;/h2&gt;
&lt;p&gt;Conditional variance can be similarly defined. &lt;span class=&#34;math inline&#34;&gt;\(\Var(X|Y=y)\)&lt;/span&gt; is the conditional variance
of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Var(X|Y)\)&lt;/span&gt; is a random variable depending
on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Var(X|Y) \coloneq \E[(X - \mu_{X|Y})^2 | Y] = \E(X^2|Y) - \E(X|Y)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;laws-of-total-expectation-and-variance&#34;&gt;Laws of Total
Expectation and Variance&lt;/h2&gt;
&lt;p&gt;If all the expectations below exist, then for any random variable
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \E_{y \sim p_Y} [\E(X|Y=y)] \quad \textbf{Law of Total
Expectation}
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\Var(X) = \E_{y \sim p_Y} [\Var(X|Y=y)] + \Var_{y \sim p_Y} [\E(X|Y=y)]
\quad \textbf{Law of Total Variance}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>总览</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</link>
      <pubDate>Tue, 08 Nov 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/</guid>
      <description>

&lt;h2 id=&#34;概率论与统计总览&#34;&gt;概率论与统计：总览&lt;/h2&gt;
&lt;p&gt;随机变量其实是一个从事件（概率空间的子集）到数字的映射，是一个具体事件数字化的过程。概率论和统计的一个共同的主要话题就是随机变量，可以说它们像是随机变量的一体两面。&lt;/p&gt;
&lt;p&gt;概率论更加注重随机变量取值集合相对应事件的概率。为此，概率论需要讨论事件所有可能的试验结果、事件的运算、事件的独立性、随机变量的取值范围、随机变量的概率分布等话题。&lt;/p&gt;
&lt;p&gt;统计中的随机变量来自于对总体的随机抽样，这个过程中，我们会得到一组样本，每个样本在被观测之前，都是服从总体分布的随机变量；观测（observation）之后，它们便有了一个具体的观测值（realization）。我们可以将观测行为类比概率论中的试验，而这种观测行为将会导致一个随机变量坍缩成为一个具体的观测值。&lt;/p&gt;
&lt;p&gt;可以这样理解概率论与统计：概率论是根据事件总体的自身属性，正向推导所有事件对应概率分布的分布函数；统计是根据某个概率分布（即总体分布）采样得到的结果，反向推导该分布的属性。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;概率论&lt;/th&gt;
&lt;th&gt;统计&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;概率空间（事件总体）&lt;/td&gt;
&lt;td&gt;样本空间（总体）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;试验&lt;/td&gt;
&lt;td&gt;样本&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;随机变量的数字特征&lt;/td&gt;
&lt;td&gt;样本的数字特征（统计量）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;渐进理论&lt;/td&gt;
&lt;td&gt;统计推段&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;统计推断是统计的终极话题。从任务角度，统计推断主要包括&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/&#34;&gt;参数估计&lt;/a&gt;和&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/&#34;&gt;假设检验&lt;/a&gt;；从方法角度，统计推断分为频率学派和贝叶斯学派。由于笔者习惯采用频率学派视角，故将&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/&#34;&gt;贝叶斯推断&lt;/a&gt;单独抽出，另放它处。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Generating Covariance Matrix</title>
      <link>https://chunxy.github.io/blogs/generating-covariance-matrix/</link>
      <pubDate>Sat, 13 Aug 2022 11:23:21 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/generating-covariance-matrix/</guid>
      <description>

&lt;p&gt;Covariance matrix of a random vector can usually be deduced from the
distribution’s property, or estimated from samples. But how to generate
an arbitrary covariance matrix? How should we populate the entries in a
square matrix so that it makes a legitimate covariance matrix?&lt;/p&gt;
&lt;h2 id=&#34;first-method&#34;&gt;First Method&lt;/h2&gt;
&lt;p&gt;In general, we construct the target covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; by giving its eigenvalues and its
orthonormal eigenvectors (&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Orthogonal Eigenvectors&#34;&gt;any real
symmetric matrix, including the covariance matrix of course, can be
constructed in this way&lt;/a&gt;). A diagonal matrix of eigenvalues (denoted
as &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;) are easy to synthesize. It
remains that how to synthesize a square matrix that has orthonormal
column vectors.&lt;/p&gt;
&lt;p&gt;Let the dimension of the target covariance matrix be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Given an arbitrary square matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, we can decompose &lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;, which is a real symmetric matrix,
into &lt;span class=&#34;math inline&#34;&gt;\(U \Lambda U^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the orthonormal matrix consisting of
&lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is the diagonal matrix populated
with &lt;span class=&#34;math inline&#34;&gt;\(M M^T\)&lt;/span&gt;’s eigenvalues, due to
the property of real symmetric matrix.&lt;/p&gt;
&lt;p&gt;Then we define &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
(M M^T)^{1/2} &amp;amp;\coloneqq U \Lambda^{1/2} U^T \\
(M M^T)^{-1/2} &amp;amp;\coloneqq U \Lambda^{-1/2} U^T \\
\end{align}
\]&lt;/span&gt; We take &lt;span class=&#34;math inline&#34;&gt;\(E = (M M^T)^{-1/2}
M\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; will contain the
orthonormal column vectors as expected. To verify, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;E E^T = \left[ (M M^T)^{-1/2} M \right] \left[ M^T ((M
M^T)^{-1/2})^T \right] \\
&amp;amp;= \left[ (M M^T)^{-1/2} \right] \left[ M M^T \right] \left[ ((M
M^T)^{-1/2})^T \right] \\
&amp;amp;= \left[ U \Lambda^{-1/2} U^T \right] \left[ U \Lambda U^T \right]
\left[ U \Lambda^{-1/2} U^T \right] \\
&amp;amp;= U \Lambda^{-1/2} \underbrace{\left[ U^T U \right]}_{I} \Lambda
\underbrace{\left[ U^T U \right]}_{I} \Lambda^{-1/2} U^T \\
&amp;amp;= U \Lambda^{-1/2} \Lambda \Lambda^{-1/2} U^T = U U^T = I
\end{aligned}
\]&lt;/span&gt; Thus, the targeting covariance matrix can be constructed as
&lt;span class=&#34;math inline&#34;&gt;\(\Sigma = E D E^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;second-method&#34;&gt;Second Method&lt;/h2&gt;
&lt;p&gt;The easiest way to generate a legitimate covariance matrix would be
to arbitrarily synthesize a square matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, and take &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = A A^T\)&lt;/span&gt; (see &lt;a href=&#34;https://stats.stackexchange.com/questions/215497/how-to-create-an-arbitrary-covariance-matrix&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;By doing so, we can obtain a covariance matrix very fast. But you
lose the control over it. Since &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
is totally arbitrary, you can tell little about &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’ s eigenvalues, eigenvectors,
etc.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>椭圆曲线加密算法</title>
      <link>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</link>
      <pubDate>Thu, 23 Jun 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;本文为&lt;a href=&#34;https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/&#34;&gt;Elliptic
Curve
Cryptography&lt;/a&gt;系列文章的翻译，原文深入浅出，非常值得一读。这篇译文刨去了背景知识，相当于是一篇纯纯的学习笔记了。不过由于我本人完全是一个密码学门外汉，专业水平有限，不足之处多多包涵。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#加密算法分支&#34;&gt;加密算法分支&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线&#34;&gt;椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#群group&#34;&gt;群（Group）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积scalar-multiplication&#34;&gt;标量积（Scalar
Multiplication）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看群&#34;&gt;再看群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#标量积与子群&#34;&gt;标量积与子群&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete
Logarithm）&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve
Diffie-Hellman&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve
Digital Signature Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#再看离散对数运算&#34;&gt;再看离散对数运算&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ecc与rsa&#34;&gt;ECC与RSA&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;加密算法分支&#34;&gt;加密算法分支&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于椭圆曲线&lt;/p&gt;
&lt;p&gt;基于椭圆曲线的加密算法包括ECC（Elliptic Curve
Cryptography）、ECDH和ECDSA。ECDH与ECDSA是基于ECC发展而来。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于模余运算&lt;/p&gt;
&lt;p&gt;基于模余运算的加密算法包括RSA、DSA、DH以及其他衍生算法。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;椭圆曲线与群&#34;&gt;椭圆曲线与群&lt;/h2&gt;
&lt;h3 id=&#34;椭圆曲线&#34;&gt;椭圆曲线&lt;/h3&gt;
&lt;p&gt;一条椭圆曲线就是一组满足&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax +
b\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne
0\)&lt;/span&gt;的二维平面点集。&lt;span class=&#34;math inline&#34;&gt;\(4a^3 + 27b^2 \ne
0\)&lt;/span&gt;的条件是为了保证曲线不存在&lt;strong&gt;奇点（singularity）&lt;/strong&gt;；&lt;span class=&#34;math inline&#34;&gt;\(y^2 = x^3 + ax +
b\)&lt;/span&gt;又被称作椭圆曲线的&lt;strong&gt;Weierstrass normal
form&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;除了这条曲线上的点，我们还需要一个无穷远处的点，我们用&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;这个特殊的符号来表示这个点，所以椭圆曲线更准确的表达式为
&lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 椭圆曲线的一条显而易见的性质是，它是关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴对称的。&lt;/p&gt;
&lt;h3 id=&#34;群group&#34;&gt;群（Group）&lt;/h3&gt;
&lt;p&gt;一个集合&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;加上一个二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，若满足以下条件，就构成了数学上的一个&lt;strong&gt;群&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;封闭性（closure）：&lt;span class=&#34;math inline&#34;&gt;\(a \in G, b \in G \to
a \oplus b \in G\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;结合律（associativity）：&lt;span class=&#34;math inline&#34;&gt;\((a + b) + c = a
+ (b + c)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;存在一个单位元（identity element）&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(a + 0
= 0 + a =
a\)&lt;/span&gt;，即单位元与任何元素进行运算，不改变该元素的值；&lt;/li&gt;
&lt;li&gt;每个数都存在一个逆元（inverse）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若该群进一步满足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;交换律（commutativity）：&lt;span class=&#34;math inline&#34;&gt;\(a + b = b +
a\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则称该群为&lt;strong&gt;阿贝尔群（Abelian group）&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;椭圆曲线上的群&#34;&gt;椭圆曲线上的群&lt;/h3&gt;
&lt;p&gt;对于我们定义的椭圆曲线集合，我们&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义无穷远处的&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;为单位元；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义逆元为该点关于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;轴另一侧的对称点；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;如下：&lt;/p&gt;
&lt;p&gt;若一条直线与椭圆曲线的三个交点分别为&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(P
\oplus Q \oplus R =
0\)&lt;/span&gt;，我们称这三个点是&lt;strong&gt;对齐的（aligned）&lt;/strong&gt;。在此处我们没有规定三个点之间的顺序，即三个点之间可以任意交换位置，也就是说我们的定义的二元运算是满足交换律的，我们定义的群是一个阿贝尔群。&lt;/p&gt;
&lt;p&gt;给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q
= (x_q, y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R
= P \oplus Q\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= m^2 - x_P - x_Q \\
y_R &amp;amp;= y_P + m(x_R - x_P) \\
&amp;amp;= y_Q + m(x_R - x_Q)
\end{align}
\]&lt;/span&gt; 其中： &lt;span class=&#34;math display&#34;&gt;\[
m = \begin{cases}
\frac{y_P - y_Q}{x_P - xQ}, &amp;amp; P \ne Q \\
\frac{3x_P^2 + a}{2y_P}, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;标量积scalar-multiplication&#34;&gt;标量积（Scalar
Multiplication）&lt;/h3&gt;
&lt;p&gt;给定之前的二元加法运算，我们可以定义出相应的群中元素与标量之间的乘法运算：
&lt;span class=&#34;math display&#34;&gt;\[
n P  = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 这样的乘法运算可以在&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;时间内完成。&lt;/p&gt;
&lt;h3 id=&#34;对数运算logarithm&#34;&gt;对数运算（Logarithm）&lt;/h3&gt;
&lt;p&gt;给定&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们可以很高效地完成标量积运算&lt;span class=&#34;math inline&#34;&gt;\(Q = nP\)&lt;/span&gt;；但如果给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，我们如何计算出对数运算（虽然这里是除法，但是为了和密码学中的标记保持一致，这里使用了对数）&lt;span class=&#34;math inline&#34;&gt;\(n = Q \div P\)&lt;/span&gt;呢？&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线与有限域&#34;&gt;椭圆曲线与有限域&lt;/h2&gt;
&lt;h3 id=&#34;有限域finite-field&#34;&gt;有限域（Finite Field）&lt;/h3&gt;
&lt;p&gt;有限域首先是一系列元素的集合，比如说由整数模余某个质数&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;得到的集合（通常表示为&lt;span class=&#34;math inline&#34;&gt;\(\Z/p\)&lt;/span&gt;或&lt;span class=&#34;math inline&#34;&gt;\(\newcommand{F}{\mathbb F}
\F_p\)&lt;/span&gt;）；有限域还定义了两种二元运算：加法和乘法，且这两种运算应该满足如下条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在有限域上都是封闭的、满足结合律以及交换律的；&lt;/li&gt;
&lt;li&gt;存在单位元；&lt;/li&gt;
&lt;li&gt;每个元素都存在相应的逆元。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，乘法运算还应该满足分配律（distributive）：&lt;span class=&#34;math inline&#34;&gt;\(x \cdot (y + z) = x \cdot y + x \cdot
z\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;包含了从&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(p-1\)&lt;/span&gt;的所有整数，而加法、乘法操作之后要追加模余（除数为&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;）操作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(a + b = 0 \pmod p\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;加法逆元（additive
inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=-b,
b=-a\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若&lt;span class=&#34;math inline&#34;&gt;\(ab = 1 \pmod o\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;互为&lt;strong&gt;乘法逆元（multiplicative
inverse）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a=b^{-1},b=a^{-1}\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(xy^{-1}\)&lt;/span&gt;有时也表示为&lt;span class=&#34;math inline&#34;&gt;\(x/y\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元可以通过Extended Euclidean
Algorithm，其时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以证明，&lt;span class=&#34;math inline&#34;&gt;\(\F_p\)&lt;/span&gt;也是一个阿贝尔群。&lt;/p&gt;
&lt;h3 id=&#34;有限域上的椭圆曲线&#34;&gt;有限域上的椭圆曲线&lt;/h3&gt;
&lt;p&gt;椭圆曲线本身的定义为： &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \R^2 | y^2 = x^3 + ax + b, 4a^3 + 27b^2 \ne 0\} \cup \{0\}
\]&lt;/span&gt; 加上有限域的限制之后，变为 &lt;span class=&#34;math display&#34;&gt;\[
\{(x,y) \in \F^2 | y^2 = x^3 + ax + b \pmod p, 4a^3 + 27b^2 \ne 0 \pmod
p, a, b \in \F_p \} \cup \{0\}
\]&lt;/span&gt;
由于有限域的限制，此时所有的点全部出现第一象限。该图像关于&lt;span class=&#34;math inline&#34;&gt;\(y = p / 2\)&lt;/span&gt;对称，因为若&lt;span class=&#34;math inline&#34;&gt;\(y_1 + y_2 = p\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_1^2 &amp;amp;= (p - y_2)^2 \\
&amp;amp;= p^2 - 2py_2 + y_2^2 \\
&amp;amp;= y_2^2 \pmod p
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看群&#34;&gt;再看群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于一个点&lt;span class=&#34;math inline&#34;&gt;\(Q = (x_Q,
y_Q)\)&lt;/span&gt;，其逆元&lt;span class=&#34;math inline&#34;&gt;\(-Q\)&lt;/span&gt;定义为&lt;span class=&#34;math inline&#34;&gt;\(-Q = (x_Q, -y_Q \mod p)\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们这样定义有限域上椭圆曲线上的点之间的二元运算&lt;span class=&#34;math inline&#34;&gt;\(\oplus\)&lt;/span&gt;，同之前一样，三个对齐的点（aligned
points）&lt;span class=&#34;math inline&#34;&gt;\(P,Q,R\)&lt;/span&gt;满足 &lt;span class=&#34;math display&#34;&gt;\[
P \oplus Q \oplus R = 0
\]&lt;/span&gt;
只不过这里“对齐”的含义与之前有所不同，之前的对齐指的是几何上的共线，即三个点满足&lt;span class=&#34;math inline&#34;&gt;\(ax + by + c = 0\)&lt;/span&gt;；而这里的对齐指的是：
&lt;span class=&#34;math display&#34;&gt;\[
ax + by + c = 0 \pmod p
\]&lt;/span&gt; 有趣的是，计算加法的公式和之前没有发生太大变化（&lt;a href=&#34;https://arxiv.org/pdf/1710.00214&#34;&gt;证明&lt;/a&gt;）。给定两个非零、非对称的点&lt;span class=&#34;math inline&#34;&gt;\(P = (x_P, y_p), Q = (x_q,
y_Q)\)&lt;/span&gt;，我们可以很轻松地找到&lt;span class=&#34;math inline&#34;&gt;\(R = P
\oplus Q\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_R &amp;amp;= (m^2 - x_P - x_Q) \mod p \\
y_R &amp;amp;= (y_P + m(x_R - x_P)) \mod p \\
&amp;amp;= (y_Q + m(x_R - x_Q)) \mod p
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math display&#34;&gt;\[
m =
\begin{cases}
(y_P - y_R)(x_P - x_R)^{-1} \mod p, &amp;amp; P \ne Q \\
(3x_P^2 + a)(2y_P)^{-1} \mod p, &amp;amp; P = Q
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;群中元素的个数叫做群的&lt;strong&gt;秩（order）&lt;/strong&gt;，可以通过&lt;a href=&#34;https://en.wikipedia.org/wiki/Schoof%27s_algorithm&#34;&gt;Schoof’s
algorithm&lt;/a&gt;计算求得。&lt;/p&gt;
&lt;h3 id=&#34;标量积与子群&#34;&gt;标量积与子群&lt;/h3&gt;
&lt;p&gt;标量积依旧遵循之前的定义，给定正整数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;和群中的点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
nP = \underbrace{P + \dots + P}_{n \text{ times}}
\]&lt;/span&gt; 标量积其实就是对某个点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;不断做加法，其中一个有趣的性质是，&lt;span class=&#34;math inline&#34;&gt;\(0P, 1P, 2P,
\dots\)&lt;/span&gt;的结果会以某个最小正周期周期&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;循环（&lt;a href=&#34;https://en.wikipedia.org/wiki/Subgroup#Basic_properties_of_subgroups&#34;&gt;证明&lt;/a&gt;）。这也就意味着，群中对加法&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;的倍数是关于加法封闭的（closed under
addition），它们又构成了一个循环子群（cyclic subgroup），&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;又称作这个循环子群的&lt;strong&gt;基点（base
point/generator）&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;是这个&lt;strong&gt;循环子群的秩（subgroup
order）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrange%27s_theorem_(group_theory)&#34;&gt;Lagrange’s
theorem&lt;/a&gt;，子群的秩是其父群的秩的约数。&lt;/p&gt;
&lt;h4 id=&#34;寻找基点&#34;&gt;寻找基点&lt;/h4&gt;
&lt;p&gt;在ECC算法中，我们一般会先计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，找出它一个比较大的约数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，让&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;作为子群的秩，&lt;span class=&#34;math inline&#34;&gt;\(h = N /
n\)&lt;/span&gt;称作这个子群的余因子（cofactor），再根据这个子群的秩去找这个子群的基点。一般来说，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;会从&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子中选取，基本算法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算父群的秩&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，从大到小排列进行试验：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;计算余因子&lt;span class=&#34;math inline&#34;&gt;\(h = N /
n\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;随机选择椭圆曲线上的一点&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(G = hP\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，则重新选择&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;进行试验；否则这意味着&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;就是秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的子群的基点。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，ECC算法能够运行的前提是，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;必须是&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;的质因子。&lt;/p&gt;
&lt;h3 id=&#34;离散对数运算discrete-logarithm&#34;&gt;离散对数运算（Discrete
Logarithm）&lt;/h3&gt;
&lt;p&gt;现在我们解答之前提出的对数运算问题，给定&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，目前没有算法能够在多项式时间之内求解满足&lt;span class=&#34;math inline&#34;&gt;\(Q = kP\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。这个问题有点类似于给定整数&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，如何求解满足&lt;span class=&#34;math inline&#34;&gt;\(b = a^k \pmod p\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;？这两个问题目前都没有算法能在多项式时间之内求解，这也是ECC算法安全的根本。&lt;/p&gt;
&lt;h2 id=&#34;椭圆曲线加密算法&#34;&gt;椭圆曲线加密算法&lt;/h2&gt;
&lt;p&gt;寻找到之前秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的子群后，我们就可以生成私钥和公钥了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;私钥是从&lt;span class=&#34;math inline&#34;&gt;\(\{1,\dots,n-1\}\)&lt;/span&gt;中随机抽取的数字&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;公钥是点&lt;span class=&#34;math inline&#34;&gt;\(H = dG\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面介绍两个基于ECC的公钥加密算法。&lt;/p&gt;
&lt;h3 id=&#34;elliptic-curve-diffie-hellman&#34;&gt;Elliptic Curve
Diffie-Hellman&lt;/h3&gt;
&lt;p&gt;ECDH是DH算法在椭圆曲线中的变体，它实际上是一种密钥交换算法，而不是加密算法。它的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice和Bob各自随机生成私钥和公钥：&lt;span class=&#34;math inline&#34;&gt;\(H_A =
d_A G, H_B = d_B G\)&lt;/span&gt;，注意，Alice和Bob使用了相同的基点；&lt;/li&gt;
&lt;li&gt;Alice和Bob在非安全信道上交换各自的公钥，即使中间人拦截到了&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，如果他不能求解出对数运算问题，他也不会知道Alice和Bob的私钥；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_A
H_B\)&lt;/span&gt;，Bob计算&lt;span class=&#34;math inline&#34;&gt;\(S = d_B
H_A\)&lt;/span&gt;，根据子群对加法的封闭性，二者应该得到相同的结果；&lt;/li&gt;
&lt;li&gt;中间人即使知道&lt;span class=&#34;math inline&#34;&gt;\(H_A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(H_B\)&lt;/span&gt;，也无法得到密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，Alice和Bob便可以通过密钥&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;加密内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elliptic-curve-digital-signature-algorithm&#34;&gt;Elliptic Curve
Digital Signature Algorithm&lt;/h3&gt;
&lt;p&gt;ECDSA是一种公钥加密算法，可以用于数字签名。ECDSA的作用对象是消息的哈希值，而不是消息本身，所以在使用ECDSA时，也要选取一个安全的哈希函数。消息的哈希值在签名过程中会被截断，使得该剩余哈希值的比特位数等于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的比特位数，我们用&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;来表示剩余哈希值所代表的整数。ECDSA的大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alice从&lt;span class=&#34;math inline&#34;&gt;\(\{1, \dots,
n\}\)&lt;/span&gt;中随机抽取数字&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(P = kG\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod
n\)&lt;/span&gt;，如果&lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;Alice计算&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + rd_A) \mod
n\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(d_A\)&lt;/span&gt;是Alice的私钥，&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;关于&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的乘法逆元（我们前面选取&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;为质因子的目的就在于，保证这里的&lt;span class=&#34;math inline&#34;&gt;\(k^{-1}\)&lt;/span&gt;一定存在），如果&lt;span class=&#34;math inline&#34;&gt;\(s=0\)&lt;/span&gt;，则重新选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;元组&lt;span class=&#34;math inline&#34;&gt;\((r,s)\)&lt;/span&gt;就是Alice对应的签名。Bob拿到这样的签名之后，作以下验证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_1 = s^{-1}z \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(u_2 = s^{-1}r \mod n\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算点&lt;span class=&#34;math inline&#34;&gt;\(P = u_1G + u_2H_A\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;仅当&lt;span class=&#34;math inline&#34;&gt;\(r = x_P \mod
n\)&lt;/span&gt;时，Bob可以验证这确实是Alice的签名。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;验证过程的正确性证明如下， &lt;span class=&#34;math display&#34;&gt;\[
\label{P} \begin{aligned}
P &amp;amp;= u_1 G + u_2 H_A \\
&amp;amp;= u_1 G + u_2 d_A G \\
&amp;amp;= (s^{-1} z + s^{-1} r d_A) G \\
&amp;amp;= s^{-1}(z + r d_A) G
\end{aligned}
\]&lt;/span&gt; 之前我们定义&lt;span class=&#34;math inline&#34;&gt;\(s = k^{-1} (z + r d_A)
\mod n\)&lt;/span&gt;，将两边同乘&lt;span class=&#34;math inline&#34;&gt;\(ks^{-1}\)&lt;/span&gt;，我们可以得到&lt;span class=&#34;math inline&#34;&gt;\(k = s^{-1}(z + r d_A) \mod
n\)&lt;/span&gt;，将此式代入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{P}\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
P = kG
\]&lt;/span&gt; 这也就是Alice签名过程中得到的&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，证毕。&lt;/p&gt;
&lt;h4 id=&#34;k的选取&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的选取&lt;/h4&gt;
&lt;p&gt;在使用ECDSA时，我们必须注意不能使用相同的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密多份消息，也不能暴露我们选取&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的方式（比如说随机数生成方式），否则就会有很大的私钥泄露风险。比如说我们用同一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;加密两份消息，Bob就可以通过这两次签名过程得到&lt;span class=&#34;math inline&#34;&gt;\((r, s_1), (r,
s_2)\)&lt;/span&gt;，如果Bob还有额外途径获取两次消息的哈希&lt;span class=&#34;math inline&#34;&gt;\(z_1, z_2\)&lt;/span&gt;，那么： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
s_1 = k^{-1}(z_1 + r d_A), s_2 = k^{-1}(z_2 + r d_A) \to \\
(s_1 - s_2) = k^{-1}(z_1 - z_2) \mod n \to \\
k = (z_1 - z_2)(s_1 - s_2)^{-1}
\end{gather}
\]&lt;/span&gt; 再根据&lt;span class=&#34;math inline&#34;&gt;\(s_1 = k^{-1}(z_1 + r d_A)
\mod n\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_A &amp;amp;= r^{-1}(s_1k - z_1) \mod n \\
&amp;amp;= r^{-1}(s_1(z_1 - z_2)(s_1 - s_2)^{-1} - z_1) \mod n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;再看离散对数运算&#34;&gt;再看离散对数运算&lt;/h3&gt;
&lt;p&gt;给定秩为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;、基点为&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的椭圆曲线子群，以及该子群上的两点&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，离散对数运算求解的是满足&lt;span class=&#34;math inline&#34;&gt;\(Q = xP\)&lt;/span&gt;的整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。我们接下来了解两个求解离散对数运算的算法。&lt;/p&gt;
&lt;h4 id=&#34;baby-step-giant-step&#34;&gt;Baby-step-giant-step&lt;/h4&gt;
&lt;p&gt;首先任意一个整数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，都可以写成&lt;span class=&#34;math inline&#34;&gt;\(x = am + b\)&lt;/span&gt;，由&lt;span class=&#34;math inline&#34;&gt;\(a,m,b\)&lt;/span&gt;这三个满足关系的任意整数表示，那么，我们就可以考虑这样解决离散对数运算问题：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q &amp;amp;= xP \\
Q &amp;amp;= (am + b)P \\
Q - amP &amp;amp;= bP
\end{aligned}
\]&lt;/span&gt;
Baby-step-giant-step算法采取了从两边夹逼的方式解决问题，过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(m = \lceil \sqrt n
\rceil\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots,
m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，计算&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，并将&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;存储到哈希表中；&lt;/li&gt;
&lt;li&gt;对所有&lt;span class=&#34;math inline&#34;&gt;\(\{0, \dots,
m\}\)&lt;/span&gt;中的数字&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(Q - amP\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;检查哈希表中是否存在某个&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(Q -
amP = bP\)&lt;/span&gt;，如果存在，就意味着我们找到了一个解。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(bP\)&lt;/span&gt;的计算对应着baby-step，&lt;span class=&#34;math inline&#34;&gt;\(amP\)&lt;/span&gt;的计算对应着giant-step，该算法的合理性在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=0\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(0, P,
\dots, mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(mP, P
+ mp, \dots, mP + mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=2\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\(2mP, P
+ 2mp, \dots, mP + 2mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a=m-1\)&lt;/span&gt;时，我们检查&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;是否和&lt;span class=&#34;math inline&#34;&gt;\((m-1)mP, P + (m-1)mp, \dots, mP +
(m-1)mP\)&lt;/span&gt;相等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，我们检查了&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(m^2P=nP\)&lt;/span&gt;之间的所有点，也就是所有可能的点。而检查的过程我们并不需要做实际的加法运算，只需要检查哈希表中有没有对应的差值。在baby-step中，我们需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加法，在giant-step中，由于哈希表查询速度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;，并且至多需要做&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;次加减法，所以整体上该算法的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;，而哈希表带来的空间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;。尽管看上去这个多项式时间的算法还不错，但是由于一般&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;非常大，这个算法实际需要的时间成本以及存储成本远远超出当前计算机的水平。&lt;/p&gt;
&lt;h4 id=&#34;pollards-rho&#34;&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法的时间复杂度也是&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\sqrt
n)\)&lt;/span&gt;，但是它的空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(1)\)&lt;/span&gt;。和Baby-step-giant-step算法一样，我们实际解决的问题与原问题稍微有所不同，在Pollard’s
&lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中，给定&lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt;，我们想要找到整数&lt;span class=&#34;math inline&#34;&gt;\(a,b,A,B\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
aP + bQ = AP + BQ
\]&lt;/span&gt; 找到这四个整数之后，我们代入&lt;span class=&#34;math inline&#34;&gt;\(Q =
xP\)&lt;/span&gt;来求解&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
aP + bxP &amp;amp;= AP + BxP \\
(a-A)P &amp;amp;= (b-B)xP \\
&amp;amp;\Downarrow \\
(a-A) &amp;amp;= (b-B)x \pmod n \\
x &amp;amp;= (a-A)(b-B)^{-1} \mod n
\end{aligned}
\]&lt;/span&gt; Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法思路是这样的：我们生成一系列伪随机点&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(X_i = a_iP +
b_iQ\)&lt;/span&gt;。这样的序列可以由一个伪随机函数&lt;span class=&#34;math inline&#34;&gt;\(f(X_i) = (a_{i+1},
b_{i+1})\)&lt;/span&gt;生成，也就是说下一点是由当前点决定的，而&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部如何工作并不重要。通过这样的&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;产生序列，我们的序列迟早会出现一个回环，也就是说&lt;span class=&#34;math inline&#34;&gt;\(X_j =
X_i\)&lt;/span&gt;，而这时我们也就能够找到相应的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;。出现回环的原因也很好理解：我们点的个数是有限的，问题其实在于如何找到回环入口。&lt;/p&gt;
&lt;h5 id=&#34;龟兔赛跑&#34;&gt;龟兔赛跑&lt;/h5&gt;
&lt;p&gt;Pollard’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;算法中的回环入口查找，其实类似单向链表中的回环入口查找：在链表开头设置一快一慢两个指针，我们让快指针每次前进两步，慢指针每次前进一步；二者相遇时，从相遇点和起点再设置两个新的慢指针，这两个新的慢指针相遇之处即为环的入口。&lt;/p&gt;
&lt;h4 id=&#34;量子计算shors-algorithm&#34;&gt;量子计算：Shor’s Algorithm&lt;/h4&gt;
&lt;p&gt;理论上，Shor’s Algorithm的时间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron((\log n)^3)\)&lt;/span&gt;，空间复杂度为&lt;span class=&#34;math inline&#34;&gt;\(\Omicron(\log
n)\)&lt;/span&gt;，但是目前的量子计算机还不能进行像Shor’s
Algorithm这样复杂的运算。&lt;/p&gt;
&lt;h3 id=&#34;ecc与rsa&#34;&gt;ECC与RSA&lt;/h3&gt;
&lt;p&gt;RSA的密钥长度在数量级上大于ECC的密钥长度，这不仅意味着更多的内存占用，还意味着更慢的计算速度。这其中的原因在于，RSA算法的离散对数运算是快于ECC算法的离散对数运算（参考&lt;a href=&#34;https://en.wikipedia.org/wiki/General_number_field_sieve&#34;&gt;General
number field
sieve&lt;/a&gt;），这也就意味着RSA算法不得不采用更长的密钥来加大破解难度。更少的内存占用，更快的计算速度，这就是在已经有了成熟的RSA算法的情况下，ECC仍被提出的原因。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/entropy/</link>
      <pubDate>Thu, 28 Apr 2022 22:47:16 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/entropy/</guid>
      <description>

&lt;p&gt;The entropy of discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (probability mass function) is defined
as &lt;span class=&#34;math display&#34;&gt;\[
H(p) = -\mathrm{E}_{x \sim p}\log p(x)
\]&lt;/span&gt; The entropy reaches its maximum when the underlying
distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a uniform
distribution. The maximum value is &lt;span class=&#34;math inline&#34;&gt;\(\log
k\)&lt;/span&gt; if the support is finite and has &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; many distinct values. This can be
derived with the Jenson’s inequality and understood via the level of
chao of a distribution. From an analysis point of view, the entropy is
defined on a &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-tuple whose domain
is compact so that the maxima must exist; we can always find a larger
entropy if any two of &lt;span class=&#34;math inline&#34;&gt;\(p_1,
\dots,p_k\)&lt;/span&gt; are not equal.&lt;/p&gt;
&lt;p&gt;The entropy of continuous distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; (probability density function) is
usually similarly defined as &lt;span class=&#34;math display&#34;&gt;\[
H(q) = -\E_{x\sim q}\log q(x) = -\int q(x)\log q(x)\d x
\]&lt;/span&gt; This is actually the &lt;strong&gt;differential entropy&lt;/strong&gt;
introduced by Shannon. In fact, it is not a good continuous analog of
discrete entropy and it was not rigorously derived. For example, this
formula can be negative. Therefore, in the case of entropy, the random
variable had better be discrete, despite the wide usage of differential
entropy.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;The entropy of a &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional
&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(x) = \frac{e^{-\frac 1 2 (x-\mu)^T
\Sigma^{-1}(x-\mu)}}{\sqrt{|2\pi\Sigma|}}\)&lt;/span&gt; can be derived as
follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
H(p) &amp;amp;\triangleq -\int p(x) \log p(x) \d x = -\int p(x) [-\frac 1 2
(x-\mu)^T \Sigma^{-1} (x-\mu) - \frac 1 2 \log |2\pi\Sigma|] \d x \\
&amp;amp;= \frac 1 2 \int p(x) (x-\mu)^T \Sigma^{-1}(x-\mu) \d x + \frac 1 2
\log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 \int p(x) x^T \Sigma^{-1} x \d x + \frac 1 2 \int p(x)
\mu^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad - \frac 1 2 \int p(x) \mu^T \Sigma^{-1} x \d x - \frac 1 2
\int p(x) x^T \Sigma^{-1} \mu \d x \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= [\frac 1 2 \tr(\Sigma^{-1} \Sigma) + \frac 1 2 \mu^T \Sigma^{-1}
\mu] + \frac 1 2 \mu^T \Sigma^{-1} \mu \\
&amp;amp;\quad - \frac 1 2 \mu^T \Sigma^{-1} \mu - \frac 1 2 \mu^T
\Sigma^{-1} \mu \\
&amp;amp;\quad + \frac 1 2 \log |2\pi\Sigma| \\
&amp;amp;= \frac 1 2 n + \frac 1 2 \log |2\pi\Sigma|
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier</title>
      <link>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</link>
      <pubDate>Fri, 07 Jan 2022 13:00:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/</guid>
      <description>

&lt;h2 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h2&gt;
&lt;p&gt;The standard form of the Lagrange multiplier optimization problem is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\inf f_0(x) \\
s.t.\quad &amp;amp; f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp; h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Denote the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; that satisfies all the requirements in
the above problem as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X \subseteq
\R^n\)&lt;/span&gt;. Then the Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(L:\R^n \times \R^r \times \R^s \mapsto \R\)&lt;/span&gt;
(there is an implicit constraint that the variables must reside in the
natural domain of the functions):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(x, \lambda, \mu) = f_0(x) + \sum_{i=1}^r\lambda_if_i(x) +
\sum_{i=1}^s\mu_ih_i(x)
\]&lt;/span&gt; Define the function &lt;span class=&#34;math inline&#34;&gt;\(P: \mathcal X
\mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda,\mu;\lambda_i \ge 0}L(x,\lambda,\mu)
\]&lt;/span&gt; The primal problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\inf_{x \in \mathcal X} P(x) \label{primal}
\end{equation}
\]&lt;/span&gt; It is easy to have &lt;span class=&#34;math inline&#34;&gt;\(P(x)
=f_0(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt;.
Thus the primal problem is equivalent to the original problem. Denote
primal problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;’s
optimal value as &lt;span class=&#34;math inline&#34;&gt;\(p^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define the dual function &lt;span class=&#34;math inline&#34;&gt;\(D: \R^r \times
\R^s \mapsto \R\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
D(\lambda, \mu) = \inf_{x \in \mathcal \R^n}L(x, \lambda, \mu)
\]&lt;/span&gt; The Lagrangian dual problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\sup_{\lambda,\mu;\lambda_i \ge 0} D(\lambda, \mu) \label{dual}
\end{equation}
\]&lt;/span&gt; Denote the dual problem &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;’s optimal value as &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt;. We always have &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;For any feasible &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathcal X,
(\lambda, \mu) \in {\R^r}^+ \times \R^s\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(x) = \sup\limits_{\lambda&amp;#39;,\mu&amp;#39;;\lambda&amp;#39;_i \ge
0}L(x,\lambda&amp;#39;,\mu&amp;#39;) \ge L(x,\lambda,\mu) \ge \inf_{x&amp;#39; \in
\R^n}L(x&amp;#39;, \lambda, \mu) =D (\lambda, \mu)
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge
d^\star\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p^\star \ge d^\star\)&lt;/span&gt; is called
weak duality because it always holds. &lt;span class=&#34;math inline&#34;&gt;\(p^\star = d^\star\)&lt;/span&gt; is called strong
duality because it does not hold in general. Assume, though, a strong
duality holds, let &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; be the
primal optima, &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star,
\mu^\star)\)&lt;/span&gt; be the dual optima, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
f_0(x^\star) = p^\star = d^\star = D(\lambda^\star, \mu^\star) = \inf_{x
\in \R^n}L(x,\lambda^\star,\mu^\star) \le
L(x^\star,\lambda^\star,\mu^\star) \\
f_0(x^\star) = p^\star = P(x^\star) = \sup\limits_{\lambda,\mu;\lambda_i
\ge 0}L(x^\star,\lambda,\mu) \ge L(x^\star,\lambda^\star,\mu^\star) \\
L(x^\star,\lambda^\star,\mu^\star) \le f_0(x^\star) \le
L(x^\star,\lambda^\star,\mu^\star)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
f_0(x^\star) = P(x^\star) = D(\lambda^\star, \mu^\star) =
L(x^\star,\lambda^\star,\mu^\star)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the strong duality holds and &lt;span class=&#34;math inline&#34;&gt;\((x^\star,\lambda^\star,\mu^\star)\)&lt;/span&gt; is the
optima, they must satisfy the KKT conditions, and we can leverage the
KKT conditions to solve the optima and optimal value:&lt;/p&gt;
&lt;h4 id=&#34;karush-kuhn-tucker-conditions&#34;&gt;Karush-Kuhn-Tucker
Conditions&lt;/h4&gt;
&lt;p&gt;In standard optimization problem, KKT conditions refer to the below
four:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;primal constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(f_1(x) \dots,f_r(x) \le 0, h_1(x),\dots,h_s(x) =
0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;dual constraint&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\dots,\lambda_r \ge 0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;complementary slackness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1f_1(x),\dots,\lambda_rf_r(x) =
0\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;vanishing gradient&lt;/strong&gt; of Lagrangian w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; : &lt;span class=&#34;math display&#34;&gt;\[
\nabla f_0(x) + \sum_{i=1}^r\lambda_i\nabla f_i(x) +
\sum_{i=1}^s\mu_i\nabla h_i(x) = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that solutions satisfying KKT conditions do not imply a strong
duality or an optimal point. For a better discussion between strong
duality and KKT conditions, please go to &lt;a href=&#34;https://math.stackexchange.com/questions/3616646/question-about-kkt-conditions-and-strong-duality&#34;&gt;this
discussion&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;when-to-apply&#34;&gt;When-to-apply&lt;/h3&gt;
&lt;h4 id=&#34;slater-condition&#34;&gt;Slater Condition&lt;/h4&gt;
&lt;p&gt;Strong duality does not hold generally. But it does hold in &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/convex-optimization/#Standard Form&#34;&gt;standard convex optimization
problem&lt;/a&gt;. In such case, KKT conditions are also sufficient for strong
duality provided that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is an
interior point of the feasible region.&lt;/p&gt;
&lt;h4 id=&#34;general-case&#34;&gt;General Case&lt;/h4&gt;
&lt;p&gt;In cases where we cannot tell strong duality directly, we may still
try to apply Lagrangian multiplier to convert the primal problem to the
less-constrained dual problem (&lt;span class=&#34;math inline&#34;&gt;\(\lambda \ge
0\)&lt;/span&gt; is much looser than the constraints in the original problem).
That is, we solve &lt;span class=&#34;math inline&#34;&gt;\(d^\star\)&lt;/span&gt; first,
and then check if &lt;span class=&#34;math inline&#34;&gt;\(d^\star =
p^\star\)&lt;/span&gt;. We do so with the following process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;firstly take derivative of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;
w.r.t. the unconstrained &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and make
it zero (vanishing gradient) to obtain the closed-form expression of
&lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;find &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star \ge 0\)&lt;/span&gt; (dual
constraint) and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt; that
maximizes the &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda, \mu)\)&lt;/span&gt; and
solve &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu^\star\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;finally verify that &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;
satisfies the constraints (primal constraint and the implied
complementary slackness) in the original problem and &lt;span class=&#34;math inline&#34;&gt;\(f_0(x^\star) = d^\star\)&lt;/span&gt; (strong
duality).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we can successfully go through the above process, we can still
solve the problem with Lagrangian multiplier.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bullet Points</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bullet-points/</link>
      <pubDate>Sun, 19 Dec 2021 13:59:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bullet-points/</guid>
      <description>

&lt;h2 id=&#34;bullet-points&#34;&gt;Bullet Points&lt;/h2&gt;
&lt;p&gt;This post lists out various topics under the machine learning
subject.&lt;/p&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;data modalities
&lt;ul&gt;
&lt;li&gt;numbers&lt;/li&gt;
&lt;li&gt;texts&lt;/li&gt;
&lt;li&gt;images&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;audios&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data cleaning&lt;/li&gt;
&lt;li&gt;imbalanced data&lt;/li&gt;
&lt;li&gt;data normalization (one &lt;a href=&#34;https://cs231n.github.io/neural-networks-2/#:~:text=Common%20pitfall.&#34;&gt;pitfall&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;standardization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data augmentation&lt;/li&gt;
&lt;li&gt;data splitting
&lt;ul&gt;
&lt;li&gt;cross validation&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/ransac/&#34;&gt;RANSAC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;feature extraction/engineering
&lt;ul&gt;
&lt;li&gt;domain expertise&lt;/li&gt;
&lt;li&gt;kernel method&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;supervised vs. unsupervised
&lt;ul&gt;
&lt;li&gt;supervised learning
&lt;ul&gt;
&lt;li&gt;classification
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/&#34;&gt;logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/&#34;&gt;support vector machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/&#34;&gt;Bayes classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/&#34;&gt;linear discriminant
analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regression (&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-regression/&#34;&gt;linear&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/&#34;&gt;non-linear&lt;/a&gt; case)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;unsupervised learning
&lt;ul&gt;
&lt;li&gt;discovers inherent properties (latent variables) in the data&lt;/li&gt;
&lt;li&gt;includes
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/clustering/&#34;&gt;clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/&#34;&gt;dimensionality
reduction&lt;/a&gt;/manifold embedding&lt;/li&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;li&gt;latent-variable model
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/&#34;&gt;hidden Markov model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/&#34;&gt;independent component
analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;discriminative vs. generative&lt;/li&gt;
&lt;li&gt;classification vs. regression
&lt;ul&gt;
&lt;li&gt;from classification model to regression model&lt;/li&gt;
&lt;li&gt;from binary classification to multi-class classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;linear vs. non-linear&lt;/li&gt;
&lt;li&gt;parametric vs. non-parametric&lt;/li&gt;
&lt;li&gt;ensemble method
&lt;ul&gt;
&lt;li&gt;bootstrap aggregating
&lt;ul&gt;
&lt;li&gt;random forest&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;gradient boosting
&lt;ul&gt;
&lt;li&gt;least square boosting&lt;/li&gt;
&lt;li&gt;AdaBoost&lt;/li&gt;
&lt;li&gt;LogitBoost&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regularization and overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;training criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean squared error&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;cross entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;impurity
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/entropy/&#34;&gt;entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;testing criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;confusion matrix&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Real\Pred&lt;/th&gt;
&lt;th&gt;Positive&lt;/th&gt;
&lt;th&gt;Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Positive&lt;/td&gt;
&lt;td&gt;TP&lt;/td&gt;
&lt;td&gt;FN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Negative&lt;/td&gt;
&lt;td&gt;FP&lt;/td&gt;
&lt;td&gt;TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;F-score&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;precision-recall curve&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;receiver operation curve&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/&#34;&gt;mean average
precision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Machine Learning Bullet Points</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</link>
      <pubDate>Sun, 19 Dec 2021 13:59:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/machine-learning-bullet-points/</guid>
      <description>

&lt;p&gt;This post lists out various topics under the machine learning
subject.&lt;/p&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;data modalities
&lt;ul&gt;
&lt;li&gt;numbers&lt;/li&gt;
&lt;li&gt;texts&lt;/li&gt;
&lt;li&gt;images&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;audios&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data sampling
&lt;ul&gt;
&lt;li&gt;bootstrap aggregation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;data cleaning&lt;/li&gt;
&lt;li&gt;imbalanced data&lt;/li&gt;
&lt;li&gt;data augmentation&lt;/li&gt;
&lt;li&gt;data splitting&lt;/li&gt;
&lt;li&gt;feature extraction/engineering
&lt;ul&gt;
&lt;li&gt;domain expertise&lt;/li&gt;
&lt;li&gt;statistics&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;feature normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;supervised vs. unsupervised
&lt;ul&gt;
&lt;li&gt;unsupervised learning
&lt;ul&gt;
&lt;li&gt;discovers inherent properties (latent variables) in the data&lt;/li&gt;
&lt;li&gt;includes:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/clustering/&#34;&gt;clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/&#34;&gt;dimensionality
reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;manifold embedding&lt;/li&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;discriminative vs. generative&lt;/li&gt;
&lt;li&gt;classification vs. regression
&lt;ul&gt;
&lt;li&gt;from classification model to regression model&lt;/li&gt;
&lt;li&gt;from binary classification to multi-class classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;linear vs. non-linear&lt;/li&gt;
&lt;li&gt;parametric vs. non-parametric&lt;/li&gt;
&lt;li&gt;boosting method
&lt;ul&gt;
&lt;li&gt;ensemble method&lt;/li&gt;
&lt;li&gt;Adaboost&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;regularization and overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;training criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean squared error&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;cross entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;impurity
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/entropy/&#34;&gt;entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;testing criterion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;confusion matrix&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Real/Pred&lt;/th&gt;
&lt;th&gt;Positive&lt;/th&gt;
&lt;th&gt;Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Positive&lt;/td&gt;
&lt;td&gt;TP&lt;/td&gt;
&lt;td&gt;FN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Negative&lt;/td&gt;
&lt;td&gt;FP&lt;/td&gt;
&lt;td&gt;TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;F-score&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate System and Change of Basis</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</link>
      <pubDate>Sat, 18 Dec 2021 20:47:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/</guid>
      <description>
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} = \{b_1, b_2, ...,
b_n\}\)&lt;/span&gt; be a basis for a vector space &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Then for &lt;span class=&#34;math inline&#34;&gt;\(x
= [x_1, x_2, ..., x_n]^T\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, there exists a unique set of scalars
&lt;span class=&#34;math inline&#34;&gt;\(q_1, q_2, ..., p_n\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
x = q_1b_1 + q_2b_2 + ... + q_nb_n
\]&lt;/span&gt; These scalars are called the coordinates of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{B} =
\begin{bmatrix}
q_1
\cdots
q_n
\end{bmatrix}^T
\]&lt;/span&gt; is the coordinate vectors of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;. The mapping &lt;span class=&#34;math inline&#34;&gt;\(x \mapsto [x]_\mathcal{B}\)&lt;/span&gt; is called the
coordinate mapping determined by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{B} = [b_1, b_2, ...,
b_n]\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x =
P_\mathcal{B}[x]_\mathcal{B}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt; both be a basis for an
n-dimensional vector space &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;.
Then there is a unique &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt;
matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C
\leftarrow B}\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[
[x]_\mathcal{C} = \mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B}
\]&lt;/span&gt; The columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C \leftarrow
B}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;-coordinate vectors of the
vectors in the basis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\mathop{P}\limits_\mathcal{C \leftarrow B} = [[b_1]_\mathcal{C},
[b_2]_\mathcal{C}, ..., [b_n]_\mathcal{C}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B} =  P_\mathcal{C}\mathop{P}\limits_\mathcal{C
\leftarrow B}
\begin{bmatrix}
q_1
\cdots
q_n
\end{bmatrix}^T \\
&amp;amp;= P_\mathcal{C}(q_1[b_1]_\mathcal{C} + q_2[b_2]_\mathcal{C} + ... +
q_n[b_n]_\mathcal{C}) \\
&amp;amp;= q_1P_\mathcal{C}[b_1]_\mathcal{C} +
q_2P_\mathcal{C}[b_2]_\mathcal{C} + ... +
q_nP_\mathcal{C}[b_n]_\mathcal{C} \\
&amp;amp;= q_1b_1 + q_2b_2 + ... + q_nb_n \\
&amp;amp;= x
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B} =
\mathcal{I}\)&lt;/span&gt; is the standard basis, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;= [[e_1]_\mathcal{C},
[e_2]_\mathcal{C}, ..., [e_n]_\mathcal{C}] \\
P_\mathcal{C}\mathop{P}\limits_\mathcal{C \leftarrow I} &amp;amp;=
P_\mathcal{C}[[e_1]_\mathcal{C}, [e_2]_\mathcal{C}, ...,
[e_n]_\mathcal{C}] \\
&amp;amp;= [e_1, e_2, ..., e_n] \\
&amp;amp;= I
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(P_\mathcal{C}\)&lt;/span&gt; is
invertible, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{C
\leftarrow I} = P_\mathcal{C}^{-1}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\([x]_\mathcal{B} = P_\mathcal{B}^{-1}x\)&lt;/span&gt; in
equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \\
[x]_\mathcal{C} &amp;amp;= \mathop{P}\limits_\mathcal{C \leftarrow
B}[x]_\mathcal{B} \\
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1}[x]_\mathcal{C} &amp;amp;=
(\mathop{P}\limits_\mathcal{C \leftarrow
B})^{-1}\mathop{P}\limits_\mathcal{C \leftarrow B}[x]_\mathcal{B} \\
[x]_\mathcal{B} &amp;amp;= (\mathop{P}\limits_\mathcal{C \leftarrow
B})^{-1}[x]_\mathcal{C}  \\
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(\mathop{P}\limits_\mathcal{B \leftarrow C} =
(\mathop{P}\limits_\mathcal{C \leftarrow B})^{-1},
\mathop{P}\limits_\mathcal{B \leftarrow C}\mathop{P}\limits_\mathcal{C
\leftarrow B} = I\)&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>单源最短路径问题</title>
      <link>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 21 Jun 2021 09:33:12 +0000</pubDate>
      <guid>https://chunxy.github.io/blogs/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;其实各种算法问题，在《算法导论》中已经有了很精确的定义以及严谨的论证了。但是我个人认为，真正理解一个算法，除了严谨的符号运算之外，还要有一些粗颗粒的认知作为引子，从而能够在必要的时间串起整个论证过程。所以我写下这篇博客，也是对自己认知的检验，如果有幸能被更多人看到，那自然再好不过。&lt;/p&gt;
&lt;p&gt;当然，减少严谨的符号运算，并不意味着完全不出现符号，因为算法本身就是对问题的抽象，剥掉这层抽象，就没办法进行架构在抽象之上的信息传递了。&lt;/p&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#问题描述及定义&#34;&gt;问题描述及定义&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决思路&#34;&gt;解决思路&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#正文之前&#34;&gt;正文之前&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#负权边&#34;&gt;负权边&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#负权环路&#34;&gt;负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#非负权环路&#34;&gt;非负权环路&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#放缩操作&#34;&gt;放缩操作&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#解决方法&#34;&gt;解决方法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#dijkstra算法&#34;&gt;Dijkstra算法&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#对比&#34;&gt;对比&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;p&gt;&lt;img src=&#34;sssp.png&#34;/&gt;&lt;/p&gt;
&lt;h2 id=&#34;问题描述及定义&#34;&gt;问题描述及定义&lt;/h2&gt;
&lt;p&gt;单源最短路径问题，旨在求解&lt;strong&gt;带权有向图&lt;/strong&gt;（weighted
directed graph）中&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34; role=&#34;doc-noteref&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;，从某个&lt;strong&gt;点&lt;/strong&gt;（vertex）出发，到图中任意一&lt;strong&gt;点&lt;/strong&gt;的最短距离，某些情况下，还需要找出这一条最短距离的&lt;strong&gt;路径&lt;/strong&gt;，称之为&lt;strong&gt;最短路径&lt;/strong&gt;，若无特殊指明且不致歧义，以下最短路径问题均指代单源最短路径问题。&lt;/p&gt;
&lt;p&gt;更严格一些，设&lt;span class=&#34;math inline&#34;&gt;\(G(V,
E)\)&lt;/span&gt;表示带权有向图，&lt;span class=&#34;math inline&#34;&gt;\(w : E \to
\mathbb{R}\)&lt;/span&gt;表示&lt;strong&gt;权重&lt;/strong&gt;，&lt;strong&gt;路径&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(p = \left &amp;lt;v_0, v_1, ... v_k\right
&amp;gt;\)&lt;/span&gt;的&lt;strong&gt;距离&lt;/strong&gt;定义为： &lt;span class=&#34;math display&#34;&gt;\[
W(p) = \sum\limits_{i = 1}^k w(v_{k-1}, v_k)
\]&lt;/span&gt; 其中， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\forall i \in [0, k], v_i \in V \\
\forall i \in [1, k], (v_{i-1}, v_i) \in E
\end{gather*}
\]&lt;/span&gt; 我们从某一点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;（叫作&lt;strong&gt;起点&lt;/strong&gt;，或者源点）出发，记其到图中任意一&lt;strong&gt;点&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径距离为&lt;span class=&#34;math inline&#34;&gt;\(\delta(v)\)&lt;/span&gt;，&lt;strong&gt;单源最短路径&lt;/strong&gt;求解的就是任意一条从&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(W(p) =
\delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;为了方便，我们为每一个点&lt;span class=&#34;math inline&#34;&gt;\(v \in
V\)&lt;/span&gt;设立一个中间变量&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;，用&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;来表示求解过程中的最短距离的&lt;strong&gt;可行上界&lt;/strong&gt;，也就是说始终有&lt;span class=&#34;math inline&#34;&gt;\(\delta(v) \leq v.d\)&lt;/span&gt;，算法初始化时，&lt;span class=&#34;math inline&#34;&gt;\(v.d =
+\infty\)&lt;/span&gt;，算法运行过程中，我们通过寻找路径使&lt;span class=&#34;math inline&#34;&gt;\(v.d\)&lt;/span&gt;这个上界不断减小，直到&lt;span class=&#34;math inline&#34;&gt;\(v.d = \delta(v)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;解决思路&#34;&gt;解决思路&lt;/h2&gt;
&lt;p&gt;最短路径问题（包括多源最短路径问题）都隐含着一个最优子结构（optimal
substructure），即：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;是一条连接两个点的最短路径，那么&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的任意一条&lt;strong&gt;子路径&lt;/strong&gt;，一定也是连接其两个端点的最短路径。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这条性质可由反证法轻松得到，也将是后续寻找最短路径所需要理解的一个重要概念。&lt;/p&gt;
&lt;h2 id=&#34;正文之前&#34;&gt;正文之前&lt;/h2&gt;
&lt;h3 id=&#34;负权边&#34;&gt;负权边&lt;/h3&gt;
&lt;p&gt;负权边指的是图中某些边的权重为负。虽然负权边不会对最短路径的最优子结构性质产生任何影响，但是后面我们会看到，负权边会导致Dijkstra算法失效。&lt;/p&gt;
&lt;h3 id=&#34;负权环路&#34;&gt;负权环路&lt;/h3&gt;
&lt;p&gt;负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;（loop），并且这条环路上的所有边的权重相加结果为负。&lt;/p&gt;
&lt;p&gt;一旦从起点可以到达这个环路上的点，那么最短路径问题就变得没有意义了：我们可以不断重复地走这条环路，然后
“拐出”
环路，到达目标点，使得到达目标点的路径的权重变得任意小（arbitrarily
short），所以也就不存在什么 “最短路径” 了。&lt;/p&gt;
&lt;p&gt;一个成熟的算法应当能够检测出图中是否有可以由&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;到达的负权环路，如果没有，则算法照常进行；如果有，应予以通报。&lt;/p&gt;
&lt;h3 id=&#34;非负权环路&#34;&gt;非负权环路&lt;/h3&gt;
&lt;p&gt;非负权环路指的是图中某些边构成一条&lt;strong&gt;环路&lt;/strong&gt;，并且这条环路上的所有边的权重相加的结果大于等于0。&lt;/p&gt;
&lt;p&gt;负权环路会使最短路径问题没有意义，那么非负权环路呢？或者说，最短路径是否包含非负权环路呢？&lt;/p&gt;
&lt;p&gt;答案是否，如果一条最短路径包含了非负权环路，我们大可将这段环路从路径中
“拿掉”，得到的路径和原路径可以达到同样的终点，并且新路径的权重不大于原路径的权重。&lt;/p&gt;
&lt;h3 id=&#34;放缩操作&#34;&gt;放缩操作&lt;/h3&gt;
&lt;p&gt;放缩操作的对象是边，对于边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;，放缩操作将检测能否优化点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界：&lt;/p&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;RELAX(u, v, w):
if v.d &amp;gt; u.d + w(u, v):
    v.d = u.d + w(u, v)
    v.predecessor = u&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即如果路径&lt;span class=&#34;math inline&#34;&gt;\(s \sim u \to
v\)&lt;/span&gt;的长度小于当前&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，我们便可以借此优化&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的上界，并同时通过将的&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;前继设为&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;来记录这一次优化。&lt;/p&gt;
&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;
&lt;h3 id=&#34;bellman-ford算法&#34;&gt;Bellman-Ford算法&lt;/h3&gt;
&lt;p&gt;Bellman-Ford算法是最短路径问题中最为robust的一种了，能处理负权边、能检测负权环路、不要求当前图为有向无环图（directed
acyclic graph）。Bellman-Ford算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原图中存在可由起点抵达的负权环路，返回
false，用以告知存在负权环路，最短路径问题无意义&lt;/li&gt;
&lt;li&gt;原图中不存在可由起点抵达的负权环路，返回
true，用以告知最短路径问题已解决，并将结果蕴含在相应的数据结构中&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;// 算法主体
for i = 1 to |V| - 1
    for each edge (u, v) in E
        RELAX(u, v, w)

// 检测是否存在负权回路
for each edge (u, v) in E
    if v.d &amp;gt; u.d + w(u, v)
        return false

return true&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;算法主体&lt;/p&gt;
&lt;p&gt;我们不妨先假设原图中不存在负权环路，先思考Bellman-Ford在解决最短路径问题时的正确性。&lt;/p&gt;
&lt;p&gt;根据以上的讨论，任意一点的最短路径中不存在环，故任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径最多由&lt;span class=&#34;math inline&#34;&gt;\(|V|-1\)&lt;/span&gt;条边，&lt;span class=&#34;math inline&#34;&gt;\(|V|\)&lt;/span&gt;个点构成。设： &lt;span class=&#34;math display&#34;&gt;\[
p=\left &amp;lt;v_0,v_1, ... v_k\right &amp;gt;，其中v_0 = s，v_k = t
\]&lt;/span&gt; 在寻找&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径时，任一&lt;span class=&#34;math inline&#34;&gt;\(v \in \{u | (u,t) \in E, u.d = \delta(s,
u)\}\)&lt;/span&gt;（即此时&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的最短路径已找到，且点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;有一条连向点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的边） ，都是&lt;span class=&#34;math inline&#34;&gt;\(v_{k-1}\)&lt;/span&gt;（也就是&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在其最短路径中的前继）的一个候选，我们需要证明的是，&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的实际前继能够在Bellman-Ford算法运行之下被发现，从而被真正地选为&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的前继。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bellman.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;根据前面的讨论，路径&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的前缀&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right &amp;gt;, \left &amp;lt; v_0,
v_1, v_2\right &amp;gt;...\)&lt;/span&gt;分别是&lt;span class=&#34;math inline&#34;&gt;\(v_1,
v_2, ...\)&lt;/span&gt;的最短路径，在外侧第一轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_0, v_1)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0,
v_1\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_1.d =
\delta(v_1)\)&lt;/span&gt;且将不再变化（因为这已经是最小）；在外侧第二轮for-loop后，边&lt;span class=&#34;math inline&#34;&gt;\((v_1, v_2)\)&lt;/span&gt;一定会被放缩，而由于&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1,
v_2\right&amp;gt;\)&lt;/span&gt;实际是最短路径，故放缩之后，&lt;span class=&#34;math inline&#34;&gt;\(v_2.d =
\delta(v_2)\)&lt;/span&gt;且将不再变化（因为这已经是最小）&lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ...
v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;负权回路检测&lt;/p&gt;
&lt;p&gt;至于负权回路检测部分的正确性，则不得不引入一些公式，但其实并不复杂。&lt;/p&gt;
&lt;p&gt;假设原图存在可由到达的负权回路&lt;span class=&#34;math inline&#34;&gt;\(c = \left
&amp;lt;v_0, v_1, ... v_k\right &amp;gt;, v_0 = v_k\)&lt;/span&gt;，其中，&lt;span class=&#34;math inline&#34;&gt;\(W(c) = \sum_{i=0}^{k-1}w(v_i, v_{i+1}) &amp;lt;
0\)&lt;/span&gt;。运用反证法，即假设最终不存在点&lt;span class=&#34;math inline&#34;&gt;\(u,v\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(v.d
&amp;gt; u.d + w(u, v)\)&lt;/span&gt;，则有： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_i.d &amp;amp;\leq v_{i-1}.d + w(v_{i-1}, v_i), \forall i \in [1, k]
\Rightarrow  \\
\sum_{i=1}^k v_i.d &amp;amp;\leq \sum_{i=1}^k (v_{i-1}.d + w(v_{i-1},v_i))
\\
v_k.d + \sum_{i=1}^{k-1} v_i.d &amp;amp;\leq v_0.d + \sum_{i=1}^{k-1}
v_{i}.d + \sum_{i=1}^k w(v_{i-1},v_i) \\
0 &amp;amp;\leq \sum_{i=1}^k w(v_{i-1},v_i) = W(c)
\end{aligned}
\]&lt;/span&gt; 与&lt;span class=&#34;math inline&#34;&gt;\(W(c) &amp;lt;
0\)&lt;/span&gt;矛盾，故得证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dijkstra算法&#34;&gt;Dijkstra算法&lt;/h3&gt;
&lt;p&gt;Dijkstra算法相对于Bellman-Ford算法来说，可以在时间复杂度上有所优化，但是能够处理的情形也就少了一些：Dijkstra算法不能处理负权边（所以更不用提负权环路了）。Dijkstra算法基本框架如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;维持一个点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;由最短路径已确定的点构成；&lt;/li&gt;
&lt;li&gt;不断向中加入能够确定最短路径的点，直到所有中的点都被加入。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;pseudocode&#34;&gt;&lt;code&gt;S = {}
Q = G.V
while Q is not empty
    u = EXTRACT-MIN(Q)
    add u to S
    for each v in G.adj[u]
        RELAX(u, v, w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然，难点在于如何根据&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;找出能够确定最短路径的点。寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dijkstra.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in S\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\((u, t) \in E\)&lt;/span&gt;，对&lt;span class=&#34;math inline&#34;&gt;\((u, t)\)&lt;/span&gt;进行放缩后得到的值&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;，都是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的一个备选，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中每加入一个点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;（非&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的点），若边&lt;span class=&#34;math inline&#34;&gt;\((v,t)\)&lt;/span&gt;存在，对该边进行放缩后，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选（也就是放缩后的&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;）就会多一个，而&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;自然是这些备选中最小的那个。而当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径确定后，便可以将&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;不断扩展，直至最终包含整个点集&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;，也就是所有点的最短路径都被找到。&lt;/p&gt;
&lt;p&gt;之前提到过，Dijkstra算法不能处理负权边的情况，但上述
Dijkstra算法的讨论中似乎也没有涉及到负权边，为什么它就不能处理了呢？并且，我们只知道放缩后&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的是&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;的备选，那么对&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;的放缩要进行到什么时候，才能确认&lt;span class=&#34;math inline&#34;&gt;\(t.d=\delta(t)\)&lt;/span&gt;呢？Dijkstra算法告诉我们，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(t.d \leq u.d\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(\delta(t) = t.d\)&lt;/span&gt;，也就是当&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界小于所有待确认点&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;的上界时，我们就能确定&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径，也就能够将点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;加入到&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中。&lt;/p&gt;
&lt;p&gt;为什么？如果没有负权边，我们可以会发现，&lt;span class=&#34;math inline&#34;&gt;\(\forall u \in V - S\)&lt;/span&gt;，其上界&lt;span class=&#34;math inline&#34;&gt;\(u.d\)&lt;/span&gt;总是由放缩操作得到的，所以在算法运行过程中，它必然是单调递减的，而且它代表了一条具体的到达&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的路径。但为什么&lt;span class=&#34;math inline&#34;&gt;\(V -
S\)&lt;/span&gt;中的所有点的上界的最小值，却能够成为某个特定点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径呢？&lt;/p&gt;
&lt;p&gt;我们来看看&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为最小上界的之前之后都发生了什么，换言之，在此之前，或者在此之后，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;有没有可能更小？之前是不会更小了，因为我们说过，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;是单调递减的；那么之后呢？如果在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的上界成为&lt;span class=&#34;math inline&#34;&gt;\((V-S)\)&lt;/span&gt;中的最小上界、从而被加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;之后，我们在新的某一轮中选取另外一个点&lt;span class=&#34;math inline&#34;&gt;\(u \in V - S\)&lt;/span&gt;，作为此轮加入&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的点，在随后的操作中&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;会不会因为某个由&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;出发的放缩操作继续减小呢？&lt;/p&gt;
&lt;p&gt;不会的，对于某个新加入的点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\delta(u)\)&lt;/span&gt;必然不小于任何一个&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点的最短距离。运用数学归纳法，假设某一时刻&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;按加入顺序排列为&lt;span class=&#34;math inline&#34;&gt;\(\{ u_0, \dots, u_k \}\)&lt;/span&gt;，且有&lt;span class=&#34;math inline&#34;&gt;\(u_0.d \le \dots \le u_k.d\)&lt;/span&gt;。若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;“引荐”进来（也就是说进行了&lt;span class=&#34;math inline&#34;&gt;\(RELAX(u_k, u, w)\)&lt;/span&gt;操作）的，则必有&lt;span class=&#34;math inline&#34;&gt;\(u.d = u_k.d + w(u_k, u) \ge
u_k.d\)&lt;/span&gt;；而若&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;是由非&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;的某个&lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;引荐而来，则也必然应该有&lt;span class=&#34;math inline&#34;&gt;\(u.d \ge u_k.d\)&lt;/span&gt;，运用反证法：如果&lt;span class=&#34;math inline&#34;&gt;\(u.d &amp;lt; u_k.d\)&lt;/span&gt;，则在我们的算法中，&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;至少应该在选择&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;加入的一轮中，因优于&lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt;被加入，和&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;的当前情况矛盾，从而得证。&lt;/p&gt;
&lt;p&gt;而如果有负权边，则不能保证在后续加入的点的最短距离单调递增，故不能用以上论证来证明Dijkstra算法的正确性了。何况，这种情况下，强行使用Dijkstra算法，很轻易地就能举出反例来证明结果的错误，比如对下图以&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;为起点应用Dijkstra算法，就会得到错误结果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;counterexample.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;当然我们也得证明，每一轮加入新点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(t.d =
\delta(t)\)&lt;/span&gt;，因为此时虽然&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;达到整个算法流程中的最小，但这个最小值尚未被证明等于&lt;span class=&#34;math inline&#34;&gt;\(\delta(t)\)&lt;/span&gt;。但正如前面所说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;寻找到一个点的最短路径的必要条件是：在对到达这个点的最短路径中的前继节点进行放缩操作时，该前继节点的最短路径已经确定，而点集&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，正是一个最短路径已经确定的点的集合。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们可以使用数学归纳法，来证明&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;始终是一个最短路径已经确定的点的集合，也就是说，假设此时&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;是一个最短路径已经确定的点的集合，加入&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;后，&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;依然保持它的性质。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;前继的候选无非就是图中所有点，我们已经证明，&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;不可能再由于&lt;span class=&#34;math inline&#34;&gt;\(V-S\)&lt;/span&gt;中的点放缩而变小了，所以&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;在最短路径中的前继只可能来自于&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正是由&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;中的点放缩而来，故&lt;span class=&#34;math inline&#34;&gt;\(t.d\)&lt;/span&gt;正式所有可能中最小的那一个，证毕。&lt;/p&gt;
&lt;h3 id=&#34;有向无环图中的最短路径&#34;&gt;有向无环图中的最短路径&lt;/h3&gt;
&lt;p&gt;有向无环图，从定义上就排除了负权环路存在的可能，故所有点的最短路径必然存在，问题就在于如何寻找到这些最短路径。&lt;/p&gt;
&lt;p&gt;我们当不能对其直接应用Dijkstra算法，因为有向无环图并不排除负权边存在的可能——那就直接用Bellman-Ford算法咯？&lt;/p&gt;
&lt;p&gt;也不尽然。有向无环图显然只是Bellman-Ford算法能够处理的情况中的一小部分，并且这一小部分具有一些特殊的性质：无环。设任意一点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的最短路径为&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1, ... v_k\right
&amp;gt;\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(v_0 = s，v_k =
t\)&lt;/span&gt;，既然不存在环路，从起点&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;出发，只要沿着边走，一步一步放缩，必然是先放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left &amp;lt;v_0, v_1\right
&amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_1.d =
\delta(v_1)\)&lt;/span&gt;；然后放缩边&lt;span class=&#34;math inline&#34;&gt;\(\left
&amp;lt;v_1, v_2\right &amp;gt;\)&lt;/span&gt;并由此得到&lt;span class=&#34;math inline&#34;&gt;\(v_2.d = \delta(v_2)\)&lt;/span&gt;……如此放缩&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;轮后，我们便按顺序寻找到了&lt;span class=&#34;math inline&#34;&gt;\(v_1, ...
v_k\)&lt;/span&gt;一众节点的最短路径以及其最短路径的前继。&lt;/p&gt;
&lt;p&gt;“沿着边走”
有一个专业名词，叫做按&lt;strong&gt;拓扑顺序&lt;/strong&gt;遍历。事实上，我们放缩过边&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},
v_i)\)&lt;/span&gt;之后，并不一定要马上放缩边&lt;span class=&#34;math inline&#34;&gt;\((v_i,
v_{i+1})\)&lt;/span&gt;，只要我们能够保证&lt;span class=&#34;math inline&#34;&gt;\((v_i,
v_{i+1})\)&lt;/span&gt;一定在&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},
v_i)\)&lt;/span&gt;之后放缩即可，至于中间是否穿插其他边的放缩操作，都无所谓。而拓扑顺序正是满足上述性质的一组顺序，为了得到一组拓扑顺序，我们需要对原图进行&lt;strong&gt;拓扑排序&lt;/strong&gt;，然后按照得到的拓扑顺序进行放缩。&lt;/p&gt;
&lt;p&gt;如何进行拓扑排序呢？实际很简单，首先对原图进行&lt;strong&gt;深度优先遍历&lt;/strong&gt;（还有一种基于&lt;strong&gt;入度&lt;/strong&gt;的拓扑排序，此处不表），将完成遍历的点依次插入队列的首部，便可得到按照拓扑顺序排列的一个队列，拓扑顺序的实际意义是，如果边&lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;存在，那么对点&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;的访问必须先于对点&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;的访问。我们每次从队列中取出一个点，并对从其出发的所有边进行放缩操作即可。虽说拓扑顺序是对点的一个排序，但从该点出发的边和这个点是关联的，所以，先访问点，也就相当于先访问从这个点出发的边了，我们先&lt;span class=&#34;math inline&#34;&gt;\(v_{i}\)&lt;/span&gt;访问，也就必然先于&lt;span class=&#34;math inline&#34;&gt;\((v_i,v_{i+1})\)&lt;/span&gt;放缩&lt;span class=&#34;math inline&#34;&gt;\((v_{i-1},v_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;对比&#34;&gt;对比&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 9%&#34;/&gt;
&lt;col style=&#34;width: 26%&#34;/&gt;
&lt;col style=&#34;width: 34%&#34;/&gt;
&lt;col style=&#34;width: 29%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;条目&lt;/th&gt;
&lt;th&gt;Bellman-Ford&lt;/th&gt;
&lt;th&gt;Dijkstra&lt;/th&gt;
&lt;th&gt;DAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;复杂度&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|E| \log |V|)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Theta(|V| + |E|)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;条件&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;无负权边&lt;/td&gt;
&lt;td&gt;有向无环&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;section class=&#34;footnotes footnotes-end-of-document&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;无向图可以很便捷的转换为带权有向图。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34; role=&#34;doc-backlink&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>Source Coding Theorem</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theorem/</guid>
      <description>

&lt;h2 id=&#34;notations-and-concepts&#34;&gt;Notations and Concepts&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt;
(we specifically use the random variable symbol to denote the ensemble)
is a triplet &lt;span class=&#34;math inline&#34;&gt;\((X, \newcommand{A}{\mathcal A}
\newcommand{P}{\mathcal P} \A_X, \P_X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; denotes the random variable, which
takes on values from &lt;span class=&#34;math inline&#34;&gt;\(\A_X = \{a_1, a_1,
\dots \}\)&lt;/span&gt;, that has probability &lt;span class=&#34;math inline&#34;&gt;\(\P_X
= \{p_1, p_2, \dots \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Shannon information content of an outcome &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/strong&gt; is: &lt;span class=&#34;math display&#34;&gt;\[
h(x) = \log_2 \frac{1}{P(x)}
\]&lt;/span&gt; The &lt;strong&gt;raw bit content of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_0(X) = \log |\mathcal A_X|
\]&lt;/span&gt; The &lt;strong&gt;smallest &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset &lt;span class=&#34;math inline&#34;&gt;\(S_\delta\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal A_X\)&lt;/span&gt;&lt;/strong&gt; is the smallest
subset satisfying &lt;span class=&#34;math display&#34;&gt;\[
P(X \in S_\delta) \ge 1 - \delta
\]&lt;/span&gt; The &lt;strong&gt;essential bit content of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X) = \log |S_\delta|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;shannons-source-coding-theorem&#34;&gt;Shannon’s Source Coding
Theorem&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be an ensemble with
entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; bits, and &lt;span class=&#34;math inline&#34;&gt;\(X^N\)&lt;/span&gt; be the ensemble composed of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such i.i.d. random variables. Given
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \delta &amp;lt; 1\)&lt;/span&gt;, there exists a
positive integer &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; such that for
&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
|\frac{1}{N} H_\delta (X^N) - H| &amp;lt; \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or put it in a verbal way,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. random variables each
with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; can be
compressed into more than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits
with negligible information loss as &lt;span class=&#34;math inline&#34;&gt;\(N \to
\infty\)&lt;/span&gt;; conversely if they are compressed fewer than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits, it is virtually certain that
there is information loss.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let the random variable &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; be defined
for the ensemble &lt;span class=&#34;math inline&#34;&gt;\(Y = X^N\)&lt;/span&gt;, which
composes of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. ensembles
&lt;span class=&#34;math inline&#34;&gt;\(X_1 \dots X_N\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; can be
re-written as the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;
information contents &lt;span class=&#34;math inline&#34;&gt;\(h_i = \log \frac 1
{P(X_i)}, i \in 1,2,\dots,N\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots
P(X_N)} = \frac 1 N (\log \frac{1}{P(X_1)} + \dots + \log
\frac{1}{P(X_N)})
\]&lt;/span&gt; Each of these information contents is in turn a random
variable with mean &lt;span class=&#34;math inline&#34;&gt;\(\bar h_i = H(X) =
H\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{h_i}^2 =
\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A long string of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; symbols would
usually contain roughly &lt;span class=&#34;math inline&#34;&gt;\(p_1 N\)&lt;/span&gt;
occurrences of symbol &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_2 N\)&lt;/span&gt; occurrences of symbol &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt;… The probability of those elements is
roughly &lt;span class=&#34;math inline&#34;&gt;\((p_1)^{p_1 N} (p_2)^{p_2 N}
\dots\)&lt;/span&gt; The information content of each such element is thus
roughly &lt;span class=&#34;math inline&#34;&gt;\(N \sum_i p_i \log \frac{1}{p_i} = N
H\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define the typical elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{A}_X^N\)&lt;/span&gt; to be just those element
that have probability close to &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;. Note that interestingly, the
most probable string is not usually typical because its probability is
far away from &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We introduce another parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to control how close the
probability has to be to &lt;span class=&#34;math inline&#34;&gt;\(2^{-NH}\)&lt;/span&gt;
for an element to be typical. The set of typical elements is called
&lt;strong&gt;typical set&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\beta}\)&lt;/span&gt; and is defined as &lt;span class=&#34;math display&#34;&gt;\[
T_{N \beta} = \left\{y \in \mathcal A_X^N: [\frac 1 N \log
\frac{1}{P(y)} - H]^2 &amp;lt; \beta^2 \right\}
\]&lt;/span&gt; By the &lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;Weak Law of Large
Numbers&lt;/a&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P \left( \left( \frac 1 N \log \frac{1}{P(Y)} - H \right)^2 \ge \beta^2
\right) \le \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; and thus &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; As &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, the
probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; falls in &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; draws near to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. We need to relate this to the theorem
that for any given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta\)&lt;/span&gt;, there is a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \simeq NH\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;lt; N(H +
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; is not the
best sufficient subset for compression (because it doesn’t include those
most probable). Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\log |T_{N
\beta}|\)&lt;/span&gt; upper-bounds the &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N)\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;. On the other hand, for all &lt;span class=&#34;math inline&#34;&gt;\(y \in T_{N \beta}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P(y) &amp;gt; 2^{-N(H + \beta)}\)&lt;/span&gt;, thus &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;amp;&amp;lt; \sum_{y \in T_{N \beta}} P(y)
&amp;lt; 1 \\
&amp;amp;\Downarrow \\
|T_{N \beta}| &amp;amp;&amp;lt; 2^{N(H + \beta)} \\
\end{align*}
\]&lt;/span&gt; If we set &lt;span class=&#34;math inline&#34;&gt;\(\beta =
\epsilon\)&lt;/span&gt; and set &lt;span class=&#34;math inline&#34;&gt;\(N \ge N_0\)&lt;/span&gt;
in a way such that &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{\epsilon^2 N_0} \le
\delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in T_{N
\epsilon}) \ge 1 - \delta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\epsilon}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset. Then, &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \le \log |T_{N \epsilon}| \le N(H +
\epsilon)\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(N \ge
N_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H -
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This part is reached by contradiction. Suppose instead there exists a
&lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;#39;\)&lt;/span&gt; such that there exists
a sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; which
results in &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^{N}) \le N(H -
\epsilon)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\epsilon
&amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon/2\)&lt;/span&gt;, we now have
&lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]&lt;/span&gt; Denote the associated subset by &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;. We are to disprove &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; can
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. We break this probability into two parts: &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39;) = P(Y \in S&amp;#39; \cap T_{N \beta}) + P(Y \in S&amp;#39; \cap
\overline{T_{N \beta}})
\]&lt;/span&gt; For the first part, we have &lt;span class=&#34;math display&#34;&gt;\[
|S&amp;#39; \cap T_{N \beta}| \le |S&amp;#39;| \le 2^{N(H - 2\beta)}
\]&lt;/span&gt; Thus, the maximum value of the first part is obtained when
&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap T_{N \beta}\)&lt;/span&gt; contains
&lt;span class=&#34;math inline&#34;&gt;\(2^{N(H - 2\beta)}\)&lt;/span&gt; outcomes all with
probability &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)}\)&lt;/span&gt;
(property of elements in &lt;span class=&#34;math inline&#34;&gt;\(T_{N
\beta}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;As for the second part, since &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap
\overline{T_{N \beta}} \subseteq \overline{T_{N \beta}}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39; \cap \overline{T_{N \beta}}) \le P(Y \in \overline{T_{N
\beta}}) &amp;lt; \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
P(Y \in S&amp;#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} +
\frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; For arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and
a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we can have
&lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \le 1 - \delta\)&lt;/span&gt;
instead of &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. Then we shall conclude that &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; cannot
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(Y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt;
N(H - \epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind Shannon’s source coding theorem
is that, as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; grows, the deviation
of the random variable may grow in a lower order than that of the number
of values the random variable can take on. Therefore, the resulting
outcomes will fall in a narrower range, making the typical set smaller.
Encoding the elements inside the typical set is &lt;em&gt;almost&lt;/em&gt; enough
for the purpose of communication.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Source Coding Theory</title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/</guid>
      <description>

&lt;h2 id=&#34;notations-and-concepts&#34;&gt;Notations and Concepts&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/strong&gt;
is a triple &lt;span class=&#34;math inline&#34;&gt;\((X, \newcommand{A}{\mathcal A}
\newcommand{P}{\mathcal P} \A_X, \P_X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the outcome of a random variable,
which takes on values from &lt;span class=&#34;math inline&#34;&gt;\(\A_X = \{a_1,
a_1, \dots \}\)&lt;/span&gt;, that has probability &lt;span class=&#34;math inline&#34;&gt;\(\P_X = \{p_1, p_2, \dots \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The raw bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_0(X) = \log |\mathcal A_X|
\]&lt;/span&gt; &lt;strong&gt;The smallest &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_\delta\)&lt;/span&gt; is the smallest subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal A_X\)&lt;/span&gt; satisfying &lt;span class=&#34;math display&#34;&gt;\[
P(X \in S_\delta) \ge 1 - \delta
\]&lt;/span&gt; &lt;strong&gt;The essential bit content&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X) = \log |S_\delta|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;shannons-source-coding-theorem&#34;&gt;Shannon’s source coding
theorem&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be an ensemble with
entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; bits, and &lt;span class=&#34;math inline&#34;&gt;\(X^N\)&lt;/span&gt; be the ensemble composed of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. such random variables. Given
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \delta &amp;lt; 1\)&lt;/span&gt;, there exists a
positive integer &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; such that for
&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt; N_0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
|\frac{1}{N} H_\delta (X^N) - H| &amp;lt; \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or put it in a verbal way,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. random variables each
with entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X) = H\)&lt;/span&gt; can be
compressed into more than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits
with negligible information loss as &lt;span class=&#34;math inline&#34;&gt;\(N \to
\infty\)&lt;/span&gt;; conversely if they are compressed fewer than &lt;span class=&#34;math inline&#34;&gt;\(NH\)&lt;/span&gt; bits, it is virtually certain that
there is information loss.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The random variable &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; is defined
for the ensemble &lt;span class=&#34;math inline&#34;&gt;\(Y = X^N\)&lt;/span&gt; which
composes of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; i.i.d. &lt;span class=&#34;math inline&#34;&gt;\(X_1 \dots X_N\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 N \log \frac 1 {P(Y)}\)&lt;/span&gt; can be
re-written as the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;
information contents &lt;span class=&#34;math inline&#34;&gt;\(h_i = \log \frac 1
{P(X_i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots
P(X_N)} = \frac 1 N (\log \frac{1}{P(X_1)} + \dots + \log
\frac{1}{P(X_N)})
\]&lt;/span&gt; Each of these information contents is in turn a random
variable with mean &lt;span class=&#34;math inline&#34;&gt;\(\bar h_i = H(X) =
H\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{h_i}^2 =
\sigma^2\)&lt;/span&gt;. The &lt;strong&gt;typical set&lt;/strong&gt; with parameters
&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
T_{N \beta} = \{y \in \mathcal A_X^N: [\frac 1 N \log \frac{1}{P(y)} -
H]^2 &amp;lt; \beta^2 \}
\]&lt;/span&gt; By the &lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;Weak Law of Large
Numbers&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P((\frac 1 N \log
\frac{1}{P(y)} - H)^2 \ge \beta^2) = \frac{\sigma^2}{\beta^2
N}\)&lt;/span&gt;, and thus &lt;span class=&#34;math display&#34;&gt;\[
P(y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; As &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, the
probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; falls in &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; draws near to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. We need to relate this to the theorem
that for any given &lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta\)&lt;/span&gt;, there is a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \simeq NH\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;lt; N(H +
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(T_{N \beta}\)&lt;/span&gt; is not the
best sufficient subset for compression. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\log |T_{N \beta}|\)&lt;/span&gt; upper-bounds the &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N)\)&lt;/span&gt;. On the other hand, for all
&lt;span class=&#34;math inline&#34;&gt;\(y \in T_{N \beta}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)} &amp;lt; P(y) &amp;lt; 2^{-N(H +
\beta)}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;amp;&amp;lt; 1 \\
|T_{N \beta}| &amp;amp;&amp;lt; 2^{N(H + \beta)} \\
\end{align*}
\]&lt;/span&gt; If we set &lt;span class=&#34;math inline&#34;&gt;\(\beta =
\epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N = N_0\)&lt;/span&gt; in a
way such that &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{\epsilon^2
N_0} \le \delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(P(y \in
T_{N_0 \epsilon}) \ge 1 - \delta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_{N_0 \epsilon}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-sufficient subset. Then, &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) \le \log |T_{N_0 \epsilon}| \le
N_0(H + \epsilon)\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(N
\ge N_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt; N(H -
\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This part is reached by contradiction. Suppose instead there exists a
&lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;#39;\)&lt;/span&gt; such that there exists
a sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt; which
results in &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^{N_0}) \le N_0(H -
\epsilon)\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\beta = \epsilon/2\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]&lt;/span&gt; Denote the associated subset by &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt;. We are to disprove &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; can
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) = P(y \in S&amp;#39; \cap T_{N \beta}) + P(y \in S&amp;#39; \cap
\overline{T_{N \beta}})
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39; \cap T_{N \beta}| \le
|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt;. The maximum value of the first
term is obtained when &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap T_{N
\beta}\)&lt;/span&gt; contains &lt;span class=&#34;math inline&#34;&gt;\(2^{N(H -
2\beta)}\)&lt;/span&gt; outcomes all with probability &lt;span class=&#34;math inline&#34;&gt;\(2^{-N(H - \beta)}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39; \cap \overline{T_{N \beta}} \subseteq
\overline{T_{N \beta}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(P(y \in
S&amp;#39; \cap \overline{T_{N \beta}}) \le P(y \in \overline{T_{N \beta}})
&amp;lt; \frac{\sigma^2}{\beta^2 N}\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
P(y \in S&amp;#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} +
\frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]&lt;/span&gt; For arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and
a sufficiently-large &lt;span class=&#34;math inline&#34;&gt;\(N_0\)&lt;/span&gt;, we can
have &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \le 1 - \delta\)&lt;/span&gt;
instead of &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt;. We shall conclude that &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(|S&amp;#39;| \le 2^{N(H - 2\beta)}\)&lt;/span&gt; cannot
achieve &lt;span class=&#34;math inline&#34;&gt;\(P(y \in S&amp;#39;) \ge 1 -
\delta\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H_\delta(X^N) &amp;gt;
N(H - \epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;source-coding-theorem-for-symbol-codes&#34;&gt;Source coding theorem
for symbol codes&lt;/h2&gt;
&lt;h3 id=&#34;kraft-mcmillan-inequality&#34;&gt;Kraft-McMillan inequality&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Denote the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. If &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt; there exists a set of uniquely-decodable prefix coding with
these lengths as their symbol codes’ lengths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The proof is done by construction. The
number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; should
be less than &lt;span class=&#34;math inline&#34;&gt;\(2^{l+1}\)&lt;/span&gt;, or else the
above condition will be violated. Therefore we can loosely arrange all
the codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; to be unique.
Then the uniqueness condition is checked.&lt;/p&gt;
&lt;p&gt;Denote the number of codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt;. For any two consecutive lengths
&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;amp;\le 1 \\
C_{l+1} &amp;amp;\le 2^{l+1} - 2 C_l \\
C_{l+1} &amp;amp;\le 2(2^l - C_l) \\
\end{aligned}
\]&lt;/span&gt; This means we can append these unused &lt;span class=&#34;math inline&#34;&gt;\(2^l - C_l\)&lt;/span&gt; codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to meet the number of codes of length
&lt;span class=&#34;math inline&#34;&gt;\(l+1\)&lt;/span&gt;. Construction
completes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we have a set of uniquely-decodable prefix coding. Denote
the length of each symbol code as &lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt; and there are &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; symbols. Then, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(S =
\sum_{i=1}^I 2^{-l_i} \le 1\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S^N &amp;amp;= (\sum_{i=1}^I 2^{-l_i})^N \\
  &amp;amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots +
l_{i_N})}
  \end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\((l_{i_1} + \dots +
l_{i_N})\)&lt;/span&gt; term can be treated as the length of encoding of &lt;span class=&#34;math inline&#34;&gt;\(a_{i_1} \dots a_{i_N}\)&lt;/span&gt; of arbitrary length
&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(l_\min = \min_i l_i, l_\max = \max_i l_i\)&lt;/span&gt;,
the above can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
  S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(C_l\)&lt;/span&gt; represents the
number of symbol codes of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;.
Since the coding is uniquely-decodable, &lt;span class=&#34;math inline&#34;&gt;\(C_l
\le 2^l\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(S &amp;gt; 1\)&lt;/span&gt;, the above
cannot hold for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.
Therefore &lt;span class=&#34;math inline&#34;&gt;\(S \le 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-source-coding-theorem&#34;&gt;The source coding theorem&lt;/h3&gt;
&lt;p&gt;For an ensemble &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, there exists
a prefix code &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; with expected
length satisfying &lt;span class=&#34;math display&#34;&gt;\[
H(X) \le L(C,X) &amp;lt; H(X) + 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We define the implicit probabilities
&lt;span class=&#34;math inline&#34;&gt;\(q_i = 2^{-l_i} / z\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z = \sum_i 2^{-l_i}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(C,X) = \sum_i p_i l_i &amp;amp;= -\sum_i [p_i \log (q_iz)] \\
&amp;amp;=\sum_i [p_i \log 1/q_i] - \log z \\
&amp;amp;\ge H(X)
\end{aligned}
\]&lt;/span&gt; The equality holds when &lt;span class=&#34;math inline&#34;&gt;\(z =
1\)&lt;/span&gt; (the code is complete) and &lt;span class=&#34;math inline&#34;&gt;\(q =
p\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(l_i = \log 1/p_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;From another perspective, suppose the coding is complete but not
optimal, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;L(C,X) = -\sum_i [p_i \log (q_iz)] = -\sum_i [p_i \log (q_i)] \\
&amp;amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i
\log (q_i)] \\
&amp;amp;= H(X) + D_{KL}(p || q)
\end{aligned}
\]&lt;/span&gt; where the cost is the extra &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p || q)\)&lt;/span&gt; bits, which is brought by
instead treating &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as the real
distribution. &lt;span class=&#34;math inline&#34;&gt;\(D_{KL(p||q)}\)&lt;/span&gt; is
termed as relative entropy or the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</link>
      <pubDate>Thu, 12 May 2022 11:26:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/noise-contrastive-estimation/</guid>
      <description>

&lt;h2 id=&#34;three-perspectives-on-nce&#34;&gt;Three Perspectives on NCE&lt;/h2&gt;
&lt;h3 id=&#34;non-parametric-estimation&#34;&gt;Non-parametric estimation&lt;/h3&gt;
&lt;p&gt;The traditional log-likelihood function will be &lt;span class=&#34;math inline&#34;&gt;\(\ell = \sum_x \ln p_\theta(x)\)&lt;/span&gt;. In NCE, we
learn &lt;span class=&#34;math display&#34;&gt;\[
p_\theta(1|x) = \sigma(G(x;\theta) - \gamma) = \frac{1}{1 +
e^{-G(x;\theta) + \gamma}}
\]&lt;/span&gt; And corresponding loss function becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal {L} &amp;amp;= -\E_{x \sim \tilde p(x)} \ln p_\theta(1|x) - \E_{x
\sim q(x)} \ln p_\theta(0|x) \\
&amp;amp;= -\int \tilde p(x) \ln p_\theta(1|x) dx - \int q(x) \ln
p_\theta(0|x) dx \\
&amp;amp;= - \int [\tilde p(x) + q(x)] [\frac{\tilde p(x)}{\tilde p(x) +
q(x)} \ln p_\theta(1|x) + \frac{q(x)}{\tilde p(x) + q(x)} \ln
p_\theta(0|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(P(y|x) = \begin{cases}\frac{\tilde
p(x)}{\tilde p(x) + q(x)}, y=1 \\\frac{q(x)}{\tilde p(x) +
q(x)},y=0\end{cases}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\label{loss} \begin{aligned}
\arg \min_{\theta, \gamma} \mathcal{L} &amp;amp;= \arg \min_{\theta, \gamma}
-\int [\tilde p(x) + q(x)][\frac{\tilde p(x)}{\tilde p(x) + q(x)} \ln
p_\theta(1|x) + \int \frac{q(x)}{\tilde p(x) + q(x)} \ln
p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} -\int [\tilde p(x) + q(x)][P(1|x) \ln
p_\theta(1|x) + P(0|x)\ln p_\theta(0|x)]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)][P(1|x) \ln
\frac{1}{p_\theta(1|x)} + P(0|x)\ln \frac{1}{p_\theta(0|x)}]dx \\
&amp;amp;= \arg \min_{\theta, \gamma} \int [\tilde p(x) + q(x)]
H[P(y|x)||p_\theta(y|x)]dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since cross entropy &lt;span class=&#34;math inline&#34;&gt;\(H(p||q) \ge
H(p)\)&lt;/span&gt; and the minimum is reached only when &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;, the global minimum for equation
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is reached when &lt;span class=&#34;math inline&#34;&gt;\(p_\theta(y|x) = P(y|x)\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_\theta(1|x) = \frac{1}{1 + e^{-G(x;\theta) + \gamma}} &amp;amp;=
\frac{\tilde p(x)}{\tilde p(x) + q(x)} = P(1|x) \\
\frac{q(x)}{\tilde p(x)} &amp;amp;= e^{-G(x;\theta) + \gamma} \\
\tilde p(x) &amp;amp;= \frac{q(x) e^{G(x;\theta)}}{e^\gamma}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; are learnt so that &lt;span class=&#34;math inline&#34;&gt;\(q(x) e^{G(x;\theta) - \gamma}\)&lt;/span&gt; fit the
real distribution. It becomes more intuitive when &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a uniform distribution and &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is a constant &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(e^\gamma\)&lt;/span&gt; will be the learnt normalizing
factor.&lt;/p&gt;
&lt;p&gt;https://kexue.fm/archives/5617/comment-page-1&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation-the-original-papers-view&#34;&gt;Maximum
likelihood estimation (the original paper’s view)&lt;/h3&gt;
&lt;p&gt;The model is defined as &lt;span class=&#34;math inline&#34;&gt;\(\ln p_\theta(x) =
\ln p^0_\alpha(x) + c\)&lt;/span&gt;. The MLE will maximize the objective
function &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J_T(\theta) &amp;amp;= \frac{1}{T_d} [\sum_{i=1}^{T_d \\} \ln h(x_i;\theta)
+ \sum_{i=1}^{T_n} \ln (1 - h(y_i;\theta)) \text{, ($x_i$&amp;#39;s are
samples, $y_i$&amp;#39;s are noises, $T_n = \nu T_d$)} \\
&amp;amp;\stackrel{P}\to J(\theta) \triangleq \E_{x \sim \tilde p} \ln
h(x;\theta) + \nu \E_{x \sim q} \ln (1 - h(x;\theta)) \\
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
r_\nu(u) = \frac{1}{1 + \nu e^{-u}} \\
G(x; \theta) = \ln p_\theta(x) - \ln q(x) \\
h(x;\theta) = r_\nu(G(x;\theta)) = \frac{1}{1 + \nu e^{-G(x;\theta)}} \\
\end{gather}
\]&lt;/span&gt; Denote by &lt;span class=&#34;math inline&#34;&gt;\(\tilde J\)&lt;/span&gt; the
objective function &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; seen as a
function of &lt;span class=&#34;math inline&#34;&gt;\(f(.) = \ln p_\theta(.)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f) = \E_{x \sim \tilde p} \ln r_\nu(f(x) - \ln q(x)) + \nu
\E_{x \sim q} \ln (1 - r_\nu[f(x) - q(x)])
\]&lt;/span&gt; For &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; a perturbation of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\tilde J(f + \epsilon \phi) = \E_{x \sim \tilde p} \ln r_\nu(f(x) +
\epsilon \phi(x) - \ln q(x)) + \nu \E_{x \sim q} \ln (1 - r_\nu[f(x) +
\epsilon \phi - q(x)])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By Taylor’s expansion,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln r_\nu(u + \epsilon u_1 + \epsilon^2 u_2) &amp;amp;\approx \ln r_\nu (u)
+ r_{\frac{1}{\nu}}(-u)(\epsilon u_1 + \epsilon^2 u_2) - \frac{r_\nu
(u)r_\frac{1}{\nu}(-u)}{2}(\epsilon u_1 + \epsilon^2 u_2)^2 + \Omicron
\big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln r_\nu (u) + \epsilon u_1r_{\frac{1}{\nu}}(-u) +
\epsilon^2(u_2r_{\frac{1}{\nu}}(-u) - \frac{1}{2}u_1^2
r_{\frac{1}{\nu}}(-u)r_\nu (u)) + \Omicron \big(\epsilon^3 \big) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ln \big(1 - r_v(u + \epsilon u_1 + \epsilon^2 u_2) \big) &amp;amp;\approx
\ln \big(1 - r_v(u) \big) - r_v(u)(\epsilon u_1 + \epsilon^2 u_2) -
\frac{r_{\frac{1}{\nu}}(-u) r_\nu(u)}{2} (\epsilon u_1 + \epsilon^2
u_2)^2 + \Omicron \big((\epsilon u_1 + \epsilon^2 u_2)^3 \big) \\
&amp;amp;= \ln(1 - r_v(u)) - \epsilon u_1 r_v(u) - \epsilon^2 \big( u_2
r_v(u) + \frac{1}{2} u_1^2 r_{\frac{1}{\nu}}(-u) r_\nu(u) \big) +
\Omicron(\epsilon^3)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde J(f + \epsilon \phi) \approx &amp;amp;\E_{x \sim \tilde p} \ln r_\nu
\big(f(x) + \epsilon \phi(x) - \ln q(x) \big) + \nu \E_{x \sim q} \ln (1
- r_\nu[f(x) + \epsilon \phi - q(x)]) \\
= &amp;amp;\E_{x \sim \tilde p} \{\ln r_\nu \big(f(x) - \ln q(x) \big) +
\epsilon \phi(x) r_{\frac{1}{\nu}} \big(\ln q(x) - f(x) \big) \} \\
&amp;amp;+\nu \E_{x \sim q} \{ \ln \big(1 - r_\nu[f(x) -\ln q(x)] \big) -
\epsilon \phi(x) r_\nu \big( f(x) - \ln q(x) \big) \} +
\Omicron(\epsilon^2) \\
= &amp;amp;\tilde J(f) + \epsilon \int \phi(x) \big(r_\frac{1}{\nu} [\ln
q(x) - f(x)] \tilde p(x) - \nu r_\nu[f(x) - \ln q(x)] q(x) \big) +
\Omicron(\epsilon^2)
\end{aligned}
\]&lt;/span&gt; The above equation attains the local maximum at &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; only if the term of order &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
r_\frac{1}{\nu} [\ln q(x) - f(x)] \tilde p(x) &amp;amp;= \nu r_\nu[f(x) -
\ln q(x)] q(x) \\
\frac{\nu \tilde p(x)}{\nu + e^{f(x) - \ln q(x)}} &amp;amp;= \frac{\nu
q(x)}{1 + \nu e^{\ln q(x) - f(x)}} \\
\frac{\tilde p(x)q(x)}{\nu q(x) + p_\theta(x)} &amp;amp;= \frac{q(x)
p_\theta(x)}{p_\theta(x) + \nu q(x)} \\
\end{aligned}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt; is nonzero, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\tilde p(x)}{\nu q(x) + p_\theta(x)} =
\frac{p_\theta(x)}{p_\theta(x) + \nu q(x)} \iff p_\theta(x) = \tilde
p(x)\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;https://leimao.github.io/article/Noise-Contrastive-Estimation/&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Stonesjtu/Pytorch-NCE&#34;&gt;PyTorch-NCE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;other-reference&#34;&gt;Other Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/334772391&#34;&gt;NCE与语言模型&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jacobian Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</link>
      <pubDate>Mon, 09 May 2022 22:09:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/</guid>
      <description>
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \to \R^m\)&lt;/span&gt;, with
input &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt; and output &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f =
\begin{cases}
y_1 = f_1(x_1, x_2, ..., x_n) \\
y_2 = f_2(x_1, x_2, ..., x_n) \\
... \\
y_m = f_m(x_1, x_2, ..., x_n) \\
\end{cases}
\]&lt;/span&gt; Then Jacobian matrix is &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
J &amp;amp;=
\begin{bmatrix}
\frac{\partial f}{\partial x_1} &amp;amp; \frac{\partial f}{\partial x_2}
&amp;amp; \cdots &amp;amp; \frac{\partial f}{\partial x_n} \\
\end{bmatrix} \\
&amp;amp;=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp;amp; \frac{\partial f_m}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \\
\end{bmatrix}
\end{aligned}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear
transformation, i.e., &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y =
Tx\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
J = T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a linear
transformation and &lt;span class=&#34;math inline&#34;&gt;\(n = m\)&lt;/span&gt;, i.e.,
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; square matrix &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
dy_1 \\
dy_2 \\
\vdots \\
dy_n \\
\end{bmatrix} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}
\begin{bmatrix}
dx_1 \\
dx_2 \\
\vdots \\
dx_n \\
\end{bmatrix}
\]&lt;/span&gt; That is, &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{
\begin{bmatrix}
dy_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dy_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dy_n
\end{bmatrix}}_A
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix} =
\underbrace{
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp;amp; \frac{\partial f_n}{\partial
x_2} &amp;amp; \cdots &amp;amp; \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}}_J
\underbrace{
\begin{bmatrix}
dx_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; dx_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; dx_n
\end{bmatrix}}_B
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(JB\)&lt;/span&gt; are both diagonal. From above equation
we can find that &lt;span class=&#34;math inline&#34;&gt;\(A = JB\)&lt;/span&gt;. Therefore,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|A| &amp;amp;= |JB| \\
dy_1 \dots dy_n &amp;amp;= |J|dx_1 \dots dx_n \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spherical Coordinates</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</link>
      <pubDate>Mon, 09 May 2022 20:26:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/</guid>
      <description>

&lt;p&gt;The conversion between the 2-d Cartesian coordinate system and the
2-d polar coordinate system can be extended to a higher dimension, say
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d. In &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-d case, their conversion can be
described as below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Spherical to Cartesian &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
r &amp;amp;= \sqrt{x_1^2 + \dots + x_k^2} &amp;amp;&amp;amp; \\
\varphi_1 &amp;amp;= \arccot \frac{x_1} {\sqrt{x_k^2 + \dots + x_2^2}}
&amp;amp;&amp;amp;=  \arccos \frac{x_1} {\sqrt{x_k^2 + \dots + x_1^2}} \\
\varphi_2 &amp;amp;= \arccot \frac{x_2} {\sqrt{x_k^2 + \dots + x_3^2}}
&amp;amp;&amp;amp;=  \arccos \frac{x_2} {\sqrt{x_k^2 + \dots + x_2^2}} \\
&amp;amp; \vdots &amp;amp;&amp;amp;\vdots\\
\varphi_{k-2} &amp;amp;= \arccot \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}
&amp;amp;&amp;amp;= \arccos \frac{x_{k-2}} {\sqrt{x_k^2 + x_{k-1}^2 +
x_{k-2}^2}} \\
\varphi_{k-1} &amp;amp;= 2 \arccot \frac{x_{k-1} + \sqrt{x_k^2 +
x_{k-1}^2}}{x_k} &amp;amp;&amp;amp;=
\begin{cases}
\arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n \ge 0 \\
2\pi - \arccos \frac{x_{k-1}} {\sqrt{x_k^2 + x_{k-1}^2}}, &amp;amp;x_n &amp;gt;
0\\
\end{cases}
\end{alignat}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cartesian to spherical &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
x_1 &amp;amp;= r \cos(\varphi_1) \\
x_2 &amp;amp;= r \sin(\varphi_1) \cos(\varphi_2) \\
\notag &amp;amp;\vdots \\
x_{k-1} &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\cos(\varphi_{k-1}) \\
x_k &amp;amp;= r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\sin(\varphi_{k-1}) \\
\end{align}
\]&lt;/span&gt; The corresponding &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian
Matrix&lt;/a&gt; is &lt;span class=&#34;math display&#34;&gt;\[
J^{(k)} = \left[ \begin{array}{ccccc|c}
\cos (\varphi_1) &amp;amp; -r \sin(\varphi_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots
&amp;amp; 0 \\
\sin(\varphi_1) \cos(\varphi_2) &amp;amp; r \cos(\varphi_1) \cos(\varphi_2)
&amp;amp; -r \sin(\varphi_1) \sin(\varphi_2) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ &amp;amp; \ddots &amp;amp; \vdots \\
\hline
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \cos(\varphi_{k-1}) &amp;amp;
\cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; -r \sin(\varphi_1) \dots
\sin(\varphi_{k-2}) \sin(\varphi_{k-1}) \\
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1}) &amp;amp;
\cdots &amp;amp; \cdots &amp;amp; \ &amp;amp; \ &amp;amp; r \sin(\varphi_1) \dots
\sin(\varphi_{k-2}) \cos(\varphi_{k-1})
\end{array} \right]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(|J^{(2)}|\)&lt;/span&gt; can be easily
derived as &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;; and &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k)}|\)&lt;/span&gt; can be constructed from &lt;span class=&#34;math inline&#34;&gt;\(|J^{(k-1)}|\)&lt;/span&gt;. Comparing &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt;,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra column
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and an extra row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\cos(\varphi_{k-1})\)&lt;/span&gt; term in each element
than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on
row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;On row &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, before column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; has an extra &lt;span class=&#34;math inline&#34;&gt;\(\sin(\varphi_{k-1})\)&lt;/span&gt; term in each element
than the elements of &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; on
row &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(J^{(k)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J^{(k-1)}\)&lt;/span&gt; are totally the same on the
region delimited by row &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, row
&lt;span class=&#34;math inline&#34;&gt;\(k-2\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apply the Laplace expansion along column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and combine the property of determinant
to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J^{(k)}| =\ &amp;amp; \underbrace{0 + \dots + 0}_{n-2} + (-1)^{(n-1)+n} [-r
\sin(\varphi_1) \dots \sin(\varphi_{k-2}) \sin(\varphi_{k-1})
\sin(\varphi_{k-1}) \big( \sin(\varphi_{k-1}) |J^{(k-1)}| \big)] + \\
&amp;amp; (-1)^{n+n} [r \sin(\varphi_1) \dots \sin(\varphi_{k-2})
\cos(\varphi_{k-1}) \big( \cos(\varphi_{k-1}) |J^{(k-1)}| \big)] \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) \big(
\sin_{\varphi_{k-1}}^2 + \cos{\varphi_{k-1}}^2 \big) |J^{(k-1)}| \\
=\ &amp;amp; r \sin(\varphi_1) \dots \sin(\varphi_{k-2}) |J^{(k-1)}| \\
\end{aligned}
\]&lt;/span&gt; By induction, &lt;span class=&#34;math display&#34;&gt;\[
|J^{(k)}| = r^{k-1} \sin^{k-2}(\varphi_1) \sin^{k-3}(\varphi_2) \dots
\sin(\varphi_{k-2})
\]&lt;/span&gt; Therefore when changing basis from orthogonal coordinate
system to polar coordinate system, &lt;span class=&#34;math display&#34;&gt;\[
\d x_1 \dots \d x_k = r^{k-1} \sin^{k-2}(\varphi_1) \sin^{k-3}
(\varphi_2) \dots \sin(\varphi_{k-2}) \d r \d \varphi_1 \dots \d
\varphi_{k-1}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates&#34;&gt;Wiki&lt;/a&gt;
|| &lt;a href=&#34;https://wuli.wiki//online/SphCar.html&#34;&gt;3d Case Blog 1&lt;/a&gt; ||
&lt;a href=&#34;https://www.cnblogs.com/hans_gis/archive/2012/11/21/2755126.html&#34;&gt;3d
Case Blog 2&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;volume-of-sphere&#34;&gt;Volume of Sphere&lt;/h3&gt;
&lt;p&gt;With change of basis between spherical coordinate and Cartesian, we
may compute the volume of sphere in any &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimension. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_n &amp;amp;= \int_{B_n} 1 \; \d x_1 \d x_2 \dots \d x_n \\
&amp;amp;= \int_{0}^{R} \int_{0}^{2 \pi}
\underbrace{\int_{0}^{\pi} \dots \int_{0}^{\pi}}_{n-2} \\
&amp;amp;\quad r^{n-1} \sin(\varphi_1)^{n-2} \sin(\varphi_2)^{n-3} \dots
\sin(\varphi_{n-2}) \d r \d \varphi_1 \dots \d \varphi_{n-1} \\
&amp;amp;= \int_{0}^{R} r^{n-1} \d r
\int_{0}^{2 \pi} \d \varphi_{n-1} \\
&amp;amp;\quad
\int_{0}^{\pi} \sin(\varphi_{n-2}) \d \varphi_{n-2}
\int_{0}^{\pi} \sin^2(\varphi_{n-3}) \d \varphi_{n-3}
\dots
\int_{0}^{\pi} \sin^{n-2}(\varphi_{1}) \d \varphi_{1} \\
\end{aligned}
\]&lt;/span&gt; Notice that &lt;span class=&#34;math display&#34;&gt;\[
\int_{0}^{\pi} \sin^{n}(x) \d x = \sqrt{\pi} \frac{\Gamma
(\frac{n-1}{2})}{\Gamma (\frac{n}{2})}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_n &amp;amp;= \int_{0}^{R} r^{n-1} \d r
\int_{0}^{2 \pi} \d \varphi_{n-1} \\
&amp;amp;\quad
\int_{0}^{\pi} \sin(\varphi_{n-2}) \d \varphi_{n-2}
\int_{0}^{\pi} \sin^2(\varphi_{n-3}) \d \varphi_{n-3}
\dots
\int_{0}^{\pi} \sin^{n-2}(\varphi_{1}) \d \varphi_{1} \\
&amp;amp;= \frac{R^n}{n} \cdot 2\pi \cdot \sqrt{\pi}
\frac{\Gamma(0)}{\Gamma(1/2)} \cdot \sqrt{\pi}
\frac{\Gamma(1/2)}{\Gamma(2/2)} \cdots \sqrt{\pi}
\frac{\Gamma((n-3)/2)}{\Gamma((n-2)/2)} \\
&amp;amp;= R^n \frac{1}{n/2} \sqrt{\pi^n} \frac{1}{\Gamma((n-2)/2)} \\
&amp;amp;= \frac{R^n \sqrt{\pi^n}}{\Gamma(n/2)}
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1v8411J7WQ/&#34;&gt;《三体》中的数学——为什么很高维度的单位球体积为0？_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/fourier-transform/</guid>
      <description>

&lt;h2 id=&#34;continuous-time-fourier-transform&#34;&gt;Continuous-time Fourier
Transform&lt;/h2&gt;
&lt;h3 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h3&gt;
&lt;p&gt;In Euclidean space, we usually represent a vector by a set of
independent and orthogonal base vectors (basis). Orthogonality means the
inner product between two basis is zero. Inner product can also be
defined on some common interval between two functions, and thus the
orthogonality.&lt;/p&gt;
&lt;h4 id=&#34;frequency-domain&#34;&gt;Frequency Domain&lt;/h4&gt;
&lt;p&gt;It is intuitive to model after the inner product between vectors.
Function (signal) on its domain can be viewed as an “infinite-dimension”
vector. We represent this infinity in the definition of function inner
product by integration. In particular, given two functions &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, an interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[
\int\limits_{x \in I}s(x)g(x)dx
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are orthogonal on interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; if their inner product &lt;span class=&#34;math inline&#34;&gt;\(\int_{x \in I}s(x)g(x)dx = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set of basis of &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt; Euclidean
space contains &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent
orthogonal basis. For an “infinite-dimension” function space, there are
an infinite number of basis, among which a group of sine and cosine
functions satisfy. For integer &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;
and positive integers &lt;span class=&#34;math inline&#34;&gt;\(m, n\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\left\{
\begin{array} \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = \pi, m = n, m, n
\ge 1 \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx = 0, m \ne n, m, n
\ge 1 \\
\end{array}
\right. \\
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx = 0, m, n \ge 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m = n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \cos^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 - \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin^2(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{1 + \cos(2nx)}{2}dx \\
&amp;amp;= \pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \sin(nx) \cos(nx)dx \\
&amp;amp;= \int_{-\pi + 2k\pi}^{\pi + 2k\pi} \frac{\sin(2nx)}{2}dx \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m \ne n\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \cos(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) + \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \sin(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\cos((m+n)x) - \cos((m-n)x)}{2}dx \\
&amp;amp;= \frac{\frac{\sin((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} - \frac{\sin((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\pi + 2k\pi}^{\pi + 2k\pi} \cos(mx) \sin(nx)dx &amp;amp;= \int_{-\pi
+ 2k\pi}^{\pi + 2k\pi} \frac{\sin((n+m)x) + \sin((n-m)x)}{2}dx \\
&amp;amp;= -\frac{\frac{\cos((m+n)x)}{m+n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi} + \frac{\cos((m-n)x)}{m-n}\bigg|^{x=\pi + 2k\pi}_{x=-\pi +
2k\pi}}{2} \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, we can use the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(x), \sin(x), \cos(2x), \sin(2x),
\dots\)&lt;/span&gt; to fit any &lt;strong&gt;continuous function&lt;/strong&gt; on
interval &lt;span class=&#34;math inline&#34;&gt;\([-\pi, \pi]\)&lt;/span&gt;. Or use &lt;span class=&#34;math inline&#34;&gt;\(1, \cos(2\pi fx), \sin(2\pi fx), \cos(2\pi f2x),
\sin(2\pi f2x), \dots\)&lt;/span&gt; to fit any function on interval &lt;span class=&#34;math inline&#34;&gt;\([\frac{-1}{2f} + \frac{k}{f}, \frac{1}{2f} +
\frac{k}{f}]\)&lt;/span&gt;, which can be any interval by choosing the value
of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (frequency) and the integer
&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A continuous function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;
approximated with such series up to level &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; can be written as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{split}
s_N(x) &amp;amp;= a_0 + \sum_{i=1}^N \big(
\underbrace{a_n}_{A_n\sin(\varphi_n)} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\cos(\varphi_n)} \sin(2\pi fnx) \big) \\
&amp;amp;= a_0 + \sum_{n=1}^N \bigg( A_n\sin(2\pi fnx + \varphi_n) \bigg)
\text{, where} \\
\end{split} \\
\notag &amp;amp;A_n = \sqrt{a_n^2 + b_n^2}, \sin(\varphi_n) =
\frac{a_n}{\sqrt{a_n^2 + b_n^2}}, \cos(\varphi_n) =
\frac{b_n}{\sqrt{a_n^2 + b_n^2}}
\end{align}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(A_n\)&lt;/span&gt; can be interpreted as
the amplitude, &lt;span class=&#34;math inline&#34;&gt;\(\varphi_n\)&lt;/span&gt; as the
phase, &lt;span class=&#34;math inline&#34;&gt;\(nf\)&lt;/span&gt; as the frequency.&lt;/p&gt;
&lt;h4 id=&#34;complex-frequency-domain&#34;&gt;Complex Frequency Domain&lt;/h4&gt;
&lt;p&gt;By Euler’s Formula we have &lt;span class=&#34;math display&#34;&gt;\[
e^{ix} = \cos x + i\sin x
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(s_N(x)\)&lt;/span&gt; can be
re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{alignat}{2}
s_N(x) &amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg(
\underbrace{a_n}_{A_n\cos \phi_n} \cos(2\pi fnx) +
\underbrace{b_n}_{A_n\sin \phi_n} \sin(2\pi fnx) \bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \bigg( A_n\cos(2\pi fnx - \phi_n)
\bigg) \\
&amp;amp;= a_0 &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) + i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;+ \sum_{n=1}^N \frac{A_n}{2} \bigg( \cos(2\pi fnx -
\phi_n) - i\sin(2\pi fnx - \phi_n) \bigg) \\
&amp;amp; &amp;amp;&amp;amp;\Downarrow_\text{by multiplication rule between complex
numbers in polar form}  \\
&amp;amp;= &amp;amp;&amp;amp; \sum_{n=-N}^N c_ne^{i 2\pi fnx} \\
\end{alignat}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
c_n &amp;amp;=
\begin{cases}
\frac{A_n}{2}(\cos \phi_n - i\sin \phi_n) = \frac{1}{2}(a_n - ib_n),
&amp;amp;n &amp;gt; 0 \\
\overline{c_{|n|}} =\frac{A_n}{2}(\cos \phi_n + i\sin \phi_n), &amp;amp;n
&amp;lt; 0 \\
a_0, &amp;amp;n = 0
\end{cases}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(N \to +\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; can be reconstructed as the Fourier
Series: &lt;span class=&#34;math display&#34;&gt;\[
s(x) = \lim_{N \to +\infty} s_N(x) = a_0 + \sum_{n=1}^{+\infty} \bigg(
a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg) \\
\]&lt;/span&gt; The problem comes how &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt;
can be computed. By the orthogonality mentioned before, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \d x = \int_{-1/2f+f/k}^{1/2f+f/k} [a_0
+ \sum_{n=1}^{+\infty} \bigg( a_n \cos(2\pi fnx) + b_n \sin(2\pi fnx)
\bigg)] \d x = \frac{1}{f} a_0 \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \cos(2\pi fkx) \d x =
\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \cos (2\pi fkx) \d x =
\frac{1}{2f} a_k \\
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) \sin(2\pi fkx) \d x
=\int_{-1/2f+f/k}^{1/2f+f/k} [a_0 + \sum_{n=1}^{+\infty} \bigg( a_n
\cos(2\pi fnx) + b_n \sin(2\pi fnx) \bigg)] \sin (2\pi fkx) \d x =
\frac{1}{2f} b_k \\
\end{gather}
\]&lt;/span&gt; The computation of &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; can be combined together by
the Euler’s Formula: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-1/2f+f/k}^{1/2f+f/k} s(x) e^{-i 2\pi fkx} \d x &amp;amp; =
\int_{-1/2f+f/k}^{1/2f+f/k} (a_k \cos(2\pi fkx) + b_k \sin(2\pi fkx))
(\cos(2\pi fkx) - i \sin(2\pi fkx)) \d x \\
&amp;amp;= \frac{1}{2f} (a_k - i b_k) = \frac{1}{f} c_k
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;fourier-transform&#34;&gt;Fourier Transform&lt;/h3&gt;
&lt;p&gt;We have been through representing a continuous function on a certain
interval using the Fourier Series. This can be quite useful for periodic
functions. As long as we figure out the representation on its repeating
interval, we obtain the representation on its whole domain. The problem
is more of computing the factor for each sine and cosine function.&lt;/p&gt;
&lt;p&gt;The process of finding out factors for an arbitrary continuous
function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is called Fourier
Transform. It transforms the function &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; from the time domain to the frequency
domain. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is periodic, it can
be easily represented by the Fourier Series as discussed in previous
section. When &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is not periodic, we
can treat the periodic interval as &lt;span class=&#34;math inline&#34;&gt;\([-\infty,
+\infty]\)&lt;/span&gt;. Its Fourier Transform and the inverse will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\hat s \stackrel{\mathcal F}\Longleftrightarrow s \\
\hat s(f) = \int_{-\infty}^{+\infty} s(t) e^{-i 2\pi f x} \d t \\
s(x) = \int_{-\infty}^{+\infty} \hat s(f) e^{i 2\pi f x} \d f
\end{gather*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;discrete-time-fourier-transform&#34;&gt;Discrete-time Fourier
Transform&lt;/h2&gt;
&lt;p&gt;The domain (time axis) of the function (signal) is continuous in our
discussion by now. When the time axis is discrete (and usually takes on
a series of integers), we are facing the Discrete-time Fourier
Transform. We will be using the term &lt;strong&gt;signal&lt;/strong&gt; instead of
the function from now on.&lt;/p&gt;
&lt;p&gt;For a discrete signal &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, its
Fourier Transform is &lt;span class=&#34;math display&#34;&gt;\[
\hat s(\omega) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i\omega k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the
angular speed. &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is in the
unit of radian/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat
s(\omega)\)&lt;/span&gt; is the weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; between two signal samples. &lt;span class=&#34;math display&#34;&gt;\[
\hat s(f) = \sum_{k=-\infty}^{+\infty} s[k] e^{-i 2\pi f k}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the
“frequency”. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the unit of
cycles/sample. &lt;span class=&#34;math inline&#34;&gt;\(\hat s(f)\)&lt;/span&gt; is the
weight of the component which walks &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; cycles between two signal samples.&lt;/p&gt;
&lt;p&gt;Todo&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Laplace Transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;-transform&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Fast Fourier Transform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jezzamon.com/fourier/zh-cn.html&#34;&gt;傅里叶变换交互式入门
(jezzamon.com)&lt;/a&gt; || &lt;a href=&#34;https://charlesliuyx.github.io/2018/02/18/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E8%AE%A9%E4%BD%A0%E6%B0%B8%E8%BF%9C%E5%BF%98%E4%B8%8D%E4%BA%86%E7%9A%84%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E8%A7%A3%E6%9E%90/&#34;&gt;傅立叶变换与群&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/1119070975&#34;&gt;如何理解傅里叶变换公式？
- 苗华栋的回答 - 知乎&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/19714540/answer/334686351&#34;&gt;如何理解傅里叶变换公式？
- 马同学的回答 - 知乎&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Contrastive Predictive Coding</title>
      <link>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</link>
      <pubDate>Fri, 29 Apr 2022 11:32:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/contrastive-predictive-coding/</guid>
      <description>

&lt;h2 id=&#34;rationale&#34;&gt;Rationale&lt;/h2&gt;
&lt;p&gt;CPC was initially proposed in autoregressive models. It enhances the
autoencoder by lifting the lower bound of mutual information between the
encoded representation and the original data. The original data can
either be the data before the encoding, or the future data after various
steps.&lt;/p&gt;
&lt;p&gt;CPC learns the representation by minimizing the following loss
function: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\c}{\mathrm{c}} \label{loss}
\mathcal L_N = -\E_{t \sim \Phi} \log \frac{f_\theta(\x_l,\c)}
{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)} =
-\E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\frac{f_\theta(\x_l,\c)} {\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(t = (\x_1, \dots, \x_{N+1}, \c^*;
\ell)\)&lt;/span&gt; is a tuple of random variables and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the distribution from which &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is drawn. &lt;span class=&#34;math inline&#34;&gt;\((\x:\c)_{1:N+1}\)&lt;/span&gt; are drawn from the joint
distribution &lt;span class=&#34;math inline&#34;&gt;\(\tilde p(\x,\c)\)&lt;/span&gt;. All
&lt;span class=&#34;math inline&#34;&gt;\(\c_i\)&lt;/span&gt;’s but one randomly-chosen
&lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; are trimmed from the original
samples. &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; is known but it is
unknown which sample it is associated with. &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; denotes the index of this unique
sample we are trying to predict. In essence, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is parameterizing the
representation &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. The formal score
function &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt; is simply chosen
to be a deterministic cosine similarity between &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\x_{1:N+1}\)&lt;/span&gt; consists of one
positive sample &lt;span class=&#34;math inline&#34;&gt;\(\x^*\)&lt;/span&gt; that is
matched with &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt; and more other
independent negative (noise) samples &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt;’s that are not matched with &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed ratio of the number of
negative samples to the number of positive samples. Let &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i|\x_{1:N+1},\c^*)\)&lt;/span&gt; represent the
probability that &lt;span class=&#34;math inline&#34;&gt;\(\x_i\)&lt;/span&gt; is the
positive sample given &lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots
x_{N+1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\c^*\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\ell=i,
\x_{1:N+1},\c^*)}{\sum_{j=1}^{N+1} P(\ell=j, \x_{1:N+1},\c^*)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitute &lt;span class=&#34;math inline&#34;&gt;\(P(\ell=i,\x_{1:N+1},\c^*) =
P(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} P(\x_j)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(\ell=i|\x_{1:N+1},\c^*) &amp;amp;= \frac{P(\x_i,\c^*) \prod_{j=1,j \ne
i}^{N+1} P(\x_j)}{\sum_{j=1}^{N+1} [P(\x_j,\c^*) \prod_{k=1,k \ne
j}^{N+1} P(\x_k)]} \\
&amp;amp;= \frac{\tilde p(\x_i,\c^*) \prod_{j=1,j \ne i}^{N+1} \tilde
p_X(\x_j)} {\sum_{j=1}^{N+1} [\tilde p(\x_j,\c^*) \prod_{k=1,k \ne
j}^{N+1} \tilde p_X(\x_j)]} \\
&amp;amp;= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} }
{\sum_{j=1}^{N+1} \frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} \\
\end{aligned}
\]&lt;/span&gt; The loss function &lt;span class=&#34;math inline&#34;&gt;\(\eqref{loss}\)&lt;/span&gt; is in fact the expectation
(the outer &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of the categorical
cross entropy (the inner &lt;span class=&#34;math inline&#34;&gt;\(\E\)&lt;/span&gt;) of
identifying the sample as positive or negative. The minimum of loss
function is thus reached when the two categorical distributions are
identical. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_\theta(l = i|x_{1:N+1},\c^*) =
\frac{f_\theta(\x_i,\c)}{\sum_{\x^\prime \in X}f_\theta(\x^\prime, \c)}
= \frac{\frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} } {\sum_{j=1}^{N+1}
\frac{\tilde p(\x_j,\c^*)}{\tilde p_X(\x_j)}} =
P(\ell=i|\x_{1:N+1},\c^*) \\
f_\theta(\x_i,\c) = \frac{\sum_{\x^\prime \in X}f_\theta(\x^\prime,
\c)}{\sum_{\x^\prime \in X} \frac{\tilde p(\x^\prime|\c)}{\tilde
p_X(\x^\prime)}} \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)} \\
f_\theta(\x_i,\c) \propto \frac{\tilde p(\x_i,\c^*)}{\tilde p_X(\x_i)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;bounding-the-mutual-information&#34;&gt;Bounding the Mutual
Information&lt;/h2&gt;
&lt;p&gt;CPC helps estimate the lower bound of the mutual information between
the encoded representation and the original data when optimizing the
InfoNCE loss: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\mathcal L_N^{\text{opt}} = -\E_{p(\x_{1:N+1},\c^*)}
\E_{p(l|\x_{1:N+1}, \c^*)} \log \frac{\frac{\tilde p(\x_l, \c^*)}{\tilde
p_X(\x_l)}} {\sum_{\x&amp;#39; \in X} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde
p_X(\x&amp;#39;)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\frac{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)} + \sum_{\x&amp;#39; \in
X, \x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}}
{\frac{\tilde p(\x_l, \c^*)}{\tilde p_X(\x_l)}} \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 +
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} \sum_{\x&amp;#39; \in X,
\x&amp;#39; \ne x_l} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde p_X(\x&amp;#39;)}
\big) \\
&amp;amp;\approx \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log
\big( 1 + \frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1)
\E_{\tilde p_X(\x&amp;#39;)} \frac{\tilde p(\x&amp;#39;, \c^*)}{\tilde
p_X(\x&amp;#39;)} \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big( 1 +
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;\ge \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} \log \big(
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)} (N - 1) \big) \\
&amp;amp;= \E_{p(\x_{1:N+1},\c^*)} \E_{p(l|\x_{1:N+1}, \c^*)} [\log
\frac{\tilde p_X(\x_l)} {\tilde p(\x_l, \c^*)}] + \log (N - 1) \\
&amp;amp;= -I(\x;\c^*) + \log (N - 1)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(\x;\c^\star) \ge
\log(N-1) - \mathcal L^{\mathrm{opt}}_{N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://anilkeshwani.github.io/CPC/&#34;&gt;Paper Review&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=zNKMHj1eLa0&#34;&gt;CPC Formulation&lt;/a&gt;
|| &lt;a href=&#34;https://jxmo.io/posts/nce&#34;&gt;NCE and InfoNCE&lt;/a&gt; || &lt;a href=&#34;https://colab.research.google.com/github/google-research/google-research/blob/master/vbmi/vbmi_demo.ipynb#scrollTo=DUd0hFZ5Js8k&#34;&gt;Demo
of Bounding Mutual Information&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Conditional Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</link>
      <pubDate>Wed, 13 Apr 2022 15:03:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</guid>
      <description>
&lt;p&gt;The &lt;strong&gt;conditional entropy&lt;/strong&gt; measures the the amount of
information needed to describe the outcome of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that the value of another random
variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is known. The entropy of
&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(Y|X) = -\sum_{(x,y) \in \mathcal{X} \times \mathcal{Y}} p_{(X,Y)}(x,y)
\log \frac{p_{(X,Y)}(x,y)}{p_X(x)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Lipschitz Continuity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</link>
      <pubDate>Mon, 31 Jan 2022 00:02:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/calculus/lipschitz-continuity/</guid>
      <description>

&lt;p&gt;For a continuous mapping &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, it
is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous if there
exists a number &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in \dom(f)\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
||f(x) - f(y)|| \le K||x - y||
\]&lt;/span&gt; If the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-Lipschitz continuous, we further
say &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-smooth.&lt;/p&gt;
&lt;h3 id=&#34;lipschitz-constant&#34;&gt;Lipschitz Constant&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the minimum number to
make the above condition hold, then &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is called the &lt;strong&gt;Lipschitz
constant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Lipschitz constant for a general differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will be the maximum spectral norm of
its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt; over its
domain. &lt;span class=&#34;math display&#34;&gt;\[
||f||_{Lip} = \sup_x \sigma[\nabla f(x)] = \sup_x \sup_{||v||=1} \nabla
f(x) \cdot v
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma[\nabla f(x)]\)&lt;/span&gt;
denotes the spectral norm of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s
gradient at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Lipschitz constant for a matrix transformation will be the
matrix’s &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The Lipschitz constant for a &lt;span class=&#34;math inline&#34;&gt;\(\R \mapsto
\R\)&lt;/span&gt; function will be its largest &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;subgradient&lt;/a&gt; over its domain&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;composition-of-functions&#34;&gt;Composition of Functions&lt;/h3&gt;
&lt;p&gt;Suppose two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; are Lipschitz continuous
respectively. Then, &lt;span class=&#34;math display&#34;&gt;\[
\nabla (f \circ g)(x) = \nabla f [g(x)] \nabla g(x)
\]&lt;/span&gt; by the chain rule of derivatives. &lt;span class=&#34;math inline&#34;&gt;\(f \circ g\)&lt;/span&gt;’s Lipschitz constant will be
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sigma(\nabla (f \circ g)(x)) &amp;amp;= \sup_{||v|| = 1} ||\{\nabla f[g(x)]
\nabla g(x)\} v|| \\
&amp;amp;=  \sup_{||v|| = 1} \{||\nabla g(x) v||\} \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;\le  \sup_{||v|| = 1} \sigma[\nabla g(x)] \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \sup_{||v|| = 1} \{||\nabla f[g(x)]
\frac{\nabla g(x) v}{||\nabla g(x) v||}||\} \\
&amp;amp;= \sigma[\nabla g(x)] \cdot \sigma\{\nabla f[g(x)]\}
\end{aligned}
\]&lt;/span&gt; In other words, &lt;span class=&#34;math inline&#34;&gt;\(||f \circ
g||_{Lip} = ||f||_{Lip} \cdot ||g||_{Lip}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/&#34;&gt;Lipschitz
Continuity, convexity, subgradients – Marco Tulio Ribeiro –
(washington.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/Lipschitz-gradient&#34;&gt;Lipschitz
continuous gradient · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Spectral Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/</link>
      <pubDate>Sat, 29 Jan 2022 21:23:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/</guid>
      <description>
&lt;p&gt;Spectral normalization of an &lt;span class=&#34;math inline&#34;&gt;\(M \times
N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
||A||_2 = \max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} =
\sqrt{\lambda_{\max}(A^TA)} = \sigma_{\max}(A)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\rm z \in \R^N\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\max}(A^TA)\)&lt;/span&gt; is the maximum
eigenvalue of matrix &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;, which is
exactly &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s largest singular value
&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\max}(A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To prove it, firstly note that: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\mathrm z} \frac{||A\mathrm z||_2}{||\mathrm z||_2} \iff
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2}
\]&lt;/span&gt; We may force a constraint on &lt;span class=&#34;math inline&#34;&gt;\(\mathrm z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(||\mathrm z||^2_2 = 1\)&lt;/span&gt;. This is because
&lt;span class=&#34;math display&#34;&gt;\[
\frac{||Ac\mathrm z||^2_2}{||c\mathrm z||^2_2} = \frac{c^2||A\mathrm
z||^2_2}{c^2||\mathrm z||^2_2} = \frac{||A\mathrm z||^2_2}{||\mathrm
z||^2_2}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\mathrm z} \frac{||A\mathrm z||^2_2}{||\mathrm z||^2_2} =
||A\mathrm z||^2_2 = \mathrm z^TA^TA\mathrm z \\
s.t. ||\mathrm z||^2_2 = 1
\end{gather}
\]&lt;/span&gt; This can be solved by Lagrange multiplier, where the
Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
L(\mathrm z, \lambda) = \mathrm z^TA^TA\mathrm z + \lambda(||\mathrm
z||^2_2 - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extrapolation of the spectral normalization will be related to
Rayleigh quotient.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://math.stackexchange.com/questions/2723629/why-is-the-maximum-rayleigh-quotient-equal-to-the-maximum-eigenvalue&#34;&gt;matrices
- Why is the maximum Rayleigh quotient equal to the maximum eigenvalue?
- Mathematics Stack Exchange&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Convex Optimization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</link>
      <pubDate>Fri, 07 Jan 2022 12:56:57 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-optimization/</guid>
      <description>

&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convex Set&lt;/p&gt;
&lt;p&gt;A set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X\)&lt;/span&gt; is called a
convex set if &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, x^{(2)} \in \mathcal
X\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall \alpha \in [0,1],
\alpha x^{(1)} + (1 - \alpha)x^{(2)} \in \mathcal X\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convex Function&lt;/p&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is
convex if &lt;span class=&#34;math inline&#34;&gt;\(dom(f)\)&lt;/span&gt; is a convex set
and &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x^{(1)} + (1 - \alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1 -
\alpha)f(x^{(2)}), \forall x^{(1)}, x^{(2)} \in dom(f), \alpha \in [0,1]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is concave if &lt;span class=&#34;math inline&#34;&gt;\(-f\)&lt;/span&gt; is convex.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with a
convex domain is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + \nabla^T f(x)(y - x), \forall x, y \in dom(f)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is twice differentiable
if &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2f(x)_{ij} =
\frac{\partial^2f(x)}{\partial x_ix_j}\)&lt;/span&gt; exists at each &lt;span class=&#34;math inline&#34;&gt;\(x \in dom(f)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla^2f(x) \succeq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Any local minima of a convex function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^N \mapsto \R\)&lt;/span&gt; is also a global
minima.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a local minima.
Then there exists a number &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt; 0\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(\forall x \in dom(f) \land
||x-y||_2 \le r \Rightarrow f(x) \ge f(y)\)&lt;/span&gt;. Suppose instead
there exists a global minima &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.
Then it must hold that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||y-z||_2 &amp;gt; r \\
0 &amp;lt; \frac{r}{||y-z||_2} &amp;lt; 1
\end{gather}
\]&lt;/span&gt; There exists some &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \epsilon
&amp;lt; r\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta = 1
- \frac{r - \epsilon}{||y-z||_2} &amp;lt; 1\)&lt;/span&gt;. Then the distance from
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to point &lt;span class=&#34;math inline&#34;&gt;\((\theta y + (1 - \theta)z)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
||y - (\theta y + (1 - \theta)z)||_2 = (1 - \theta)||y-z||_2 = \frac{r -
\epsilon}{||y-z||_2}||y-z||_2 &amp;lt; r
\]&lt;/span&gt; That is to say &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \ge f(y)
\]&lt;/span&gt; However, since &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
convex and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta &amp;lt; 1\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the global
minima, &lt;span class=&#34;math display&#34;&gt;\[
f(\theta y + (1 - \theta)z) \le \theta f(y) + (1 - \theta)f(z) &amp;lt;
\theta f(y) + (1 - \theta)f(y) = f(y)
\]&lt;/span&gt; which contradicts that &lt;span class=&#34;math inline&#34;&gt;\(f(\theta y
+ (1 - \theta)z) \ge f(y)\)&lt;/span&gt; derived. In other words, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is a global minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;Standard form of the optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;h_i(x) = 0, i = 1,\dots,s \\
\end{aligned}
\]&lt;/span&gt; Standard form of the convex optimization problem is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\min f_0(x) \\
s.t.\quad &amp;amp;f_i(x) \le 0, i = 1,\dots,r \\
&amp;amp;a_i^Tx = b_i (Ax = b), i = 1,\dots,s \\
&amp;amp;\text{$f_0,\dots,f_r$ are convex, and $a_0,\dots,a_s \in \R^N$}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</link>
      <pubDate>Fri, 07 Jan 2022 12:42:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/</guid>
      <description>
&lt;p&gt;Linear discriminant analysis is another approximation to the Bayes
optimal classifier. Instead of assuming independence between each pair
of input dimensions given certain label, LDA assumes a single common
shared covariance matrix among the input dimensions, no matter the label
is. Take the Gaussian distribution as an example: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(x|y=C_j) &amp;amp;= \mathcal{N}(x;\mu_j, \Sigma) \\
&amp;amp;=
\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}e^{-\frac{1}{2}(x-\mu_j)^T\Sigma^{-1}(x-\mu_j)}
\end{aligned}
\]&lt;/span&gt; Then the classification function will be &lt;span class=&#34;math inline&#34;&gt;\(f_{LDA} = \arg
\max\limits_{j}p(x|y=C_j)p(y=C_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;LDA’s parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\mu, \Sigma,
\varphi)\)&lt;/span&gt; is also learned with Maximum Likelihood Estimation:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\theta) &amp;amp;= \prod_{i=1}^mp(x^{(i)}, y^{(i)};\theta) \\
&amp;amp;= \prod_{i=1}^mp(x^{(i)}|y^{(i)};\theta)p(y^{(i)})
\end{aligned}
\]&lt;/span&gt; The log-likelihood function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log L(\theta) \\
&amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) + \sum_{i=1}^m\log
p(y^{(i)}, \varphi) \\
\end{aligned}
\]&lt;/span&gt; We can maximize above two summations separately.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_1(\mu,\Sigma) &amp;amp;= \sum_{i=1}^m\log p(x^{(i)}|y^{(i)};\mu,\Sigma) \\
&amp;amp;=
-\frac{1}{2}\sum_{i=1}^m(x^{(i)}-\mu_{j|y^{(i)}})^T\Sigma^{-1}(x^{(i)}-\mu_{j|y^{(i)}})
+ \log|\Sigma| + n\log2\pi\\
\end{aligned}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \mu_j} =
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) \\
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)\Sigma^{-1}(x^{(i)} - \mu_j) &amp;amp;= 0
\\
\Sigma^{-1}\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) &amp;amp;= 0
\\
\end{aligned}
\]&lt;/span&gt; Since the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is real-symmetric and invertible
and thus its nullspace is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)(x^{(i)} - \mu_j) = 0 \\
\mu_j =
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)x^{(i)}}{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial l_1}{\partial \Sigma} =
-\frac{1}{2}\sum_{i=1}^m(\Sigma^{-1}
-\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) \\
\sum_{i=1}^m(\Sigma^{-1}
-\Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T\Sigma^{-1}) = 0 \\
\Sigma^{-1}\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T)
= 0 \\
\sum_{i=1}^m(I - \Sigma^{-1}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T) = 0 \\
\sum_{i=1}^mI = \Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T
\\
\sum_{i=1}^m\Sigma
=  \Sigma\Sigma^{-1}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l_2(\varphi) &amp;amp;= \sum_{i=1}^m\log p(y^{(i)}; \varphi) \\
&amp;amp;= \sum_{i=1}^m\log\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)}
\\
&amp;amp;= \sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(p(y^{(i)};\varphi) =
\prod_{j=1}^c\varphi_j^{\mathbb{I}(y^{(i)}=C_j)} =
\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\varphi_j\)&lt;/span&gt;. We chose the
product notation because it could be easily “logged”.&lt;/p&gt;
&lt;p&gt;There is also a constraint in maximization of &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j = 1\)&lt;/span&gt;. We can form
the corresponding Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
J(\varphi, \lambda) =
\sum_{i=1}^m\sum_{j=1}^c\mathbb{I}(y^{(i)}=C_j)\log \varphi_j +
\lambda(-1 + \sum_{j=1}^c\varphi_j)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\varphi_j\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\varphi_j} + \lambda = 0 \\
\varphi_j = -\frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda}
\end{gather}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^c\varphi_j =
1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
-\frac{\sum_{j=1}^c\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{\lambda} = 1 \\
\lambda = -M
\end{gather}
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
\varphi_j = \frac{\sum_{i=1}^m\mathbb{I}(y^{(i)}=C_j)}{M}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Frobenius Normalization</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/</link>
      <pubDate>Mon, 20 Dec 2021 15:43:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/</guid>
      <description>
&lt;p&gt;Frobenius Normalization of an &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
||A||_F \triangleq \sqrt{\sum_{i,j}A_{ij}^2}
\]&lt;/span&gt; It can be found that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||A||_F^2 = \sum_{ij}A_{ij}^2 &amp;amp;=
\sum_{i=1}^m\sum_{j=1}^nA_{ij}A_{ij} =
\sum_{i=1}^n\sum_{j=1}^mA_{ji}A_{ji} \\
&amp;amp;= \sum_{i=1}^m(\sum_{j=1}^nA_{ij}A_{ji}^T) =
\sum_{i=1}^n(\sum_{j=1}^mA_{ij}^TA_{ji}) \\
&amp;amp;= \sum_{i=1}^m(A_{i:}A_{:i}^T) = \sum_{i=1}^n(A_{i:}^TA_{:i})\\
&amp;amp;= \sum_{i=1}^m(AA^T)_{ii} = \sum_{i=1}^n(A^TA)_{ii}\\
&amp;amp;= \tr(AA^T) = \tr(A^TA)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Chebyshev Distance</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/</link>
      <pubDate>Sat, 18 Dec 2021 20:28:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/</guid>
      <description>

&lt;p&gt;Chebyshev distance is a specific form of Minkowski norm (&lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norm): &lt;span class=&#34;math display&#34;&gt;\[
d_p(x, x^\prime) = ||x - x^\prime||_p \coloneq (\sum_{i=1}^n|x_i -
x^\prime_i|^p)^{1/p}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(p \to \infty\)&lt;/span&gt;.
Chebyshev distance is in effect &lt;span class=&#34;math inline&#34;&gt;\(\max\limits_i (|x_i - x^\prime_i|)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;proof-of-discrete-form&#34;&gt;Proof of Discrete Form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = |x_i - x^\prime_i|\)&lt;/span&gt; and
without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(a_1 =
\max\limits_ia_i = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
d_p(x, x^\prime) &amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^n|x_i -
x^\prime_i|^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(\sum_{i=1}^na_i^p)^{1/p} \\
&amp;amp;= \lim_{p \to \infty}(a_1^p\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p}
\\
&amp;amp;= \lim_{p \to \infty}(a_1^p)^{1/p} \cdot \lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\
&amp;amp;= a_1 \cdot \lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \\
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\forall i, a_1 &amp;gt;
a_i\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\frac{a_i}{a_1} \le
1\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 \le &amp;amp;\sum_{i=1}^n(\frac{a_i}{a_1})^p \le n \\
1^{1/p} \le &amp;amp;(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le n^{1/p}
\text{, where $p &amp;gt; 1$} \\
\lim_{p \to \infty}1^{1/p} \le &amp;amp;\lim_{p \to
\infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p} \le \lim_{p \to
\infty}n^{1/p} \\
1 \le &amp;amp;\lim_{p \to \infty}(\sum_{i=1}^n(\frac{a_i}{a_1})^p)^{1/p}
\le 1 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(d_p(x, x^\prime) = a_1 \cdot 1
= a_1 = \max\limits_i|x_i - x^\prime_i|\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;proof-of-continuous-form&#34;&gt;Proof of Continuous Form&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; be continuous and
bounded on interval &lt;span class=&#34;math inline&#34;&gt;\((a, b)\)&lt;/span&gt;, then,
&lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = \sup\limits_{x
\in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S = \sup\limits_{x \in
(a,b)}|f(x)|\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\forall\varepsilon &amp;gt; 0, \exists x_0 \in (a,b),
|f(x_0)| &amp;gt; S - \varepsilon\)&lt;/span&gt;. By continuity, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{x \to x_0}|f(x)| &amp;gt; S -
\varepsilon\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\exists\delta
&amp;gt; 0, \forall x \in U(x_0, \delta), ||f(x)| - \lim\limits_{x \to
x_0}|f(x)|| &amp;lt; \varepsilon \rightarrow |f(x)| &amp;gt; S -
2\varepsilon\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_{x_0 - \delta}^{x_0 + \delta} |f(x)|^pdx)^{1/p} &amp;amp;\ge
(\int_{x_0 - \delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} &amp;amp;\ge \lim\limits_{p \to +\infty}(\int_{x_0 -
\delta}^{x_0 + \delta} (S - 2\varepsilon)^pdx)^{1/p} \\
&amp;amp;= \lim\limits_{p \to +\infty}(2\delta(S - 2\varepsilon)^p)^{1/p} \\
&amp;amp;= S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(U(x_0, \delta)\)&lt;/span&gt; in
within the interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(|f(x)|^p \ge 0\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge (\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} \\
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} &amp;amp;\ge
\lim\limits_{p \to +\infty}(\int_{x_0 - \delta}^{x_0 + \delta}
|f(x)|^pdx)^{1/p} \\
&amp;amp;\ge S - 2\varepsilon
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is
arbitrarily and positively valued, &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{p \to +\infty}(\int_a^b
|f(x)|^pdx)^{1/p} \ge S\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} \le
\lim\limits_{p \to +\infty}(\int_a^b S^pdx)^{1/p} = \lim\limits_{p \to
+\infty}((b - a)S^p)^{1/p} = S
\]&lt;/span&gt; then, &lt;span class=&#34;math display&#34;&gt;\[
\lim\limits_{p \to +\infty}(\int_a^b |f(x)|^pdx)^{1/p} = S =
\sup\limits_{x \in (a,b)}|f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/267575473&#34;&gt;p范数的极限（无穷范数）为什么是极大值范数？
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</link>
      <pubDate>Fri, 17 Jun 2022 20:32:59 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/logistic-regression/</guid>
      <description>

&lt;h2 id=&#34;two-class-logistic-regression&#34;&gt;Two-class Logistic
Regression&lt;/h2&gt;
&lt;p&gt;Logistic Regression is a binary linear classifier. Suppose the
feature space is &lt;span class=&#34;math inline&#34;&gt;\(\R^N\)&lt;/span&gt;, then it
processes the feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with
a linear function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = w^Tx + b\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R\)&lt;/span&gt;. It takes a probabilistic
approach and maps &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to a
probability value between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; by applying a sigmoid function
&lt;span class=&#34;math inline&#34;&gt;\(\sigma(z) = \frac{1}{1 + e^{-z}}\)&lt;/span&gt;.
That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(y = +1|x) &amp;amp;= \sigma(f(x)) \\
p(y = -1|x) &amp;amp;= 1 - \sigma(f(x)) = \sigma(-f(x)) \\
\end{aligned}
\]&lt;/span&gt; Or equivalently, &lt;span class=&#34;math inline&#34;&gt;\(p(y|x) =
\sigma(yf(x))\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f_{LR}(x) =
\arg \max\limits_{y \in \{-1, 1\}} \sigma(y(f(x)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Logistic regression is a discriminative classifier because we are
directly modelling &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;, with no
intermediate step on &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{ (x^{(i)},
y^{(i)}): i=1, \dots, M \}\)&lt;/span&gt;, logistic regression is learning by
maximizing the &lt;strong&gt;conditional&lt;/strong&gt; log-likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
l(w, b) = \log L(w, b) = \log \prod_{i=1}^Mp(y^{(i)}|x^{(i)};w,b) =
\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b)
\]&lt;/span&gt;&lt;/p&gt;
$$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp;(w^\star, b^\star) = \arg \max\limits_{w, b}l(w, b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log p(y^{(i)}|x^{(i)};w,b) \\
&amp;amp;= \arg \max\limits_{w, b}\sum_{i=1}^M\log
\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\

&amp;amp;= \arg \min\limits_{w, b}-\sum_{i=1}^M\log
\sigma(y^{(i)}(w^Tx^{(i)}+b)) \\
\end{aligned}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Let $J(w, b) = -\sum_{i=1}^M\log \sigma(y^{(i)}(w^Tx^{(i)}+b))$ be the
target function. There is no closed-form solution to this optimization
problem. Rather, it is to be solved by some iterative algorithm, e.g.
gradient descent. For each iteration, parameters are updated by
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{gather}
w \leftarrow w - \eta\frac{\partial J}{\partial w} \\
b \leftarrow b - \eta\frac{\partial J}{\partial b}
\end{gather}\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
Specifically,
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\frac{\partial J}{\partial w} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))}
\frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))}
\frac{\partial (y^{(i)}f(x^{(i)}))}{\partial w} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))}
\sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)}
\\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)}x^{(i)} \\
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial b} &amp;amp;= -\sum_{i=1}^M\frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{\partial\log
\sigma(y^{(i)}f(x^{(i)}))}{\partial \sigma(y^{(i)}f(x^{(i)}))}
\frac{\partial \sigma(y^{(i)}f(x^{(i)}))}{\partial (y^{(i)}f(x^{(i)}))}
\frac{\partial (y^{(i)}f(x^{(i)}))}{\partial b} \\
&amp;amp;= -\sum_{i=1}^M \frac{1}{\sigma(y^{(i)}f(x^{(i)}))}
\sigma(y^{(i)}f(x^{(i)}))(1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
&amp;amp;= -\sum_{i=1}^M (1 - \sigma(y^{(i)}f(x^{(i)}))) y^{(i)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the posterior obtained by binary &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/linear-discriminant-analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt; also
has the form of &lt;span class=&#34;math display&#34;&gt;\[
p(y|x) = \frac{1}{1 + e^{-(w^Tx + b)}}
\]&lt;/span&gt; This is not to say LDA and LR are the same in binary case. LDA
is a generative model which learns &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;, LR is a discriminative model which
only learns &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)\)&lt;/span&gt;. One can
discriminate with a generative model, but not reversely.&lt;/p&gt;
&lt;h3 id=&#34;some-history&#34;&gt;Some History&lt;/h3&gt;
&lt;p&gt;Historically, there has been efforts on adapting linear regression to
the classification task where the output is a probability value between
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, instead of between &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. Many attempts have been given to
mapping &lt;span class=&#34;math inline&#34;&gt;\((0, 1)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((-\infty, +\infty)\)&lt;/span&gt; first and then apply
linear regression on transformed values. An early work uses the quantile
function of standard normal distribution. The model was named as
“&lt;strong&gt;prob&lt;/strong&gt;abilistic un&lt;strong&gt;it&lt;/strong&gt;” (probit).
However, this model is too computationally-expensive at that time. Later
on a work that uses the quantile function of logistic distribution
followed on, naming its model as “&lt;strong&gt;log&lt;/strong&gt;istic
un&lt;strong&gt;it&lt;/strong&gt;” (logit). Essentially, the logit function is the
log of the odds: &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{logit} (p) = \ln \frac{p}{1-p}
\]&lt;/span&gt; In machine learning, un-normalized scores for different
classes are usually called logits too. It makes some sense since these
unbounded values are to be mapped to &lt;span class=&#34;math inline&#34;&gt;\((0,
1)\)&lt;/span&gt;, which means they are logit-ted values.&lt;/p&gt;
&lt;h2 id=&#34;multi-class-logistic-regression&#34;&gt;Multi-class Logistic
Regression&lt;/h2&gt;
&lt;p&gt;One way to train use Logistic Regression in multi-class
classification in to for each class &lt;span class=&#34;math inline&#34;&gt;\(i = {1,
\dots, C}\)&lt;/span&gt;, assign a weight vector &lt;span class=&#34;math inline&#34;&gt;\(w^{(i)}\)&lt;/span&gt;, the probability is define by the
softmax function: &lt;span class=&#34;math display&#34;&gt;\[
p(y = c|x) =\frac{\exp(w^{(c)}\cdot x + b^{(c)})}{\sum_{i=1}^C
\exp(w^{(i)} \cdot x + b^{(i)})}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(f_\text{Multiclass LR}(x) =
\arg \max\limits_{y \in \{1,\dots,C\}}p(y|x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To prevent overfitting, a prior distribution is usually added to
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose each dimension of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is independently sampled from a
Gaussian distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(0,
\frac{C}{2})\)&lt;/span&gt;, then, &lt;span class=&#34;math display&#34;&gt;\[
p(w) \propto \exp(-\frac{1}{C}w^Tw)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Bounding Mutual Information</title>
      <link>https://chunxy.github.io/notes/papers/bounding-mutual-information/</link>
      <pubDate>Thu, 02 Jun 2022 14:01:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/bounding-mutual-information/</guid>
      <description>

&lt;h2 id=&#34;i_textba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;The very basic bound on the Mutual Information is based on the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/overview/#KL-divergence Entropy and Conditional Entropy&#34;&gt;non-negativity&lt;/a&gt; of KL-divergence.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) = &amp;amp;\E_{p(x,y)} \log \frac{p(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)p(x|y)}{p(x)q(x|y)} \\
= &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} + \\
&amp;amp;\underbrace{\E_{p(x,y)} \log \frac{p(x|y)}{q(x|y)}}_{\E_{p(y)}
D_{KL}(p(x|y) || q(x|y)} \\
\ge &amp;amp;\E_{p(x,y)} \log \frac{q(x|y)}{p(x)} \\
= &amp;amp;\E_{p(x,y)} \log q(x|y) + H(X) \triangleq I_\text{BA}
\end{aligned}
\]&lt;/span&gt; This bound is not usually tractable since &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(H(X)\)&lt;/span&gt; has no closed-form expression.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) =
p(x|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;i_textuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; is replaced with an
unnormalized model, that is &lt;span class=&#34;math display&#34;&gt;\[
q(x|y) = \frac{p(x)}{Z(y)} e^{f(x,y)}, \text{where } Z(y) = \E_{p(x)}
e^{f(x,y)}
\]&lt;/span&gt; Substituting this back to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{BA}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} \log \frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \triangleq I_\text{UBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that by scaling &lt;span class=&#34;math inline&#34;&gt;\(q(x|y)\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; term is canceled.&lt;/p&gt;
&lt;p&gt;This bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(q(x|y) =
\frac{p(x)}{Z(y)} e^{f(x,y)} = p(x|y)\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{f(x,y)} &amp;amp;= \frac{p(x, y)}{p(x)p(y)} Z(y) \\
e^{f(x,y)} &amp;amp;= \frac{Z(y)}{p(y)} p(y|x) \\
f(x,y) &amp;amp;= \ln p(y|x) + \underbrace{\ln \frac{Z(y)}{p(y)}}_{c(y)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textdv&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By further applying Jensen’s inequality to the &lt;span class=&#34;math inline&#34;&gt;\(\E_{p(y)} Z(y)\)&lt;/span&gt; term in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log\E_{p(y)} [Z(y)] \triangleq
I_\text{DV}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The original &lt;span class=&#34;math inline&#34;&gt;\(I_\text{DV}\)&lt;/span&gt; bound
is in effect derived from &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/#Variational Lower-bound&#34;&gt;the
variational lower bound of KL-divergence&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_\text{KL}(p_{(x,y)} || p(x) \otimes p(y)) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \log [\E_{p(x) \otimes p(y)} f(x,y)] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log [\E_{p(y)} \E_{p(x)} e^{f(x,y)}] \\
&amp;amp;= \E_{p(x,y)} f(x,y) - \log \E_{p(y)} Z(y) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_texttuba&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt; function also has the
following property: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\forall x,a&amp;gt;0,\log(x) &amp;amp;\le \frac{x}{a} + \log(a) - 1 &amp;amp;\iff \\
a + a\log(x) &amp;amp;\le x + a\log(a) &amp;amp;\iff \\
a\log(x) - x &amp;amp;\le a\log(a) - a \\
\end{aligned}
\]&lt;/span&gt; Therefore, we can insert the inequality &lt;span class=&#34;math inline&#34;&gt;\(\log Z(y) \le \frac{Z(y)}{a(y)} + \log a(y) -
1\)&lt;/span&gt; into the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; to
get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log a(y) -
1 \big)] \triangleq I_\text{TUBA}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This bound is tight when&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(f(x,y) = \log p(y|x) + \underbrace{\log
\frac{Z(y)}{p(y)}}_{c(y)}\)&lt;/span&gt;, and&lt;/li&gt;
&lt;li&gt;the tight condition of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt; holds: &lt;span class=&#34;math inline&#34;&gt;\(a(y) = Z(y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;i_textnwj&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;By setting &lt;span class=&#34;math inline&#34;&gt;\(a(y) = e\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;\ge \E_{p(x,y)} f(x,y) - \E_{p(y)} [\frac{Z(y)}{a(y)} + \log
a(y) - 1 \big)] \\
&amp;amp;\ge \E_{p(x,y)} f(x,y) - e^{-1}\E_{p(y)} Z(y)\triangleq
I_\text{NWJ}
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt; is a
special case of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{TUBA}\)&lt;/span&gt;, its
bound is tight when &lt;span class=&#34;math inline&#34;&gt;\(Z(y)\)&lt;/span&gt;
self-normalizes to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. In this case
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x,y) &amp;amp;= \log p(y|x) + \log \frac{e}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= 1 + \log \frac{p(x|y)}{p(x)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;i_textnce&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate the &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; given one sample from &lt;span class=&#34;math inline&#34;&gt;\(p(x_1)p(y|x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; additional independent samples &lt;span class=&#34;math inline&#34;&gt;\(x_{2:K} \sim p^{K-1}(x_{2:K})\)&lt;/span&gt;. For any
random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is
independent from &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_1,Z;Y) &amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,z,y)}{p(x_1,z) p(y)} \\
&amp;amp;= \E_{p(x_1,z,y)} \frac{p(x_1,y) p(z)}{p(x_1) p(z) p(y)} \\
&amp;amp;= I(X_1;Y)
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y) =
I(X_1,X_{2:K};Y)\)&lt;/span&gt;. Bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1;Y)\)&lt;/span&gt; becomes bounding &lt;span class=&#34;math inline&#34;&gt;\(I(X_1,X_{2:K};Y)\)&lt;/span&gt;, which can be estimated
using any of the preceding methods.&lt;/p&gt;
&lt;p&gt;Set the critic to &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 +
\overbrace{\log \frac{e^{g(x,y)}}
{a(y;x_{1:K})}}^{h(x_{1:K},y)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sample among &lt;span class=&#34;math inline&#34;&gt;\(x_{1:K}\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Usually &lt;span class=&#34;math inline&#34;&gt;\((x,y)_{1:K}\)&lt;/span&gt; are sampled from the same
marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tilde
p(x,y)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is then
uniformly drawn among &lt;span class=&#34;math inline&#34;&gt;\(y_{1:K}\)&lt;/span&gt;.
Substitute these to the &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\label{infonce} \begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K},y)} f(x_{1:K},y) - e^{-1}\E_{p(y)}
Z(y) \\
&amp;amp;= \E_{p(x_{1:K},y)} [1 + \log \frac{e^{g(x,y)}}{a(y,x_{1:K})}] -
e^{-1} \E_{p(y)} [\E_{p(x_{1:K})} e^{1 + \log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - e^{-1} \E_{p(y)} [e\E_{p(x_{1:K})}
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] \\
&amp;amp;= 1 + \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}}{a(y,x_{1:K})}] - \E_{p(x_{1:K})p(y)}
\frac{e^{g(x,y)}}{a(y,x_{1:K})} \\
\end{aligned}
\]&lt;/span&gt; Further set &lt;span class=&#34;math inline&#34;&gt;\(a(y;x_{1:K}) =
\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}\)&lt;/span&gt;. Substitute this into
the last term in equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{infonce}\)&lt;/span&gt; will give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E_{p(x_{1:K})p(y)} \frac{e^{g(x,y)}}{a(y,x_{1:K})} &amp;amp;= \E_{p(y)}
\frac{\E_{p(x_{1:K})} e^{g(x,y)}}{\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)}
} \\
&amp;amp;\stackrel{P}{\to} \E_{p(y)} \frac{\E_{p(x_{1:K})} e^{g(x,y)}}
{\E_{p(x_{1:K})} e^{g(x,y)} } \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X_{1:K};Y) &amp;amp;\ge \E_{p(x_{1:K})p(y|x_{1:K})} [\log
\frac{e^{g(x,y)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y)} }] \\
&amp;amp;\approx \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log
\frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)} }]
\triangleq I_\text{NCE}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;
is tightly bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log K\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K \log
\frac{e^{g(x_j,y_j)}} {\frac{1}{K} \sum_{i=1}^K e^{g(x_i, y_j)}} ] \\
&amp;amp;= \E_{p(x_{1:K})} [\frac{1}{K} \sum_{j=1}^K (\log
\frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} )] + \log K \\
&amp;amp;\le \E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{e^{g(x_j,y_j)}} {\sum_{i=1}^K e^{g(x_i, y_j)}} ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{\sum_{i=1}^K e^{g(x_i, y_j)}} {e^{g(x_j,y_j)}} ]} + \log K
\end{aligned}
\]&lt;/span&gt; The equality is reached when &lt;span class=&#34;math inline&#34;&gt;\(f(x_{1:K},y) = 1 + {h(x_{1:K},y)} = 1 + \log
\frac{p(x|y)}{p(x)}\)&lt;/span&gt; as in &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NWJ}\)&lt;/span&gt;. In this case, it can be
derived that &lt;span class=&#34;math inline&#34;&gt;\(g(x,y) = g^\star(x,y) =
\frac{p(y|x)}{p(y)}\)&lt;/span&gt;. And then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_\text{NCE} &amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{\frac{p(y_j|x_j)}{p(y_j)} + \sum_{i=1,i \ne j}^K
\frac{p(y_j|x_i)}{p(y_j)}} {\frac{p(y_j|x_j)}{p(y_j)}} ]} + \log K \\
&amp;amp;\le -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K (1 +
\frac{p(y_j)}{p(y_j|x_j)} \sum_{i=1,i \ne j}^K
\frac{p(y_j|x_i)}{p(y_j)}) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(1 +
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \E_{p(y)} \frac{p(y|x_i)}{p(y)} \big) ]}
+ \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[1 + \frac{1}{K} \sum_{j=1}^K \big(
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;\approx -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K \big(
\frac{p(y_j)}{p(y_j|x_j)} (K-1) \big) ]} + \log K \\
&amp;amp;= -\E_{p(x_{1:K})} \log {[\frac{1}{K} \sum_{j=1}^K
\frac{p(y_j)}{p(y_j|x_j)} ]} - \log(K-1) + \log K \\
&amp;amp;= I(X_1;Y) - \log(K-1) + \log K
\end{aligned}
\]&lt;/span&gt; This derivation is much looser than &lt;a href=&#34;https://chunxy.github.io/notes/papers/contrastive-predictive-coding/#Bounding the Mutual Information&#34;&gt;that in the InfoNCE’s original
paper&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;with-i_textuba&#34;&gt;With &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;There is another approach to the derivation of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{NCE}\)&lt;/span&gt;, that stems from an estimate
of &lt;span class=&#34;math inline&#34;&gt;\(I_\text{UBA}\)&lt;/span&gt; (which I won’t call
empirical estimate) and that may be more intuitive: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;I(X;Y) \ge \E_{p(x,y)} f(x,y) - \E_{p(y)} \log Z(y) \\
&amp;amp;= \E_{p^N(x,y)} f(x_1,y_1) - \E_{p^N(x,y)} \log Z(y_1) \\
&amp;amp;= \E_{p^N(x,y)} \log e^{f(x_1,y_1)} \\
&amp;amp;\quad\;- \E_{p^N(x,y)} \log \E_{p(x&amp;#39;)} e^{f(x&amp;#39;,y_1)} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}}{\E_{p(x&amp;#39;)}
e^{f(x&amp;#39;,y)}} \\
&amp;amp;\simeq \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\frac{1}{N} \sum_i
e^{f(x_i,y_1)}} \\
&amp;amp;= \E_{p^N(x,y)} \log \frac{e^{f(x_1,y_1)}} {\sum_i e^{f(x_i,y_1)}}
+ \log N \\
&amp;amp;\triangleq I_\text{NCE} \le \log N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Cross Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:46 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</guid>
      <description>
&lt;p&gt;The &lt;strong&gt;cross entropy&lt;/strong&gt; between two distributions over the
same underlying set of events measures the average number of bits to
identify the event drawn from the set if a coding scheme is used for the
set is optimized for probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, instead of the true distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Cross Entropy of distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; relative to a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) = -\mathrm{E}_{x \sim p} \log q(x)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</link>
      <pubDate>Tue, 11 Jan 2022 20:26:40 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/gradient-descent/</guid>
      <description>

&lt;p&gt;If a convex function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
differentiable and satisfies certain “regularity conditions”, we can get
a nice guarantee that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; will
converge by gradient descent.&lt;/p&gt;
&lt;h3 id=&#34;l-smoothness&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness&lt;/h3&gt;
&lt;p&gt;Qualitatively, smoothness means that the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; changes in a controlled, bounded
manner. Quantitatively, smoothness assumes that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has a Lipschitz gradient. That means
there exists an &lt;span class=&#34;math inline&#34;&gt;\(L &amp;gt; 0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
||\nabla f(x) - \nabla f(y)||_2 \le L||x - y||_2, \forall x,y \in
\mathrm{dom}(f)
\]&lt;/span&gt; We will say that such function is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth or strongly smooth. &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled=&#34;&#34;/&gt;
Proof&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness is
equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\frac{L}{2}||x||^2_2 -
f(x)\)&lt;/span&gt; being convex.&lt;/p&gt;
&lt;p&gt;With the smoothness, or the Lipschitz gradient, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from above.&lt;/p&gt;
&lt;p&gt;We will assume &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness in
all cases below. Meanwhile, the local minima (in convex case, the global
minima) is denoted as &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-convexity&#34;&gt;Convergence with Convexity&lt;/h3&gt;
&lt;p&gt;With convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with linear
approximation: &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;Now consider a &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth function
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We adopt a fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{1}{L}\)&lt;/span&gt; so that the update
in each iteration will be &lt;span class=&#34;math display&#34;&gt;\[
x_{k+1} = x_k - \frac{1}{L}\nabla f(x)
\]&lt;/span&gt; By the definition of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness, &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation} \label{diff}
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\frac{1}{L}\nabla f(x_k))^T \nabla f(x_k)
+ \frac{L}{2}||-\frac{1}{L}\nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2
\end{aligned}
\end{equation}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_k)\)&lt;/span&gt;
holds in each iteration.&lt;/p&gt;
&lt;p&gt;By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
f(x^\star) \ge f(x_k) + (x^\star - x_k)^T \nabla f(x_k) \\
\label{convexity} f(x_k) \le f(x^\star) + (x_k - x^\star)^T \nabla
f(x_k)
\end{gather}
\]&lt;/span&gt; Substituting the result back to the right-hand side of
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x^\star) + (x_k - x^\star)^T \nabla f(x_k) -
\frac{1}{2M}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; (x_k - x^\star)^T L(x_k - x_{k+1}) -
\frac{1}{2M}||L(x_k - x_{k+1})||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;&amp;lt; \frac{L}{2}(2(x_k - x^\star)^T(x_k -
x_{k+1}) - ||(x_k - x_{k+1})||^2_2) \\
\end{aligned}
\]&lt;/span&gt; By using the fact that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||a - b||^2_2 = ||a||^2_2 - 2a^Tb + ||b||^2_2 \\
2a^Tb - ||b||^2_2 = ||a||^2_2 - ||a - b||^2_2
\end{aligned}
\]&lt;/span&gt; We have &lt;span class=&#34;math display&#34;&gt;\[
\label{closer} f(x_{k+1}) - f(x^\star) \le \frac{L}{2}(||x_k -
x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This indicates that &lt;span class=&#34;math inline&#34;&gt;\(x_{k+1}\)&lt;/span&gt; is
closer to the global minima &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt;
than &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k [f(x_{i+1}) - f(x^\star)] &amp;amp;\le \sum_{i=0}^k
\frac{L}{2}(||x_i - x^\star||^2_2 - ||x_{i+1} - x^\star||^2_2) \\
&amp;amp;= \frac{L}{2}(||x_0 - x^\star||^2_2 - ||x_{k+1} - x^\star||^2_2) \\
&amp;amp;\le \frac{L}{2}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(f(x_{i+1}) \le f(x_i), \forall
i=0,\dots,k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x_{k+1}) \le f(x_i),
\forall i=0,\dots,k\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k(f(x_{k+1}) - f(x^\star)) \le \frac{L}{2}||x_0 - x^\star||^2_2 \\
f(x_{k+1}) - f(x^\star) \le \frac{L}{2k}||x_0 - x^\star||^2_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&#34;variant-step-size&#34;&gt;Variant Step Size&lt;/h5&gt;
&lt;p&gt;In real application scenario, it may be difficult to know &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; exactly. However, as long as we choose
&lt;span class=&#34;math inline&#34;&gt;\(\alpha_k \le \frac{1}{L}\)&lt;/span&gt; in each
iteration, the convergence and the rate still holds. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) + (-\alpha_k \nabla f(x_k))^T \nabla f(x_k) +
\frac{L}{2}||-\alpha_k \nabla f(x_k)||^2_2 \\
&amp;amp;= f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2
\\
f(x_{k+1}) &amp;amp;\le f(x_k) - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla
f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) - (\alpha_k -
\frac{L}{2} \alpha_k^2)||\nabla f(x_k)||^2_2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1} = f(x_{k+1}) - f(x^\star)
\ge 0\)&lt;/span&gt;, the above becomes &lt;span class=&#34;math inline&#34;&gt;\(\eta_{k+1}
\le \eta_{k} - (\alpha_k - \frac{L}{2} \alpha_k^2)||\nabla
f(x_k)||^2_2\)&lt;/span&gt;. From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{convexity}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\eta_k = f(x_k) - f(x^\star) \le (x_k - x^\star)^T \nabla f(x_k) \le
||x_k - x^\star||_2||\nabla f(x_k)||_2 \\
0 \le \frac{\eta_k}{||x_k - x^\star||_2} \le ||\nabla f(x_k)||_2 \\
-||\nabla f(x_k)||^2_2 \le -\frac{\eta^2_k}{||x_k - x^\star||^2_2} \\
\end{gather}
\]&lt;/span&gt; Substituting it back to give &lt;span class=&#34;math display&#34;&gt;\[
\eta_{k+1} \le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)
\eta^2_k}{||x_k - x^\star||^2_2}
\]&lt;/span&gt; From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{closer}\)&lt;/span&gt; we already have &lt;span class=&#34;math display&#34;&gt;\[
||x_{k+1} - x^\star||^2_2 \le ||x_k - x^\star||^2_2 \le \dots \le ||x_0
- x^\star||^2_2
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\eta_{k+1} &amp;amp;\le \eta_k - \frac{(\alpha_k - \frac{L}{2}\alpha^2_k)
\eta^2_k}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_k} &amp;amp;\le \frac{1}{\eta_{k+1}} - \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot
\frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \cdot
\frac{\eta_k}{\eta_{k+1}} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_k} &amp;amp;\ge \frac{(\alpha_k -
\frac{L}{2}\alpha^2_k)}{||x_0 - x^\star||^2_2} \text{, by } \eta_k \ge
\eta_{k+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k (\frac{1}{\eta_{i+1}} - \frac{1}{\eta_i}) &amp;amp;\ge
\sum_{i=0}^k \frac{(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 -
x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} - \frac{1}{\eta_0} &amp;amp;\ge \frac{\sum_{i=0}^k
(\alpha_i - \frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
\frac{1}{\eta_{k+1}} &amp;amp;\ge \frac{\sum_{i=0}^k (\alpha_i -
\frac{L}{2}\alpha^2_i)}{||x_0 - x^\star||^2_2} \\
f(x_{k+1} - f(x^\star)) &amp;amp;\le \frac{||x_0 -
x^\star||^2_2}{\sum_{i=0}^k (\alpha_i - \frac{L}{2}\alpha^2_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i =
\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha^2_i
&amp;lt; \infty\)&lt;/span&gt;, the Variant Step Size can be shown to converge.
Particularly when &lt;span class=&#34;math inline&#34;&gt;\(\alpha_k =
\frac{1}{k}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
f(x_{k+1} - f(x^\star)) \le \Theta(\frac{1}{\log k})||x_0 -
x^\star||^2_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-with-strong-convexity&#34;&gt;Convergence with Strong
Convexity&lt;/h3&gt;
&lt;h4 id=&#34;strong-convexity&#34;&gt;Strong Convexity&lt;/h4&gt;
&lt;p&gt;A function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly convex
if there exists a &lt;span class=&#34;math inline&#34;&gt;\(l &amp;gt; 0\)&lt;/span&gt; such
that &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2,
\forall x,y \in \mathrm{dom}(f)
\]&lt;/span&gt; Further, strong convexity is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(f(x) - \frac{l}{2}||x||^2_2\)&lt;/span&gt; being
convex.&lt;/p&gt;
&lt;p&gt;With strong convexity, we can bound &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; from below with a convex quadratic
approximation.&lt;/p&gt;
&lt;h4 id=&#34;convergence-rate-1&#34;&gt;Convergence Rate&lt;/h4&gt;
&lt;h5 id=&#34;fixed-step-size-1&#34;&gt;Fixed Step Size&lt;/h5&gt;
&lt;p&gt;We have assumed that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth above. That is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{sm} f(y) \le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y -
x||^2_2, \forall x,y \in \mathrm{dom}(f)
\end{equation}
\]&lt;/span&gt; In this section we further assume that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smooth as well as strong convex: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{sc} f(y) \ge f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y -
x||^2_2, \forall x,y \in \mathrm{dom}(f)
\end{equation}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(F(y) = f(x) + (y - x)^T \nabla
f(x) + \frac{l}{2}||y - x||^2_2\)&lt;/span&gt;. Take derivative of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla f(x) + l(y - x) = 0 \\
y_0 - x = -\frac{\nabla f(x)}{l}
\end{aligned}
\]&lt;/span&gt; Because the second derivative of &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(l
\succeq 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; reaches
global minimum at the local minima &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt;. Substitute &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; back to &lt;span class=&#34;math inline&#34;&gt;\(F(y)\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
f(y) \ge F(y) \ge F(y_0) = f(x) - \frac{1}{2l}||\nabla f(x)||^2_2
\]&lt;/span&gt; This holds when &lt;span class=&#34;math inline&#34;&gt;\(y =
x^\star\)&lt;/span&gt;. That is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) &amp;amp;\ge f(x) - \frac{1}{2l}||\nabla f(x)||^2_2 \\
||\nabla f(x)||^2_2 &amp;amp;\ge 2l(f(x) - f(x^\star))
\end{aligned}
\]&lt;/span&gt; The above inequality is referred to as Polyak-Lojasiewicz
inequality. By combining it with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt;, where we implicitly assume
a fixed step-size &lt;span class=&#34;math inline&#34;&gt;\(\alpha =
\frac{1}{L}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) &amp;amp;\le f(x_k) - \frac{1}{2L}||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) +
\frac{1}{2L}(-||\nabla f(x_k)||^2_2) \\
f(x_{k+1}) - f(x^\star) &amp;amp;\le f(x_k) - f(x^\star) +
\frac{1}{2L}(-2l(f(x) - f(x^\star))) \\
&amp;amp;= (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x_{k+1}) - f(x^\star) &amp;amp;\le (1 - \frac{l}{L}) (f(x_k) - f(x^\star))
\\
&amp;amp;\le (1 - \frac{l}{L})(1 - \frac{l}{L}) (f(x_{k-1}) - f(x^\star)) \\
&amp;amp;\le \dots \\
&amp;amp;\le (1 - \frac{l}{L})^{k+1} (f(x_0) - f(x^\star))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sm}\)&lt;/span&gt; and
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{sc}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x) + (y - x)^T \nabla f(x) + \frac{l}{2}||y - x||^2_2 \le f&amp;amp;(y)
\le f(x) + (y - x)^T \nabla f(x) + \frac{L}{2}||y - x||^2_2 \\
l &amp;amp;\le L \\
\end{aligned}
\]&lt;/span&gt; Usually it would be the case that &lt;span class=&#34;math inline&#34;&gt;\(l &amp;lt; L\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; 1 - \frac{l}{L} &amp;lt; 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;non-convex-case&#34;&gt;Non-convex Case&lt;/h3&gt;
&lt;p&gt;Without convexity condition, we can still exploit the property of
&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;-smoothness. Sum up on both sides
of equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(k = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=0}^k f(x_{k+1}) &amp;amp;\le \sum_{i=0}^k f(x_k) -
\frac{1}{2L}||\nabla f(x_k)||^2_2 \\
\sum_{i=0}^k f(x_{k+1}) - f(x_k) &amp;amp;\le -\frac{1}{2L} \sum_{i=0}^k
||\nabla f(x_k)||^2_2 \\
f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k ||\nabla
f(x_k)||^2_2
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(x^\star\)&lt;/span&gt; is a
local minima, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x^\star) \le f(x_{k+1}) &amp;amp;\le f(x_0) - \frac{1}{2L}\sum_{i=0}^k
||\nabla f(x_k)||^2_2 \\
f(x^\star) - f(x_0) &amp;amp;\le - \frac{1}{2L}\sum_{i=0}^k ||\nabla
f(x_k)||^2_2 \\
\sum_{i=0}^k ||\nabla f(x_k)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
\end{aligned}
\]&lt;/span&gt; In this case, the error is measured by the first derivative.
Suppose &lt;span class=&#34;math inline&#34;&gt;\(\tilde x = \arg \min_i ||\nabla
f(x_i)||^2_2\)&lt;/span&gt;, where the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the closest to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k||\nabla f(\tilde x)||^2_2 &amp;amp;\le 2L(f(x_0) - f(x^\star)) \\
||\nabla f(\tilde x)||^2_2 &amp;amp;\le \frac{2L(f(x_0) - f(x^\star))}{k} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Convergence%20of%20Gradient%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Convergence
of Gradient Descent.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rkganti.wordpress.com/2015/08/21/convergence-rate-of-gradient-descent-algorithm/&#34;&gt;Convergence
rate of gradient descent algorithm | bps/Hz (wordpress.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xingyuzhou.org/blog/notes/strong-convexity&#34;&gt;Strong
convexity · Xingyu Zhou’s blog&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>事件与概率</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/</link>
      <pubDate>Fri, 09 Dec 2022 17:37:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/</guid>
      <description>

&lt;h2 id=&#34;事件的运算&#34;&gt;事件的运算&lt;/h2&gt;
&lt;h3 id=&#34;事件的包含和相等&#34;&gt;事件的包含和相等&lt;/h3&gt;
&lt;p&gt;同一试验下的两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，如果&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;发生时B也发生，则称&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;包含&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(A
\subseteq B\)&lt;/span&gt;。如果&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;互相包含，则二者相等。&lt;/p&gt;
&lt;h3 id=&#34;事件的互斥和对立&#34;&gt;事件的互斥和对立&lt;/h3&gt;
&lt;p&gt;若两事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;不能再同一试验中同时发生，则称它们是互斥的。互斥事件的一个特例是对立事件，对于事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，其对立事件&lt;span class=&#34;math inline&#34;&gt;\(\bar A\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\{
\text{$A$不发生} \}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;事件的和并与加法定理&#34;&gt;事件的和（并）与加法定理&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$发生，或$B$发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的和，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A + B\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;概率论中的&lt;strong&gt;加法定理&lt;/strong&gt;描述的则是：若干互斥事件之和的概率，等于各事件的概率之和。&lt;/p&gt;
&lt;h4 id=&#34;全概率公式&#34;&gt;全概率公式&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(B_1, B_2,
\cdots\)&lt;/span&gt;为有限或无限个事件，它们两两互斥且在每次实验中至少发生一个（mutually
exclusive and collectively exhaustive），即 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_i B_j = \emptyset (i \ne j) \\
B_1 + B_2 + \cdots = \Omega \\
P(B_1 + B_2 + \cdots) = P(\Omega) = 1
\end{gather}
\]&lt;/span&gt; 根据事件运算的性质，&lt;span class=&#34;math inline&#34;&gt;\(A = A \Omega =
A B_1 + A B_2 + \cdots\)&lt;/span&gt;，由于&lt;span class=&#34;math inline&#34;&gt;\(B_1,
B_2, \cdots\)&lt;/span&gt;两两互斥，则显然&lt;span class=&#34;math inline&#34;&gt;\(A B_1, A
B_2, \cdots\)&lt;/span&gt;也两两互斥，故依据加法定理，有 &lt;span class=&#34;math display&#34;&gt;\[
P(A) = \sum_i P(A B_i) = P(A B_1) + P(A B_2) + \cdots
\]&lt;/span&gt; 再根据条件概率公式，有 &lt;span class=&#34;math display&#34;&gt;\[
P(A) = \sum_i P(B_i) P(A | B_i) = P(B_1) P(A|B_1) + P(B_2) P(A|B_2) +
\cdots
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这便是全概率公式。&lt;/p&gt;
&lt;h3 id=&#34;事件的积交&#34;&gt;事件的积（交）&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$，$B$都发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的积，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A B\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;事件的差&#34;&gt;事件的差&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，定义新事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(C = \{
\text{$A$发生，$B$不发生} \}\)&lt;/span&gt;，则称事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的差，记作&lt;span class=&#34;math inline&#34;&gt;\(C
= A - B\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我们对事件引入了和、积、差、对立运算。显然，这些运算符在数字运算中成立的运算规律，不一定对事件运算成立，比如说&lt;span class=&#34;math inline&#34;&gt;\(A + A = A\)&lt;/span&gt;而非&lt;span class=&#34;math inline&#34;&gt;\(2 A\)&lt;/span&gt;（无意义），&lt;span class=&#34;math inline&#34;&gt;\(A A = A\)&lt;/span&gt;而非&lt;span class=&#34;math inline&#34;&gt;\(A^2\)&lt;/span&gt;（无意义），&lt;span class=&#34;math inline&#34;&gt;\((A - B) + B = A + B\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;基于这些基本运算，我们也可以表示出更多的事件，比如：&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 72%&#34;/&gt;
&lt;col style=&#34;width: 27%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;表达式&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;含义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(ABC\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者同时发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A B \bar C +
A B \bar C + \bar A B C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者有且仅有两件发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \bar B
\bar C + \bar A B \bar C + \bar A \bar B C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者有且仅有一件发生&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(A + B +
C\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;三者至少其中之一发生&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;条件概率与独立性&#34;&gt;条件概率与独立性&lt;/h2&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(P(B) \ne
0\)&lt;/span&gt;，记&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)\)&lt;/span&gt;为“在给定&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;发生的条件下&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;的条件概率”，则 &lt;span class=&#34;math display&#34;&gt;\[
P(A|B) = \frac{P(AB)}{P(B)}
\]&lt;/span&gt; 在计数测度之下，这个式子很好证明。记&lt;span class=&#34;math inline&#34;&gt;\(M_A\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M_B\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M_{AB}\)&lt;/span&gt;为使得事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;同时发生的基本事件个数，记&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;为所有基本事件个数，则 &lt;span class=&#34;math display&#34;&gt;\[
P(A|B) = \frac{M_{AB}}{M_B} = \frac{M_{AB}/M}{M_B/M} =
\frac{P(AB)}{P(B)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;两个事件的独立&#34;&gt;两个事件的独立&lt;/h3&gt;
&lt;p&gt;设两个事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)\)&lt;/span&gt;可能是有差异的，这个差异便反映了二者之间的关联。而如果&lt;span class=&#34;math inline&#34;&gt;\(P(A) =
P(A|B)\)&lt;/span&gt;，则称这两事件独立。根据条件概率的定义，两事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;独立时，有 &lt;span class=&#34;math display&#34;&gt;\[
P(AB) = P(A) P(B)
\]&lt;/span&gt;
我们往往是根据事件属性，而非根据以上公式，来确定事件的独立性。比如说在连续抛两次硬币的试验中，这两次试验的结果之间确实不应该有什么关联，我们自然而然地认为它们之间是相互独立的（&lt;strong&gt;相互独立同分布&lt;/strong&gt;）。&lt;/p&gt;
&lt;p&gt;有时我们会觉得所有的独立事件可能都来自于这样的多次相互独立同分布试验，但其实独立事件也可以来自一次试验，比如说从52张扑克牌中随机抽取一张，记事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为抽中红桃花色、事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;为抽中数字6，则可以验证，这两个事件也是相互独立的。&lt;/p&gt;
&lt;h3 id=&#34;相互独立与乘法定理&#34;&gt;相互独立与乘法定理&lt;/h3&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
\cdots\)&lt;/span&gt;为有限或无限个事件，如果从其中任意取出有限个&lt;span class=&#34;math inline&#34;&gt;\(A_{i_1}, A_{i_2}, \cdots, A_{i_m}\)&lt;/span&gt;，都有
&lt;span class=&#34;math display&#34;&gt;\[
P(A_{i_1} A_{i_2} \cdots A_{i_m}) = P(A_{i_1}) P(A_{i_2}) \cdots
P(A_{i_m})
\]&lt;/span&gt; 则称事件&lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
\cdots\)&lt;/span&gt;相互独立。概率论中的&lt;strong&gt;乘法定理&lt;/strong&gt;描述的是：多个事件相互独立时，它们同时发生的概率，等于各自概率的乘积。&lt;/p&gt;
&lt;p&gt;需要注意的是，多个事件之间两两独立并不意味着它们相互独立，比如在掷两次硬币的实验中，定义以下事件：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
A：第一次为正， P(A) = 1/2 \\
B：第二次为反， P(B) = 1/2 \\
C：两次结果相同， P(C) = 1/2
\end{gathered}
\]&lt;/span&gt; 则可以轻易验证三者两两独立，但是&lt;span class=&#34;math inline&#34;&gt;\(P(ABC) = 0 \ne 1/8 = P(A) P(B)
P(C)\)&lt;/span&gt;。这三者的关系有如下图中的三个环：两两之间本可以相互分开（独立），但三者同在，便互相捆绑住了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./borromean-rings-illusion.png&#34; style=&#34;max-width: 30%;&#34;/&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mutual Information</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</link>
      <pubDate>Thu, 19 May 2022 12:20:04 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</guid>
      <description>

&lt;p&gt;Mutual information of &lt;strong&gt;two random variables&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a measure of the mutual independence
between them. It quantifies the amount of information obtained about one
random variable by observing the other random variable. It is defined as
&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]&lt;/span&gt; By its definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I(X;Y) = I(Y;X) \\
I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \otimes p_Y) \\
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;To better illustrate the formula of mutual information between two &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian-distributed&lt;/a&gt; random
variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. We can concatenate them to form, say
an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional random variable
&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, which is also
Gaussian-distributed. Then the mutual information between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be computed as: &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det
\Sigma_Z}
\]&lt;/span&gt; The key to the derivation is that mutual information is the
KL-divergence between the joint distribution and the product of the
marginal distributions.&lt;/p&gt;
&lt;p&gt;The joint can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X:Y} = N(\underbrace{\mu_X:\mu_Y}_\mu,
\underbrace{
\begin{bmatrix} \
\Sigma_{X} &amp;amp; \Cov_{XY} \\
\Cov_{YX} &amp;amp; \Sigma_{Y} \\
\end{bmatrix}
}_\Sigma
)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The product of marginals can be described as &lt;span class=&#34;math display&#34;&gt;\[
p_{X} \times p_{Y} = N(\mu_x:\mu_y,
\begin{bmatrix} \
\Sigma_{xx} &amp;amp; 0 \\
0 &amp;amp; \Sigma_{yy} \\
\end{bmatrix}
)
\]&lt;/span&gt; The probability density function of an &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;-dimensional Gaussian distribution
is &lt;span class=&#34;math inline&#34;&gt;\(p(x&amp;#39;) = \frac{1}{\sqrt{|2\pi
\Sigma&amp;#39;|}}e^{-\frac{1}{2}(x&amp;#39;-\mu&amp;#39;)^T\Sigma&amp;#39;^{-1}(x&amp;#39;-\mu&amp;#39;)}\)&lt;/span&gt;.
The entropy of this Gaussian distribution is &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 n&amp;#39; + \frac 1 2 \log
|2\pi\Sigma&amp;#39;|\)&lt;/span&gt;. In view of above, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I&amp;amp;(X;Y) = D_{KL}(p_{X:Y} || p_X \times p_Y)
= \int p_{X:Y}(\underbrace{x:y}_z) \log \frac{p_{X:Y}(x:y)} {p_X(x)
p_{Y}(y)} \d z \\
&amp;amp;= \int p_{X:Y}(\underbrace{x:y}_z) \log p_{X:Y}(x:y) \d z -
    \int p_{X:Y}(\underbrace{x:y}_z) \log p_X(x) \d z \\
&amp;amp;\quad\quad\quad -\int p_{X:Y}(\underbrace{x:y}_z) \log p_Y(y) \d z
\\
&amp;amp;= \int p_{Z}(z) \log p_{Z}(z) \d z -
    \int p_{X}(x) \log p_X(x) \d x -
    \int p_{Y}(y) \log p_Y(y) \d y \\
&amp;amp;= -(\log \sqrt{\det(2\pi \Sigma)} + \frac n 2) +
    (\log \sqrt{\det(2\pi \Sigma_{X}}) + \frac {n_X} 2) \\
&amp;amp;\quad\quad\quad +(\log \sqrt{\det(2\pi \Sigma_{Y}}) + \frac {n_Y}
2) \\    
&amp;amp;= \frac 1 2 \log \frac{\det \Sigma_X \det \Sigma_Y}{\det \Sigma_Z}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;kronecker-gaussian&#34;&gt;Kronecker Gaussian&lt;/h4&gt;
&lt;p&gt;Consider the multivariate Gaussian distribution random vector &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; of the same length &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. Suppose they are both independent
internally and they have the component-wise correlation &lt;span class=&#34;math inline&#34;&gt;\(corr(X_i, Y_j) = \delta_{ij} \rho\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(\rho \in (-1, 1)\)&lt;/span&gt; (open to ensure
the covariance matrix is invertible), &lt;span class=&#34;math inline&#34;&gt;\(1 \le
i, j \le k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\delta_{ij}\)&lt;/span&gt; is
the Kronecker’s delta: &lt;span class=&#34;math display&#34;&gt;\[
\delta_{ij} =
\begin{cases}
0, &amp;amp; i \ne j \\
1, &amp;amp; i = j
\end{cases}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z_k\)&lt;/span&gt; be the vector
concatenated by &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;. It is easy to draw its covariance
matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{Z_k}\)&lt;/span&gt; like &lt;span class=&#34;math display&#34;&gt;\[
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{pmatrix}
\]&lt;/span&gt; The mutual information between the &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \frac 1 2 \log \frac{\det \Sigma_{X_k} \det \Sigma_{Y_k}}{\det
\Sigma_{Z_k}} = -\frac 1 2 \log \det \Sigma_{Z_k}
\]&lt;/span&gt; The problem remains as how to compute &lt;span class=&#34;math inline&#34;&gt;\(\det \Sigma_{Z_{k}}\)&lt;/span&gt;. After applying the
Laplacian expansion along the first column, it remains to deal with the
determinants of following two matrices (dashed lines rule out the
row/column to be deleted): &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
A_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right)
}_\text{$2k$ columns},
B_k = \underbrace{
\left( \begin{array}{c:ccccccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right)
}_\text{$2k$ columns}, \\
\det Z_k =
\left.
\begin{cases}
\det A_k - \rho \det B_k, &amp;amp; \text{$k$ is odd} \\
\det A_k + \rho \det B_k, &amp;amp; \text{$k$ is even} \\
\end{cases}
\right\}
\Rightarrow \det Z_k = \det A_k + (-1)^k \rho \det B_k
\end{gather}
\]&lt;/span&gt; Applying the Laplacian expansion along the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th row of &lt;span class=&#34;math inline&#34;&gt;\(A_k\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\det A_k =
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right|
}_\text{$2k$ columns}
= \det Z_{k-1}
\]&lt;/span&gt; Applying the Laplacian expansion along the first row of &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\det B_k = (-1)^k \rho
\underbrace{
\left| \begin{array}{c:ccc:c:ccc}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; \rho &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho
\\
\hdashline
\rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\\
\hdashline
0 &amp;amp; \rho  &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0
\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho  &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1
\\
\end{array} \right|
}_\text{$2k$ columns}
= (-1)^k \rho \det Z_{k-1}
\]&lt;/span&gt; In all, &lt;span class=&#34;math inline&#34;&gt;\(\det Z_k = \det Z_{k-1} -
(-1)^{2k} \rho^2 \det Z_{k-1} = (1 - \rho^2) Z_{k-1}\)&lt;/span&gt;. Because
&lt;span class=&#34;math inline&#34;&gt;\(\det Z_1 = 1 - \rho^2\)&lt;/span&gt;, we finally
have &lt;span class=&#34;math display&#34;&gt;\[
\det Z_k = (1 - \rho^2)^k
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = -\frac 1 2 \log \det \Sigma_{Z_k} = -\frac k 2 \log (1 -
\rho^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/438607/mutual-information-between-subsets-of-variables-in-the-multivariate-normal-distr&#34;&gt;Mutual
information between subsets of variables in the multivariate normal
distribution - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.nyu.edu/~kleeman/infolect7.pdf&#34;&gt;Information
Theory and Predictability Lecture 7: Gaussian Case&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</link>
      <pubDate>Fri, 07 Jan 2022 12:52:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/</guid>
      <description>

&lt;p&gt;Support vector machine is used in binary classification task. It aims
to find a &lt;strong&gt;linear hyperplane&lt;/strong&gt; that separates the data
with different labels with the maximum margin. By symmetry, there should
be at least one margin point on both side of the decision boundary.
These points are called &lt;strong&gt;support vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hard-margin-svm&#34;&gt;Hard-margin SVM&lt;/h3&gt;
&lt;p&gt;Suppose the data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)},
y^{(i)}), i=1, \dots, M\}\)&lt;/span&gt;, and is linearly separable. The
separation hyperplane will be in the form of &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = 1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is above the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;gt; 0\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} = -1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is below the hyperplane (&lt;span class=&#34;math inline&#34;&gt;\(w^Tx^{(i)} + b &amp;lt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The distance (margin) from a data point to the separation hyperplane
will be &lt;span class=&#34;math display&#34;&gt;\[
d^{(i)} = \frac{|w^Tx^{(i)} + b|}{||w||_2} = \frac{y^{(i)}(w^Tx^{(i)} +
b)}{||w||_2}\\
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d^{(i)}\)&lt;/span&gt; is called the
&lt;strong&gt;geometric distance&lt;/strong&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(|w^Tx^{(i)} + b|\)&lt;/span&gt; is called the
&lt;strong&gt;functional distance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;margin&lt;/strong&gt; of the hyperplane &lt;span class=&#34;math inline&#34;&gt;\(w^Tx + b = 0\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[
d = \min\limits_{i \in \{1,\dots,M\}} d^{(i)} = \min\limits_{i \in
\{1,\dots,M\}} \frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; The maximum margin solution is found by solving &lt;span class=&#34;math display&#34;&gt;\[
\max\limits_{w,b} \min\limits_{i \in \{1,\dots,M\}}
\frac{y^{(i)}(w^Tx^{(i)}+b)}{||w||_2}
\]&lt;/span&gt; Suppose &lt;span class=&#34;math inline&#34;&gt;\((w^\prime,
b^\prime)\)&lt;/span&gt; is one solution to the above. Then &lt;span class=&#34;math inline&#34;&gt;\((\lambda w^\prime, \lambda b^\prime)\)&lt;/span&gt; is
also a solution, because &lt;span class=&#34;math display&#34;&gt;\[
\frac{y^{(i)}((\lambda w^{\prime})^Tx^{(i)} + \lambda
b^\prime)}{||\lambda w^\prime||_2} = \frac{\lambda
y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{\lambda ||w^\prime||_2} =
\frac{y^{(i)}((w^{\prime})^Tx^{(i)} + b^\prime)}{||w^\prime||_2}
\]&lt;/span&gt; There is an extra degree of freedom in this problem.
Therefore, we may impose &lt;span class=&#34;math display&#34;&gt;\[
\min\limits_{i \in \{1,\dots,M\}} y^{(i)}(w^Tx^{(i)}+b) = 1
\]&lt;/span&gt; to consume this freedom. Consequently, the problem becomes
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max \frac{1}{||w||_2} \iff \min \frac{1}{2}||w||^2_2 \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1, i = 1,\dots,M
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-margin-svm&#34;&gt;Soft-margin SVM&lt;/h3&gt;
&lt;p&gt;It may not happen that the real data are linearly-separable, or there
may exist noisy samples that disrupt this linear separability. In such
case, we may allow some samples to violate the margin. We define some
slack variables &lt;span class=&#34;math inline&#34;&gt;\(\xi_i \ge 0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\xi_i &amp;gt; 0\)&lt;/span&gt; means that the sample is
inside the margin (or even this sample will be misclassified), &lt;span class=&#34;math inline&#34;&gt;\(\xi_i = 0\)&lt;/span&gt; means that the sample is
outside the margin.&lt;/p&gt;
&lt;p&gt;Of course, these slack variables should be as small as possible. Then
the problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\min \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i \\
s.t.\quad y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, \xi_i \ge 0,
i=1,\dots,M
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is a penalty factor on
violating the margin. To some extent, it avoid overfitting.&lt;/p&gt;
&lt;p&gt;To solve this, first convert the problem into the standard form:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \frac{1}{2}||w||^2_2 &amp;amp;+ C\sum_{i=1}^M\xi_i \\
s.t.\quad 1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b) &amp;amp;\le 0, i=1,\dots,M \\
-\xi_i &amp;amp;\le 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; This problem gives a strong duality. The solution will satisfy
the KKT conditions. We first write down the Lagrangian: &lt;span class=&#34;math display&#34;&gt;\[
L(w,b,\xi,\lambda,\mu) = \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M\xi_i +
\sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}(w^Tx^{(i)} + b)) -
\sum_{i=1}^M\mu_i\xi_i
\]&lt;/span&gt; Then we form the dual problem: &lt;span class=&#34;math display&#34;&gt;\[
\max_{\lambda,\mu;\lambda_i,\mu_i \ge 0}
\min_{w,b,\xi}L(w,b,\xi,\lambda,\mu)
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial L}{\partial w} = w - \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)},
\frac{\partial^2 L}{\partial w\partial w} = I \succeq 0 \\
\frac{\partial L}{\partial b} = -\sum_{i=1}^M\lambda_iy^{(i)},
\frac{\partial^2 L}{\partial b\partial b} = 0 \succeq 0 \\
\frac{\partial L}{\partial \xi} = C - \lambda - \mu, \frac{\partial^2
L}{\partial \xi\partial \xi} = 0 \succeq 0
\end{gather}
\]&lt;/span&gt; The Hessian matrices of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; are positive semi-definite.
Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min\limits_{w,b,\xi}L(w,\xi,\lambda,\mu)\)&lt;/span&gt;
is obtained at its local minimum, i.e. where its first-order derivative
meets &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
w = \sum_{i=1}^M\lambda_iy^{(i)}x^{(i)} \\
\sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda + \mu = C
\]&lt;/span&gt; Substitute above back to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; to transform the dual problem into
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}
\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + C\sum_{i=1}^M\xi_i -
\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i(1 - \xi_i - y^{(i)}b) +
\sum_{i=1}^M(\lambda_i - C)\xi_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
\lambda_i,\mu_i \ge 0, i=1,\dots,M \\
\Downarrow \\
\max_{\lambda,\mu}D(\lambda,\mu) =
-\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; Since we know the optimal solution exists, instead of taking
derivative of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to solve it, we assume the
solution to above problem is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
w^\star = \sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} \\
\mu^\star = C - \lambda^\star
\]&lt;/span&gt; Complementary constraints will give &lt;span class=&#34;math display&#34;&gt;\[
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} +
b^\star)) = 0 \\
\mu_i\xi_i = 0
\]&lt;/span&gt; There is at least one &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j &amp;gt; 0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(w^\star =
\sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)} = 0\)&lt;/span&gt;, which means the
primal constraints &lt;span class=&#34;math display&#34;&gt;\[
1 - \xi_i - y^{(i)}((w^\star)^Tx^{(i)} + b^\star) =  1 - \xi_i -
y^{(i)}b^\star\le 0
\]&lt;/span&gt; won’t hold no matter what value &lt;span class=&#34;math inline&#34;&gt;\(b^\star\)&lt;/span&gt; takes. This specific &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_j\)&lt;/span&gt;’s slackness complementary
will give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
0 &amp;amp;= (1 - \xi^\star_j - y^{(j)}((w^\star)^Tx^{(j)} + b^\star)) \\
b^\star &amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} - (w^\star)^Tx^{(j)} \\
&amp;amp;= \frac{1 - \xi^\star_j}{y^{(j)}} -
\sum_{i=1}^M\lambda^\star_iy^{(i)}x^{(i)}\cdot x^{(j)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
According to the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt; (not know yet), added
with primal constraints and the complementary constraints, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0 \\
\xi_i \ge 0 \\
\lambda^\star_i(1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} +
b^\star)) = 0 \\
\mu^\star_i\xi^\star_i = 0 \\
\lambda^\star_i + \mu^\star_i = C
\end{gather}
\]&lt;/span&gt; we can make interpretations on the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star_i\)&lt;/span&gt;: $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\lambda^\star_i = 0 &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i = C \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \ge 1 \\
0 &amp;lt; \lambda^\star_i &amp;lt; C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i &amp;gt; 0 \Rightarrow \xi^\star_i = 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 1 \\
\lambda^\star_i = C &amp;amp;\Rightarrow
\begin{cases}
\mu^\star_i = 0 \Rightarrow \xi^\star_i \ge 0 \\
1 - \xi^\star_i - y^{(i)}((w^{\star T})x^{(i)} + b^\star) = 0
\end{cases}
\Rightarrow y^{(i)}((w^{\star T})x^{(i)} + b^\star) \le 1 \\

\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ They corresponds to cases where sample &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is outside margin, is on the
margin, violates the margin, respectively.&lt;/p&gt;
&lt;p&gt;The key to the above derivation is &lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt;, which is to be solved in
Solving SVM section.&lt;/p&gt;
&lt;p&gt;Soft Margin SVM is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\min \frac{1}{C}||w||^2_2 + \sum_{i=1}^M\max(0, 1 - y^{(i)}(w^Tx^{(i)} +
b))
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l_h(z) = \max(0, 1 -
z)\)&lt;/span&gt; is called Hinge loss.&lt;/p&gt;
&lt;h3 id=&#34;kernel-svm&#34;&gt;Kernel SVM&lt;/h3&gt;
&lt;p&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \cdot x^{(j)}\)&lt;/span&gt;
in &lt;span class=&#34;math inline&#34;&gt;\(D(\lambda,\mu)\)&lt;/span&gt; is the inner
product between two samples. We can make a table storing these inner
products. This naturally introduces the kernel trick, which means we can
manipulate the entry in this table by remapping the &lt;span class=&#34;math inline&#34;&gt;\((x^{(i)}, x^{(j)})\)&lt;/span&gt; so that the entry
represents the inner product of some higher-dimensional features. &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x^{(i)}, x^{(j)}) = \phi(x^{(i)}) \cdot \phi(x^{(j)})
\]&lt;/span&gt; This saves us from explicitly defining the mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a higher-dimensional feature. This
also saves the time of the computation of the inner products of these
higher-dimensional features, than that of transform-then-inner-product
method.&lt;/p&gt;
&lt;p&gt;SVM is a linear classifier, which means its decision boundary is a
linear hyperplane in input feature space. By applying kernel trick, we
implicitly map the input feature to a higher dimensional one. Therefore
the decision boundary would become a linear hyperplane in this
higher-dimensional space: &lt;span class=&#34;math display&#34;&gt;\[
w_\phi\phi(x) + b_\phi = 0
\]&lt;/span&gt; And thus this hyperplane is not linear in original feature
space.&lt;/p&gt;
&lt;h4 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input feature &lt;span class=&#34;math inline&#34;&gt;\(x = [x_1,\dots,x_N] \in
\R^N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order polynomial: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x, x^\prime) = (x^Tx^\prime)^p =
(\sum_{i=1}^Nx_ix^\prime_i)^p
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
K(x,x^\prime) = (\sum_{i=1}^Nx_ix^\prime_i)^2 =
(\sum_{i=1}^N\sum_{j=1}^Nx_ix_jx^\prime_ix^\prime_j)^2 =
\phi(x)\phi(x^\prime)
\]&lt;/span&gt; The transformation applied on original input feature will be
&lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = [x_1x_1,x_1x_2,\dots,x_1x_N,x_2x_1,\dots
x_2x_N,\dots,x_Nx_1,\dots,x_Nx_N] \in \R^{N^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kernel between two features is similar to a Gaussian &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = e^{-\gamma||x - x^\prime||^2_2}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a
hyper-parameter to be determined.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This kernel is the case where it is hard to define the
transformation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;
explicitly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-svm&#34;&gt;Solving SVM&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda^\star\)&lt;/span&gt; is the key to the
final solution and by far it is still remained unsolved: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\lambda,\mu}D(\lambda,\mu) =
-\frac{1}{2}\sum_{i=1}^M\sum_{j=1}^M\lambda_i\lambda_jy^{(i)}y^{(j)}x^{(i)}\cdot
x^{(j)} + \sum_{i=1}^M\lambda_i \\
s.t.\quad \sum_{i=1}^M\lambda_iy^{(i)} = 0 \\
0 &amp;lt; \lambda_i &amp;lt; C, i=1,\dots,M \\
\end{gather}
\]&lt;/span&gt; We can attack it by sequential minimal optimization.&lt;/p&gt;
&lt;h3 id=&#34;multi-class-svm&#34;&gt;Multi-class SVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 vs. 1&lt;/p&gt;
&lt;p&gt;Train 1-vs-1 SVM for all pairs of classes. Then decide by majority
voting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1 vs. rest&lt;/p&gt;
&lt;p&gt;Train 1-vs-rest SVM for all classes (&lt;span class=&#34;math inline&#34;&gt;\(y =
1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; belongs to the
“1”). Choose the class with the largest geometric margin.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>Coordinate Descent</title>
      <link>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</link>
      <pubDate>Mon, 03 Jan 2022 14:50:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/coordinate-descent/</guid>
      <description>

&lt;h3 id=&#34;convex-and-differentiable-function&#34;&gt;Convex and Differentiable
Function&lt;/h3&gt;
&lt;p&gt;Given a convex, differentiable &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n
\mapsto \R\)&lt;/span&gt;, if we are at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is minimized along each coordinate
axis, have we found a global minima? That is, does &lt;span class=&#34;math display&#34;&gt;\[
\forall d, i,f(x + d \cdot e_i) \ge f(x) \Rightarrow f(x) = \min_zf(z),
\text{ where $e_i$ is the $i$-th standard basis ?}
\]&lt;/span&gt; The answer is yes. This is because &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(x) = \left[
\begin{array} \\
\frac{\partial f}{\partial x_1},
\cdots,
\frac{\partial f}{\partial x_n}
\end{array}
\right]
= 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;convex-but-non-differentiable-function&#34;&gt;Convex but
Non-differentiable Function&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is only convex but not
differentiable, the above will not necessarily hold. However, if &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; can be decomposed such that &lt;span class=&#34;math display&#34;&gt;\[
f(x) = g(x) + \sum_{i=1}^nh_i(x_i)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; is convex and
differentiable, and each &lt;span class=&#34;math inline&#34;&gt;\(h_i(x_i)\)&lt;/span&gt;
is convex but possibly non-differentiable, the global minima still
holds. For any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;= g(y) - g(x) + \sum_{i=1}^nh_i(y_i) -
\sum_{i=1}^nh_i(x_i) \\
&amp;amp;\ge \nabla g(x)^T(y - x) + \sum_{i=1}^n[h_i(y_i) - h_i(x_i)] \\
&amp;amp;= \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) - h_i(x_i)]
\end{aligned}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; obtains
minimum along each axes, for any &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x + d \cdot e_k) &amp;amp;\ge f(x) \\
g(x + d \cdot e_k) + \sum_{i=1,i\ne k}^nh_i(x_i) + h_k(x_k + d) &amp;amp;\ge
g(x) + \sum_{i=1}^nh_i(x_i) \\
g_k(x_k + d) - g_k(x_k) + h_k(x_k + d) - h_k(x_k) &amp;amp;\ge 0 \\
\end{aligned}
\]&lt;/span&gt; By &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/subgradient/#Properties&#34;&gt;the linearity of
subgradient&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial(g_k + h_k) =
\nabla_k g + \partial h_k \Rightarrow -\nabla_k g \in \partial
h_k\)&lt;/span&gt;. That is &lt;span class=&#34;math display&#34;&gt;\[
h_k(y_k) - h_k(x_k) \ge -\nabla_k g(x)(y_k - x_k)
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
f(y) - f(x) = \sum_{i=1}^n[\nabla_i g(x)(y_i - x_i) + h_i(y_i) -
h_i(x_i)] \ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD
&lt;a href=&#34;https://chunxy.github.io/uploads/Coordinate%20Descent.pdf&#34; target=&#34;_blank&#34;&gt;Coordinate
Descent.pdf&lt;/a&gt; ======= ## External Materials
&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; notes&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;Coordinate%20Descent.pdf&#34;&gt;Coordinate Descent.pdf&lt;/a&gt; || &lt;a href=&#34;https://zeyuan.wordpress.com/2016/06/13/coordinate-descent/&#34;&gt;Coordinate
Descent in One Line, or Three if Accelerated | A Butterfly Valley
(wordpress.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>FlatNCE</title>
      <link>https://chunxy.github.io/notes/papers/flatnce/</link>
      <pubDate>Sat, 27 Aug 2022 21:42:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/papers/flatnce/</guid>
      <description>

&lt;p&gt;FlatNCE provides a way to compute the gradient of InfoNCE without
introducing the rounding error when subtracting between two similar
numbers.&lt;/p&gt;
&lt;p&gt;Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(g^\ominus_{ij}\)&lt;/span&gt;
is the affinity score between reference sample &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and negative (noise) sample &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; is the affinity score
between positive sample and itself/its transformation &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the batch index. Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{InfoNCE}\)&lt;/span&gt; the batch estimate
of the loss from InfoNCE: &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{detach}{\mathop{\text{detach}}}
\newcommand{logsumexp}{\mathop{\text{logsumexp}}}
\begin{gather}
\hat l_\text{InfoNCE} = \logsumexp_j g^\ominus_{ij} - g^\oplus_{ii} =
\log (\sum_{j \ne i} \exp g^\ominus_{ij} ) -g^\oplus_{ii} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually, the above is calculated as &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} }
\exp(g^\ominus_{ij} - \max_k g^\ominus_{ik}) ] + \max_k g^\ominus_{ik} -
g^\oplus_{ii}
\]&lt;/span&gt; When the learning saturates, &lt;span class=&#34;math inline&#34;&gt;\(\hat
l_\text{InfoNCE}\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which means &lt;span class=&#34;math inline&#34;&gt;\(\max_k g^\ominus_{ik}\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(g^\oplus_{ii}\)&lt;/span&gt; and thus &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{InfoNCE} = \log[\sum_{j \in {\oplus, \ominus} }
\exp(g^\ominus_{ij} - g^\oplus_{ii}) ] + \underbrace{g^\oplus_{ii} -
g^\oplus_{ii} }_{\text{error-prone}}
\]&lt;/span&gt; A rounding error will very likely happen when &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_of_significance&#34;&gt;subtracting
two near numbers&lt;/a&gt;. Such error will accumulate and fail the InfoNCE.
As said, FlatNCE provides a way to circumvent this rounding error.&lt;/p&gt;
&lt;h2 id=&#34;gradient-perspective&#34;&gt;Gradient Perspective&lt;/h2&gt;
&lt;p&gt;Denote by &lt;span class=&#34;math inline&#34;&gt;\(\hat l_\text{FlatNCE}\)&lt;/span&gt;
the batch estimate of the negative loss from FlatNCE: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat l_\text{FlatNCE} &amp;amp;= \exp [ \logsumexp_{j \ne i} (
g^\ominus_{ij} - g^\oplus_{ii} ) - \detach \logsumexp_{j \ne i} (
g^\ominus_{ij} - g^\oplus_{ii} ) ] \\
&amp;amp;= \frac{\exp \logsumexp_{j \ne i} ( g^\ominus_{ij} - g^\oplus_{ii}
) } {\detach [ \exp \logsumexp_{j \ne i} ( g^\ominus_{ij} -
g^\oplus_{ii} ) ] } \\
&amp;amp;= \frac{\exp \log \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) -
g_\theta (x_i, y_i) ]} {\detach \{\exp \log \sum_{j \ne i} \exp [
g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta
(x_i, y_i) ]} {\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j)
- g_\theta (x_i, y_i) ] \} }
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By putting the positive sample into the contrasting samples, &lt;span class=&#34;math display&#34;&gt;\[
\hat l_\text{FlatNCE}^\oplus =  \frac{1 + \sum_j \exp \big(
g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) \big)} {1 +
\text{detach}[\sum_j \exp \big( g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i,
y_i) \big)]}
\]&lt;/span&gt; where the &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; comes from
adding the positive sample &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; to
the set of negative samples (let’s denote this “negative” sample by
&lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;_0\)&lt;/span&gt;). It can be easily found
that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_\theta \hat l_\text{FlatNCE}^\oplus (g_\theta) = \nabla_\theta
\hat l_\text{InfoNCE} (g_\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may further find that the gradient of FlatNCE is an
importance-weighted estimator of the form &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\nabla_\theta \hat l^\oplus_\text{FlatNCE} &amp;amp;= \frac{\sum_{j \ne i}
\{ \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta (x_i, y_i) ] [\nabla_\theta
g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i, y_i)] \} }
{\detach \{ \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) - g_\theta
(x_i, y_i) ] \} } \\
&amp;amp;= \frac{\sum_{j \ne i} \{ \exp [ g_\theta(x_i, y&amp;#39;_j) ]
[\nabla_\theta g_\theta(x_i, y&amp;#39;_j) - \nabla_\theta g_\theta(x_i,
y_i)] \} } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] } \\
&amp;amp;= \sum_{k \ne i} \left\{ \underbrace {\frac{ \exp [ g_\theta(x_i,
y&amp;#39;_k) ] } { \sum_{j \ne i} \exp [ g_\theta(x_i, y&amp;#39;_j) ] }
}_{w_k} \nabla_\theta g_\theta(x_i, y&amp;#39;_k) \right\} - \nabla_\theta
g_\theta(x_i, y_i) \\
\end{aligned}
\]&lt;/span&gt; As the learning progresses, &lt;span class=&#34;math inline&#34;&gt;\(w_k\)&lt;/span&gt;’s other than &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; will go to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, which will cause the gradient to
vanish.&lt;/p&gt;
&lt;h2 id=&#34;lower-bound-perspective&#34;&gt;Lower-bound Perspective&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{InfoNCE}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\hat l_\text{FlatNCE}\)&lt;/span&gt; are part of the
lower bounds to the mutual information in two methods. Given &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; the positive sample and &lt;span class=&#34;math inline&#34;&gt;\(y_{j&amp;gt;0}\)&lt;/span&gt; are the negative samples,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\label{lemma3.3} \begin{aligned}
-\hat l^{K, \theta}_\text{InfoNCE} &amp;amp;= -\log \{ \frac 1 K \sum_{j
&amp;gt; 0} \exp[g_\theta(x_0,y_j) - g_\theta(x_0,y_0)] \} \\
&amp;amp;= \sup_v (v \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)] - (-1 - \log (-v)) \\
&amp;amp;\Downarrow_{v = -e^{-u}} \\
&amp;amp;\ge  -e^{-u} \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)] - (-1 - \log (e^{-u}) \\
&amp;amp;= 1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0) - u]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; as the primal
critic and &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; as the dual critic.
Since arbitrary choice of &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; lower-bounds the mutual
information, we can either jointly optimize &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt; or more preferably, train in an
iterative fashion. Given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;,
set &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat u(g_\theta) = \log ({\frac 1 K \sum_j \exp[g_\theta(x,y_j) -
g_\theta(x, y)]})
\]&lt;/span&gt; Then we fix &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and only
update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is fixed, the only gradient comes from
&lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt;. Plugin &lt;span class=&#34;math inline&#34;&gt;\(\hat u\)&lt;/span&gt; to the right-hand side of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;1 - u - \frac 1 K \sum_{j &amp;gt; 0} \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0) - u] \\
&amp;amp;= 1 - \log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0, y_0)]}) \notag \\
&amp;amp;\quad - \frac 1 K \frac{\sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0,y_0)}{{\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) -
g_\theta(x_0, y_0)]}} \\
&amp;amp;= -\log ({\frac 1 K \sum_j \exp[g_\theta(x_0,y_j) - g_\theta(x_0,
y_0)]}) \label{obj} \\
&amp;amp;= -\hat l^K_\text{InfoNCE}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which tightly lower-bounds the &lt;span class=&#34;math inline&#34;&gt;\(-\hat
l^K_\text{InfoNCE}\)&lt;/span&gt;. However we update &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lemma3.3}\)&lt;/span&gt; will always hold. The
objective of the whole FlatNCE is to enlarge &lt;span class=&#34;math inline&#34;&gt;\(\eqref{obj}\)&lt;/span&gt; after substituting &lt;span class=&#34;math inline&#34;&gt;\(u = \hat u(g_\theta&amp;#39;)\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(-\hat l^{K, \theta&amp;#39;}_\text{InfoNCE}\)&lt;/span&gt;
can float up.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kexue.fm/archives/8586&#34;&gt;FlatNCE：小批次对比学习效果差的原因竟是浮点误差？
- 科学空间|Scientific Spaces (kexue.fm)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gaussian Distribution</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/</link>
      <pubDate>Sun, 08 May 2022 19:09:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/</guid>
      <description>

&lt;h2 id=&#34;gaussian-distribution&#34;&gt;Gaussian Distribution&lt;/h2&gt;
&lt;h3 id=&#34;one-dimensional&#34;&gt;One-dimensional&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;-d random variable
&lt;span class=&#34;math inline&#34;&gt;\(x \sim N(\mu, \sigma^2)\)&lt;/span&gt;, then its
density function is &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x -
\mu}{\sigma})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To verify that it integrates to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} p(x) \d x &amp;amp;=
\sqrt{(\int_{-\infty}^{+\infty} p(x) \d x) \cdot
(\int_{-\infty}^{+\infty} p(y) \d y)} \\
&amp;amp;= \sqrt {\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x)p(y)
\d x \d y} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I^2 =
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} p(x)p(y) \d x \d
y\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\int_{-\infty}^{+\infty} p(x) \d x = \sqrt{I^2} \\
I^2 = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x - \mu}{\sigma})^2}
\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{y - \mu}{\sigma})^2}
\d x \d y
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = \frac{x-\mu}{\sigma}, v =
\frac{y-\mu}{\sigma}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
\frac{1}{\sigma\sqrt{2\pi}}
e^{-\frac{1}{2}u^2}\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}v^2} \d u
\d v \\
&amp;amp;= \frac{1}{2\pi} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
e^{-\frac{1}{2}(u^2 + v^2)} \d u \d v
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u = r \sin \theta, v = r \cos
\theta\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
e^{-\frac{1}{2}(u^2+v^2)} \d u \d v &amp;amp;=
\int_{0}^{2\pi} \int_{0}^{+\infty} e^{-\frac{1}{2} (r^2 \sin^2\theta +
r^2 \cos^2\theta)} r \d r \d\theta \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} -e^{-\frac{1}{2} r^2}
\d(-\frac{1}{2} r^2) \d\theta \\
&amp;amp;= \int_{0}^{2\pi}-e^{t}\Big|_{t=0}^{t=-\infty}d\theta \\
&amp;amp;= \int_{0}^{2\pi}d\theta \\
&amp;amp;= 2\pi
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(I^2 =
\frac{1}{2\pi}2\pi = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\int_{-\infty}^{+\infty}p(x)dx = \sqrt{I^2} =
1\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independent-standard-n-dimensional&#34;&gt;Independent standard &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z = [Z_1, Z_2, ..., Z_n]^T\)&lt;/span&gt;,
suppose &lt;span class=&#34;math inline&#34;&gt;\(Z_i, Z_j (i,j=1,...,n \and i \ne
j)\)&lt;/span&gt; are independent and &lt;span class=&#34;math inline&#34;&gt;\(Z_i
(i=1,...,n)\)&lt;/span&gt; observes standard Gaussian distribution, we can
derive the joint distribution density function for random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; to be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\newcommand{z}{\mathrm{z}}
p(\z) &amp;amp;= p(z_1, z_2, ..., z_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}
e^{-\frac{1}{2} z_i^2} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2}\z^T\z} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;first-order-correlated-n-dimensional&#34;&gt;First-order correlated
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional&lt;/h3&gt;
&lt;p&gt;We have given the joint distribution function of independent &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional standard Gaussian
distribution. What if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; dimensions
of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; are not standard, are not
independent with each other, but are correlated only in first order?&lt;/p&gt;
&lt;p&gt;We may begin with standard Gaussian random variables &lt;span class=&#34;math inline&#34;&gt;\(X = [X_1, \dots, X_n]\)&lt;/span&gt;. Then we can shift
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and linearly transform it with an
invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(B^{-1}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; mentioned above will just be such a
matrix as &lt;span class=&#34;math inline&#34;&gt;\(Z = B^{-1} (X - \mu)\)&lt;/span&gt; and
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
X = B^{-1}(Z - \mu) \sim \mathcal{N}(0, I) \\
p_X(\x) = \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \x^T\x}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is to take on values
in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Z}\)&lt;/span&gt;, which is a subset
of &lt;span class=&#34;math inline&#34;&gt;\(\R^{n}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{z}{\mathrm{z}} P_Z(Z \in \mathcal{Z}) = \int_\mathcal{Z}
p_Z(\z) \d \z
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(Z = f(X) = BX + \mu\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is invertible, the mapping
&lt;span class=&#34;math inline&#34;&gt;\(X \to Z\)&lt;/span&gt; is one-to-one, therefore
the multivariate &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/jacobian-matrix/&#34;&gt;Jacobian
transformation&lt;/a&gt; is &lt;span class=&#34;math display&#34;&gt;\[
J(X \to Z) = B^{-1} \\
\]&lt;/span&gt; with its determinant &lt;span class=&#34;math inline&#34;&gt;\(J = |J(X \to
Z)| = |B^{-1}| = |B|^{-1}\)&lt;/span&gt;. Note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|J| &amp;amp;= \sqrt{|B|^{-1}|B|^{-1}} \\
&amp;amp;= \sqrt{|B|^{-1}|B^T|^{-1}} \\
&amp;amp;= \sqrt{|BB^T|^{-1}} \\
&amp;amp;= |BB^T|^{-\frac{1}{2}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Z (Z \in \mathcal{Z}) &amp;amp;= P_X (X \in f^{-1}(\mathcal{Z})) \\
P_Z (Z \in \mathcal{Z}) &amp;amp;= \int_{f^{-1}(\mathcal{Z})} p_X(\x) \d \x
\\
&amp;amp;\Downarrow_{\x = f^{-1}(\z)} \\
\int_\mathcal{Z} p_Z (\z) \d \z &amp;amp;= \int_\mathcal{Z} p_X (f^{-1}(\z))
|J| \d \z \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p_Z (\z) = p_X (f^{-1}(\z))|J| = p_X (B^{-1} (\z - \mu) |J| \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2}
(\z-\mu)^T(B^{-1})^T B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T
(B^T)^{-1} B^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} (\z-\mu)^T
(BB^T)^{-1} (\z-\mu)} |BB^T|^{-\frac{1}{2}} \\
&amp;amp;= \frac{1}{(2\pi)^{\frac{n}{2}}|BB^T|^\frac{1}{2}} e^{-\frac{1}{2}
(\z-\mu)^T (BB^T)^{-1} (\z-\mu)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also note that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= E[(\z-\mu) (\z-\mu)^T] \\
&amp;amp;= E[B X X^T B^T] \\
&amp;amp;= B E[X X^T] B^T \\
&amp;amp;= B I B^T \\
&amp;amp;= B B^T
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, &lt;span class=&#34;math display&#34;&gt;\[
p_Z(\z) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_Z|}} e^{-\frac{1}{2} (\z-\mu)^T
\Sigma_Z^{-1} (\z-\mu)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/40225646&#34;&gt;为什么高斯分布概率密度函数的积分等于1
- 知乎 (zhihu.com)&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/58987388&#34;&gt;多元高斯分布完全解析 - 知乎
(zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>KL-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</link>
      <pubDate>Mon, 25 Apr 2022 16:39:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</guid>
      <description>

&lt;h2 id=&#34;kl-divergence&#34;&gt;KL-divergence&lt;/h2&gt;
&lt;p&gt;KL-divergence, denoted as &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|q)\)&lt;/span&gt;, is statistical distance,
measuring how the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is different from the reference
probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, both
defined on &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathcal{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In information theory, it measures the &lt;strong&gt;relative
entropy&lt;/strong&gt; from &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which is the average number of extra
bits required to represent a message with &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In discrete form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\sum_{x \in \mathcal{X}} p(x)\log
\frac{q(x)}{p(x)} = \sum_{x \in \mathcal{X}} p(x)\log \frac{p(x)}{q(x)}
\]&lt;/span&gt; In continuous form, &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p \| q) = -\int_{x \in \mathcal{X}} p(x) \log
\frac{q(x)}{p(x)}dx = \int_{x \in \mathcal{X}} p(x) \log
\frac{p(x)}{q(x)}dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;variational-lower-bound&#34;&gt;Variational Lower-bound&lt;/h3&gt;
&lt;p&gt;One property of KL-divergence is &lt;span class=&#34;math display&#34;&gt;\[
D_\text{KL}(p || q) = \sup_{T: \Omega \to \R} \E_{p} [T] - \log
(\E_q[e^T])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proof is as follows. Given a distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; and a function &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, construct the Gibbs distribution &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(g(x) = \frac{q(x)e^{T(x)}}{Z}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z = \E_{q(x)} e^{T(x)}\)&lt;/span&gt;. Then &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\E&amp;amp;_{p(x)} T(x) - \log Z = \E_{p(x)} [T(x) - \log Z] \\
&amp;amp;= \E_{p(x)} [\log e^{T(x)} - \log \E_{q(x)} e^{T(x)}] \\
&amp;amp;= \E_{p(x)} \log \frac{e^{T(x)}} {\E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{q(x) e^{T(x)}} {q(x) \E_{q(x)} e^{T(x)}} \\
&amp;amp;= \E_{p(x)} \log \frac{g(x)} {q(x)} \\
\end{aligned}
\]&lt;/span&gt; Finally KL-divergence minus above gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;D_\text{KL}(p || q) - (\E_{p(x)} T(x) - \log Z) \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {q(x)} - \E_{p(x)} \log \frac{g(x)}
{q(x)} \\
&amp;amp;= \E_{p(x)} \log \frac{p(x)} {g(x)} \triangleq D_\text{KL}(p || g)
\ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;gaussian-case&#34;&gt;Gaussian Case&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are random variables, both of some
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/&#34;&gt;Gaussian distribution&lt;/a&gt;. Then the
KL-divergence between them can be formulated as: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_\text{KL}(p_X || p_Y) &amp;amp;= \int p_X(x) \log \frac{p_X(x)} {p_Y(x)}
\d x = \int p_X(x) \log [
    \sqrt \frac{|\Sigma_X|}{|\Sigma_Y|}
    \frac {
        e^{-\frac 1 2 (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X)}
    } {
        e^{-\frac 1 2 (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)}
    }
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} -
\frac 1 2 \int p_X(x) [
    (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) +
    (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y)
]
\d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} -
\frac 1 2 \int p_X(x) (x-\mu_X)^T \Sigma_X^{-1} (x-\mu_X) \d x \\
&amp;amp;\quad -\frac 1 2 \int p_X(x) (x-\mu_Y)^T \Sigma_Y^{-1} (x-\mu_Y) \d
x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2 \int p_X(x)
x^T \Sigma_X^{-1} x \d x + \frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \int p_X(x) x^T \Sigma_Y^{-1} x \d x + \frac 1 2
\mu_Y^T \Sigma_Y^{-1} \mu_Y \d x \\
&amp;amp;= \frac 1 2 \log \frac{\Sigma_X}{\Sigma_Y} - \frac 1 2
\tr(\Sigma_X^{-1} \Sigma_X) - \frac 1 2 \mu_X \Sigma_X^{-1} \mu_X +
\frac 1 2 \mu_X^T \Sigma_X^{-1} \mu_X \\
&amp;amp;\quad - \frac 1 2 \tr(\Sigma_Y^{-1} \Sigma_X) - \frac 1 2 \mu_X
\Sigma_Y^{-1} \mu_X + \frac 1 2 \mu_Y^T \Sigma_Y^{-1} \mu_Y     \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y) + \mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu_X \Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y)] \\
&amp;amp;\quad + \frac 1 2 [\mu_Y^T \Sigma_Y^{-1} \mu_Y - \mu^T_X
\Sigma_Y^{-1} \mu_X + \mu_X^T \Sigma_Y^{-1} \mu_Y - \mu_Y^T
\Sigma_Y^{-1} \mu_X] \\
&amp;amp;= \frac 1 2 [\log \frac{\Sigma_X}{\Sigma_Y} - n - \tr(\Sigma_X^{-1}
\Sigma_Y) + (\mu_Y^T - \mu_X^T) \Sigma_Y^{-1} (\mu_Y - \mu_X)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</link>
      <pubDate>Fri, 07 Jan 2022 12:58:37 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/expectation-maximization/</guid>
      <description>
&lt;p&gt;If a probabilistic model contains only observable variables, maximum
likelihood estimation or Bayesian methods can be adopted to derive the
model parameters. However it is also possible that a probabilistic model
contains unobservable variables (called &lt;strong&gt;latent
variables&lt;/strong&gt;). Latent variables are those that you cannot observe
but you know its existence and its influence in a random trial. In such
case, &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange multipliers&lt;/a&gt; may
be hard to apply because of the existence of the “log of sum” term.&lt;/p&gt;
&lt;p&gt;Given observed samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{X} =
\{\x^{(1)}, \x^{(2)}, ..., \x^{(m)}\}\)&lt;/span&gt; (with unobservable latent
variable samples &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Z}\)&lt;/span&gt;), MLE
tries to the find best parameters &lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = \arg\max_{\theta}\log(p(\mathrm{X};\theta))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The log-likelihood function is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\theta) &amp;amp;= \log p(\mathrm{X};\theta) = \sum_{\x \in \mathrm{X}}
\log p(\x; \theta) \\
&amp;amp;= \sum_{\x \in \mathrm{X}} \log \E_{\z \sim q} \frac{p(\x, \z;
\theta)}{q(\z)} \\
&amp;amp;\Downarrow_\text{by Jensen&amp;#39;s Inequality} \\
&amp;amp;\ge \sum_{\x \in \mathrm{X}} \E_{\z \sim q} \log \frac{p(\x, \z;
\theta)}{q(\z)} \triangleq B(q, \theta) \\
&amp;amp;\text{where $q$ is an arbitrary reference probability measure}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we enlarge the &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt;,
the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; can be
lifted, which gives us a gentle guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l(\theta)\)&lt;/span&gt; may increase.&lt;/p&gt;
&lt;p&gt;Expectation maximization lifts this bound iteratively. To begin with,
we choose a random initial estimation for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(\theta^0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\((q, \theta)\)&lt;/span&gt;. We can specifically set
&lt;span class=&#34;math inline&#34;&gt;\(q(\z) = p(\z | \x; \theta^{t})\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt; is the estimation in
current iteration. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(B(q,
\theta)\)&lt;/span&gt; becomes &lt;span class=&#34;math display&#34;&gt;\[
B(q, \theta) = \sum_{\x \in \mathrm{X}} \E_{\z \sim p(\cdot | \x;
\theta^{t})} \log \frac{p(\x, \z; \theta)}{p(\z | \x; \theta^{t})}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\log \frac{1}{p(\z | \x;
\theta^{t})}\)&lt;/span&gt; is irrelevant to the optimization of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can simplify the problem as
maximizing &lt;span class=&#34;math display&#34;&gt;\[
Q(\theta^t, \theta) \triangleq \sum_{\x \in \mathrm{X}} \E_{\z \sim
p(\cdot | \x; \theta^{t})} \log p(\x, \z; \theta) \label{q-function}
\]&lt;/span&gt; The above step is called the &lt;strong&gt;expectation&lt;/strong&gt; step
because we are choosing a probability measure for the expectation term
in &lt;span class=&#34;math inline&#34;&gt;\(B(q, \theta)\)&lt;/span&gt;. The next step is
the &lt;strong&gt;maximization&lt;/strong&gt; step where we fix &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt; and maximize &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;-function&lt;/strong&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{q-function}\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This accounts for the estimation
in the next iteration: &lt;span class=&#34;math display&#34;&gt;\[
\theta^{t+1} = \arg\max_{\theta} Q(\theta^t, \theta)
\]&lt;/span&gt; We do these two steps back and forth, comprising the whole
expectation maximization algorithm.&lt;/p&gt;
&lt;p&gt;The reason we choose &lt;span class=&#34;math inline&#34;&gt;\(q(\z) = p(\z | \x;
\theta^{t})\)&lt;/span&gt;, which is conditioned on &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;, is that we want to inject some
dynamics into the algorithm. Say if we choose &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to be some static measure not relative
to &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;, the algorithm will end
at the very first iteration. On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(p(\z | \x; \theta^t)\)&lt;/span&gt; is the most
approachable probability measure w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\z\)&lt;/span&gt; and relative to &lt;span class=&#34;math inline&#34;&gt;\(\theta^t\)&lt;/span&gt;. After all, in latent models,
&lt;span class=&#34;math inline&#34;&gt;\(p(\z | \x; \theta^t)\)&lt;/span&gt; is much easier
to compute than &lt;span class=&#34;math inline&#34;&gt;\(p(\z;
\theta^t)\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eigenvectors and Eigenvalues</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</guid>
      <description>

&lt;h2 id=&#34;eigenvectors-and-eigenvalues&#34;&gt;Eigenvectors and Eigenvalues&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;eigenvector&lt;/strong&gt; of a &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a
&lt;strong&gt;nonzero&lt;/strong&gt; vector &lt;span class=&#34;math inline&#34;&gt;\(\rm
x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\rm x = \lambda \rm
x\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.
This scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the
corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt;. Note that by definition,
&lt;span class=&#34;math inline&#34;&gt;\((\lambda, \vec 0)\)&lt;/span&gt; is a pair of
eigenvalue and eigenvector of any square matrix. However, &lt;span class=&#34;math inline&#34;&gt;\(\vec 0\)&lt;/span&gt; is just too trivial an eigenvector
that people exclude it from the eigen discussion.&lt;/p&gt;
&lt;p&gt;Note, though, &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; can be an
eigenvalue. Also note that if &lt;span class=&#34;math inline&#34;&gt;\((\lambda,
v)\)&lt;/span&gt; is a pair of eigen of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\((\lambda, kv)\)&lt;/span&gt; is also a pair of
eigen.&lt;/p&gt;
&lt;h3 id=&#34;similarity&#34;&gt;Similarity&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrices, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
is similar to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; if there is an
invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(PAP^{-1} = B\)&lt;/span&gt;, or equivalently &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}BP = A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are similar, then they have the same
characteristic polynomial and hence then the same eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
B - \lambda I = PAP^{-1} - \lambda PP^{-1} = P(A - \lambda I)P^{-1} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(B - \lambda I) = \det(P(A - \lambda I)P^{-1}) \\
&amp;amp;= \det(P) \cdot \det(A - \lambda I) \cdot \det(P^{-1}) \\
&amp;amp;= \det(A - \lambda I)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independence-between-eigenvectors&#34;&gt;Independence between
Eigenvectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are
the eigenvectors corresponding to the distinct eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, ..., \lambda_r\)&lt;/span&gt; of
an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2,
... , v_r\)&lt;/span&gt; are also called eigenvectors from different
&lt;strong&gt;eigenspaces&lt;/strong&gt;), then &lt;span class=&#34;math inline&#34;&gt;\(v_1,
v_2, ..., v_r\)&lt;/span&gt; are linearly independent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;Suppose instead these vectors are dependent. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the least index such that &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is a linear combination of
previous vectors. Then there exists scalars &lt;span class=&#34;math inline&#34;&gt;\(c_1, c_2, ..., c_p\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1v_1 + c_2v_2 + \cdots + c_pv_p = v_{p+1} \label{lincom}
  \end{equation}
  \]&lt;/span&gt; Multiply both sides with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to obtain &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1\lambda_1v_1 + c_2\lambda_2v_2 + \cdots + c_p\lambda_pv_p =
\lambda_{p+1}v_{p+1} \label{eq1}
  \end{equation}
  \]&lt;/span&gt; Multiply both sides of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{p+1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1\lambda_{p+1}v_1 + c_2\lambda_{p+1}v_2 + \cdots +
c_p\lambda_{p+1}v_p = \lambda_{p+1}v_{p+1} \label{eq2}
  \end{equation}
  \]&lt;/span&gt; Subtract &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt;
from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  \begin{equation}
  c_1(\lambda_1 - \lambda_{p+1})v_1 + c_2(\lambda_2 - \lambda_{p+1})v_2
+ \cdots + c_p(\lambda_p - \lambda_{p+1})v_p = 0 \label{diff}
  \end{equation}
  \]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ...,
v_p\)&lt;/span&gt; are independent, the weights in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; are all zeros. None of &lt;span class=&#34;math inline&#34;&gt;\((\lambda_i - \lambda_{p+1})\)&lt;/span&gt; is zero, so
&lt;span class=&#34;math inline&#34;&gt;\(c_i = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; says &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which is impossible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, as the eigenvector, is
nonzero so that the conclusion can hold even for &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagonalization&#34;&gt;Diagonalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; linearly independent
eigenvectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(AP = PD\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\p}{\mathrm{p}} P = [\p_1, \p_2, ...,
\p_n]\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a
diagonal matrix, &lt;span class=&#34;math display&#34;&gt;\[
AP = [A\p_1, A\p_2, ..., A\p_n] = [D_{11} \p_1, D_{22} \p_2, ..., D_{nn}
\p_n]
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible,
&lt;span class=&#34;math inline&#34;&gt;\(\p_i\)&lt;/span&gt;’s are linearly independent,
which indicates that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;’s
are &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors.&lt;/p&gt;
&lt;p&gt;From this, we could also see that if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, then &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; must be the matrix concatenated with
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; must be the diagonal matrix filled with
corresponding eigenvalues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Easy to show.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all matrices are diagonalizable. For example, the matrix &lt;span class=&#34;math inline&#34;&gt;\(\begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1
\end{bmatrix}\)&lt;/span&gt; is not diagonalizable.&lt;/p&gt;
&lt;p&gt;The diagonalization of a square matrix is also referred to as
&lt;strong&gt;eigen decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-rank-trace-and-determinant&#34;&gt;Eigenvalues: Rank, Trace
and Determinant&lt;/h3&gt;
&lt;p&gt;Since the characteristic equation of an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; matrix is a polynomial of degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the equation always has exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; roots, counting multiplicities,
including complex roots. There are some relations between eigenvalues
and matrix’s rank, trace and determinant: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\rank = \text{the number of nonzero real eigenvalues, including
multiplicities} \\
\tr = \text{sum of the eigenvalues} \\
\det = \text{product of the eigenvalues}
\end{gather}
\]&lt;/span&gt; ## Power Iteration&lt;/p&gt;
&lt;p&gt;We may obtain the eigenvalues of an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; diagonalizable matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s by solving &lt;span class=&#34;math inline&#34;&gt;\(\det (A - \lambda I) = 0\)&lt;/span&gt;. Then the
corresponding eigenvectors can be solved. The order of complexity of
this method is cubic.&lt;/p&gt;
&lt;p&gt;But chances are that we don’t want all the eigenpairs, but instead
only those with largest eigenvalues, like when we find the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/#Eckart-Young-Mirsky Theorem&#34;&gt;best rank-&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; approximation using SVD&lt;/a&gt;. It is an
overkill to solve all eigenpairs. Luckily there is another lightweight
iterative method that can help.&lt;/p&gt;
&lt;p&gt;Begin with an arbitrary vector &lt;span class=&#34;math inline&#34;&gt;\(x_0 = x
\in \R^n\)&lt;/span&gt;. The iteration rule is &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1} = \frac{A x_t}{||A x_t||}
\]&lt;/span&gt; Unroll &lt;span class=&#34;math inline&#34;&gt;\(x_{t+1}\)&lt;/span&gt; to get
&lt;span class=&#34;math inline&#34;&gt;\(x_t = \frac{A^t x}{||A^t x||}\)&lt;/span&gt;.
Because &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; can be written as a linear combination
of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; normalized independent eigenvectors:
&lt;span class=&#34;math display&#34;&gt;\[
x = \sum_{i=1}^n \alpha_i v_i
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_n\)&lt;/span&gt; be
arranged such that corresponding eigenvalues go from large to small.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;A^t x = A^t \sum_{i=1}^N \alpha_i v_i \\
&amp;amp;= \sum_{i=1}^N \alpha_i \lambda_i^t v_i \\
&amp;amp;= \alpha_1 \lambda_1^t \sum_{i=1}^N \frac{\alpha_i}{\alpha_1}
(\frac{\lambda_i}{\lambda_1})^t v_i
\end{aligned}
\]&lt;/span&gt; Under the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt; is strictly larger than other
eigenvalues, &lt;span class=&#34;math inline&#34;&gt;\((\frac{\lambda_i}{\lambda_1})^t
\to 0, A^t x \to \alpha_1 \lambda_1^t v_1\)&lt;/span&gt;. Thus, &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1} = \frac{A^t x}{||A^t x||} \to v_1
\]&lt;/span&gt; To find the corresponding eigenvalue, observe that &lt;span class=&#34;math display&#34;&gt;\[
x_{t+1}^T A x_{t+1} \to \lambda_1
\]&lt;/span&gt; This gives the eigenpair with the largest eigenvalue. To find
the next one, repeat the above with &lt;span class=&#34;math display&#34;&gt;\[
A&amp;#39; = A - \lambda_1 v_1 v_1^T
\]&lt;/span&gt; The process above is called the deflation for the power
method. Refer to &lt;a href=&#34;Finding%20Eigenvalues.pdf&#34;&gt;Finding
Eigenvalues.pdf&lt;/a&gt; for the proof of correctness. This might also be
related to &lt;a href=&#34;https://www.wikiwand.com/en/Min-max_theorem&#34;&gt;Courant–Fischer
theorem&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD
&lt;a href=&#34;https://chunxy.github.io/uploads/Eigen%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Eigen
Decomposition.pdf&lt;/a&gt; ======= &lt;a href=&#34;Eigen%20Decomposition.pdf&#34;&gt;Eigen
Decomposition.pdf&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; notes&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Orthogonality and Projection</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</link>
      <pubDate>Mon, 20 Dec 2021 10:19:06 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/</guid>
      <description>

&lt;h2 id=&#34;orthogonality-and-independence&#34;&gt;Orthogonality and
Independence&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; are
orthogonal to each other, then they are independent with each other.&lt;/p&gt;
&lt;h2 id=&#34;orthonormality&#34;&gt;Orthonormality&lt;/h2&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; has orthonormal columns if and only if
&lt;span class=&#34;math inline&#34;&gt;\(U^TU = I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;orthogonal matrix&lt;/strong&gt; is a square invertible matrix
&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(U^{-1} = U^T\)&lt;/span&gt;. By its definition, it has
orthonormal columns and orthonormal rows.&lt;/p&gt;
&lt;h2 id=&#34;projection&#34;&gt;Projection&lt;/h2&gt;
&lt;h3 id=&#34;projection-onto-orthogonal-basis&#34;&gt;Projection onto Orthogonal
Basis&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\{u_1, u_2, ..., u_k\}\)&lt;/span&gt; be an
orthogonal basis for a subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;
of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. Then for each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, the weights in the linear combination
&lt;span class=&#34;math display&#34;&gt;\[
y = c_1u_1 + c_2u_2 + ... + c_ku_k
\]&lt;/span&gt; are given by &lt;span class=&#34;math display&#34;&gt;\[
c_i = \frac{y \cdot u_i}{u_i \cdot u_i} \label{coef}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-vector&#34;&gt;Projection onto Vector&lt;/h3&gt;
&lt;p&gt;Given a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; in
&lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; and another vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, we wish to decompose &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \alpha u\)&lt;/span&gt;
for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is some vector orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
z &amp;amp;= y - \hat y \\
z \cdot u &amp;amp;= (y - \alpha u) \cdot u \\
y \cdot u - \alpha u \cdot u &amp;amp;= 0 \\
\alpha &amp;amp;= \frac{y \cdot u}{u \cdot u}
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \frac{y \cdot u}{u \cdot
u}u\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z = y -
\hat y\)&lt;/span&gt; is called the component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(cu\)&lt;/span&gt; for any scalar &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is the projection onto the
subspace &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; spanned by &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;. In this sense, &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also denoted as &lt;span class=&#34;math inline&#34;&gt;\(\Pi_L(y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-subspace&#34;&gt;Projection onto Subspace&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; be a linear subspace of
&lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, then each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; can be uniquely written in the form:
&lt;span class=&#34;math display&#34;&gt;\[
y = \hat y + z
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(W^\perp\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\hat
y\)&lt;/span&gt; is called the orthogonal projection of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\Pi_W(y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also called the best
approximation to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, in the sense that: &lt;span class=&#34;math display&#34;&gt;\[
||y - \hat y||_2 \le ||y - v||_2, \forall v \in W
\]&lt;/span&gt; It can be shown by &lt;span class=&#34;math display&#34;&gt;\[
y - v = (y - \hat y) + (\hat y - v)
\]&lt;/span&gt; which gives &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||y - v||_2^2 = ||(y - \hat y) + (\hat y - v)||_2^2 \\
&amp;amp;= ||y - \hat y||_2^2 + ||\hat y - v||_2^2 + 2(y - \hat y)^T (\hat y
- v) \\
&amp;amp;\Downarrow_{y - \hat y \in W^\perp, \hat y - v \in W} \\
&amp;amp;= ||y - \hat y||_2^2 + ||\hat y - v||_2^2 \\
&amp;amp;&amp;gt; ||y - \hat y||_2^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;projection-onto-column-space&#34;&gt;Projection onto Column Space&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(A \in \R^{n \times m}\)&lt;/span&gt; is a
matrix, for any &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; we may
still want to find its projection onto the column space (also a linear
subspace) spanned by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. As
mentioned above, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be
decomposed into &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\hat y \in \Col A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z \in (\Col A)^\perp = \Nul A^T\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
y = \hat y + z \label{decomp} \\
A^T z = 0 \label{perp} \\
\exists x \in \R^n, A x = \hat y \label{proj}
\end{gather}
\]&lt;/span&gt; Substitute &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; in
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{proj}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{decomp}\)&lt;/span&gt;, and then substitute &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{decomp}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{perp}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^T (y - A x) &amp;amp;= 0 \\
A^T A x &amp;amp;= A^T y \\
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\rank {A^T A} = \rank
A\)&lt;/span&gt;. If either &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s columns
are independent or &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is of full
rank, we can solve the above equation as &lt;span class=&#34;math display&#34;&gt;\[
x = (A^T A)^{-1} A^T y
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
\hat y = A x = A (A^T A)^{-1} A^T y
\]&lt;/span&gt; For any &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt;, its
projection onto &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; can be found
by left-multiplying &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s
&lt;strong&gt;projection matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(P
\triangleq A (A^T A)^{-1} A^T\)&lt;/span&gt;. And we can verify that &lt;span class=&#34;math inline&#34;&gt;\(z \in \Nul A^T\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A^T z = A^T (y - \hat y) = A^T (y - A (A^T A)^{-1} A^T y) = A^T y - A^T
y = 0
\]&lt;/span&gt; There are some interesting properties with this projection
matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is symmetric;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is
&lt;strong&gt;idempotent&lt;/strong&gt; in that &lt;span class=&#34;math inline&#34;&gt;\(P^2 =
P\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Col P = \Col A\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For every vector &lt;span class=&#34;math inline&#34;&gt;\(x \in \Col A\)&lt;/span&gt;,
by the definition of projection matrix, we have &lt;span class=&#34;math display&#34;&gt;\[
P x = x
\]&lt;/span&gt; which means &lt;span class=&#34;math inline&#34;&gt;\(x \in \Col P\)&lt;/span&gt;
and thus &lt;span class=&#34;math inline&#34;&gt;\(\Col A \subseteq \Col
P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For every vector &lt;span class=&#34;math inline&#34;&gt;\(y \in \Col P\)&lt;/span&gt;,
there exists a vector &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
P z = y
\]&lt;/span&gt; By the definition of projection matrix, we know that &lt;span class=&#34;math inline&#34;&gt;\((P z) \in \Col A\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\Col P \subseteq \Col A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\Col P = \Col A\)&lt;/span&gt;.
Interestingly and as a direct result, we have &lt;span class=&#34;math display&#34;&gt;\[
A^T P = A^T
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-projection-matrix&#34;&gt;General Projection Matrix&lt;/h3&gt;
&lt;p&gt;The projection matrix derived above for a column space is actually an
&lt;strong&gt;orthogonal projection matrix&lt;/strong&gt;. The residual &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; of the original vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; after projection is perpendicular to
&lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; and thus to &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a more general case, the residual is not necessarily perpendicular
to &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; is also the closest point in &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. This instead is the definitive
property of a &lt;strong&gt;projection&lt;/strong&gt;, suitable for any set of
vectors like linear subspace or non-convex set.&lt;/p&gt;
&lt;p&gt;We can similarly develop the notion of a general projection matrix.
If an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; satisfies that &lt;span class=&#34;math inline&#34;&gt;\(P^2 = P\)&lt;/span&gt;, then it is a &lt;strong&gt;projection
matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Note that we develop the concept of orthogonal projection matrix from
the orthogonal projection onto a linear subspace. But for a general
projection matrix, we only require that &lt;span class=&#34;math inline&#34;&gt;\(P^2
= P\)&lt;/span&gt;. We don’t associate general projection matrix with general
projection, because for a vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(P y\)&lt;/span&gt; may not be the projection of
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; onto the projection space &lt;span class=&#34;math inline&#34;&gt;\(\{ P x: x \in \R^n \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math display&#34;&gt;\[
P = \begin{pmatrix}
0 &amp;amp; 1 \\
0 &amp;amp; 1
\end{pmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is a projection matrix
and it transforms all 2-D vectors &lt;span class=&#34;math inline&#34;&gt;\(y = [y_1,
y_2]^T\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\hat y = [y_2,
y_2]^T\)&lt;/span&gt;. The projection space formed by &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P} = \{ x \in \R^2: x_1 = x_2
\}\)&lt;/span&gt;. Clearly, neither &lt;span class=&#34;math inline&#34;&gt;\(\hat
y\)&lt;/span&gt; is the closest point to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt;, nor the residual &lt;span class=&#34;math inline&#34;&gt;\(y - \hat y\)&lt;/span&gt; is perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are some properties with a general projection matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is an orthogonal
projection matrix if and only if it is symmetric;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is an orthogonal
projection matrix if and only if its singular values are either &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;Refer to &lt;a href=&#34;https://math.stackexchange.com/questions/1109755/a-projection-p-is-orthogonal-if-and-only-if-its-spectral-norm-is-1&#34;&gt;this&lt;/a&gt;
or &lt;a href=&#34;https://math.stackexchange.com/questions/4407294/singular-values-of-projection-matrix&#34;&gt;this&lt;/a&gt;
for proof.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible only if it
is the identity matrix.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


</description>
    </item>
    
    <item>
      <title>$f$-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence can be treated as
the generalization of the KL-divergence. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has to satisfy
that &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a convex function. The reason for
these two constraints is that we hope &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
D_f (p||q) = 0 \text{ when $p=q$} \\
\forall p, q, D_f (p||q) \ge 0
\end{gather}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x\log x\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence becomes
KL-divergence.&lt;/p&gt;
&lt;h2 id=&#34;variational-f-divergence&#34;&gt;Variational &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; have no closed-form expression, it is
difficult to compute the &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence. Therefore in practice &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence is computed with a
variational expression: &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ
T(x)] \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the convex
conjugate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. The derivation is
as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int f^{**}(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int \sup_t [\frac{p(x)}{q(x)} t - f^*(t)] q(x)\ \d x \\
&amp;amp;= \int \sup_t[p(x) t - f^*(t) q(x)]\ \d x \\
&amp;amp;\Downarrow_{T(x) = \arg \sup_t[p(x) t - f^*(t) q(x)]} \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \int [p(x) T(x) - f^*(T(x)) q(x)]\ \d
x \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)]
\}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</link>
      <pubDate>Fri, 14 Jan 2022 21:19:17 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/linear-regression/</guid>
      <description>

&lt;p&gt;Given a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{(x^{(i)},
y^{(i)}),i=1,\dots,M\}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \in \R^N\)&lt;/span&gt; is the feature vector
and &lt;span class=&#34;math inline&#34;&gt;\(y^{(i)} \in R\)&lt;/span&gt; is the output,
learn a linear function &lt;span class=&#34;math inline&#34;&gt;\(f = w^Tx + b: \R^N
\mapsto \R\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^N, b \in
\R\)&lt;/span&gt;, that best predicts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for any feature vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The “best” is usually measured by the
mean square error: &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{MSE}(f,\mathcal D) = \frac{1}{M}\sum_{i=1}^M(y^{(i)} -
f(x^{(i)}))^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a sidenote, it is very convenient to standardize the feature (so
that the solution won’t depend to unit used in the measurement) and
pre-center the label (so that the intercept term, or the bias term &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be omitted).&lt;/p&gt;
&lt;h3 id=&#34;ordinary-least-squares&#34;&gt;Ordinary Least Squares&lt;/h3&gt;
&lt;p&gt;OLS selects the linear regression parameters &lt;span class=&#34;math inline&#34;&gt;\(w, b\)&lt;/span&gt; that minimizes the mean squared
error: &lt;span class=&#34;math display&#34;&gt;\[
w^\star, b^\star = \arg \min_{w,b}\frac{1}{M}\sum_{i=1}^M(y^{(i)} -
w^Tx^{(i)} - b)^2
\]&lt;/span&gt; This is equivalent to the below least squares problem. Let
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
X =
\left [
\begin{array}{c|c}
(x^{(1)})^T &amp;amp; 1 \\
(x^{(2)})^T &amp;amp; 1 \\
\vdots &amp;amp; \vdots \\
(x^{(M)})^T &amp;amp; 1
\end{array}
\right ],
W = \begin{bmatrix}
w \\
b \\
\end{bmatrix},
Y = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(M)}
\end{bmatrix} \\
\\
XW =  Y
\end{gathered}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may not lie in the
column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Therefore we
have to approximate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X\hat W = \Pi_{\Col(X)}Y \Rightarrow Y &amp;amp;- X\hat W \in \Nul(X^T) \\
\Downarrow\\
X^T(Y - X\hat W) &amp;amp;= 0 \\
X^TX\hat W &amp;amp;= X^TY
\end{aligned}
\]&lt;/span&gt; Columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are
independent &lt;span class=&#34;math inline&#34;&gt;\(\iff\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is invertible. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is not invertible, there are
infinite many solutions to &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt;.
We can get one specific &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt; by
enforcing regularization on &lt;span class=&#34;math inline&#34;&gt;\(\hat W\)&lt;/span&gt;
or adding more samples when &lt;span class=&#34;math inline&#34;&gt;\(M &amp;lt; N +
1\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is
invertible, there is a unique solution that &lt;span class=&#34;math inline&#34;&gt;\(\hat W = (X^TX)^{-1}X^TY\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This gives the same result with minimizing the MSE: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
W^\star = \arg \min_{W} \mathrm{MSE}(W) = \frac{1}{M}(Y - XW)^T(Y - XW)
\label{target} \\
\Downarrow \notag \\ \notag \\
\begin{aligned}
\frac{\partial \mathrm{MSE}}{\partial W} &amp;amp;= \frac{\partial(Y^TY -
Y^TXW - W^TX^TY + W^TX^TXW)}{\partial W} \\
&amp;amp;= \frac{\partial(Y^TY - Y^TXW - Y^TXW + (XW)^TXW)}{\partial W} \\
&amp;amp;= -2X^TY + 2X^TXW \\
\end{aligned} \notag \\
\Downarrow_{\text{making it zero}} \notag \\ \notag \\
0 = -2X^TY + 2X^TXW^\star \notag \\
X^TXW^\star = X^TY \notag
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are chances that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is too
large, making equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{target}\)&lt;/span&gt; much computationally
expensive. In this case, we can use gradient descent. The update rule
will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
W^{(t+1)} &amp;amp;= W^{(t)} - \frac{\eta}{2} \nabla \mathrm{MSE}(W^{(t)})
\\
&amp;amp;= W^{(t)} - \eta(X^TXW^{(t)} -X^TY)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-probabilistic-view&#34;&gt;A Probabilistic View&lt;/h3&gt;
&lt;h4 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood
Estimation&lt;/h4&gt;
&lt;p&gt;In classification task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the
feature, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the label; in
regression task, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the “label”,
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the “feature”. From a
probabilistic point of view, we would like estimate &lt;span class=&#34;math inline&#34;&gt;\(p(\text{feature}|\text{label})\)&lt;/span&gt;. In this
case, we treat &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as the “feature”
composed of a deterministic function and a noise sampled from an
identical and independent Gaussian distribution, i.e., for random
variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal X, \mathcal Y\)&lt;/span&gt; (to
distinguish from matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), &lt;span class=&#34;math display&#34;&gt;\[
\mathcal Y = \mathcal XW + \epsilon, \text{ where }\epsilon \sim
\mathcal N(0, \sigma^2)
\]&lt;/span&gt; Thus, &lt;span class=&#34;math display&#34;&gt;\[
p(Y|X;W,\sigma^2) = p(\epsilon=Y - XW;\sigma^2) = \mathcal N(Y -
XW;0,\sigma^2I)
\]&lt;/span&gt; Then OLS can be attacked by maximum likelihood estimation. The
log-likelihood function will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(W,\sigma^2) &amp;amp;= \log(L(W, \sigma^2)) = \log(p(Y|X;W,\sigma^2)) =
\log \mathcal N(Y - XW;0,\sigma^2I) \\
&amp;amp;=
\log(\frac{1}{\sqrt{(2\pi)^M|\sigma^2I|}}e^{-\frac{1}{2\sigma^2}(Y -
XW)^T(Y - XW)}) \\
&amp;amp;= -\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) - \frac{M}{2}\log \sigma^2
- \frac{M}{2}\log 2\pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\arg \max_{W,\sigma^2}l(W, \sigma^2) &amp;amp;= \arg
\max_{W,\sigma^2}-\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) -
\frac{M}{2}\log \sigma^2 - \frac{M}{2}\log 2\pi \\
&amp;amp;= \arg \min_{W,\sigma^2}\frac{1}{2\sigma^2}(Y - XW)^T(Y - XW) +
\frac{M}{2}\log \sigma^2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and
make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\frac{1}{\sigma^2}(X^TXW^\star - X^TY) = 0 \\
X^TXW^\star = X^TY
\end{gathered}
\]&lt;/span&gt; Substitute &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; back,
take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and
make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{\star2}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\frac{M}{2\sigma^{\star2}} - \frac{(Y - XW^\star)^T(Y -
XW^\star)}{2(\sigma^{\star2})^2} = 0 \\
\sigma^{\star2} = \frac{1}{M}(Y - XW^\star)^T(Y - XW^\star)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may further analyze the efficacy of &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; as a point estimator. Suppose
&lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt; is invertible. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;W^\star = (X^T X)^{-1} X^T Y \\
&amp;amp;= (X^T X)^{-1} X^T (X W_\text{real} + Z) \\
&amp;amp;= W_\text{real} + (X^T X)^{-1} X^T Z
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; contains the
noise term for each sample. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;
is unbiased because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[W^\star] = \E[W_\text{real} + (X^T X)^{-1} X^T Z] \\
&amp;amp;= W_\text{real} + (X^T X)^{-1} X^T \E[Z] = W_\text{real}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt;
will be: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\Cov[W^\star] = \Cov[W_\text{real} + (X^T X)^{-1} X^T Z] \\
&amp;amp;= 0 + (X^T X)^{-1} X^T \Cov[Z] X (X^T X)^{-1} \\
&amp;amp;= (X^T X)^{-1} X^T {\sigma^\star}^2 I X (X^T X)^{-1} \\
&amp;amp;= {\sigma^\star}^2 (X^T X)^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;maximum-a-posteriori&#34;&gt;Maximum a Posteriori&lt;/h4&gt;
&lt;p&gt;If we take the Bayesian view and add a prior to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(W
\sim \mathcal N(0,\frac{C}{2}I)\)&lt;/span&gt;, or rather &lt;span class=&#34;math inline&#34;&gt;\(p(W) \propto \exp(-\frac{1}{C}W^TW)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p(W|X, Y) = \frac{p(W, X, Y)}{p(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(X, W)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)P(X)}{P(X, Y)} \\
&amp;amp;= \frac{p(Y|X, W)P(W)}{P(Y|X)} \\
&amp;amp;\propto p(Y|X,W)p(W)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
W^\star = \arg \max_W p(W|X,Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;regularized-variants&#34;&gt;Regularized Variants&lt;/h2&gt;
&lt;p&gt;Linear regression tends to have high variance, due to the sum of
random variables in its inference formula. This is why we tend to
restrict the magnitude of coefficients.&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;By adding a regularization term to OLS objective we obtain the ridge
regression: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \frac{\alpha}{2}||W||^2_2
\]&lt;/span&gt; By regularization, we are “shrink” the amount of &lt;span class=&#34;math inline&#34;&gt;\(||W||_2\)&lt;/span&gt;, whose magnitude is controlled by
the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We prefer smaller
weights because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smaller weights are more robust to the perturbations of input;&lt;/li&gt;
&lt;li&gt;there are better chances to zero out some dimensions of the input
feature, which may be uninformative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The constant term before the sum of square errors that before &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not of significance. We choose
them to be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; because this
gives a nicer closed-form solution to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Take derivative of the objective
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and make it &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
-X^TY + X^TXW^\star + \alpha W^\star = 0 \\
(X^TX + \alpha I)W^\star = X^TY
\end{gathered}
\]&lt;/span&gt; The “ridge” comes from the &lt;span class=&#34;math inline&#34;&gt;\(\alpha
I\)&lt;/span&gt; term, which is added to the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h3&gt;
&lt;p&gt;In ridge regression, there is still chance that some weights are
small but not zero, because the regularization term is small so far as
the weights are fairly small. Think in this way: &lt;span class=&#34;math inline&#34;&gt;\(0.01^2 = 0.0001 \ll 0.01\)&lt;/span&gt;, though it is
not rigid.&lt;/p&gt;
&lt;p&gt;To get better regularization, we can change the regularization term
to &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;-norm: &lt;span class=&#34;math display&#34;&gt;\[
\min_{W} \frac{1}{2}||Y - XW||^2_2 + \alpha||W||_1
\]&lt;/span&gt; The reason to choose &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt;
norm is that, &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is the least number
that preserves convexity among the &lt;span class=&#34;math inline&#34;&gt;\(l_p\)&lt;/span&gt; norms. Interestingly, when the
penalty term on the &lt;span class=&#34;math inline&#34;&gt;\(l_1\)&lt;/span&gt; norm is
large enough, the resulting estimation will be zero (refer &lt;a href=&#34;https://stats.stackexchange.com/questions/280823/what-is-the-mathematical-rigorous-proof-that-l1-regularization-will-give-sparse&#34;&gt;this&lt;/a&gt;).
The solution to the lasso regression can be efficiently approached using
&lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/coordinate-descent/&#34;&gt;coordinate descent&lt;/a&gt; or &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/least-angle-regression/&#34;&gt;least angle regression&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6018889.html&#34;&gt;Lasso回归算法：
坐标轴下降法与最小角回归法小结 - 刘建平Pinard - 博客园
(cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/76055830&#34;&gt;LASSO回归求解 - 知乎
(zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hastie.su.domains/StatLearnSparsity/&#34;&gt;Statistical
Learning with Sparsity: the Lasso and Generalizations
(su.domains)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Subgradient</title>
      <link>https://chunxy.github.io/notes/articles/optimization/subgradient/</link>
      <pubDate>Tue, 11 Jan 2022 16:09:23 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/subgradient/</guid>
      <description>

&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Subgradient of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
\partial f(x) = \{g|f(y) \ge f(x) + g^T(y - x), \forall y \in dom(f) \}
\]&lt;/span&gt; Or put it another way, the subgradient defines a hyper-plane
passing through &lt;span class=&#34;math inline&#34;&gt;\((x,f(x))\)&lt;/span&gt;: $$ ^T ( -
) = 0,&lt;/p&gt;
y ^n, t \ &lt;span class=&#34;math display&#34;&gt;\[
And
\]&lt;/span&gt; ^T ( - ) , (y, t) epi(f) &lt;span class=&#34;math display&#34;&gt;\[
This is because by definition,
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
g^T(y - x) - (f(y) - f(x)) &amp;amp;\le 0, \forall y &amp;amp;\Rightarrow \\
g^T(y - x) - (t - f(x)) &amp;amp;\le 0, \forall y,\forall t \ge f(y)
&amp;amp;\Rightarrow \\
\left[
\begin{array} \\
g \\
-1 \\
\end{array}
\right]^T
\Big(
\left[
\begin{array} \\
y \\
t \\
\end{array}
\right] -
\left[
\begin{array} \\
x \\
f(x) \\
\end{array}
\right]
\Big)
&amp;amp;\le 0, \forall (y, t) \in epi(f)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\left[\begin{array}\\ g \\ -1
\\\end{array}\right]\)&lt;/span&gt; is the normal of the tangent plane of
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;The subgradient &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is
a set, instead of a single number like the gradient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Subgradient is a monotonic operator: &lt;span class=&#34;math display&#34;&gt;\[
(u - v)^T(y - x) \ge 0, \forall u \in \partial f(y), \forall v \in
\partial f(x)
\]&lt;/span&gt; This can be shown by: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left.
\begin{array} \\
f(y) \ge f(x) + v^T(y - x) \\
f(x) \ge f(y) + u^T(x - y) \\
\end{array}
\right\}
&amp;amp;\Rightarrow
f(y) + f(x) \ge f(x) + f(y) - (u - v)^T(y - x) \\
&amp;amp;\Rightarrow (u - v)^T(y - x) \ge 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x^\star = \arg \min_x f(x) \iff 0 \in
\partial f(x^\star)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) = \{\nabla f(x)\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x) \and p \in
\partial f(x)\)&lt;/span&gt;, then for &lt;span class=&#34;math inline&#34;&gt;\(y = x + r(p
- \nabla f(x))\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y) - f(x) &amp;amp;\ge p^T(y - x) \\
f(x + r(p - \nabla f(x))) - f(x) &amp;amp;\ge rp^T(p - \nabla f(x)) \\
\frac{f(x + r(p - \nabla f(x))) - f(x)}{r} &amp;amp;\ge p^T(p - \nabla f(x))
\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(p - \nabla f(x))^T(p - \nabla f(x)) &amp;amp;\le \frac{f(x + r(p - \nabla
f(x))) - f(x)}{r} - \nabla f(x)^T(p - \nabla f(x)) \\
||p - \nabla f(x)||^2_2 &amp;amp;\le \frac{f(x + r(p - \nabla f(x))) - f(x)-
\nabla f(x)^Tr(p - \nabla f(x))}{r} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take limit on &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; on both sides to
give &lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}||p - \nabla f(x)||^2_2 \le \lim_{r \to 0}\Big( \frac{f(x
+ r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))}{r} \Big)
\\
\]&lt;/span&gt; By the definition of gradient, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f&amp;amp;(x + r(p - \nabla f(x))) - f(x)- \nabla f(x)^Tr(p - \nabla f(x))
\\
&amp;amp;= \mathcal{O}(||r(p - \nabla f(x))||^2_2) \\
&amp;amp;= ||p - \nabla f(x)||^2\mathcal{O}(r^2) \\
&amp;amp;= \mathcal{O}(r^2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{r \to 0}\Big( \frac{f(x + r(p - \nabla f(x))) - f(x)- \nabla
f(x)^Tr(p - \nabla f(x))}{r} \Big) = \lim_{r \to
0}\frac{\mathcal{O}(r^2)}{r} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
||p - \nabla f(x)||^2_2 \le 0 \\
0 \le ||p - \nabla f(x)||^2_2 \le 0 \\
||p - \nabla f(x)||^2_2 = 0
\end{gather}
\]&lt;/span&gt; contradicting the assumption that &lt;span class=&#34;math inline&#34;&gt;\(p \ne \nabla f(x)\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(p = \nabla f(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \alpha_1 f_1(x) + \alpha_2
f_2(x)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x) =
\alpha_1 \partial f_1(x) + \alpha_2 f_2(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/139083837&#34;&gt;凸优化笔记16：次梯度
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Non-linear Regression</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</link>
      <pubDate>Fri, 07 Jan 2022 12:46:20 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/non-linear-regression/</guid>
      <description>

&lt;p&gt;The basic idea about non-linear regression is to perform the feature
mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;, followed by linear
regression in the new feature space.&lt;/p&gt;
&lt;h3 id=&#34;polynomial-regression&#34;&gt;Polynomial Regression&lt;/h3&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a scalar, the feature
mapping in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-th order Polynomial
Regression is &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) =
\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
\]&lt;/span&gt; The regression function and the parameters are then like those
in linear regression: &lt;span class=&#34;math display&#34;&gt;\[
f(x) = [w_0,w_1,w_2,\dots,w^p]\begin{bmatrix}
1 \\
x \\
x^2 \\
\vdots \\
x^p
\end{bmatrix}
= w^T\phi(x)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; captures all
features in each degree from &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; up
to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. In higher-dimensional input
feature space, there are many more entries for feature mapping in each
degree. Take a 2-D input for example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;degree 1: &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;degree 2: &lt;span class=&#34;math inline&#34;&gt;\([x_1^2, x_1x_2,
x_2^2]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;degree 3: &lt;span class=&#34;math inline&#34;&gt;\([x_1^3, x_1^2x_2, x_1x_2^2,
x_2^3]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-trick-in-non-linear-regression&#34;&gt;Kernel Trick in
Non-linear Regression&lt;/h3&gt;
&lt;p&gt;Many Linear Regression algorithms learning algorithms depend on the
calculation of inner products between feature vectors, either during
training or in prediction, without directly depending on the feature
vector. We can transform the feature vector by applying a feature
mapping &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; to do the Non-linear
Regression task.&lt;/p&gt;
&lt;p&gt;However, it is not necessary to explicitly define the feature mapping
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; when only the inner products
are needed. Instead, we can define a function &lt;span class=&#34;math inline&#34;&gt;\(\mathcal K: \R^N \times \R^N \mapsto \R\)&lt;/span&gt;
that directly calculate the inner products of pseudo-transformed
features: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal K(x,x^\prime) = \phi_{pseudo}(x)^T\phi_{pseudo}(x^\prime)
\]&lt;/span&gt; This will be more flexible and computationally-efficient.&lt;/p&gt;
&lt;h4 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h4&gt;
&lt;p&gt;Recall Ridge Regression: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \min_{W}\frac{1}{2}||Y - X^TW||^2_2 +
\frac{\alpha}{2}||W||^2_2 \iff (XX^T + \alpha I_{N+1})W^\star = XY
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\((XX^T + \alpha
I_{N+1})\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(W^\star =
(XX^T + \alpha I_{N+1})^{-1}XY\)&lt;/span&gt;. By &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/&#34;&gt;matrix identity&lt;/a&gt; &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; we can obtain that &lt;span class=&#34;math inline&#34;&gt;\(W^\star =
X(X^TX + \alpha I_M)^{-1}Y\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(X \in
\R^{(N+1)\times M}, (X^TX + \alpha I_M)^{-1}Y \in \R^{M}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; can be rewritten as a linear
combination of columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;,
i.e. &lt;span class=&#34;math inline&#34;&gt;\(W^\star\)&lt;/span&gt; lies in the span of
input vectors: &lt;span class=&#34;math display&#34;&gt;\[
W^\star = \sum_{i=1}^M\lambda_ix^{(i)}
\]&lt;/span&gt; For a new data &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we
make prediction by &lt;span class=&#34;math display&#34;&gt;\[
y^* = (x^*)^TW = (x^*)^TX(X^TX + \alpha I_M)^{-1}Y
\]&lt;/span&gt; which totally depends on the inner products.&lt;/p&gt;
&lt;h4 id=&#34;support-vector-regression&#34;&gt;Support Vector Regression&lt;/h4&gt;
&lt;p&gt;Support Vector Regression learns a hyper-plane that incorporates
within its margin as many points as possible. As a comparison, &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/support-vector-machine/&#34;&gt;Support Vector Machine&lt;/a&gt; learns a
hyper-plane that excludes outside its margin as many points as possible.
Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i)
\\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b \le \epsilon + \xi_i, i=1,\dots,M \\
y^{(i)} - w^Tx^{(i)} - b \ge \epsilon + \xi^*_i, i=1,\dots,M \\
\xi_i, \xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a
hyper-parameter to be determined. Transform the problem into the
standard optimization form: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{w,\xi,\xi^*} \frac{1}{2}||w||^2_2 + C\sum_{i=1}^M(\xi_i + \xi^*_i)
\\
s.t.\quad y^{(i)} - w^Tx^{(i)} - b - \epsilon - \xi_i \le 0, i=1,\dots,M
\\
w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i \le 0, i=1,\dots,M \\
-\xi_i, -\xi^*_i \ge 0, i=1,\dots,M
\end{aligned}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(w,\xi,\xi^*,\lambda,\mu,\nu,\nu^*) = &amp;amp;\frac{1}{2}||w||^2_2 +
C\sum_{i=1}^M(\xi_i + \xi^*_i) \\
&amp;amp;+ \sum_{i=1}^M\lambda_i(y^{(i)} - w^Tx^{(i)} - b - \epsilon -
\xi_i) \\
&amp;amp;+ \sum_{i=1}^M\mu_i(w^Tx^{(i)} + b - y^{(i)} + \epsilon + \xi^*_i)
\\
&amp;amp;- \sum_{i=1}^M\nu_i\xi_i -\sum_{i=1}^M\nu^*_i\xi^*_i
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Support%20Vector%20Regression.pdf&#34; target=&#34;_blank&#34;&gt;Support
Vector Regression.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>常见分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</link>
      <pubDate>Thu, 11 Aug 2022 17:12:01 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;h2 id=&#34;离散型&#34;&gt;离散型&lt;/h2&gt;
&lt;h3 id=&#34;二项分布binomial-distribution&#34;&gt;二项分布（binomial
distribution）&lt;/h3&gt;
&lt;p&gt;如果离散型随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从二项分布，一般记作&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n, p)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(x;n,p) = {n \choose x} p^x (1-p)^{n-x}, x = 0,1,\dots \\
\E[X] = np \\
\Var[X] = np(1-p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;二项分布可以帮助纠正一个生活中很常见的谬误，比如说身高高于两米的人占人类总体的&lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;，那么是否说明随机选取的100个人中一定至少有1个人高于两米呢？记&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为100个人中身高高于两米的人数，显然&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(100, 0.01)\)&lt;/span&gt;，经计算可得&lt;span class=&#34;math inline&#34;&gt;\(P(X=0) \approx
0.366\)&lt;/span&gt;。其实也就意味着，100个人中，能至少看到1个身高高于两米的人的概率其实大约是&lt;span class=&#34;math inline&#34;&gt;\(1-0.366 = 63.4\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;泊松分布poisson-distribution&#34;&gt;泊松分布（Poisson
distribution）&lt;/h3&gt;
&lt;p&gt;泊松分布产生于&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;用来表示在一定时间或空间内出现的事件个数的场景中。泊松分布有一些基本假设，设观察的这一单位时间或空间为&lt;span class=&#34;math inline&#34;&gt;\([0, 1)\)&lt;/span&gt;，取一个很大的自然数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，将&lt;span class=&#34;math inline&#34;&gt;\([0,1)\)&lt;/span&gt;平分为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;段窗口：&lt;span class=&#34;math inline&#34;&gt;\(l_1
= [0, \frac{1}{n}), l_2 = [\frac{1}{n}, \frac{2}{n}), \dots, l_n =
[\frac{n-1}{n}, 1)\)&lt;/span&gt;，则：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;在每段&lt;span class=&#34;math inline&#34;&gt;\(l_i\)&lt;/span&gt;内，恰发生一个事件的概率正比于这段的长度&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;，即可取为&lt;span class=&#34;math inline&#34;&gt;\(\frac{\lambda}{n}\)&lt;/span&gt;；又假定&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;很大故&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;很小时，不可能发生两次以上事件；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(l_1, l_2, \dots,
l_n\)&lt;/span&gt;中是否发生时间是相互独立的；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样的基本假设下，单位窗口内发生事件的总数记为随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;。此时&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;应当服从二项分布，而当&lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;则服从泊松分布，故泊松分布也可以看作是某种形式的二项分布取极限而得到：
&lt;span class=&#34;math display&#34;&gt;\[
P(X = i; \lambda) = \lim_{n \to \infty} {n \choose i}
(\frac{\lambda}{n})^i (1 - \frac{\lambda}{n})^{n-i}
\]&lt;/span&gt; 将&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} {n \choose
i} / n^i = 1 / i!\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to
\infty} (1 - \frac{\lambda}{n})^{n-i} =
e^{-\lambda}\)&lt;/span&gt;代入即可得到泊松分布的分布律。&lt;/p&gt;
&lt;p&gt;一般如果&lt;span class=&#34;math inline&#34;&gt;\(X \sim B(n,p)\)&lt;/span&gt;且&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;较大、&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;较小、&lt;span class=&#34;math inline&#34;&gt;\(np =
\lambda\)&lt;/span&gt;不太大时，&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布接近于泊松分布&lt;span class=&#34;math inline&#34;&gt;\(P(\lambda)\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(x;\lambda) = e^{-\lambda} \frac{\lambda^x}{x!}, x = 0,1,\dots \\
\E[X] = \lambda \\
\Var[X] = \lambda
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;伯努利分布bernoulli-distribution&#34;&gt;伯努利分布（Bernoulli
distribution）&lt;/h3&gt;
&lt;p&gt;伯努利分布&lt;span class=&#34;math inline&#34;&gt;\(B(1,
p)\)&lt;/span&gt;实际上是二项分布中&lt;span class=&#34;math inline&#34;&gt;\(n =
1\)&lt;/span&gt;的一个特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
B(1;1,p) = p, B(0;1,p) = 1 - p \\
\E[X] = p \\
\Var[X] = p(1 - p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;多项分布multinomial-distribution&#34;&gt;多项分布（multinomial
distribution）&lt;/h3&gt;
&lt;p&gt;多项分布其实就是二项分布的推广，不像二项分布，多项分布的取值的是多值的而不是二值的（binary）。假设有&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果，且这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果互相对立、完备穷举（mutually
exclusive and collectively exhaustive），此时它们的概率之和为&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，即&lt;span class=&#34;math inline&#34;&gt;\(p_1 +
\dots + p_k = 1\)&lt;/span&gt;，多项分布计算的则是这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;种结果分别发生&lt;span class=&#34;math inline&#34;&gt;\(n_1, \dots, n_k\)&lt;/span&gt;次时的概率。令&lt;span class=&#34;math inline&#34;&gt;\(N = n_1 + \dots + n_k, \vec p = [p_1, \dots, p_k],
\vec n = [n_1, \dots, n_k]\)&lt;/span&gt;，则： &lt;span class=&#34;math display&#34;&gt;\[
P(\vec n; \vec p) = \frac{N!}{n_1! \dots n_k!} p_1^{n_1} \dots p_k^{n_k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;多项分布可以拓展到连续情况，此时&lt;span class=&#34;math inline&#34;&gt;\(n_1,
\dots, n_k \in \R_+\)&lt;/span&gt;，而概率质量函数变为 &lt;span class=&#34;math display&#34;&gt;\[
p(\vec n; \vec p) = \frac{\Gamma(N + 1)}{\Gamma(n_1 + 1) \dots
\Gamma(n_k + 1)} p_1^{n_1} \dots p_k^{n_k}
\]&lt;/span&gt;
连续情况下的多项分布也是&lt;code&gt;sklearn&lt;/code&gt;中能将TFIDF特征应用到&lt;code&gt;MultinomialNB&lt;/code&gt;的基本原理。&lt;/p&gt;
&lt;h3 id=&#34;分类分布categorical-distribution&#34;&gt;分类分布（categorical
distribution）&lt;/h3&gt;
&lt;p&gt;类似伯努利分布是二项分布&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时的特例，分类分布则是多项分布&lt;span class=&#34;math inline&#34;&gt;\(N=1\)&lt;/span&gt;时的特例： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(\vec n; \vec p, 1) = \prod_{i=1}^k p_i ^{n_i} \\
\E[X] = \vec p \\
\Var[X] = \vec p (1 - \vec p)
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;连续型&#34;&gt;连续型&lt;/h2&gt;
&lt;h3 id=&#34;指数分布exponential-distribution&#34;&gt;指数分布（exponential
distribution）&lt;/h3&gt;
&lt;p&gt;指数分布最常见的一个场景是寿命估计。设想一种大批生产的电器元件，其元件寿命&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;是随机变量，在“无老化”的假定下——即“若元件在时刻&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;尚正常工作，则其失效率总为某个与&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;无关的常数&lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;”，那么&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从参数为&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的指数分布。&lt;/p&gt;
&lt;p&gt;上述假设用概率语言描述则是 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{h \to 0} P(x \le X \le x+h | X &amp;gt; x) / h = \lambda
\]&lt;/span&gt; 注意到 &lt;span class=&#34;math display&#34;&gt;\[
P(x \le X \le x+h | X &amp;gt; x) = \frac{P(\{ x \le X \le x+h \} \cap \{ X
&amp;gt; x \})}{P(X &amp;gt; x)} = \frac{P(x &amp;lt; X \le x+h)}{P(X &amp;gt; x)}
\]&lt;/span&gt; 所以 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lim_{h \to 0} \frac{P(x &amp;lt; X \le x+h)}{h P(x &amp;lt; X))} &amp;amp;= \lambda
\\
\lim_{h \to 0} \frac{F(x + h) - F(x)}{h(1 - F(x))} &amp;amp;= \lambda \\
\frac{F&amp;#39;(x)}{1 - F(x)} &amp;amp;= \lambda
\end{aligned}
\]&lt;/span&gt; 上述微分方程的通解为&lt;span class=&#34;math inline&#34;&gt;\(F(x) = 1 -
Ce^{-\lambda x}\)&lt;/span&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(F(0) =
0\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(C = 1\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
p(x;\lambda) = \begin{cases}
\lambda e^{-\lambda x}, &amp;amp; x &amp;gt; 0 \\
0, &amp;amp; x \le 0
\end{cases} \\
\E[X] = \lambda^{-1} \\
\Var[X] = \lambda^{-2}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;正态分布normal-distribution&#34;&gt;正态分布（normal
distribution）&lt;/h3&gt;
&lt;p&gt;正态分布也叫作高斯分布（Gaussian distribution），一维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x -
\mu)^2}{2 \sigma^2}}
\]&lt;/span&gt; 二维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p \Big( (x,y); \mu_X, \mu_Y, \sigma_X, \sigma_Y, \sigma_{XY} \Big) =
\frac{1}{2\pi \sqrt{(\sigma_X^2 \sigma_Y^2 - \sigma_{XY}^2)}}
e^{-\frac 1 {2(1 - \sigma_{XY}^2)} \left(\frac{(x-\mu_X)^2} {\sigma_X^2}
- \frac{2\sigma_{XY}(x - \mu_X)(y - \mu_Y)} {\sigma_X \sigma_Y} +
\frac{(y-\mu_Y)^2} {\sigma_Y^2} \right)}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;维情况下： &lt;span class=&#34;math display&#34;&gt;\[
p(\x; \mu, \Sigma) = \frac{1}{\sqrt{|2\pi \Sigma|}} e^{-\frac{1}{2}
(\x-\mu)^T \Sigma^{-1} (\x-\mu)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/clustering/</link>
      <pubDate>Sat, 09 Apr 2022 14:32:03 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/clustering/</guid>
      <description>

&lt;h2 id=&#34;big-picture&#34;&gt;Big Picture&lt;/h2&gt;
&lt;p&gt;The clustering algorithms can be broadly split into two categories
depending on whether the number of clusters is given or to be determined
by user. &lt;strong&gt;Partitional&lt;/strong&gt; ones pre-set the number of
clusters; while &lt;strong&gt;hierarchical&lt;/strong&gt; ones output a dendrogram
that illustrates how clusters are built level by level. Users are free
to choose the level of clustering in this hierarchical clustering.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hierarchical algorithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bottom-up&lt;/strong&gt; agglomerative clustering&lt;/p&gt;
&lt;p&gt;This approach starts with each object in a separate cluster, and
repeatedly&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;joins the most similar pair of clusters,&lt;/li&gt;
&lt;li&gt;update the similarity of the new cluster to others until there is
only one cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are five further methods in this approach. The difference among
them lies in the way to measure inter-cluster similarity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single-linkage&lt;/strong&gt; measures the similarity between two
clusters as the distance between their closest members.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-linkage&lt;/strong&gt; measures the similarity between
two clusters as the distance between their furthest members.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average-linkage&lt;/strong&gt; measures the similarity between two
clusters as the average of distances of all the cross-cluster
pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt; measures the similarity between two
clusters as the distance between their centers.&lt;/li&gt;
&lt;li&gt;DBSCAN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Top-down&lt;/strong&gt; divisive clustering&lt;/p&gt;
&lt;p&gt;This approach starts with all the data in a single cluster, and
repeatedly split each cluster into two using a partition algorithm until
each object is in a separate cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition algorithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K-means&lt;/li&gt;
&lt;li&gt;Gaussian mixture model&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h2&gt;
&lt;p&gt;Given dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D = \{x^{(i)},
i=1,\dots,M\}\)&lt;/span&gt;, a clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; points into &lt;span class=&#34;math inline&#34;&gt;\(K (K \le M)\)&lt;/span&gt; clusters is a partition of
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; disjoint groups &lt;span class=&#34;math inline&#34;&gt;\(\{C_1,\dots,C_K\}\)&lt;/span&gt;. Suppose we have a
function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that evaluates the
clustering &lt;span class=&#34;math inline&#34;&gt;\(\mathcal C\)&lt;/span&gt; and returns
lower score with better clustering. The best clustering will be &lt;span class=&#34;math display&#34;&gt;\[
\arg \min_{\mathcal C} f(\mathcal C)
\]&lt;/span&gt; The number of all possibilities of clustering with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements is called the Bell number,
denoted as &lt;span class=&#34;math inline&#34;&gt;\(B_M\)&lt;/span&gt;. The calculation of
the Bell number is based on dynamic programming. The number of ways to
cluster &lt;span class=&#34;math inline&#34;&gt;\(M+1\)&lt;/span&gt; elements is the sum of
number of ways to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; element and cluster
it, with the rest belong to one single cluster&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; elements and cluster
them, with the rest belong to one single cluster&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;li&gt;select &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; elements and cluster
them, with the rest belong to one single cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
B_{M+1} = \sum_{i=1}^M\binom{M}{0}B_i \\
B_0 = 1
\end{gather}
\]&lt;/span&gt; The exhaustive method will be computationally intractable. We
need either an approximation algorithm or a scoring function with
special properties.&lt;/p&gt;
&lt;h3 id=&#34;k-means&#34;&gt;K-means&lt;/h3&gt;
&lt;p&gt;K-means assumes there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;
clusters. This greatly eliminates many possibilities described above.
Its objective is &lt;span class=&#34;math display&#34;&gt;\[
\min_{c_1,\dots,c_K}\sum_{i=1}^M||x^{(i)} - c_{z^{(i)}}||^2_2, \text{
where }z^{(i)} = \arg \min_{j \in \{1,\dots,K\}}||x^{(i)}-c_j||^2_2
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}\)&lt;/span&gt; is the cluster
index to which &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; is assigned.
K-means’ objective is to assign each point to its closest cluster center
and minimize the total within-cluster square errors. For cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(C_j =
\{x^{(i)}|z^{(i)} = j\}\)&lt;/span&gt; be the set of points assigned to it,
then the cluster center of cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
c_j = \frac{1}{|C_j|}\sum_{x^{(i)} \in C_j}x^{(i)}
\]&lt;/span&gt; However, both the cluster center &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\c}{\mathrm{c}} \c\)&lt;/span&gt; and the
assignment &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\z}{\mathrm{z}}
\z\)&lt;/span&gt; is initially unknown. K-means solves this by randomly pick
up initial cluster centers and enter the
assign-data-to-clusters/update-cluster-centers loop, until the cluster
centers converge or become satisfactory. Rewrite the objective of
K-means as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\z,\c}(l(\z,\c) \coloneq \sum_{i=1}^M||x^{(i)}- c_{z^{(i)}}||^2_2)
\]&lt;/span&gt; K-means is in essence a coordinate descent of the loss &lt;span class=&#34;math inline&#34;&gt;\(l(\z,\c)\)&lt;/span&gt;. The main loop of K-means is
to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assign data points to its nearest cluster center, i.e. minimizing
over the assignment &lt;span class=&#34;math inline&#34;&gt;\(\z\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;update cluster centers according to the points assigned to,
i.e. minimizing over the centroids &lt;span class=&#34;math inline&#34;&gt;\(\c\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is monotonically decreasing
after each step in the above loop. Also, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is lower-bounded by &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; and thus K-means will converge
finally.&lt;/p&gt;
&lt;p&gt;Each cluster in K-means has a circular shape because of the Euclidean
distance it uses.&lt;/p&gt;
&lt;p&gt;For more discussion and an interesting image compression method with
K-means, please refer &lt;a href=&#34;https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;gaussian-mixture-model&#34;&gt;Gaussian Mixture Model&lt;/h3&gt;
&lt;p&gt;A cluster can also be modelled by a multi-variate Gaussian with
elliptical shape: the elliptical shape is controlled by the covariance
matrix; the location is controlled by the mean. Gaussian mixture model
is a weighted sum of, say &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;,
Gaussian distributions: &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^K\pi_j\mathcal N(x;\mu_j, \Sigma_j)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; is the prior that a
sample is generated from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th
Gaussian. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x;\mu_j,
\Sigma_j)\)&lt;/span&gt; is the probability to generate the sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. Put it together, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; is the total probability of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; over its latent &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(z =
j\)&lt;/span&gt; representing the prior condition that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is sampled from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian. &lt;span class=&#34;math display&#34;&gt;\[
p(x) = \sum_{j=1}^Kp(x, z = j) = \sum_{j=1}^Kp(x|z = j)p(z = j)
\]&lt;/span&gt; GMM is learned by &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(L(\pi,\mu,\Sigma) \triangleq
\prod_{i=1}^Mp(x^{(i)})\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt; The log-likelihood function form will be &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_{\pi,\mu,\Sigma}\Big(l(\pi,\mu,\Sigma) \triangleq \log
L(\pi,\mu,\Sigma)\Big) \\
s.t.\quad \sum_{j=1}^K\pi_j = 1
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-lagrange-multiplier&#34;&gt;Attempt with &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/lagrange-multiplier/&#34;&gt;Lagrange Multiplier&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^Mp(x^{(i)}) =
\prod_{i=1}^M\sum_{j=1}^Kp(z_j)p(x|z = z_j) =
\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^K\pi_j\mathcal
N(x^{(i)};\mu_j, \Sigma_j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\end{gather}
\]&lt;/span&gt; The Lagrangian function will be &lt;span class=&#34;math display&#34;&gt;\[
J(\pi,\mu,\Sigma,\lambda) = -\sum_{i=1}^M\log\sum_{j=1}^K\pi_j\mathcal
N(x^{(i)};\mu_j, \Sigma_j) + \lambda(\sum_{j=1}^K\pi_j - 1)
\]&lt;/span&gt; This expression is just too hard to zero the derivatives. For
example, writing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal N(x^{(i)};\mu_j,
\Sigma_j)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)}_j\)&lt;/span&gt;,
then take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial J}{\partial \pi_j} &amp;amp;= -\sum_{i=1}^M\frac{\mathcal
N(x^{(i)};\mu_j, \Sigma_j)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_j,
\Sigma_j)} + \lambda \\
&amp;amp;\Downarrow \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_1,
\Sigma_1)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_1, \Sigma_1)} \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_2,
\Sigma_2)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_2, \Sigma_2)} \\
&amp;amp;\dots \\
\lambda &amp;amp;= \sum_{i=1}^M\frac{\mathcal N(x^{(i)};\mu_K,
\Sigma_K)}{\sum_{k=1}^K\pi_k\mathcal N(x^{(i)};\mu_K, \Sigma_K)}\\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By far, the whole expression is too complicated for us to continue
with.&lt;/p&gt;
&lt;h4 id=&#34;attempt-with-expectation-maximization&#34;&gt;Attempt with Expectation
Maximization&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
L(\pi,\mu,\Sigma) = \prod_{i=1}^M p(x^{(i)}) = \prod_{i=1}^M
\sum_{j=1}^K p(x^{(i)}, z^{(i)} = j) \\
l(\pi,\mu,\Sigma) = \log\big(\prod_{i=1}^M\sum_{j=1}^Kp(x^{(i)}, z^{(i)}
= j)\big)
= \sum_{i=1}^M\log\sum_{j=1}^Kp(x^{(i)}, z^{(i)} = j)
\end{gather}
\]&lt;/span&gt; By Jensen’s inequality, if &lt;span class=&#34;math inline&#34;&gt;\(\forall
i \in \{ 1,\dots,M \}, j \in \{1,\dots,K\}, \alpha_j^{(i)} \ge
0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K\alpha_j^{(i)} =
1\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
l(\pi,\mu,\Sigma) &amp;amp;= \sum_{i=1}^M \log \sum_{j=1}^Kp(x^{(i)},
z^{(i)} = j) = \sum_{i=1}^M \log \sum_{j=1}^K \alpha^{(i)}_j
\frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\alpha^{(i)}_j} \\
&amp;amp;\ge B(\alpha,\pi,\mu,\Sigma) := \sum_{i=1}^M \sum_{j=1}^K
\alpha^{(i)}_j \log \frac{\mathcal N(x^{(i)};\mu_j, \Sigma_j)
\pi_j}{\alpha^{(i)}_j} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the lower bound of
&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. If we can enlarge &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, we can gently guarantee that &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is increasing in this process. As
illustrated in &lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/expectation-maximization/&#34;&gt;expectation
maximization&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; is
chosen to be &lt;span class=&#34;math inline&#34;&gt;\(p(z^{(i)}=j | x^{(i)}; \pi^t,
\mu^t,\Sigma^t)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\pi^t,
\mu^t,\Sigma^t\)&lt;/span&gt; are estimations in current iteration. With &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; being fixed, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
B(\alpha,\pi,\mu,\Sigma) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log
\mathcal N(x^{(i)};\mu_j, \Sigma_j) \pi_j - \sum_{i=1}^M \sum_{j=1}^K
\alpha^{(i)}_j \log \alpha^{(i)}_j \\
\end{aligned}
\]&lt;/span&gt; Grouping terms in &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; that
are relevant to &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;, we are to
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_\pi \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j \\
s.t.\quad \sum_{i=1}^K \pi_j = 1
\end{gather}
\]&lt;/span&gt; We do this by Lagrangian multipliers: &lt;span class=&#34;math display&#34;&gt;\[
J(\pi, \lambda) = \sum_{i=1}^M \sum_{j=1}^K \alpha^{(i)}_j \log \pi_j +
\lambda(1 - \sum_{i=1}^K\pi_i )
\]&lt;/span&gt; Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\pi_k\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial J}{\partial \pi_k} =\sum_{i=1}^M
\frac{\alpha^{(i)}_k}{\pi_k} - \lambda \\
\Downarrow \\
\pi_k^{t+1} = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{\lambda}
\]&lt;/span&gt; With the knowledge that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^K \pi_j = 1, \sum_{i=1}^M\alpha^{(i)}_j
= 1\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\pi_k^{t+1} = \frac{\sum_{i=1}^M\alpha^{(i)}_k}{M}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \mu_k} &amp;amp;= \frac{\partial \sum_{i=1}^M
\sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j,
\Sigma_j)}{\partial \mu_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k (x^{(i)}
- \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k)}{\partial \mu_k} \\
&amp;amp;= \sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{i=1}^M \alpha^{(i)}_k \Sigma_k^{-1} (x^{(i)} - \mu_k) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k) &amp;amp;= 0, \text{ by the
invertibility of $\Sigma_j^{-1}$} \\
\Downarrow \\
\mu_k^{t+1} &amp;amp;= \frac{\sum_{i=1}^M \alpha^{(i)}_k x^{(i)}}{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k\)&lt;/span&gt;
and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial B}{\partial \Sigma_k} &amp;amp;= \frac{\partial \sum_{i=1}^M
\sum_{j=1}^K \alpha^{(i)}_j \log \mathcal N(x^{(i)};\mu_j,
\Sigma_j)}{\partial \Sigma_k} \\
&amp;amp;= \frac{-\frac{1}{2} \partial \sum_{i=1}^M \alpha^{(i)}_k ((x^{(i)}
- \mu_k)^T \Sigma_k^{-1} (x^{(i)} - \mu_k) + \log|\Sigma_k|)}{\partial
\Sigma_k} \\
&amp;amp;= -\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)}
- \mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
-\frac{1}{2} \sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} -
\mu_k) (x^{(i)} - \mu_k)^T \Sigma_k^{-1} + \Sigma_k^{-1}) &amp;amp;= 0 \\
\sum_{i=1}^M \alpha^{(i)}_k (-\Sigma_k^{-1} (x^{(i)} - \mu_k) (x^{(i)} -
\mu_k)^T + I) &amp;amp;= 0 \\
\end{aligned} \\
\Downarrow \\
\Sigma_k^{t+1} = \frac{\sum_{i=1}^M \alpha^{(i)}_k (x^{(i)} - \mu_k)
(x^{(i)} - \mu_k)^T}{\sum_{i=1}^M \alpha^{(i)}_k} \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;soft-k-means&#34;&gt;Soft K-means&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha^{(i)}_j\)&lt;/span&gt; in GMM, which
measures how likely the sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is
generated from &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th Gaussian, can
be viewed as a contribution of sample &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to the cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. That is, a sample can contribute
different portions to different clusters and these portions add up to
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the case where &lt;span class=&#34;math inline&#34;&gt;\(\pi_j = \frac{1}{K},
\Sigma_k = I\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p(x^{(i)}|z^{(i)} = j) = \mathcal N(x^{(i)};\mu_j, I) =
\frac{1}{\sqrt{(2\pi)^N}}\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2) \\
\alpha^{(i)}_j = p(z^{(i)} = j | x^{(i)}) = \frac{p(x^{(i)}|z^{(i)} =
j)p(z^{(i)} = j)}{p(x^{(i)})} = \frac{\mathcal N(x^{(i)};\mu_j, I)
\frac{1}{K}}{\frac{1}{K} \sum_{k=1}^K \mathcal N(x;\mu_k, I)} =
\frac{\exp(-\frac{1}{2}||x^{(i)}-\mu_j||^2_2)}{\sum_{k=1}^K
\exp(-\frac{1}{2}||x^{(k)}-\mu_j||^2_2)}
\end{gather}
\]&lt;/span&gt; The “soft K-means” comes from the softmax of the Euclidean
distance.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/gaussian-mixture-models-vs-k-means-which-one-to-choose-62f2736025f0&#34;&gt;Gaussian
Mixture Models vs K-Means. | by K.Kubara | Towards Data Science&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Angle Regression</title>
      <link>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</link>
      <pubDate>Sun, 19 Dec 2021 15:09:58 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/least-angle-regression/</guid>
      <description>

&lt;h2 id=&#34;forward-selection&#34;&gt;Forward Selection&lt;/h2&gt;
&lt;p&gt;In cases where we are to solve &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;
so that &lt;span class=&#34;math inline&#34;&gt;\(Y = XW\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Y \in \R^M, X \in \R^{M \times N}, W \in
\R^N\)&lt;/span&gt;, we can do it in an iterative and greedy way.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right,
do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \frac{Y \cdot \mathrm x^{(k)}}{||\mathrm x ^{(k)}||_2} \mathrm
x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; from
&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or we have used all columns in &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently this algorithm is efficient. However, it does not
guarantee an exact solution, even when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; lies in the column space of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;forward-stagewise&#34;&gt;Forward Stagewise&lt;/h2&gt;
&lt;p&gt;Like forward selection, forward stagewise selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Unlike Forward
Selection, Forward Selection does not subtract the whole projection from
&lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Instead, it proceeds along
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; with a small
step.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\},\eta\text{ is a small
constant}\)&lt;/span&gt; initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select among &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt; has the largest cosine
distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. Then for each
&lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(i)}\)&lt;/span&gt; from left to right,
do the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \eta \frac{Y \cdot \mathrm x^{(k)}}{||\mathrm x ^{(k)}||_2}
\mathrm x ^{(k)} \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;del&gt;Remove &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt;
from &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/del&gt; Go to step 2 until
&lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt; is sufficiently small.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apparently forward stagewise gives an exact solution when &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is small enough. But it is more
time-consuming.&lt;/p&gt;
&lt;h2 id=&#34;least-angle-regression&#34;&gt;Least Angle Regression&lt;/h2&gt;
&lt;p&gt;LARS a is a compromise between forward selection and forward
stagewise. Likewise, LARS selects a &lt;span class=&#34;math inline&#34;&gt;\(\mathrm
x^{(k)}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm
x^{(k)}\)&lt;/span&gt; has the largest cosine distance to &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;. LARS proceeds stagewise like
forward stagewise, however with its own methodology to determine the
step size.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_{res} = Y, C = \{\mathrm x^{(1)},
\mathrm x^{(2)}, \dots, \mathrm x^{(N)}\}, d = \arg\max_{\mathrm x \in
C}\frac{Y \cdot \mathrm x }{||\mathrm x||_2}\)&lt;/span&gt;
initially.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then do the following: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\hat Y = \eta \frac{Y \cdot d}{||d||_2} d \\
Y_{res} = Y_{res} - \hat Y
\end{gathered}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is determined such
that there is some &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)} \in
C\)&lt;/span&gt; onto which the projection of &lt;span class=&#34;math inline&#34;&gt;\((Y -
\hat Y)\)&lt;/span&gt; is the same as that onto &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Or in other words, &lt;span class=&#34;math inline&#34;&gt;\((Y - \hat Y)\)&lt;/span&gt; lies on the bisector
hyper-plane between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm x^{(k)}\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; be along this bisector.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to step 2 until &lt;span class=&#34;math inline&#34;&gt;\(Y_{res}\)&lt;/span&gt;
is sufficiently small.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


</description>
    </item>
    
    <item>
      <title>Dimension Reduction</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/</link>
      <pubDate>Fri, 07 Jan 2022 12:39:56 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/dimensionality-reduction/</guid>
      <description>

&lt;h2 id=&#34;unsupervised-dimension-reduction&#34;&gt;Unsupervised Dimension
Reduction&lt;/h2&gt;
&lt;p&gt;Dimensionality reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduces computational cost;&lt;/li&gt;
&lt;li&gt;de-noises by projecting onto lower-dimensional space and back to
original space;&lt;/li&gt;
&lt;li&gt;makes results easier to understand by reducing the
collinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compared to feature selection,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the goal of feature selection is to remove features that are not
informative with respect to the class label. This obviously reduces the
dimensionality of the feature space;&lt;/li&gt;
&lt;li&gt;dimensionality reduction can be used to find a meaningful lower-dim
feature space even when there is information in each feature dimension
so that none can be discarded;&lt;/li&gt;
&lt;li&gt;dimensionality reduction is unsupervised while feature selection is
supervised.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compared to data compression,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dimensionality reduction can be seen as a simplistic form of data
compression. But they are not equivalent, as the goal of data
compression is to reduce the entropy of the representation, which is not
limited to the dimensionality reduction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-dimensionality-reduction&#34;&gt;Linear Dimensionality
Reduction&lt;/h3&gt;
&lt;p&gt;Linear dimensionality reduction projects data onto lower-dimensional
space by representing the data with a new basis consisting of some major
components. Mathematically, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x^{(i)} = \sum_{k=1}^Kz^{(i)}_kb^{(k)}, \text{ where $z^{(i)} \in \R^K$
is the weight, $b^{(k)} \in \R^N$ is the basis vector.} \\
X = BZ, \text{ where $X = [x^{(i)}, \dots, x^{(M)}], B = [b^{(1)},
\dots, b^{(K)}], Z = [z^{(1)}, \dots, z^{(M)}]$}
\end{gather}
\]&lt;/span&gt; The objective can be set to minimize the error when recovering
from &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
\min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X - BZ)^2_{ij}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;alternating-least-squares&#34;&gt;Alternating Least Squares&lt;/h4&gt;
&lt;p&gt;By leveraging the idea of coordinate descent and OLS solution to
linear regression, we can optimize &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; alternatively, until convergence.&lt;/p&gt;
&lt;p&gt;Fix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, take derivative w.r.t.
&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
2(X - BZ)(-Z^T) = 0 \\
BZZ^T = XZ^T \\
B = XZ^T(ZZ^T)^{-1}
\end{gather}
\]&lt;/span&gt; Fix &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, take derivative
w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(Z^T\)&lt;/span&gt; and make it zero to give
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\frac{\partial||X - BZ||^2_F}{\partial Z^T} = \frac{\partial||X^T -
Z^TB^T||^2_F}{\partial Z^T} = 2(X^T - Z^TB^T)(-B) = 0 \\
2(X^T - Z^TB^T)(-B) = 0 \\
Z^TB^TB = X^TB \\
Z^T = X^TB(B^TB)^{-1} \\
Z = (B^TB)^{-1}B^TX
\end{gather}
\]&lt;/span&gt; Assume we have run the ALS to convergence and obtain the
global optimal parameters &lt;span class=&#34;math display&#34;&gt;\[
B^\star, Z^\star = \arg \min_{B,Z} = ||X - BZ||^2_F = \sum_{ij}(X -
BZ)^2_{ij}
\]&lt;/span&gt; Let any invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(R \in
\R^{K \times K}\)&lt;/span&gt;. We can construct a pair of &lt;span class=&#34;math inline&#34;&gt;\(\tilde B, \tilde Z\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde B = B^\star R, \tilde Z =
R^{-1}Z^\star\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||X - \tilde B \tilde Z||^2_F = ||X - B^\star RR^{-1} Z^\star||^2_F =
||X - B^\star Z^\star||^2_F
\]&lt;/span&gt; Thus the global optima is not unique. We may force
regularization on &lt;span class=&#34;math inline&#34;&gt;\(B, Z\)&lt;/span&gt; and obtain
the unique optima.&lt;/p&gt;
&lt;h4 id=&#34;principal-component-analysis&#34;&gt;&lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;Principal Component Analysis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Suppose the SVD for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X = U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U$ is $N \times N$, $\Sigma$ is diagonal, $V$ is $M \times M$} \\
UU^T = I \\
VV^T = I \\
\Sigma = diag_{N \times M}(\sigma_1, \sigma_2, ..., \sigma_p) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_p \ge 0 \\
p = \min\{N,M\}
\end{gather}
\]&lt;/span&gt; By only preserving the &lt;span class=&#34;math inline&#34;&gt;\(K \le
\rank(\Sigma)\)&lt;/span&gt; most prominent singular values, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be approximated as &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{$U_K \in \R^{N \times K}$ is first $K$ columns of $U$} \\
\text{$V_K \in \R^{M \times K}$ is first $K$ columns of $V$} \\
\Sigma_K \in \R^{K \times K} = diag(\sigma_1, \sigma_2, ..., \sigma_K)
\\
\end{gather}
\]&lt;/span&gt; &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/#Eckart-Young-Mirsky Theorem&#34;&gt;Eckart-Young-Mirsky
theorem&lt;/a&gt; will tell that &lt;span class=&#34;math inline&#34;&gt;\(U_K\Sigma_KV^T_K\)&lt;/span&gt; is the best
approximation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; among &lt;span class=&#34;math inline&#34;&gt;\(N \times M\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in terms of Frobenius norm.&lt;/p&gt;
&lt;p&gt;We may obtain the &lt;span class=&#34;math inline&#34;&gt;\(B^\star,
Z^\star\)&lt;/span&gt; by &lt;span class=&#34;math display&#34;&gt;\[
B^\star = U_K, Z^\star = \Sigma_K V^T_K
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the data is centered in advance, &lt;span class=&#34;math inline&#34;&gt;\(B^\star\)&lt;/span&gt; is essentially the principal
component in PCA and &lt;span class=&#34;math inline&#34;&gt;\(Z^\star\)&lt;/span&gt; is the
transformed &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in the basis formed
by &lt;span class=&#34;math inline&#34;&gt;\(B^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;random-projection&#34;&gt;Random Projection&lt;/h4&gt;
&lt;p&gt;The above methods all minimize the Frobenius norm during
reconstruction. This objective may become time-consuming when input
dimension becomes large. We may use some randomly-generated vectors as
basis to do the projection. This greatly saves time, at the expense of
losing accuracy. We can measure such projection by checking whether the
structure of the data can be preserved, e.g. the distance between
points.&lt;/p&gt;
&lt;p&gt;Johnson-Lindenstrauss Lemma tells that a random function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \frac{1}{\sqrt{K}}Ax\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(K
\times N\)&lt;/span&gt; matrix with i.i.d. entries sampled from a standard
Gaussian, can preserve the distance between any two points within error
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
(1 - \epsilon)||x^{(i)} - x^{(j)}||^2_2 \le ||f(x^{(i)}) -
f(x^{j})||^2_2 \le (1 + \epsilon)||x^{(i)} - x^{(j)}||^2_2
\]&lt;/span&gt; with the probability at least &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt; as long as &lt;span class=&#34;math display&#34;&gt;\[
K \ge \frac{8\log M}{\epsilon^2}
\]&lt;/span&gt; And usually this requirement is quite conservative.&lt;/p&gt;
&lt;h3 id=&#34;non-linear-dimensionality-reduction&#34;&gt;Non-linear Dimensionality
Reduction&lt;/h3&gt;
&lt;p&gt;We can introduce the feature mapping to handle the non-linearity
problem in Dimensionality Reduction. Similar to non-linear regression,
we can introduce the kernel trick in this case, yielding the &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/#Kernel PCA&#34;&gt;Kernel PCA&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;supervised-dimensionality-reduction&#34;&gt;Supervised Dimensionality
Reduction&lt;/h2&gt;
&lt;p&gt;There is no guarantee that the unsupervised Dimension Reduction can
throw insight into the classification problem. It may or may not help.
Otherwise supervised Dimension Reduction such as &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/&#34;&gt;Fisher’s Linear Discriminant&lt;/a&gt; finds
a lower-dimensional space so as to minimize the class overlap.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</link>
      <pubDate>Tue, 11 Jan 2022 16:10:51 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/</guid>
      <description>

&lt;p&gt;Given a data matrix &lt;span class=&#34;math inline&#34;&gt;\(X = [x^{(1)},
x^{(2)}, ..., x^{(M)}] \in \mathbb R^{N \times M}\)&lt;/span&gt;, the goal of
PCA is to identify the directions of maximum variance contained in the
data (compared with directional derivative, this is directional
variance) and project data onto those directions.&lt;/p&gt;
&lt;h3 id=&#34;linear-pca&#34;&gt;Linear PCA&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(v \in \mathbb R^{N \times 1}\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 = 1\)&lt;/span&gt;, the variance in
the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2, \text{where }\mu =
\frac{1}{M}\sum_{i=1}^Mv^Tx^{(i)} = v^T\bar x
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{M}\sum_{i=1}^Mx^{(i)} =
0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - \mu)^2 =
\frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)} - v^T\bar x)^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(x^{(i)} - \bar x)(x^{(i)} - \bar x)^Tv
\\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(x^{(i)} - \bar x)(x^{(i)} - \bar
x)^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^Mx^{(i)}(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^TXX^Tv \\
\end{aligned}
\]&lt;/span&gt; Suppose we want to identify the direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; of the maximum variance, we can
formulate the problem as &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v \quad \frac{1}{M}v^TXX^Tv \\
s.t.\quad v^Tv = 1
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\frac{1}{M}XX^T\)&lt;/span&gt;, which is real symmetric, we can form the
Lagrangian function: &lt;span class=&#34;math display&#34;&gt;\[
L(v, \lambda) = v^T\Sigma v + \lambda (1 - v^Tv)
\]&lt;/span&gt; We represent the constraint as &lt;span class=&#34;math inline&#34;&gt;\(1 -
v^Tv = 0\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(v^Tv - 1 =
0\)&lt;/span&gt;. The reason is if we take derivative w.r.t &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial v} = 2\Sigma v - 2\lambda v
\]&lt;/span&gt; which is more intuitive than &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial v} = 2\Sigma v +
2\lambda v\)&lt;/span&gt;. Let the derivative be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v = \lambda v\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(v^tv = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v
\ne 0\)&lt;/span&gt;, this means the optimal solution &lt;span class=&#34;math inline&#34;&gt;\((\lambda^\star, v^\star)\)&lt;/span&gt; must be a pair
of eigenvalue of eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v^\star = v_i, \lambda^\star = \lambda_i
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; is rescaled
such that &lt;span class=&#34;math inline&#34;&gt;\(v_i^Tv_i = 1\)&lt;/span&gt;. Substitute
the result back to the objective to give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}v^TXX^Tv &amp;amp;= (v_i)^T\Sigma v_i \\
&amp;amp;= (v_i)^T\lambda_iv_i \\
&amp;amp;= \lambda_i(v_i)^Tv_i \\
&amp;amp;= \lambda_i
\end{aligned}
\]&lt;/span&gt; So the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; largest
directions of variance will be the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_N\)&lt;/span&gt;, corresponding to the
eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 &amp;gt; \lambda_2 &amp;gt;
... &amp;gt; \lambda_N\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;compared-with-svd&#34;&gt;Compared with SVD&lt;/h4&gt;
&lt;p&gt;Intuitively, principal components can be obtained by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Diagonalizable&#34;&gt;spectral decomposition&lt;/a&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;’s covariance matrix. But in practice,
principal components are usually solved with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;, which is more efficient in
computation and can handle sparse representation. One thing worth notice
is that, before applying SVD, PCA centers the data firstly. That said,
we cannot equalize PCA and SVD. For a more detailed discussion on the
relation between PCA and SVD, please refer to &lt;a href=&#34;https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8&#34;&gt;this
blog&lt;/a&gt; and &lt;a href=&#34;https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca&#34;&gt;this
post&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;another-view-on-pca&#34;&gt;Another view on PCA&lt;/h4&gt;
&lt;p&gt;Finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\max_{v} \frac{1}{M}\sum_{i=1}^M(v^Tx^{(i)})^2 \\
\]&lt;/span&gt; is equivalent to finding &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; different &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; to &lt;span class=&#34;math display&#34;&gt;\[
\min_{v} \frac{1}{M}\sum_{i=1}^M||x^{(i)} - v^Tx^{(i)}v||^2
\]&lt;/span&gt; both subject to &lt;span class=&#34;math inline&#34;&gt;\(||v||^2 =
1\)&lt;/span&gt; and inter-orthogonality. This second notation is essentially
the projection of &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(||v|| = 1\)&lt;/span&gt; and thus the objective
indicates minimizing the overall approximation error.&lt;/p&gt;
&lt;h3 id=&#34;kernel-pca&#34;&gt;Kernel PCA&lt;/h3&gt;
&lt;p&gt;We can map the data to a higher-dimension space: &lt;span class=&#34;math inline&#34;&gt;\(x^{(i)} \to \phi(x^{(i)})\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X} = [\phi(x^{(1)}), \phi(x^{(2)}), ...,
\phi(x^{(M)})]\)&lt;/span&gt;. Kernel tricks allow us not to explicitly define
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, but to only focus on the
inner product of mapped data: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal
K(x^{(i)}, x^{(j)}) = \phi(x^{i})^T\phi(x^{j})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, the variance along direction &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(v^Tv=1\)&lt;/span&gt;, is given by &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2, \text{where }\mu =
\frac{1}{M}\sum_{i=1}^Mv^T\phi (x^{(i)}) = v^T\bar\phi(x)
\]&lt;/span&gt; Under the assumption that data is pre-centered so that &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x) = \frac{1}{M}\sum_{i=1}^M\phi(x^{(i)})
= 0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - \mu)^2 =
\frac{1}{M}\sum_{i=1}^M(v^T\phi (x^{(i)}) - v^T\bar\phi(x))^2 \\
&amp;amp;= \frac{1}{M}\sum_{i=1}^Mv^T(\phi(x^{(i)}) - \bar{\phi(x)})(\phi
(x^{(i)}) - \bar\phi(x))^Tv \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M(\phi(x^{(i)}) - \bar\phi(x))(\phi
(x^{(i)}) - \bar\phi(x))^T)v \\
&amp;amp;= \frac{1}{M}v^T(\sum_{i=1}^M\phi(x^{(i)})(\phi(x^{(i)})^T)v \\
&amp;amp;\Downarrow_{\text{by column-row expansion}} \\
&amp;amp;= \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
\end{aligned}
\]&lt;/span&gt; Then comes the standard form of linear PCA. Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\frac{1}{M}\mathcal{X}\mathcal{X}^T\)&lt;/span&gt;, we would like to solve
&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\max_v\quad \frac{1}{M}v^T\mathcal{X}\mathcal{X}^Tv \\
s.t.\quad v^T v=1
\end{gather}
\]&lt;/span&gt; This is equivalent to solving &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvectors. Unfortunately,
this cannot be directly solved like in linear PCA since we don’t know
&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. However note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
\Sigma v &amp;amp;= \lambda v \\
v &amp;amp;= \frac{1}{\lambda}\Sigma v
\end{aligned} \\
\begin{aligned}
v &amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M\phi(x^{(i)})[(\phi(x^{(i)})^Tv]
\\
&amp;amp;= \frac{1}{M\lambda}\sum_{i=1}^M[(\phi(x^{(i)})^Tv]\phi(x^{(i)})
\end{aligned}
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; and
can be written as &lt;span class=&#34;math display&#34;&gt;\[
v = \mathcal{X}\alpha, \alpha \in \R^N
\]&lt;/span&gt; Substitute this back to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma v =
\lambda v\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{1}{M}\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
\lambda\mathcal{X}\alpha \\
\frac{1}{M}\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\mathcal{X}^T\mathcal{X}\mathcal{X}^T\mathcal{X}\alpha &amp;amp;=
M\lambda\mathcal{X}^T\mathcal{X}\alpha \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(K =
\mathcal{X}^T\mathcal{X}\)&lt;/span&gt; which is invertible. Then, &lt;span class=&#34;math display&#34;&gt;\[
KK\alpha = M\lambda K\alpha \\
K\alpha = M\lambda\alpha
\]&lt;/span&gt; We can solve &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; as
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvector. Kernel trick can
be applied since that &lt;span class=&#34;math inline&#34;&gt;\(K_{ij} =
(\mathcal{X}^T\mathcal{X})_{ij} =
\phi(x^{(i)})^T\phi(x^{(j)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\lambda\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s eigenvalue, which is proportional to
&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;’s eigenvalue and thus the
objective. So &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s largest &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvalues correspond to &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; largest objective respectively. &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \alpha_2, ..., \alpha_L\)&lt;/span&gt; has to
be solved with constraint that &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i^T\alpha_i = \frac{1}{M\lambda}\)&lt;/span&gt;
because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^Tv &amp;amp;= (\mathcal{X}\alpha)^T\mathcal{X}\alpha \\
&amp;amp;= \alpha^T(\mathcal{X}^T\mathcal{X})\alpha \\
&amp;amp;= \alpha^TK\alpha \\
&amp;amp;= M\lambda\alpha^T\alpha \\
&amp;amp;= 1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given a sample &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;, we first
transform it with &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and
dot-product with &lt;span class=&#34;math inline&#34;&gt;\(v_1,\dots,v_L\)&lt;/span&gt; to
get the new coordinates. To get its &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th coordinate, &lt;span class=&#34;math display&#34;&gt;\[
\phi(x^*)^Tv_j = \phi(x^*)^T\mathcal X\alpha_j = [\mathcal K(x^*,
x^{(1)}), \mathcal K(x^*, x^{(2)}), \dots, \mathcal K(x^*,
x^{(M)})]\alpha_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&#34;centralizing-in-kernel-trick&#34;&gt;Centralizing in Kernel Trick&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde\phi(x^{(i)}) = \phi(x^{(i)}) -
\bar\phi(x), \tilde{\mathcal{X}} = [\tilde\phi(x^{(1)}),
\tilde\phi(x^{(2)}), ..., \tilde\phi(x^{(M)})]\)&lt;/span&gt;. We have assumed
that &lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt; is pre-centered
by subtracting &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.
However, as said, we didn’t explicitly define &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, so there is no way to calculate
&lt;span class=&#34;math inline&#34;&gt;\(\phi(x^{(i)})\)&lt;/span&gt;, let alone &lt;span class=&#34;math inline&#34;&gt;\(\bar\phi(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From another perspective, the objective of pre-centering is to get
&lt;span class=&#34;math inline&#34;&gt;\(\tilde K =
\tilde{\mathcal{X}}\tilde{\mathcal{X}}^T =
\sum_{i=1}^M\tilde\phi(x^{(i)})\tilde\phi(x^{(i)})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{M \times 1}\)&lt;/span&gt;
represent a &lt;span class=&#34;math inline&#34;&gt;\(M \times 1\)&lt;/span&gt; column
vector with all elements being &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\bar\phi(x) = \frac{1}{M}(\phi(x^{(1)}) + \phi(x^{(2)}) + ... +
\phi(x^{(M)})) = \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_M\)&lt;/span&gt;
represents a &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt; matrix with
all elements being &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}\)&lt;/span&gt;.
Pre-center all data to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde{\mathcal{X}} &amp;amp;= [\tilde\phi(x^{(1)}), \tilde\phi(x^{(2)}),
..., \tilde\phi(x^{(M)})]\\
&amp;amp;= [\phi(x_1) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1},
\phi(x^{(2)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}, ...,
\phi(x^{(M)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}] \\
&amp;amp;= [\phi(x^{(1)}), \phi(x^{(2)}), ..., \phi(x^{(M)})] -
\frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}\mathbb{1}_{M \times 1}^T
\\
&amp;amp;= \mathcal{X} - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times
1}\mathbb{1}_{M \times 1}^T \\
&amp;amp;= \mathcal{X} - \mathcal{X}\mathbb{1}_M \\
\tilde K &amp;amp;= \tilde{\mathcal{X}}^T\tilde{\mathcal{X}} \\
&amp;amp;= (\mathcal{X} - \mathcal{X}\mathbb{1}_M)^T(\mathcal{X} -
\mathcal{X}\mathbb{1}_M) \\
&amp;amp;= (\mathcal{X}(I - \mathbb{1}_M))^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)^T\mathcal{X}^T\mathcal{X}(I - \mathbb{1}_M) \\
&amp;amp;= (I - \mathbb{1}_M)K(I - \mathbb{1}_M) \\
\tilde K_{ij} &amp;amp;= (\tilde{\mathcal{X}}^T\tilde{\mathcal{X}})_{ij} \\
&amp;amp;= \tilde\phi(x^{(i)})^T\tilde\phi(x^{(j)})\\
&amp;amp;= (\phi(x^{(i)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times
1})^T(\phi(x^{(j)}) - \frac{1}{M}\mathcal{X}\mathbb{1}_{M \times 1}) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;explained-variance&#34;&gt;Explained Variance&lt;/h3&gt;
&lt;p&gt;It remains a question that in real application, how many principal
components to choose to represent the original data. Explained variance
can be a good measure on this. We can choose a number of principal
components such that they “explains” a certain percentage &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; of the original data.&lt;/p&gt;
&lt;p&gt;Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{M}v_i^TXX^Tv_i\)&lt;/span&gt; is the
directional variance explained along &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;, or the variance of the first
dimension of transformed data samples. We may choose the first &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; principal components such that &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^k v_i^TXX^Tv_i \ge \tau \sum_{i=1}^N \sigma_i^2
\]&lt;/span&gt; ## External Materials&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/59775730&#34;&gt;数据降维:
核主成分分析(Kernel PCA)原理解析&lt;/a&gt; || &lt;a href=&#34;https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained&#34;&gt;Explained
variance&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Stirling&#39;s Approximation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/</link>
      <pubDate>Mon, 09 May 2022 19:39:49 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/numerical-analysis/stirlings-approximation/</guid>
      <description>

&lt;h2 id=&#34;stirlings-approximation&#34;&gt;Stirling’s Approximation&lt;/h2&gt;
&lt;p&gt;Stirling’s approximation, which states that &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(n+1) \sim \sqrt{2 \pi n} \left( \frac{n}{e}
\right)^n\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;,
is useful when estimating the order of &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt;. Notably, it is quite accurate even
when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is small.&lt;/p&gt;
&lt;p&gt;The authentic proof entails Gamma function and Laplace’s method.
However in integer case, Stirling’s approximation can be approached with
Poisson distribution. Start from a Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
P(r ; \lambda) = e^{-\lambda} \frac{\lambda^r}{r!}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda_1)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(Y \sim P(\lambda_2)\)&lt;/span&gt;, and suppose
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(X + Y \sim P(\lambda_1 + \lambda_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proof &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \Pr(X + Y = r) = \sum_{k=0}^r \Pr(X = k) \Pr(Y = r-k) \\
&amp;amp;= \sum_{k=0}^r e^{-\lambda_1} \frac{\lambda_1^k}{k!} e^{-\lambda_2}
\frac{\lambda_2^{r-k}}{(r-k)!} \\
&amp;amp;= \frac{e^{-(\lambda_1 + \lambda_2)}}{r!} \sum_{k=0}^r \frac{r!}{k!
(r-k)!} \lambda_1^k \lambda_2^{r-k} \\
&amp;amp;= e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^r}{r!}
\\
&amp;amp;= P(r; \lambda_1 + \lambda_2)
\end{aligned}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, a random variable &lt;span class=&#34;math inline&#34;&gt;\(X \sim
P(\lambda)\)&lt;/span&gt; (with integer &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) can be treated as the addition
of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; independent &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim P(1)\)&lt;/span&gt;. By the central limit
theorem, for a large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\mathbb \Pr(\frac{\underbrace{\sum_i Y_i}_X - \lambda}{\sqrt{\lambda}}
\le x) \simeq \Phi(x)
\]&lt;/span&gt; Or put it another way, the mass of the Poisson distribution
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; follows is well approximated by
the density of the Gaussian distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P(x;\lambda) &amp;amp;\simeq N(x; \lambda, \lambda) \\
e^{-r} \frac{\lambda^r}{r!} &amp;amp;\approx \frac{1}{\sqrt{2\pi \lambda}}
e^{-\frac{(r - \lambda)^2}{2\lambda}}
\end{aligned}
\]&lt;/span&gt; Plug &lt;span class=&#34;math inline&#34;&gt;\(r = \lambda\)&lt;/span&gt; into
this formula and rearrange it to have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
e^{-\lambda} \frac{\lambda^\lambda}{\lambda!} &amp;amp;\approx
\frac{1}{\sqrt{2\pi \lambda}} \\
\lambda! &amp;amp;\approx \sqrt{2\pi \lambda} \left( \frac{\lambda}{e}
\right)^\lambda
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Jenson-Shannon Divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/</link>
      <pubDate>Tue, 03 May 2022 15:10:21 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/jenson-shannon-divergence/</guid>
      <description>

&lt;h2 id=&#34;jenson-shannon-divergence&#34;&gt;Jenson-Shannon Divergence&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, &lt;strong&gt;Jenson-Shannon
divergence&lt;/strong&gt; is another method of measuring the distance between
two distributions. It is based on &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; with some notable
differences. KL-divergence does not make a good measure of distance
between distributions, since in the first place it is &lt;em&gt;not
symmetric&lt;/em&gt;. Another disadvantage of KL-divergence is that it is
&lt;em&gt;not bounded from above&lt;/em&gt;. Jenson-Shannon divergence, on the other
hand, overcomes these two problems of KL-divergence.&lt;/p&gt;
&lt;p&gt;Given two distributions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; defined on the same sample space,
Jenson-Shannon divergence is defined as &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\JSD}{\mathop{\text{JSD}}}
\JSD(p;q) \coloneq \frac{1}{2} (D_\text{KL}(p || m) + D_\text{KL}(q ||
m)), \text{where $m = \frac{p+q}{2}$}
\]&lt;/span&gt; Since the JSD is the addition of two KL-divergences, it is
non-negative by the non-negativity of KL-divergence and it reaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(p=q\)&lt;/span&gt;. On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\JSD(p;q) &amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log
\frac{p(x)}{(p(x)+q(x))/2} + \E_{x \sim q} \log
\frac{q(x)}{(p(x)+q(x))/2} \big) \\
&amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log \frac{2}{( 1 + e^{\ln
\frac{q(x)}{p(x)}} )} + \E_{x \sim q} \log \frac{2}{( 1 + e^{\ln
\frac{p(x)}{q(x)}} )} \big) \\
\end{aligned}
\]&lt;/span&gt; Due to the concavity of &lt;span class=&#34;math inline&#34;&gt;\(f(x) =
\log \frac{2}{1 + e^x}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\JSD(p;q) &amp;amp;= \frac{1}{2} \big( \E_{x \sim p} \log \frac{2}{( 1 +
e^{\ln \frac{q(x)}{p(x)}} )} + \E_{x \sim q} \log \frac{2}{( 1 + e^{\ln
\frac{p(x)}{q(x)}} )} \big) \\
&amp;amp;\le \frac{1}{2} \big( \log \frac{2}{( 1 + e^{\E_{x \sim p} \ln
\frac{q(x)}{p(x)}} )} + \log \frac{2}{( 1 + e^{\E_{x \sim q} \ln
\frac{p(x)}{q(x)}} )} \big) \\
&amp;amp;\le \frac{1}{2} \big( 2 \log \frac{2}{( 1 + e^{( \E_{x \sim p} \ln
\frac{q(x)}{p(x)} + \E_{x \sim q} \ln \frac{p(x)}{q(x)} ) / 2} )} \big)
\\
&amp;amp;= \log \frac{2}{( 1 + e^{-\frac{1}{2} ( D_\text{KL}(p||q) +
D_\text{KL}(q||p) )} )}
\end{aligned}
\]&lt;/span&gt; This upper bound is attributed to Crooks. Since the
KL-divergence can go to positive infinity, we can conclude that &lt;span class=&#34;math inline&#34;&gt;\(\JSD(p;q)\)&lt;/span&gt; is upper-bounded by &lt;span class=&#34;math inline&#34;&gt;\(\log 2\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\J}{\mathop{\text{J}}} \J(p;q)
\coloneq \frac{1}{2} ( D_\text{KL}(p||q) + D_\text{KL}(q||p) )\)&lt;/span&gt;
term is also known as &lt;strong&gt;Jeffreys divergence&lt;/strong&gt; (the
coefficient &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; may be
ignored in some other place). Even more accurately, the upper bound can
be rewritten as
(&lt;a href=&#34;https://chunxy.github.io/uploads/Inequalities%20between%20the%20Jenson-Shannon%20and%20Jeffreys%20divergences.pdf&#34; target=&#34;_blank&#34;&gt;in
this note&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\JSD(p;q) \le \min (\frac{1}{4} \J(p;q), \log \frac{2}{1 +
e^{-\J(p;q)}})
\]&lt;/span&gt; A lower bound in terms of Jeffreys divergence can be derived
as
(&lt;a href=&#34;https://chunxy.github.io/uploads/A%20Note%20on%20Bound%20for%20Jensen-Shannon%20Divergence%20by%20Jeffreys.pdf&#34; target=&#34;_blank&#34;&gt;in
this note&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\JSD(p;q) \ge \frac{1}{4} \ln(1 + 2\J(p;q))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remarkably, the square root of JSD between two distributions
satisfies the &lt;a href=&#34;https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition&#34;&gt;metric
axioms&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;relation-with-entropy&#34;&gt;Relation with Entropy&lt;/h3&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\JSD(p;q) = \frac{1}{2} (D_\text{KL}(p || \frac{p+q}{2}) +
D_\text{KL}(q || \frac{p+q}{2})) \\
&amp;amp;= \frac{1}{2} [H(p||\frac{p+q}{2}) - H(p) + H(q||\frac{p+q}{2}) -
H(q)] \\
&amp;amp;=
\begin{aligned}[t]
&amp;amp;-\frac{1}{2} [\E_{x \sim p} \log \frac{p+q}{2}(x) + \E_{x \sim q}
\log \frac{p+q}{2}(x)] \\
&amp;amp;- \frac{1}{2} [H(p) + H(q)] \\
\end{aligned} \\
&amp;amp;= -\E_{x \sim \frac{p+q}{2}} \frac{p+q}{2}(x) - \frac{1}{2} [H(p) +
H(q)] \\
&amp;amp;= H(\frac{p+q}{2}) - \frac{1}{2} [H(p) + H(q)]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Unconscious Statistics</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/</link>
      <pubDate>Tue, 03 May 2022 10:50:54 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/</guid>
      <description>

&lt;h2 id=&#34;law-of-the-unconscious-statistician&#34;&gt;Law of the Unconscious
Statistician&lt;/h2&gt;
&lt;p&gt;In probability theory and statistics, the &lt;strong&gt;law of the
unconscious statistician&lt;/strong&gt; (LOTUS), is a theorem used to
calculate the expected value of a function &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt; of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; when one knows the probability
distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; but one does not
know the distribution of &lt;span class=&#34;math inline&#34;&gt;\(g(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the probability mass function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \sum_x g(x) p(x)
\]&lt;/span&gt; If the probability density function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)p(x)\ \d x
\]&lt;/span&gt; If the cumulative distribution function is known, &lt;span class=&#34;math display&#34;&gt;\[
\E[g(X)] = \int_{-\infty}^{+\infty} g(x)\ \d F(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;marginal-expectation&#34;&gt;Marginal Expectation&lt;/h2&gt;
&lt;p&gt;If the joint distribution of two random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is known, then the expectation of one
component can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\E[X] = \int_{-\infty}^{+\infty} x p_X(x)\; \d x =
\int_{-\infty}^{+\infty} x \int_{-\infty}^{+\infty} p(x,y)\; \d y\; \d x
= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xp(x,y)\ \d y\ \d x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math display&#34;&gt;\[
\E [X] = \E_{y \sim p_Y} [\E_{x \sim p(X|Y=y)}]  =
\int_{-\infty}^{+\infty} p(y) \bigg( \int_{-\infty}^{+\infty} x p(x|y)\
\d x \bigg) \d y
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/questions/185729/expected-value-of-a-marginal-distribution-when-the-joint-distribution-is-given&#34;&gt;StackExchange
Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-of-non-negative-random-variables&#34;&gt;Expectation of
Non-negative Random Variables&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable whose
value is non-negative, and &lt;strong&gt;its expectation exists&lt;/strong&gt;,
and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is continuous, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}[b]
\E (X) &amp;amp;= \int_{0}^{+\infty} x p(x)\ \d x = \int_{0}^{+\infty} x\ \d
\big( P(x) - 1 \big) \\
&amp;amp;= [x \big( P(x) - 1 \big)]\bigg|_{x=0}^{+\infty} - \int_0^{+\infty}
\big( P(x) - 1 \big)\ \d x
\end{aligned}
\]&lt;/span&gt; Because the expectation exists, the above expression and
especially the &lt;span class=&#34;math inline&#34;&gt;\([x \big( P(x) - 1
\big)]\bigg|_{x=0}^{+\infty}\)&lt;/span&gt; term must converge: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
[x \big( P(x) - 1 \big)]\bigg|_{x=0} = 0 \\
[P(x) - 1]\bigg|_{x \to +\infty} = 0 \Rightarrow [x \big( P(x) - 1
\big)]\bigg|_{x \to +\infty} = 0
\end{gather}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\E(X) = \int_{0}^{+\infty} \big (1 - P(x) \big)\ \d x
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is discrete and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; only takes on integer values, supposing
the max value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E(X) = \sum_{k=0}^{N} [k P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [(\sum_{j=0}^{k-1} 1) P(X = k)] \\
&amp;amp;= \sum_{k=0}^{N} [\sum_{j=0}^{k-1} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} [\sum_{k=j+1}^{N} P(X = k)] \\
&amp;amp;= \sum_{j=0}^{N-1} P(X &amp;gt; j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/305031/expectation-when-cumulative-distribution-function-is-given&#34;&gt;StackExchange
Discussion&lt;/a&gt; || &lt;a href=&#34;https://en.wikipedia.org/wiki/Summation_by_parts&#34;&gt;Summation by
Parts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;expectation-and-quantile-function&#34;&gt;Expectation and Quantile
Function&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be the PDF and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; be the CDF of a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(Q =
F^{-1}\)&lt;/span&gt; be the inverse of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;
is called the &lt;strong&gt;quantile function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and &lt;span class=&#34;math display&#34;&gt;\[
\int_0^1 Q(p)\ \d p \stackrel{p=F(x)}{\Longrightarrow} =
\int_{-\infty}^{+\infty} x f(x)\ \d x = \E(X)
\]&lt;/span&gt; &lt;a href=&#34;https://stats.stackexchange.com/a/18439&#34;&gt;StackExchange
Answer&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>协方差与相关系数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Sun, 01 May 2022 10:41:15 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description>

&lt;p&gt;以下以二维随机变量为例，展示协方差以及相关系数的概念。虽然协方差和相关系数相对期望、方差来说显得复杂，但是他们依旧是随机变量的数字特征。&lt;/p&gt;
&lt;h2 id=&#34;协方差&#34;&gt;协方差&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\([X,
Y]\)&lt;/span&gt;为一组二维随机变量，如果&lt;span class=&#34;math inline&#34;&gt;\(\mathrm
E\{[X - \mathrm E(X)][Y - \mathrm E(Y)]\}\)&lt;/span&gt;存在，则称 &lt;span class=&#34;math display&#34;&gt;\[
\notag \mathrm {Cov}(X, Y) \triangleq \mathrm E\{[X - \mathrm E(X)][Y -
\mathrm E(Y)]\}
\]&lt;/span&gt; 为随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的协方差。在实际中计算协方差时，更多的是使用以下公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\mathrm {Cov}(X, Y) = \mathrm E\{[X - \mathrm E(X)][Y - \mathrm
E(Y)]\} \\
&amp;amp;= \mathrm E[XY - X\mathrm E(Y) - \mathrm E(X)Y + \mathrm E(X)
\mathrm E(Y)] \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X)\mathrm E(Y) - \mathrm E(X)\mathrm
E(Y) + \mathrm E(X) \mathrm E(Y) \\
&amp;amp;= \mathrm E(XY) - \mathrm E(X) \mathrm E(Y)
\end{aligned}
\]&lt;/span&gt; 而二维随机变量[X, Y]对应的协方差矩阵即为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma = \begin{bmatrix} \Cov(X,X) &amp;amp; \Cov(X,Y) \\ \Cov(Y,X) &amp;amp;
\Cov(Y,Y) \\ \end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关系数&#34;&gt;相关系数&lt;/h2&gt;
&lt;p&gt;协方差考察了随机变量之间协同变化的关系，但如果采取不同的量纲，同样的数据产生的协方差相差非常大。为避免这种情况发生，我们可以首先将随机变量标准化：
&lt;span class=&#34;math display&#34;&gt;\[
X^\star = \frac{X - \E(X)}{\sqrt{\Var(X)}}，Y^\star = \frac{Y -
\E(Y)}{\sqrt{\Var(Y)}}
\]&lt;/span&gt; 再求协方差&lt;span class=&#34;math inline&#34;&gt;\(\Cov(X^\star,
Y^\star)\)&lt;/span&gt;，这便是随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的相关系数： &lt;span class=&#34;math display&#34;&gt;\[
\rho(X, Y) = \mathrm{Cov}(X^\star, Y^\star) = \frac{\Cov(X,
Y)}{\sqrt{\Var(X) \Var(Y)}}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 12:55:32 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</guid>
      <description>

&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be decomposed into &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\text{$U$ is $m \times m$, $V$ is $n \times n$} \\
U U^T = I \\
V V^T = I \\
\Sigma = \diag_{m \times n} (\sigma_1, \sigma_2, ..., \sigma_r) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_r \ge 0 \\
r = \rank A
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;This is shown by construction.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Construction of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real symmetric matrix, so it
can be diagonalized by an orthogonal matrix. Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be this matrix and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; be the corresponding diagonal
matrix consisting of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s
eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
V^{-1} (A^T A) V = \Lambda
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be arranged such that corresponding
eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are decreasing.
Because &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; consists of orthonormal
basis, we have &lt;span class=&#34;math inline&#34;&gt;\(V^T V = I\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
V^T (A^T A) V = \Lambda
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is positive semi-definite, it has
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; non-negative real eigenvalues,
&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; of which are positive. Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = \sqrt{\lambda_i}, i = 1, 2, ...,
n\)&lt;/span&gt; (which are defined as &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s the &lt;strong&gt;singular
values&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\lambda_1, \lambda_2, ..., \lambda_r &amp;gt; 0, \lambda_r, \lambda_{r+1},
..., \lambda_{n} = 0 \\
\sigma_1, \sigma_2, ..., \sigma_r &amp;gt; 0, \sigma_r, \sigma_{r+1}, ...,
\sigma_{n} = 0
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V_1 = [v_1, v_2, ..., v_r],
V_2 = [v_{r+1}, v_{r+2}, ..., v_n], V = [V_1, V_2]\)&lt;/span&gt;. Also
let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_1 =
\begin{bmatrix}
\sigma_1 \\
&amp;amp; \sigma_2 \\
&amp;amp; &amp;amp; \ddots \\
&amp;amp; &amp;amp; &amp;amp; \sigma_r
\end{bmatrix}
\]&lt;/span&gt; Complement &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_1\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to get the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma =
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T A)\)&lt;/span&gt; is of dimension &lt;span class=&#34;math inline&#34;&gt;\(n-r\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; comprises of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvectors whose eigenvalues are
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n -
r\)&lt;/span&gt; independent columns and &lt;span class=&#34;math inline&#34;&gt;\(A^T A V_2
= 0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; spans
&lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^TA)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1 V_1^T + V_2 V_2^T \\
A = A I = A V_1 V_1^T + A V_2 V_2^T = A V_1 V_1^T
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Construction of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
u_i \coloneq \frac{1}{\sigma_i} A v_i, i = 1, 2, ..., r \\
U_1 \coloneq [u_1, u_2, ..., u_r]
\end{gather}
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(AV_1 = U_1\Sigma_1\)&lt;/span&gt;.
Also, &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s columns are
orthonormal: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
u_i^T u_j &amp;amp;= (\frac{1}{\sigma_i} A v_i)^T (\frac{1}{\sigma_j} A v_j)
\\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} v_i^T A^T A v_j \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} v_i^T \lambda_j v_j \\
&amp;amp;= \frac{\sigma_j}{\sigma_i} v_i^T v_j \\
&amp;amp;=
\begin{cases}
0 &amp;amp; i \ne j \\
1 &amp;amp; i = j
\end{cases}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s are within &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt; and are orthonormal as shown,
plus that &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s span the &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;’s complement, we have &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp = \Nul(A^T)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\{ u_{r+1}, u_{r+2}, \dots, u_{m} \}\)&lt;/span&gt; be
an orthonormal basis of &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T)\)&lt;/span&gt;
such that they are orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s column vectors. We construct &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
U_2 \coloneq [u_{r+1}, u_{r+2}, ..., u_{m}] \\
U \coloneq [U_1, U_2]
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof of &lt;span class=&#34;math inline&#34;&gt;\(U \Sigma V^T = A\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
U \Sigma V^T &amp;amp;= [U_1, U_2]
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\
\end{bmatrix}
\\
&amp;amp;= [U_1 \Sigma_1, 0]
\begin{bmatrix}
V_1^T \\
V_2^T \\
\end{bmatrix}
\\
&amp;amp;= U_1 \Sigma_1 V_1^T \\
&amp;amp;= A V_1 V_1^T \\
&amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;p&gt;Note that SVD reveals that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
essentially the addition of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;
rank-&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
U \Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T
\]&lt;/span&gt; We have shown the construction process of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(U \Sigma
V^T\)&lt;/span&gt; in the above proof, i.e. the existence of such
decomposition, with which we can investigate into &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Notice that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A A^T &amp;amp;= U \Sigma V^T V \Sigma U^T \\
A A^T &amp;amp;= U \Sigma^2 U^T \\
A A^T U &amp;amp;= U \Sigma^2 U^T U \\
A A^T U &amp;amp;= U \Sigma^2
\end{aligned}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is a
diagonal matrix, we can conclude that &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; contains the eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; contains the eigenvalues of
&lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; as its diagonal entries.
Similarly &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^T A &amp;amp;= V \Sigma U^T U \Sigma V^T \\
A^T A &amp;amp;= V \Sigma^2 V^T \\
A^T A V &amp;amp;= V \Sigma^2 V^T V \\
A^T A V &amp;amp;= V \Sigma^2
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; contains the
eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; contains the eigenvalues of
&lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; as its diagonal entries.&lt;/p&gt;
&lt;p&gt;It is easy to verify that &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; actually have the same
eigenvalues. And if &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is an
eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A v\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; with the same eigenvalue. The
columns of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are called the
&lt;strong&gt;left-singular vectors&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and the columns of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are called the &lt;strong&gt;right-singular
vectors&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The time complexity of SVD is &lt;span class=&#34;math inline&#34;&gt;\(O(\min\{
m^2 n, n^2 m \})\)&lt;/span&gt;, depending on whether &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(A^T
A\)&lt;/span&gt; is used to solve &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;eckart-young-mirsky-theorem&#34;&gt;Eckart-Young-Mirsky Theorem&lt;/h2&gt;
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(r
\le \min(m,n)\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X = U
\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
\diag(\sigma_1, \sigma_2, ..., \sigma_r)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star
= U \Sigma_k V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k =
\diag(\sigma_1, \sigma_2, ..., \sigma_k)\)&lt;/span&gt;, when distance between
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Notice that in this case, &lt;span class=&#34;math display&#34;&gt;\[
Y^\star = \sum_{i=1}^k \sigma_k u_k v_k^T
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||U A||_F = ||A U||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= \tr((U A)^T U A) \\
&amp;amp;= \tr(A^T U U A) \\
&amp;amp;= \tr(A^T A) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||A
U||_F\)&lt;/span&gt;. Then for any &lt;span class=&#34;math inline&#34;&gt;\(m \times
n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||U \Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^T U \Sigma V^T V - U^T Y V||_F^2 \\
&amp;amp;= ||\Sigma - U^T Y V||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^T Y V\)&lt;/span&gt;, which is
also of rank &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||\Sigma - Z||_F^2 = \sum_{ij}(\Sigma_{ij} -
Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^r (\sigma_{i} - Z_{ii})^2 + \sum_{i&amp;gt;r}^{\min(m,n)}
Z_{ii}^2 + \sum_{i \ne j} Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \begin{cases}
\sigma_i, &amp;amp; 1 \le i \le r \\
0, &amp;amp; r \le i \le \min(m,n)
\end{cases} \\
Z_{ij} = 0, 1 \le i \le M, 1 \le j \le N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U \Sigma_k V^T\)&lt;/span&gt;. Q.E.D.&lt;/p&gt;
&lt;p&gt;Note that Eckart-Young-Mirsky theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;svd-and-diagonalization&#34;&gt;SVD and Diagonalization&lt;/h2&gt;
&lt;p&gt;It is easy to mix up SVD with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalization&lt;/a&gt;. Notably,
diagonalization factorizes square matrix while SVD factorizes any
matrix. Also, not all square matrices can be diagonalized but all
matrices can be applied with SVD. Finally, SVD can be interpreted as
rotation-scaling-rotation because &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(V^T\)&lt;/span&gt; are orthogonal. But in
diagonalization, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}\)&lt;/span&gt; are not necessarily orthogonal,
unless in the case of real symmetric matrix.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6251584.html&#34;&gt;奇异值分解(SVD)原理与在降维中的应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/22237507&#34;&gt;奇异值的物理意义是什么？
- 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Singular%20Value%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Singular
Value Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Gram-Schmidt Orthogonalization</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/</guid>
      <description>

&lt;h2 id=&#34;gram-schmidt-orthogonalization&#34;&gt;Gram-Schmidt
Orthogonalization&lt;/h2&gt;
&lt;p&gt;The Gram-Schmidt process is a simple algorithm for producing
orthogonal basis for any nonzero subspace of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given a basis &lt;span class=&#34;math inline&#34;&gt;\(\{ \x_1, \dots, \x_p
\}\)&lt;/span&gt; for a nonzero subspace &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, define&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned} \newcommand{\v}{\mathrm{v}}
\v_1 &amp;amp;= \x_1 \\
\v_2 &amp;amp;= \x_2 - \frac{\x_2 \cdot \v_1}{\v_1 \cdot \v_1} \v_1 \\
\v_3 &amp;amp;= \x_3 - \frac{\x_3 \cdot \v_1}{\v_1 \cdot \v_1} \v_1 -
\frac{\x_3 \cdot \v_2}{\v_2 \cdot \v_2} \v_2 \\
&amp;amp;\vdots \\
\v_p &amp;amp;= \x_p - \frac{\x_p \cdot \v_1}{\v_1 \cdot \v_1} \v_1 -
\frac{\x_p \cdot \v_{p-1}}{\v_{p-1} \cdot \v_{p-1}} \v_{p-1}
\end{aligned}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(\{ \v_1, \dots, \v_p
\}\)&lt;/span&gt; is an orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. In addition, &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\span}[1]{\mathrm{Span}\{#1\}}
\span{\v_1, \dots,\v_k} = \span{\x_1, \dots, \x_k} \text{\quad for $1
\le k \le p$}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;qr-factorization&#34;&gt;QR Factorization&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix with linearly
independent columns, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be
factored as &lt;span class=&#34;math inline&#34;&gt;\(A = QR\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m
\times n\)&lt;/span&gt; matrix whose columns form an orthonormal basis for
&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathrm{Col}}A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n
\times n\)&lt;/span&gt; upper triangular invertible matrix with positive
entries on its diagonal.&lt;/p&gt;
&lt;p&gt;Such factorization can be realized by Gram-Schmidt orthogonalization.
The columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, say denoted as
&lt;span class=&#34;math inline&#34;&gt;\(\x_1, \dots, \x_n\)&lt;/span&gt;, form the basis
of &lt;span class=&#34;math inline&#34;&gt;\(\Col A\)&lt;/span&gt;. Apply the Gram-Schmidt
process to construct an orthogonal basis &lt;span class=&#34;math inline&#34;&gt;\(\{
\v_1, \dots, \v_n \}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\Col
A\)&lt;/span&gt; and let &lt;span class=&#34;math display&#34;&gt;\[
Q = [\v_1, \dots, \v_n]
\]&lt;/span&gt; For every &lt;span class=&#34;math inline&#34;&gt;\(k = 1, \dots,
n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\x_k\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(\span{\x_1, \dots, \x_k} = \span{\v_1, \dots,
\v_k}\)&lt;/span&gt;. So there are constants &lt;span class=&#34;math inline&#34;&gt;\(r_{1k}, \dots, r_{kk}\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\x_k = [\v_1, \dots, \v_k]
\begin{bmatrix}
r_{1k} \\
\vdots \\
r_{kk}
\end{bmatrix}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt; is nonzero or else
&lt;span class=&#34;math inline&#34;&gt;\(\x_k\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(\span{\x_1, \dots, \x_{k-1}}\)&lt;/span&gt;, which
violates the linear independence condition of columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. We can assume &lt;span class=&#34;math inline&#34;&gt;\(r_{kk} &amp;gt; 0\)&lt;/span&gt;; otherwise we can multiply
both &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\v_k\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; without compromising previous
conditions. Let &lt;span class=&#34;math display&#34;&gt;\[
\newcommand{\r}{\mathrm{r}} \r_k =
\begin{bmatrix}
r_{1k} \\
\vdots \\
r_{kk} \\
0 \\
\vdots \\
0
\end{bmatrix}
\]&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(\x_k = Q \r_k\)&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(k = 1, \dots, n\)&lt;/span&gt;. Then &lt;span class=&#34;math display&#34;&gt;\[
A = [\v_1, \dots, \v_n] [\r_1, \dots, \r_n] = Q R
\]&lt;/span&gt; The fact that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is
invertible follows easily the fact that &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;’s columns are linearly independent.
Because &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th element of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column, i.e. &lt;span class=&#34;math inline&#34;&gt;\(r_{kk}\)&lt;/span&gt;, is positive, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;’s diagonal entries are positive.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Eckart-Young-Mirsky Theorem</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</link>
      <pubDate>Fri, 07 Jan 2022 12:40:50 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/eckart-young-mirsky-theorem/</guid>
      <description>
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(R
\le \min\{M,N\}\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X =
U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma =
diag(\sigma_1, \sigma_2, ..., \sigma_R)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(M \times N\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(K \le R\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star
= U\Sigma_K V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_K =
diag(\sigma_1, \sigma_2, ..., \sigma_K)\)&lt;/span&gt;, when distance between
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||UA||_F = ||AU||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= tr((UA)^TUA) \\
&amp;amp;= tr(A^TUUA) \\
&amp;amp;= tr(A^TA) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||AU||_F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(K
\le R\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||U\Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^TU\Sigma V^TV - U^TYV||_F^2 \\
&amp;amp;= ||\Sigma - U^TYV||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^TYV\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is also of rank &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||X - Y||_F^2 &amp;amp;= ||\Sigma - Z||_F^2 \\
&amp;amp;= \sum_{ij}(\Sigma_{ij} - Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^R(\sigma_{i} - Z_{ii})^2 +
\sum_{i&amp;gt;R}^{\min\{M,N\}}Z_{ii}^2 + \sum_{i \ne j}Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \sigma_{i}, i = 1, 2, \dots, K \\
Z_{ii} = \sigma_{i}, i = K, K + 1, \dots, R - 1 \\
Z_{ii} = 0, i = R, R + 1, \dots, \min\{M,N\} \\
Z_{ij} = 0, i = 1,2, .... M, j=1, 2, \dots, N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U\Sigma_KV^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Eckart-Young-Mirsky Theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt; .&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Independent Component Analysis</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</link>
      <pubDate>Sun, 19 Dec 2021 10:16:43 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/independent-component-analysis/</guid>
      <description>

&lt;h2 id=&#34;assumption-and-derivation&#34;&gt;Assumption and Derivation&lt;/h2&gt;
&lt;p&gt;Suppose observations are the linear combination of signals. Also
suppose that the number of signal sources is equal to the number of
linearly mixed channel. Given observations (along the time axis &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \{x^{(i)}\}_{i=1}^M, x^{(i)} \in
\R^{N \times 1}\)&lt;/span&gt;, independent component analysis finds the &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; mixing matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(X
= AS\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are the
propagation weights from a signal source to observers. Therefore, they
are considered independent (and are thus called independent
components).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is called mixing matrix and
&lt;span class=&#34;math inline&#34;&gt;\(W = A^{-1}\)&lt;/span&gt; is called unmixing
matrix because &lt;span class=&#34;math inline&#34;&gt;\(S = WX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = U\Sigma V^T\)&lt;/span&gt; by &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. Assume observations are
pre-centered:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^Mx^{(i)} = 0
\]&lt;/span&gt; Assume that signals are independent and the variance of each
signal is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (taking the advantage
of scale/sign ambiguity): &lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{M}\sum_{i=1}^Ms^{(i)}(s^{(i)})^T = I
\]&lt;/span&gt; Then the covariance of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;
can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
C &amp;amp;= \frac{1}{M}x^{(i)}(x^{(i)})^T \\
&amp;amp;= \frac{1}{M}As^{(i)}(As^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(U\Sigma V^Ts^{(i)})^T \\
&amp;amp;= \frac{1}{M}U\Sigma V^Ts^{(i)}(s^{(i)})^TV\Sigma^T U^T \\
&amp;amp;= U\Sigma V^TIV\Sigma^T U^T \\
&amp;amp;= U\Sigma^2U^T \text{ (by property of SVD, note that $\Sigma$ is $n
\times n$ diagonal)} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If our assumptions are correct, then the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the concatenated eigenvectors of the
matrix &lt;span class=&#34;math inline&#34;&gt;\(CC^T\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; is the diagonal matrix
consisting of corresponding eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(C^TC\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(CC^T = C^TC = C^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In other words, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be solved by eigen
decomposition. Define &lt;span class=&#34;math inline&#34;&gt;\(\hat x^{(i)} =
\Sigma^{-1}U^Tx^{(i)}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat C &amp;amp;= \frac{1}{M}\hat x^{(i)}(\hat x^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(\Sigma^{-1}U^Tx^{(i)})^T \\
&amp;amp;= \frac{1}{M}\Sigma^{-1}U^Tx^{(i)}(x^{(i)})^TU\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^T(\frac{1}{M}x^{(i)}(x^{(i)})^T)U\Sigma^{-1} \\
&amp;amp;= \Sigma^{-1}U^TU\Sigma^2U^TU\Sigma^{-1} \\
&amp;amp;= I \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = WX = A^{-1}X = V\Sigma^{-1}U^TX = V\hat X
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The remaining job is to find the &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. We resort to multi-information, a
generalization of mutual-information from Information Theory to judge
how close a distribution is to statistical independence for multiple
variables. &lt;span class=&#34;math display&#34;&gt;\[
I(s) = \sum_{s \in \mathcal{S}}p(s) \log \frac{p(s)}{\prod_jp(s_j)}
\]&lt;/span&gt; It is a non-negative quantity that reaches the minimum if and
only if all variables are statistically independent.&lt;/p&gt;
&lt;p&gt;The multi-information can be written as a function of entropy, which
is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(s) = -\sum_{s \in \mathcal{S}}p(s) \log p(s)
\]&lt;/span&gt; The multi-information can be written as the difference between
the sum of entropies of marginal distribution and the entropy of joint
distribution &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(s) &amp;amp;= \sum_jH(s_j) - H(s) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(V\hat x) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log|V|) \\
&amp;amp;= \sum_jH((V\hat x)_j) - (H(\hat x) + log1) \\
&amp;amp;= \sum_jH((V\hat x)_j) - H(\hat x) \\
\end{aligned}
\]&lt;/span&gt; Because we are assuming signal are independent from each
other, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^\star &amp;amp;= \arg \min_V\sum_jH((V\hat x)_j) - H(\hat x) \\
&amp;amp;= \arg \min_V\sum_jH((V\hat x)_j)
\end{aligned}
\]&lt;/span&gt; Calculating entropy and then taking derivative is no easy task
and therefore ICA algorithms focus on approximations or equivalences to
the above equation. For example, it can be approximated by finding the
rotation that maximizes the expected log-likelihood of the observed data
by assuming that the source distributions are known.&lt;/p&gt;
&lt;h3 id=&#34;non-gaussian-and-pca&#34;&gt;Non-Gaussian and &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The key assumption of ICA is that signals are non-Gaussian. We are
trying to recover the &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (by finding
&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;) to be as non-Gaussian as
possible. &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; consists of
independent components because its covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\hat C\)&lt;/span&gt; is shown to be an identity matrix
(and thus diagonal). &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is
reconstructed by the linear combination of independent components. Thus
it will be the most non-Gaussian when each variable is formed by exactly
one component of &lt;span class=&#34;math inline&#34;&gt;\(\hat X\)&lt;/span&gt; by Central
Limit Theorem, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;’s components
are independent.&lt;/p&gt;
&lt;p&gt;Consider the case when &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;
consists of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent Gaussian,
which invalidate the above analysis. If we forcibly calculate &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;, due to the property of
multi-variate Gaussian that its iso-density maps are spherical, then any
&lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; rotation matrix will be
a solution to Equation (9), including the left singular matrix of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e. the concatenated eigenvectors of
&lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, which is exactly the
projection basis obtained in PCA. If any &lt;span class=&#34;math inline&#34;&gt;\(N
\times N\)&lt;/span&gt; rotation matrix can be a solution, there are infinite
many solutions to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and thus ICA
will just fail.&lt;/p&gt;
&lt;p&gt;The conclusion drawn above is based on the ideal case, i.e. many
enough observations. Even in the real case where signals are Gaussian,
we may get a unique solution to &lt;span class=&#34;math inline&#34;&gt;\(V^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html&#34;&gt;Independent
Component Analysis: A Tutorial (tkk.fi)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>RANSAC</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/ransac/</link>
      <pubDate>Wed, 20 Apr 2022 16:38:31 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/ransac/</guid>
      <description>

&lt;p&gt;Outliers (noises) in the data can diverge the regression model to
reduce prediction errors for them, instead of the majority real data
points. &lt;strong&gt;RAN&lt;/strong&gt;dom &lt;strong&gt;SA&lt;/strong&gt;mple
&lt;strong&gt;C&lt;/strong&gt;onsensus is a methodology to robustly fit the model in
the presence of outliers.&lt;/p&gt;
&lt;p&gt;RANSAC does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;randomly sample a subset of data of an fairly enough amount for
training;&lt;/li&gt;
&lt;li&gt;fit a model to the this subset;&lt;/li&gt;
&lt;li&gt;determine data points in the whole data set as inliers or outliers
by comparing the residuals (prediction errors) to a threshold. The set
of inliers is called a consensus set;&lt;/li&gt;
&lt;li&gt;repeat above for some iterations and retrain the final model with
the largest consensus set (since inliers should be the majority).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters of RANSAC include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;: number of points to fit
the model;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;: threshold of the
residual;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: proportion the
outliers;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;: probability of
success (at least one iteration is finished with no outlier);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;: number of iterations to
be determined.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has no
outliers)} = (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(training subset has at least one
outlier)} = 1 - (1 - e)^s\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\text{(all T subsets have outliers)} =
(1 - (1 - e)^s)^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p\text{(all T subsets have outliers)} = (1 - (1 - e)^s)^T &amp;lt; 1 -
\delta \\
T &amp;gt; \log\frac{1 - \delta}{1 - (1 - e)^s}
\end{gather}
\]&lt;/span&gt; The threshold &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is
usually set as the median absolute deviation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/xingshansi/p/6763668.html&#34;&gt;随机抽样一致算法（Random
sample consensus，RANSAC） - 桂。 - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Fisher&#39;s Linear Discriminant</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</link>
      <pubDate>Fri, 07 Jan 2022 13:50:38 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/fishers-linear-discriminant/</guid>
      <description>

&lt;h3 id=&#34;two-class&#34;&gt;Two-class&lt;/h3&gt;
&lt;p&gt;Assume we have a set of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-dimensional samples &lt;span class=&#34;math inline&#34;&gt;\(D = \{x^{(i)}\}^M_{i=1}\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; of which belong to Class &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt; to Class &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(M_1 + M_2
= M\)&lt;/span&gt;). We seek to obtain a scalar &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; by projecting &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto a unit vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(z^{(i)} =
v^Tx^{(i)}\)&lt;/span&gt;. Of all the possibilities we would like to select
the one that maximizes the separability of the scalars between
classes.&lt;/p&gt;
&lt;p&gt;The mean of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is &lt;span class=&#34;math display&#34;&gt;\[
\mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} =
c]x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} \\
\tilde \mu_c = \frac{\sum_{i=1}^M\mathbb I[y^{(i)} = c]v^T
x^{(i)}}{\sum_{i=1}^M\mathbb I[y^{(i)} = c]} = v^T\mu_c\\
\]&lt;/span&gt; The scatter of each class in the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space and the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space is defined as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_c &amp;amp;= \sum_{i=1}^M \mathbb I[y^{(i)} = c](x^{(i)} - \mu_c)(x^{(i)}
- \mu_c)^T \\
\tilde s^2_c &amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](z^{(i)} -
\tilde u_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c](v^Tx^{(i)} - v^Tu_c)^2 \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)v^T(x^{(i)}
- u_c) \\
&amp;amp;= \sum_{i=1}^M\ \mathbb I[y^{(i)} = c]v^T(x^{(i)} - u_c)(x^{(i)} -
u_c)^Tv \\
&amp;amp;= v^TS_cv
\end{aligned}
\]&lt;/span&gt; FLD suggests in &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-space
maximizing the distance among the means of classes (inter-class scatter)
and minimizing the variance over each class (within-class scatter), i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_v J(v) &amp;amp;\coloneq \frac{(\tilde \mu_1 - \tilde \mu_2)^2}{\tilde
s^2_1 + \tilde s^2_2} \\
&amp;amp;= \frac{(v^T\mu_1 - v^T\mu_2)^2}{v^TS_1v + v^TS_2v} \\
&amp;amp;= \frac{v^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv}{v^T(S_1 + S_2)v} \\
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(S_W = S_1 + S_2\)&lt;/span&gt; is called
within-class scatter, &lt;span class=&#34;math inline&#34;&gt;\(S_B = (\mu_1 -
\mu_2)(\mu_1 - \mu_2)^T\)&lt;/span&gt; is called inter-class scatter. Then,
&lt;span class=&#34;math display&#34;&gt;\[
J(v) = \frac{v^TS_Bv}{v^TS_Wv} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take derivative w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and
make it zero to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial J}{\partial v} &amp;amp;= v^TS_Wv \frac{\partial
v^TS_Bv}{\partial v} - v^TS_Bv \frac{\partial v^TS_Wv}{\partial v}&amp;amp;
\\
&amp;amp;= (v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv  \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v^TS_Wv) 2S_Bv- (v^TS_Bv) 2S_Wv &amp;amp;= 0 \\
S_Bv - \frac{(v^TS_Bv)}{(v^TS_Wv)} S_Wv &amp;amp;= 0 \\
S_Bv &amp;amp;= J(v) S_Wv
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_W\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is some eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(S^{-1}_WS_B\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_Bv = J(v) v
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_Bv = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tv
= \alpha(\mu_1 - \mu_2)\)&lt;/span&gt; always points to the same direction as
&lt;span class=&#34;math inline&#34;&gt;\((\mu_1 - \mu_2)\)&lt;/span&gt;. Thus we can
immediately give a &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and verify:
&lt;span class=&#34;math display&#34;&gt;\[
v = S^{-1}_W(\mu_1 - \mu_2)
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
S^{-1}_WS_B\underbrace{S^{-1}_W(\mu_1 - \mu_2)}_v = S^{-1}_W\alpha(\mu_1
- \mu_2) = \underbrace{\alpha}_{J(v)} \underbrace{S^{-1}_W(\mu_1 -
\mu_2)}_v \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-class&#34;&gt;Multi-class&lt;/h3&gt;


</description>
    </item>
    
    <item>
      <title>Bias-variance Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/bias-variance-decomposition/</guid>
      <description>
&lt;p&gt;The notation used is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 18%&#34;/&gt;
&lt;col style=&#34;width: 81%&#34;/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;Symbol&lt;/th&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Notation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal
D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_\mathcal
D\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the observation of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal
D\)&lt;/span&gt;, affected by noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the real value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar
y\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the mean of the real values&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the model learned with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar
f(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the expectation of prediction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(l(f(x),
y_\mathcal D)\)&lt;/span&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;the loss function, chosen to be squared
error&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By assuming that the observation errors averages to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, the expectation of the error will be
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E_{x \sim \mathcal D}&amp;amp;[l(f(x), y_\mathcal D)] = E[(f(x) - y_\mathcal
D)^2] = E\{[(f(x) - \bar f(x) + (\bar f(x) - y_\mathcal D)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y_\mathcal D)^2] +
2E[(f(x) - \bar f(x))(\bar f(x) - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E\{[(\bar f(x) - y) + (y - y_\mathcal
D)]^2\} \\
&amp;amp;\quad + 2\underbrace{E[f(x) - \bar f(x)]}_0 E[\bar f(x) -
y_\mathcal D] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(\bar f(x) - y)^2] + E[(y -
y_\mathcal D)^2] + 2E[(\bar f(x) - y)(y - y_\mathcal D)] \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar
f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;\quad + 2E[\bar f(x) - y] \underbrace{E[y - y_\mathcal D]}_0 \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E\{[(\bar
f(x) - \bar y) + (\bar y - y)]^2\} \\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x)
- \bar y)^2] + E[(\bar y - y)^2] + 2E[(\bar f(x) - \bar y)(\bar y - y)]
\\
&amp;amp;= E[(f(x) - \bar f(x))^2] + E[(y - y_\mathcal D)^2] + E[(\bar f(x)
- \bar y)^2] + E[(\bar y - y)^2] \\
&amp;amp;\quad + 2E[\bar f(x) - \bar y]\underbrace{E[\bar y - y]}_0 \\
&amp;amp;= \underbrace{E[(f(x) - \bar f(x))^2]}_{variance} +
\underbrace{E[(\bar f(x) - \bar y)^2]}_{bias^2} + \underbrace{E[(y -
y_\mathcal D)^2]}_{noise} + \underbrace{E[(\bar y - y)^2]}_{scatter} \\
\end{aligned}
\]&lt;/span&gt; &lt;a href=&#34;https://medium.com/analytics-vidhya/5-ways-to-achieve-right-balance-of-bias-and-variance-in-ml-model-f734fea6116&#34;&gt;5
ways to achieve right balance of Bias and Variance in ML model | by
Niwratti Kasture | Analytics Vidhya | Medium&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的函数</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 31 Mar 2022 10:08:24 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/</guid>
      <description>
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一一维随机变量，&lt;span class=&#34;math inline&#34;&gt;\(f: \R \to \R\)&lt;/span&gt;为一函数，那么&lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;的期望以及分布情况会是什么样的呢？&lt;/p&gt;
&lt;p&gt;我们这里只讨论&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是单调函数的情况，令&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X)\)&lt;/span&gt;，那么 &lt;span class=&#34;math display&#34;&gt;\[
P_Y(y) = P_Y(Y \le y) = P_X(f(X) \le y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递增， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \le f^{-1}(y)) = P_X(f^{-1}(y)) \\
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial P_X(f^{-1}(y))}{\partial y} \\
&amp;amp;= p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;单调递减， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
P_Y(f(X) \le y) = P_X(X \ge f^{-1}(y)) = 1 - P_X(X \le f^{-1}(y)) = 1 -
P_X(f^{-1}(y)) \\
\nonumber \\
\begin{split}
p_Y(y) &amp;amp;= \frac{\partial P_Y(Y \le y)}{\partial y} \\
&amp;amp;= \frac{\partial [1 - P_X(f^{-1}(y))]}{\partial y} \\
&amp;amp;= -p_X(f^{-1}(y)) \cdot (f^{-1})^\prime (y) \\
&amp;amp;= p_X(f^{-1}(y)) \cdot |(f^{-1})^\prime (y)| \\
\end{split}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之，&lt;span class=&#34;math inline&#34;&gt;\(p_Y(y) = p_X(f^{-1}(y)) \cdot
|(f^{-1})^\prime (y)|\)&lt;/span&gt;。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Real Symmetric Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</link>
      <pubDate>Sun, 30 Jan 2022 13:46:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</guid>
      <description>

&lt;h2 id=&#34;real-symmetric-matrix&#34;&gt;Real Symmetric Matrix&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real-valued symmetric matrix.
We have its properties as follows.&lt;/p&gt;
&lt;h3 id=&#34;real-valued-eigenvalues-and-eigenvectors&#34;&gt;Real-valued
Eigenvalues and Eigenvectors&lt;/h3&gt;
&lt;p&gt;Its eigenvalues and thus eigenvectors are real-valued. Suppose by
contradiction that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has some
imaginary eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and
the corresponding imaginary eigenvector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A x &amp;amp;= \lambda x \\
A(x_\text{real} + x_\text{img}) &amp;amp;= (\lambda_\text{real} +
\lambda_\text{img})(x_\text{real} + x_\text{img}) \\
A x_\text{real} + A x_\text{img} &amp;amp;= (\lambda_\text{real}
x_\text{real} + \lambda_\text{img} x_\text{img}) + (\lambda_\text{real}
x_\text{real} + \lambda_\text{img} x_\text{real}) \\
\end{aligned}
\]&lt;/span&gt; Denoting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s and
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s complex conjugate by &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; respectively, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\left.
\begin{aligned}
&amp;amp;A\bar x = A x_\text{real} - A x_\text{img} \\
&amp;amp;= (\lambda_\text{real} x_\text{real} + \lambda_\text{img}
x_\text{img}) \\
&amp;amp;\quad- (\lambda_\text{real} x_\text{real} + \lambda_\text{img}
x_\text{real}) \\
&amp;amp;= \bar \lambda \bar x
\end{aligned}
\right\} \Rightarrow
\begin{aligned}
(A\bar x)^T &amp;amp;= (\bar \lambda \bar x)^T \\
\bar x^T A^T &amp;amp;= \bar \lambda \bar x^T \\
\bar x^T A &amp;amp;= \bar \lambda \bar x^T
\end{aligned}
\end{gather}
\]&lt;/span&gt; Left-multiply &lt;span class=&#34;math inline&#34;&gt;\(\bar x^T\)&lt;/span&gt; on
both sides of &lt;span class=&#34;math inline&#34;&gt;\(Ax = \lambda x\)&lt;/span&gt; to
give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar x^T \lambda x = \lambda \bar x^T x
\]&lt;/span&gt; Right-multiply &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on both
side of &lt;span class=&#34;math inline&#34;&gt;\(\bar x^TA = \bar \lambda \bar
x^T\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar \lambda \bar x^T x
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda \bar x^T x
= \lambda \bar x^T x\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\bar
x^Tx\)&lt;/span&gt; is real-value, &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda =
\lambda\)&lt;/span&gt;. In other words, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is real-valued and thus so is
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;sum-of-real-symmetric-matrices&#34;&gt;Sum of Real Symmetric
Matrices&lt;/h4&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; be two real symmetric matrices. Let
&lt;span class=&#34;math inline&#34;&gt;\(\lambda^-, \lambda^+\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s smallest and largest eigenvalue of
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu^-, \mu^+\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;’s smallest and largest eigenvalue.
Denote &lt;span class=&#34;math inline&#34;&gt;\(\gamma^-, \gamma^+\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(A+B\)&lt;/span&gt;’s smallest and largest eigenvalue.
Then it can be &lt;a href=&#34;https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/eigenvectors/sum.html&#34;&gt;derived&lt;/a&gt;
that &lt;span class=&#34;math display&#34;&gt;\[
\lambda^- + \mu^- \le \gamma^- \le \gamma^+ \le \lambda^+ + \mu^+
\]&lt;/span&gt; ### Orthogonal Eigenvectors&lt;/p&gt;
&lt;p&gt;Its eigenvectors corresponding to different eigenvalues are
orthogonal. Arbitrarily taking &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’ s
two eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2\)&lt;/span&gt; and their
eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, \lambda_1
\ne \lambda_2\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= v_1^TA^Tv_2 \\
&amp;amp;= v_1^TAv_2 \\
&amp;amp;= v_1^T \lambda_2v_2 \\
&amp;amp;= \lambda_2v_1^Tv_2 \\
\end{aligned} \\
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= \lambda_1v_1^Tv_2
\end{aligned}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda_1v_1^Tv_2 &amp;amp;= \lambda_2v_1^Tv_2 \\
(\lambda_1 - \lambda_2)v_1^Tv_2 &amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ne \lambda_2\)&lt;/span&gt;,
we have &lt;span class=&#34;math inline&#34;&gt;\(v_1^Tv_2 = 0\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; are orthogonal.&lt;/p&gt;
&lt;h3 id=&#34;diagonalizable&#34;&gt;Diagonalizable&lt;/h3&gt;
&lt;p&gt;It has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent
eigenvectors and thus &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt;. To show it,
eigenvectors in different eigenspaces are orthogonal and thus linearly
independent; and eigenvectors in the same eigenspace are also linearly
independent because they form the basis of this eigenspace.&lt;/p&gt;
&lt;h4 id=&#34;easily-invertible&#34;&gt;“Easily Invertible”&lt;/h4&gt;
&lt;p&gt;Further on the diagonalizable property, suppose &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^{-1}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is not invertible, by &lt;a href=&#34;Eigenvectors%20and%20Eigenvalues.md#Eigenvalues:%20Rank,%20Trace%20and%20Determinant&#34;&gt;the
relation between the matrix rank and the eigenvalues&lt;/a&gt;, some of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;’s entries on the diagonal are
zero. By adding &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^\prime &amp;amp;= P\Lambda P^{-1} + \lambda PIP^{-1} \\
&amp;amp;= P(\Lambda + \lambda I)P^{-1}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;
complements all the zero entries on the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(A^\prime\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; nonzero eigenvalues and is invertible.
A singular symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;
becomes invertible by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda
I\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;orthogonally-diagonalizable&#34;&gt;Orthogonally Diagonalizable&lt;/h4&gt;
&lt;p&gt;Its diagonalization can be in the form of &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(P^TP = I\)&lt;/span&gt;, by properly selecting the
orthonormal eigenvectors.&lt;/p&gt;
&lt;p&gt;Eigenvectors from different eigenspaces are already orthogonal.
Eigenvectors from the same eigenspace are independent but not
necessarily orthogonal. However, the linear combination of these
homo-spatial independent eigenvectors is still an eigenvector. Thus we
can apply the Gram-Schmidt process to these eigenvectors and obtain the
orthogonal basis for this eigenspace.&lt;/p&gt;
&lt;p&gt;Finally, we pull together all the orthogonal eigenvectors, normalize
them to unit vector, and get the orthonormal matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Orthogonal diagonalization is &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalization&lt;/a&gt;, as well as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. In fact, an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is orthogonally diagonalizable if and
only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a symmetric matrix.
Such orthogonal diagonalization is also referred to as &lt;strong&gt;spectral
decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;commutativity&#34;&gt;Commutativity&lt;/h3&gt;
&lt;p&gt;If the product of two symmetric matrices &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is symmetric, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; commute, i.e. &lt;span class=&#34;math inline&#34;&gt;\(AB = BA\)&lt;/span&gt;. This is simply because &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A B &amp;amp;= (A B)^T &amp;amp;&amp;amp;\iff \\
A B &amp;amp;= B^T A^T &amp;amp;&amp;amp;\iff \\
A B &amp;amp;= B A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;Covariance matrix is a special kind of real symmetric matrix. It is
in the form of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;. It is
positive semi-definite and thus its eigenvalues are non-negative.&lt;/p&gt;
&lt;p&gt;In fact, matrices of this form are positive semi-definite.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zybuluo.com/fsfzp888/note/1112344&#34;&gt;real-valued
eigenvalues and orthogonal eigenvectors&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Least Squares</title>
      <link>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</link>
      <pubDate>Fri, 07 Jan 2022 12:53:45 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/linear-algebra-and-its-applications/least-squares/</guid>
      <description>
&lt;p&gt;Suppose we are solving the &lt;span class=&#34;math inline&#34;&gt;\(Ax =
b\)&lt;/span&gt; problem. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; does not
always lie in the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. However, we can try to find within
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s column space a vector &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\hat x\)&lt;/span&gt; best approximates &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. By best approximation we mean to
minimize the &lt;span class=&#34;math inline&#34;&gt;\(||Ax - b||\)&lt;/span&gt; over all
&lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The best approximation can be achieved when &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b =
\mathop{proj}_{Col(A)}b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of finding a orthogonal basis for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, computing &lt;span class=&#34;math inline&#34;&gt;\(\hat b\)&lt;/span&gt; and then solving &lt;span class=&#34;math inline&#34;&gt;\(Ax = \hat b\)&lt;/span&gt;, we can derive &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in this way: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
(b - \hat b) \perp Col(A) \iff (b - \hat b) \in Nul(A^T)\iff A^T(b -
\hat b) = 0 \\
A^T(b - Ax) = 0 \\
A^TAx = A^Tb \label{solution}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will show that if columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent, then the least-square
solution &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; is uniquely given
by &lt;span class=&#34;math inline&#34;&gt;\((A^TA)^{-1}A^Tb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Firstly, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = Nul(A^TA)\)&lt;/span&gt;.
This is because &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = 0 \Rightarrow A^TAx = A^T0 = 0 \\
A^TAx = 0 \iff x^TA^TAx = 0 \iff (Ax)^TAx = 0 \Rightarrow Ax = 0
\end{gather}
\]&lt;/span&gt; When columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are
independent, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt; so that
&lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt;, which indicates that
equation &lt;span class=&#34;math inline&#34;&gt;\(\eqref{solution}\)&lt;/span&gt; has the
unique solution. Conversely, as an aside, when &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(Nul(A^TA) = 0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(Nul(A) = 0\)&lt;/span&gt;, which indicates that columns
of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are independent.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>大数定律和中心极限定理</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</link>
      <pubDate>Fri, 20 May 2022 09:27:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/</guid>
      <description>

&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)\)&lt;/span&gt;及方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X)\)&lt;/span&gt;存在，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
P(|X-\E(X)| \ge \epsilon) \le \frac{\Var(X)}{\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为连续型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \int_{|x - \E(x)| \ge \epsilon}
p(x)dx \\
&amp;amp;\le \mathop \int_{|x - \E(x)| \ge \epsilon} \bigg( \frac{X -
\E(x)}{\epsilon} \bigg)^2 p(x)dx \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \int_{|x - \E(x)| \ge \epsilon}
\big( X - \E(x) \big)^2 p(x)dx \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \int_{x \in X} \big( X - \E(x)
\big)^2 p(x)dx \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为离散型随机变量 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P(|X-\E(X)| \ge \epsilon) = \mathop \sum_{|x - \E(x)| \ge \epsilon}
P(x) \\
&amp;amp;\le \mathop \sum_{|x - \E(x)| \ge \epsilon} \bigg( \frac{x -
\E(x)}{\epsilon} \bigg)^2 P(x) \\
&amp;amp;= \frac{1}{\epsilon^2} \mathop \sum_{|x - \E(x)| \ge \epsilon}
\big( x - \E(x) \big)^2 P(x) \\
&amp;amp;\le \frac{1}{\epsilon^2} \mathop \sum_{x \in X} \big( x - \E(x)
\big)^2 P(x) \\
&amp;amp;= \frac{\Var(X)}{\epsilon^2} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式的作用是“估计随机变量偏离其期望的概率”，但显然这种估计是十分粗糙的，Chebyshev不等式的作用是作为证明其它大数定理的基础工具。&lt;/p&gt;
&lt;h3 id=&#34;依概率收敛&#34;&gt;依概率收敛&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;p&gt;随机变量是事件的映射，当试验次数足够多时，事件的频率会&lt;strong&gt;依概率收敛&lt;/strong&gt;到该事件的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots,\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) =
1\)&lt;/span&gt;，则称该随机变量序列依概率收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{P}{\to} c\)&lt;/span&gt;。或者，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| \ge \epsilon) =
0\)&lt;/span&gt;。 ### Markov不等式&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev不等式其实是Markov不等式的一个特例。令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;为一非负随机变量、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为一非负实数，Markov不等式描述的是以下关系：
&lt;span class=&#34;math display&#34;&gt;\[
P(X \ge \alpha) \le \frac{\E(X)}{\alpha}
\]&lt;/span&gt; 以连续型随机变量为例，证明如下： &lt;span class=&#34;math display&#34;&gt;\[
P(X \ge \alpha) = \int_{x \ge \alpha} p(x) \d x \le \int_{x \ge \alpha}
\frac{x}{\alpha} p(x) \d x \le \int_x \frac{x}{\alpha} p(x) \d x =
\frac{\E(X)}{\alpha}
\]&lt;/span&gt; 由于&lt;span class=&#34;math inline&#34;&gt;\(|X - \E(x)| \ge \epsilon \iff
(X - \E(X))^2 \ge \epsilon^2\)&lt;/span&gt;，将Markov不等式中的的&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\((X -
\E(X))^2\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;替换为&lt;span class=&#34;math inline&#34;&gt;\(\epsilon^2\)&lt;/span&gt;，即可得到Chebyshev不等式。不过由于Markov不等式有随机变量非负的要求，适用范围就小了一些；而且同样，Markov不等式的这种估计也是很粗糙的。&lt;/p&gt;
&lt;h2 id=&#34;弱大数定律weak-law-of-large-numbers&#34;&gt;弱大数定律（Weak Law of
large numbers）&lt;/h2&gt;
&lt;h3 id=&#34;chebyshev大数定律&#34;&gt;Chebyshev大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定理&lt;/p&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,\dots\)&lt;/span&gt;两两不相关，若存在常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\Var(X_i) \le c \ne +\infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n}
\sum_{i=1}^n \E(X_i)| &amp;lt; \epsilon) = 1
\]&lt;/span&gt; 亦即&lt;span class=&#34;math inline&#34;&gt;\(\bar X = \frac{1}{n}
\sum_{i=1}^n X_i \stackrel{P}{\to} \frac{1}{n} \sum_{i=1}^n
\E(X_i)\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;证明&lt;/p&gt;
&lt;p&gt;由于该随机序列两两不相关，故根据期望及方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\E(\frac{1}{n} \sum_{i=1}^n X_i) =  \frac{1}{n} \sum_{i=1}^n
\E(X_i),\quad \Var(\frac{1}{n} \sum_{i=1}^N X_i) = \frac{1}{n^2}
\sum_{i=1}^n \Var(X_i) \le \frac{c}{n}
\]&lt;/span&gt; 根据Chebyshev不等式， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 \le P(|\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n
\E(X_i)| \ge \epsilon) &amp;lt; \frac{\Var(\frac{1}{n} \sum_{i=1}^N
X_i)}{\epsilon^2} \le \frac{c}{n \epsilon} \\
\underbrace{\lim_{n \to \infty} 0}_0 \le \lim_{n \to \infty}
P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n} \sum_{i=1}^n \E(X_i)| \ge
\epsilon) \le \underbrace{\lim_{n \to \infty} \frac{c}{n \epsilon}}_0
\Rightarrow\\
\lim_{n \to \infty} P(\frac{1}{n} \sum_{i=1}^n X_i - \frac{1}{n}
\sum_{i=1}^n \E(X_i)| \ge \epsilon) = 0
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;khinchin大数定律&#34;&gt;Khinchin大数定律&lt;/h3&gt;
&lt;h4 id=&#34;相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu, \Var(X_i) = \sigma^2 \ne \infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt;
\epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相互独立同分布大数定律是Chebyshev大数定律的一个特例，然而在方差不存在的情况下，数学家Khinchin证明该定律依然成立，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P (|\frac{1}{n} \sum_{i=1}^n X_i - \mu| &amp;lt;
\epsilon) = 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim
B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - p| &amp;lt; \epsilon)
= 1
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然Bernoulli大数定律也是Chebyshev大数定律的一个特例。&lt;/p&gt;
&lt;h2 id=&#34;中心极限定理&#34;&gt;中心极限定理&lt;/h2&gt;
&lt;p&gt;注意，在本节中，我们用&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;表示标准正态分布的分布函数。&lt;/p&gt;
&lt;h3 id=&#34;lindburg-levy中心极限定理&#34;&gt;Lindburg-Levy中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，若&lt;span class=&#34;math inline&#34;&gt;\(\E(X_i) =
\mu, \Var(X_i) = \sigma^2 \ne \infty,
i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - n \mu}{\sqrt n \sigma}
\le x) = \Phi(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;de-moivre-laplace中心极限定理&#34;&gt;de
Moivre-Laplace中心极限定理&lt;/h3&gt;
&lt;p&gt;设随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;相互独立且同分布，且&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim
B(1,p), i=1,2,\dots\)&lt;/span&gt;，则对任意实数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}} \le
x) = \Phi(x)
\]&lt;/span&gt; 显然de
Moivre-Laplace中心极限定理是Lindburg-Levy中心极限定理的特例。&lt;/p&gt;
&lt;p&gt;前面的Bernoulli大数定律告诉我们可以用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n} \sum_{i=1}^n
X_i\)&lt;/span&gt;（频率）近似&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;（概率），而至于近似程度如何，却不得而知。de
Moivre-Laplace中心极限定理则告诉我们当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;足够大时，近似程度如何： &lt;span class=&#34;math display&#34;&gt;\[
P(|\frac{1}{n}\sum_{i=1}^n X_i - p| \le \epsilon) =
P(|\frac{\sum_{i=1}^n X_i - np}{\sqrt{np(1-p)}}| \le \frac{\sqrt n
\epsilon}{\sqrt{p(1-p)}}) \simeq 2\Phi(\frac{\sqrt n
\epsilon}{\sqrt{p(1-p)}}) - 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上式实际是在用正态分布近似二项分布（多个伯努利分布随机变量加和为伯努利分布），比如在&lt;a href=&#34;https://www.mathsisfun.com/data/quincunx-explained.html&#34;&gt;Galton
Board游戏&lt;/a&gt;中，我们就可以应用de
Moivre-Laplace中心极限定理来近似实际概率。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>三大分布与正态总体的抽样分布</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</link>
      <pubDate>Sat, 09 Jul 2022 22:21:22 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</guid>
      <description>

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布、&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布都由正态分布衍生而来，常见统计量在正态总体的假设下，都与这三种分布有关，所以他们在正态总体的统计推断中起着很大的作用。&lt;/p&gt;
&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;h3 id=&#34;gamma函数&#34;&gt;Gamma函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x) = \int_0^{+\infty} e^{-t} t^{x-1} dt \quad (x &amp;gt; 0)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt;函数具有&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x + 1) = x \Gamma(x)\)&lt;/span&gt;的性质：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = \int_0^{+\infty} e^{-t} t^{x} dt = [-e^{-t} t^x]
\bigg|^{+\infty}_{t=0} - \int_0^{+\infty} -e^{-t} xt^{x-1} dt
\]&lt;/span&gt; 根据洛必达法则，&lt;span class=&#34;math inline&#34;&gt;\(\lim_{t \to
+\infty} = \frac{-t^x}{e^t} = \lim_{t \to +\infty} \frac{x!}{e^t} =
0\)&lt;/span&gt;，故 &lt;span class=&#34;math display&#34;&gt;\[
\Gamma (x+1) = 0 + x\int_0^{+\infty} e^{-t} t^{x-1} dt = x \Gamma(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1) = \int_0^{+\infty} e^{-t}
dt = 1\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;为正整数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x) = x!\)&lt;/span&gt;；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2) = \sqrt
\pi\)&lt;/span&gt;，故&lt;span class=&#34;math inline&#34;&gt;\(x = 2k +
1\)&lt;/span&gt;为正奇数时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\frac{x}{2}) =
\sqrt \pi \prod_{i=0}^{k-1} \frac{2 * i + 1}{2}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\Gamma (\frac{1}{2}) = \int_0^{+\infty} e^{-t} t^{-\frac{1}{2}} \d t
\stackrel{u = t^\frac{1}{2}}{\Longrightarrow} \int_0^{+\infty} e^{-u^2}
u^{-1} \;2u \d u
= 2 \int_0^{+\infty} e^{-u^2} \d u
= \int_{-\infty}^{+\infty} e^{-u^2} \d u  \\
\notag \\
\begin{aligned}
&amp;amp;\Gamma^2(\frac{1}{2}) = (\int_{-\infty}^{+\infty} e^{-u^2} \d u)^2
\\
&amp;amp;= (\int_{-\infty}^{+\infty} e^{-u^2} du)(\int_{-\infty}^{+\infty}
e^{-v^2} \d v) \\
&amp;amp;= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} e^{-(u^2+v^2)}
\d u \d v \\
&amp;amp;\downarrow_{u = r\sin\theta, v = r\cos\theta} \\
&amp;amp;= \int_{0}^{2\pi} \int_{0}^{+\infty} e^{-r^2}\; r \d r \d \theta \\
&amp;amp;= \int_{0}^{2\pi} [-\frac{1}{2}e^{-r^2}] \bigg|_{r=0}^{+\infty} \d
\theta \\
&amp;amp;= \int_{0}^{2\pi} \frac{1}{2} \d \theta \\
&amp;amp;= \pi \\
&amp;amp;\Gamma (\frac{1}{2}) = \sqrt \pi
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinehw.math.ksu.edu/math340book/chap3/gamma.php&#34;&gt;The
Gamma Function&lt;/a&gt; || &lt;a href=&#34;https://www.youtube.com/watch?v=XAoe4th0F1k&#34;&gt;The derivation of
&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1/2)\)&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;chi2分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为相互独立的标准正态分布随机变量，即&lt;span class=&#34;math inline&#34;&gt;\(X_i \sim N(0,1)\)&lt;/span&gt;则称&lt;span class=&#34;math inline&#34;&gt;\(Y = X_1^2 + \dots +
X_n^2\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布，记作&lt;span class=&#34;math inline&#34;&gt;\(Y \sim \chi^2(n)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\E[Y] = n, \Var[Y] = 2n\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时，容易得到&lt;span class=&#34;math inline&#34;&gt;\(\forall y \le 0, P_Y(y) = 0, p_Y(y)= 0\)&lt;/span&gt;，
$$ &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\begin{split}
\forall y &amp;gt; 0, P_Y(y) &amp;amp;= 2P_X(\sqrt y) - 1 \\
\end{split} \\

\begin{split}
&amp;amp;p_Y(y) = 2P_X&amp;#39;(\sqrt y) \frac 1 {2 \sqrt y} \\
&amp;amp;= \frac 1 {\sqrt {2\pi y}}  e^{-\frac 1 2 y} \\
&amp;amp;= \frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1}
e^{-\frac y 2}
\end{split}
\end{align}\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
$\chi^2(1)$分布的密度函数为：
\]&lt;/span&gt; p_Y(y) =&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
\frac 1 {2^\frac{1}{2} \Gamma(\frac 1 2)} y^{\frac 1 2 - 1} e^{-\frac y
2}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=k\)&lt;/span&gt;时，令&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_k\)&lt;/span&gt;表示一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维空间中的点， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p_Y(y) = P_Y(Y \le y) &amp;amp;= \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\;
\d x_1 \dots \d x_k \\
&amp;amp;= \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots + x_k^2)}}
{(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
\end{aligned}
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(\mathcal V\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k x_i^2 \le
y\)&lt;/span&gt;的积分区域。可以看出，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal
V\)&lt;/span&gt;对应一个&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;维球体，且其半径&lt;span class=&#34;math inline&#34;&gt;\(R = \sqrt y\)&lt;/span&gt;。对此，作&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/calculus/spherical-coordinates/&#34;&gt;高维球坐标变换&lt;/a&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;P_Y(Y \le y) = \int_\mathcal V \prod_{i=1}^k N(0,1,x_i)\; \d x_1
\dots \d x_k = \int_\mathcal V \frac{e^{-\frac 1 2 (x_1^2 + \dots +
x_k^2)}} {(2\pi)^{k / 2}}\ \d x_1 \dots \d x_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2}
\int_0^\sqrt{y} \\
&amp;amp;\quad\quad\quad\frac{e^{-\frac 1 2 (r^2\cos^2 \varphi_1 +
r^2\sin^2 \varphi_1 \cos^2 \varphi_2 + \dots +
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \cos^2 \varphi_{k-1} +
r^2\sin^2 \varphi_1 \dots \sin^2 \varphi_{k-2} \sin^2 \varphi_{k-1})} }
{(2\pi)^{k / 2}}\\
&amp;amp;\quad\quad\quad r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3}
\dots \sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^{2\pi} \underbrace{\int_0^\pi \dots \int_0^\pi}_{k-2}
\int_0^\sqrt{y} \frac{e^{-\frac 1 2 r^2}} {(2\pi)^{k / 2}}
r^{k-1} \sin(\varphi_1)^{k-2} \sin(\varphi_2)^{k-3} \dots
\sin(\varphi_{k-2})
\ \d r\ \d \varphi_1 \dots \d \varphi_k \\
&amp;amp;= \int_0^\sqrt{y} \underbrace{ \int_0^{2\pi} \underbrace{\int_0^\pi
\dots \int_0^\pi}_{k-2} \frac{1} {(2\pi)^{k / 2}} \sin(\varphi_1)^{k-2}
\sin(\varphi_2)^{k-3} \dots \sin(\varphi_{k-2})\ \d \varphi_1 \dots \d
\varphi_k}_{c_k} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;= c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d r \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;是和&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;相关的常数项，并且由于&lt;span class=&#34;math inline&#34;&gt;\(P_Y(Y \le \infty) = 1\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
1 &amp;amp;= c_k \int_0^\infty e^{-\frac 1 2 r^2} r^{k-1} \d r \\
&amp;amp;\Downarrow_{r = \sqrt{2t}} \\
1 &amp;amp;= c_k \int_0^\infty e^{-t} \sqrt{2t}^{k-1} \frac{1}{\sqrt{2t}} \d
t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \int_0^\infty e^{-t} t^{(k-2)/2} \d t \\
1 &amp;amp;= 2^{(k-2)/2} c_k \Gamma(\frac{k}{2}) \\
c_k &amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故可得密度函数： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;p_Y(y) = \frac{\d P_Y(Y \le y)}{\d y} \\
&amp;amp;= \frac{\d [c_k \int_0^\sqrt{y} e^{-\frac 1 2 r^2} r^{k-1} \d
r]}{\d y} \\
&amp;amp;= \frac{1} {2^{(k-2)/2} \Gamma(\frac{k}{2})} e^{-\frac y 2}
y^\frac{k-1}{2} \frac{1}{2\sqrt y} \\
&amp;amp;= \frac{1} {2^{\frac k 2} \Gamma(\frac{k}{2})} e^{-\frac y 2}
y^{\frac{k}{2} - 1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最终可得密度函数如下： &lt;span class=&#34;math display&#34;&gt;\[
p_Y(y) =
\begin{cases}
\frac 1 {2^\frac{n}{2} \Gamma(\frac n 2)} e^{-\frac y 2} y^{\frac n 2 -
1}, &amp;amp;y &amp;gt; 0 \\
0, &amp;amp; y \le 0
\end{cases}
\]&lt;/span&gt; 另外，该密度函数也可以通过数学归纳法验证。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statlect.com/probability-distributions/chi-square-distribution&#34;&gt;Chi
Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://vtechworks.lib.vt.edu/bitstream/handle/10919/34329/10Apxb.pdf?sequence=12&#34;&gt;Generating
Function of Chi Squared Distribution&lt;/a&gt; || &lt;a href=&#34;https://www.zhihu.com/question/30020592&#34;&gt;正向推导&lt;/a&gt; || &lt;a href=&#34;https://www.bilibili.com/video/BV1e54y1v7e5&#34;&gt;数学归纳法&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;t分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim N(0,1), Y \sim
\chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z =
\frac{X}{\sqrt{Y/n}}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z
\sim t(n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \frac{\Gamma((n+1)/2)} {\sqrt{n\pi} \Gamma(n/2)} \big( 1 +
\frac{z^2} n \big)^{-(n+1)/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;f分布&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布&lt;/h2&gt;
&lt;p&gt;设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;相互独立，且&lt;span class=&#34;math inline&#34;&gt;\(X \sim \chi^2(m), Y \sim
\chi^2(n)\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(Z =
\frac{X/m}{Y/n}\)&lt;/span&gt;服从自由度为&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布，记为&lt;span class=&#34;math inline&#34;&gt;\(Z
\sim F(m,n)\)&lt;/span&gt;。其密度函数为： &lt;span class=&#34;math display&#34;&gt;\[
p_Z(z) = \begin{cases}
\frac{\Gamma((m+n)/2} {\Gamma(m/2) \Gamma(n/2)} {m \choose n}^{\frac m
2} z^{\frac m 2 - 1} (1 + \frac m n z)^{-\frac{m+n}{2}}, &amp;amp;z &amp;gt; 0
\\
0, &amp;amp;\text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;正态总体的抽样分布&#34;&gt;正态总体的抽样分布&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,
\sigma^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar X
= \frac 1 n \sum_{i=1}^n X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar
X)^2\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X \sim N(\mu, \frac{\sigma^2}{n}) \\
X_i - \bar X \sim N(0, \frac{2(n-1)^2 \sigma^2}{n^2}) \\
\frac{\sum_{i=1}^n (X_i - \bar X)^2}{\sigma^2} \sim \chi^2(n - 1),\text
即 \frac{(n-1) S^2}{\sigma^2} = \frac{n S_n^2}{\sigma^2} \sim
\chi^2(n-1) \\
\text{$\bar X$与$S^2$相互独立，$\bar X$与$S_n^2$相互独立}
\label{independence} \\
\frac{(\bar X - \mu)\sqrt{n}}{S} \sim t(n-1) \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;实际上&lt;span class=&#34;math inline&#34;&gt;\(\eqref{independence}\)&lt;/span&gt;与“总体为正态分布”互为充要条件。&lt;/p&gt;
&lt;h3 id=&#34;两个独立正态总体的抽样分布&#34;&gt;两个独立正态总体的抽样分布&lt;/h3&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_{n_X}\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu_X,
\sigma_X^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar
X = \frac 1 {n_X} \sum_{i=1}^{n_X} X_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_X^2 = \frac{1}{n_X-1} \sum_{i=1}^{n_X} (X_i -
\bar X)^2\)&lt;/span&gt;；&lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots,
Y_{n_Y}\)&lt;/span&gt;是抽自正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu_Y,
\sigma_Y^2)\)&lt;/span&gt;的一组样本，样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar
Y = \frac 1 {n_Y} \sum_{i=1}^{n_Y} Y_i\)&lt;/span&gt;，样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_Y^2 = \frac{1}{n_Y-1} \sum_{i=1}^{n_Y} (Y_i -
\bar Y)^2\)&lt;/span&gt;；且两个正态总体相互独立，令&lt;span class=&#34;math inline&#34;&gt;\(S_w = \frac{1}{n_X + n_Y - 2} (\sum_{i=1}^{n_X}
(X_i - \bar X)^2 + \sum_{i=0}^{n_Y} (Y_i - \bar Y)^2)\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X - \bar Y \sim N(\mu_X - \mu_Y, \frac{\sigma_X^2}{n_X} +
\frac{\sigma_Y^2}{n_Y}) \\
X_i - Y_j \sim N(\mu_X - \mu_Y, \sigma_X^2 + \sigma_Y^2) \\
\frac{(n_X - 1) S_X^2}{\sigma_X^2} + \frac{(n_Y - 1) S_Y^2}{\sigma_Y^2}
\sim \chi^2(n_X + n_Y - 2) \\
\frac{S_X^2 / \sigma_X^2}{S_Y^2 / \sigma_Y^2} = \frac{S_X^2 /
S_Y^2}{\sigma_X^2 / \sigma_Y^2} \sim F(n_X - 1, n_Y - 1) \\
\text{当$\sigma_X^2 = \sigma_Y^2 = \sigma^2$时，} \frac{\bar X - \bar Y
- (\mu_1 - \mu_2)}{S_w \sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}} \sim t(n_X
+ n_Y - 2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;分布计算器&#34;&gt;分布计算器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html&#34;&gt;Normal
Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/chisq.html&#34;&gt;Chi-Square
Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/t.html&#34;&gt;Student’s
t-Distribution Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://homepage.divms.uiowa.edu/~mbognar/applets/f.html&#34;&gt;F-Distribution
Applet/Calculator (uiowa.edu)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Whitening</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/</link>
      <pubDate>Thu, 11 Aug 2022 17:52:02 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/whitening/</guid>
      <description>

&lt;h2 id=&#34;whitening&#34;&gt;Whitening&lt;/h2&gt;
&lt;p&gt;Data whitening is the process of converting a random vector &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with only first-order correlation into
a new random vector &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; such that the
covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is an
identity matrix. Data whitening usually has two steps: the decorrelation
step and the standardization step.&lt;/p&gt;
&lt;p&gt;To do it, we shall first apply the &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/#Orthogonally Diagonalizable&#34;&gt;orthogonal diagonalization&lt;/a&gt; to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;’s covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_X\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma_X \Phi = \Phi \Lambda
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; contains the
normalized eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1} =
\Phi^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is
diagonal and contains the eigenvalues. Now let &lt;span class=&#34;math inline&#34;&gt;\(Y = \Phi^T X\)&lt;/span&gt;, we can verify that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Y &amp;amp;= \E \{ \Phi^T (\x - \mu_X) [\Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Phi^T (\x - \mu_X) (\x - \mu_X)^T \Phi] \\
&amp;amp;= \Phi^T \E [(\x - \mu_X) (\x - \mu_X)^T] \Phi \\
&amp;amp;= \Phi^T \Sigma_X \Phi = \Phi^T \Phi \Lambda = \Lambda
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_Y\)&lt;/span&gt; is diagonal and
we finish the decorrelation step. To further make it an identity matrix
(the standardization step), we apply &lt;span class=&#34;math inline&#34;&gt;\(Z =
\Lambda^{-1/2} Y = \Lambda^{-1/2} \Phi^T X\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Sigma_Z &amp;amp;= \E \{\Lambda^{-1/2} \Phi^T (\x - \mu_X) [\Lambda^{-1/2}
\Phi^T (\x - \mu_X)]^T \} \\
&amp;amp;= \E [\Lambda^{-1/2} \Phi^T (\x - \mu_X)  (\x - \mu_X)^T \Phi
\Lambda^{-1/2}] \\
&amp;amp;= \Lambda^{-1/2} \Phi^T \Sigma_X \Phi \Lambda^{-1/2} = I
\end{aligned}
\]&lt;/span&gt; The inverse of data whitening can be used to derive density
function of first-order correlated random variables, e.g. for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/#First-order correlated $n$-dimensional&#34;&gt;Gaussian case&lt;/a&gt;. Data whitening looks
a lot like &lt;a href=&#34;https://chunxy.github.io/notes/articles/machine-learning/principal-component-analysis/&#34;&gt;PCA&lt;/a&gt;: they both
compute the eigen pairs; they both project the original data onto the
basis formed by eigenvectors; they both can be solved with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;SVD&lt;/a&gt;. But unlike PCA, data whitening
uses all the eigenvectors as the basis instead of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; most prominent ones. Therefore, data
whitening does not reduce the data’s dimensionality as PCA does.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/overview/</link>
      <pubDate>Fri, 22 Apr 2022 21:13:30 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/overview/</guid>
      <description>

&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/cross-entropy/&#34;&gt;Cross Entropy&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt; describe the
relationship between &lt;strong&gt;two distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/&#34;&gt;conditional entropy&lt;/a&gt; and
&lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/mutual-information/&#34;&gt;mutual information&lt;/a&gt;, as well as
&lt;u&gt;joint entropy&lt;/u&gt;, describe the relationship between &lt;strong&gt;two
random variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Since the notion of entropy is the basis of all other concepts in
this section and the entropy is more well-defined on discrete random
variable, they are more meaningful to be applied in discrete case,
though they are usually trivially extended to continuous case.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-entropy-and-cross-entropy&#34;&gt;KL-divergence, Entropy
and Cross Entropy&lt;/h2&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_\text{KL}(p||q) &amp;amp;= -\mathrm{E}_{x \sim p} \log \frac{q(x)}{p(x)}
\\
&amp;amp;= -\mathrm{E}_{x \sim p} \log q(x) - (-\mathrm{E}_{x \sim p} \log
{p(x)}) \\
&amp;amp;= H(p||q) - H(p)
\end{align}
\]&lt;/span&gt; By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(-\log\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_\text{KL}(p||q) &amp;amp;= \mathrm{E}_{x \sim p} [-\log \frac{q(x)}{p(x)}]
\\
&amp;amp;\ge -\log(\mathrm{E}_{x \sim p} \frac{q(x)}{p(x)}) \\
&amp;amp;= 0
\end{align}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) \ge H(p)
\]&lt;/span&gt; The equality holds if and only if &lt;span class=&#34;math inline&#34;&gt;\(p = q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-mutual-information&#34;&gt;KL-divergence and Mutual
Information&lt;/h2&gt;
&lt;p&gt;Given that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; have the same dimension, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&amp;amp;= \sum_{x,y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x} \sum_{y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x} p(x) D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{x} D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{y} D_{KL}[p(x|y), p(x)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;joint-entropy-and-conditional-entropy&#34;&gt;Joint Entropy and
Conditional Entropy&lt;/h2&gt;
&lt;p&gt;Joint entropy is just entropy, but with the random variable usually
broken into two separate components. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;H(X, Y) = \sum_{x,y} p(x,y) \log p(x,y) \\
&amp;amp;= \sum_{x,y} p(x,y) \log p(y|x) p(x) \\
&amp;amp;= \sum_{x,y} p(x,y) \log p(y|x) \\
&amp;amp;\quad + \sum_{x,y} p(x,y) \log p(x) \\
&amp;amp;= H(Y|X) + H(X)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Difference Equation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</guid>
      <description>

&lt;h2 id=&#34;difference-equation&#34;&gt;Difference Equation&lt;/h2&gt;
&lt;p&gt;To solve difference equation like &lt;span class=&#34;math inline&#34;&gt;\(x_t =
a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \dots + a_0\)&lt;/span&gt;, we first
rewrite it into the matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left[ \begin{array} \\
x_t \\
x_{t-1} \\
\vdots \\
x_2 \\
x_1
\end{array} \right] =
\underbrace{
\left[
\begin{array} \\
a_{t-1} &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \vdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0
\end{array}
\right]
}_{A}
\left[ \begin{array} \\
x_{t-1} \\
x_{t-2} \\
\vdots \\
x_1 \\
x_0
\end{array} \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To solve it, we try to find the eigenvalues and eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A - \lambda I = \left[
\begin{array} \\
a_{t-1} - \lambda &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots  &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{array}
\right]
\]&lt;/span&gt; Apply Laplacian expansion along the first column to get its
determinant as &lt;span class=&#34;math display&#34;&gt;\[
\det (A - \lambda I) = (a_{t-1} -\lambda) (-\lambda)^{t-1} - \det
B_{t-1}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
B_{t-1} =
\underbrace{
\begin{bmatrix}
a_{t-2} &amp;amp; a_{t-3} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{bmatrix}
}_\text{$t-1$ columns}
\]&lt;/span&gt; Apply Laplacian expansion along the first column of &lt;span class=&#34;math inline&#34;&gt;\(B_{n-1}\)&lt;/span&gt; to give the following recurrence
relation: &lt;span class=&#34;math display&#34;&gt;\[
\left.
\begin{array} \\
\det B_{t-1} = a_{t-2} (-\lambda)^{t-2} - \det B_{t-2} \\
\det B_1 = a_0
\end{array}
\right\} \Rightarrow
\det B_{t-1} = (-1)^t (a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} +
\dots + a_0)
\]&lt;/span&gt; In all, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det (A - \lambda I) &amp;amp;= (a_{t-1} - \lambda)(-\lambda)^{t-1} - (-1)^t
(a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} + \dots + a_0) \\
&amp;amp;= (-1)^t (\lambda^t - a_{t-1} \lambda^{t-1} - a_{t-2} \lambda^{t-2}
- \dots - a_0)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After solving the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1
\ge \dots \ge \lambda_t\)&lt;/span&gt; and corresponding eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\v}{\mathrm{v}} \v_1, \dots,
\v_t\)&lt;/span&gt; from above equation, we can rewrite the vector formed by
the initial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; terms as the linear
combination of &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_t\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\x_0 = \left[
x_{t-1}, \cdots, x_0
\right]^T
= c_1 \v_1 + \dots c_t \v_t
\]&lt;/span&gt; Then for every &lt;span class=&#34;math inline&#34;&gt;\(n \ge t\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
x_n = \left[ A^{n-t+1} \x_0 \right]_0 = \left[ \lambda_1^{n-t+1} c_1
\v_1 + \dots + \lambda_t^{n-t+1} c_t \v_t\right]_0
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt; will be asymptotic to
&lt;span class=&#34;math inline&#34;&gt;\(\lambda_1^{n}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>统计量</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;定义：设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为取自总体的一组样本，若函数&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;不包含总体分布中的任何参数，则称&lt;span class=&#34;math inline&#34;&gt;\(g(X_1,\dots,X_n)\)&lt;/span&gt;为&lt;strong&gt;统计量&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;样本均值和样本方差&#34;&gt;样本均值和样本方差&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{样本均值：}\bar X = \frac{1}{n} \sum_{i=1}^n X_i \\
\text{样本方差：}S^2 = \frac{1}{n-1} \sum_{i=1}^N (X_i - \bar X)^2 =
\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(m_k = \frac{1}{n} \sum_{i=1}^n
X_i^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a_k = \frac{1}{n} \sum_{i=1}^n (X_i - \bar
X)^k\)&lt;/span&gt;为&lt;strong&gt;样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩&lt;/strong&gt;，这些也都是统计量。特别地，当&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;时，我们令&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq a_2 = \frac{1}{n} \sum_{i=1}^n
X_i^2 - \bar X^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;由于统计量是随机变量的函数，故统计量也是随机变量。设总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的期望&lt;span class=&#34;math inline&#34;&gt;\(\E(X)
= \mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) =
\sigma^2\)&lt;/span&gt;，关于统计量有如下定理： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
&amp;amp; \E(\bar X) = \mu, \Var(\bar X) = \frac{\sigma^2}{n} \\
\notag \\
&amp;amp; \E(S^2) = \sigma^2, \E(S_n^2) = \frac{n-1}{n} \sigma^2 \\
\notag \\
&amp;amp; \bar X \stackrel{P}{\to} \mu, S^2 \stackrel{P}{\to} \sigma^2,
S_n^2 \stackrel{P}{\to} \sigma^2
\end{gather}
\]&lt;/span&gt; 有关证明如下： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\E(\bar X) = \E (\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n}
\sum_{i=1}^n \E(X_i) = \mu \\
\Var(\bar X) = \Var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2}
\sum_{i=1}^n \Var(X_i) = \frac{\sigma^2}{n}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}[t]
\E(S^2) &amp;amp;= \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar X^2)] \\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n \E (X_i^2 ) - n \E(\bar X^2)
\big) \\
&amp;amp;\Downarrow_ {\E(X_i^2) = \Var(X_i) + \E^2(X_i) = \sigma^2 + \mu^2,
\E(\bar X^2) = \Var(\bar X) + \E^2(\bar X) = \frac{\sigma^2}{n} + \mu^2}
\\
&amp;amp;= \frac{1}{n-1} \big( \sum_{i=1}^n (\sigma^2 + \mu^2) - n
(\frac{\sigma^2}{n} + \mu^2) \big) \\
&amp;amp;= \sigma^2
\end{aligned}
\begin{aligned}[t]
\E(S_n^2) &amp;amp;= \E [\frac{n-1}{n} \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n
\bar X^2)] \\
&amp;amp;= \frac{n-1}{n} \E [\frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \bar
X^2)] \\
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;归根结底，样本方差使用&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;的原因是，其使用的“均值”为&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;而不是&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，这导致了一个自由度的缺失。而假设&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知，我们定义一个新的统计量&lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;^2 = \frac{1}{n} \sum_{i=1}^N (X_i - \mu)^2
= \frac{1}{n} (n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n
X_i^2)\)&lt;/span&gt;，我们会发现&lt;span class=&#34;math inline&#34;&gt;\(\E(S&amp;#39;^2) =
\sigma^2\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E(S&amp;#39;^2) = \frac{1}{n} \E(n \mu^2 - 2n\mu \bar X + \sum_{i=1}^n
X_i^2) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu \E(\bar X) + \sum_{i=1}^n \E
(X_i^2)) \\
&amp;amp;= \frac{1}{n} (n \mu^2 - 2n\mu^2 + \sum_{i=1}^n (\sigma^2 + \mu^2))
\\
&amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt; 至于三个统计量的依概率收敛证明，根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律&#34;&gt;相互独立同分布大数定律&lt;/a&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\bar X = \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\to} \mu \\
\frac{1}{n} \sum_{i=1}^n X_i^2 \stackrel{P}{\to} \frac{1}{n}
\sum_{i=1}^n \E (X_i^2) = \sigma^2 + \mu^2
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt;
0\)&lt;/span&gt;，存在&lt;span class=&#34;math inline&#34;&gt;\(N_1, N_2 &amp;gt;
0\)&lt;/span&gt;，使得当&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; \max(N_1,
N_2)\)&lt;/span&gt;时，始终有 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
0 &amp;lt; P(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 + \mu^2)| \ge
\epsilon / 2) &amp;lt; \delta / 2 \\
0 &amp;lt; P(|\mu^2 - \bar X^2| \ge \epsilon / 2) &amp;lt; \delta / 2 \\
\end{gather}
\]&lt;/span&gt; 记事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 +
\mu^2)| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\bar X^2
- \mu^2| \ge \epsilon / 2\)&lt;/span&gt;、事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar X^2 -
\sigma^2| \ge \epsilon / 2\)&lt;/span&gt;。由于&lt;span class=&#34;math inline&#34;&gt;\(|\frac{1}{n} \sum_{i=1}^n X_i^2 - (\sigma^2 +
\mu^2)| + |\mu^2 - \bar X^2| \ge |\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar
X^2 - \sigma^2|\)&lt;/span&gt;，则事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;发生时，事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;至少发生其中之一，即事件&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;是事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;与事件&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;并集的子集。故&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
0 &amp;lt; P(\text{事件$C$}) \le P(\text{事件$A$ 或 事件$B$}) \le
P(\text{事件$A$}) + P(\text{事件$B$}) &amp;lt; \delta
\]&lt;/span&gt; 即&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; P(|\frac{1}{n}
\sum_{i=1}^n X_i^2 - \bar X^2 \ - \sigma^2| \ge \epsilon) &amp;lt;
\delta\)&lt;/span&gt;。又由于对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta &amp;gt; 0\)&lt;/span&gt;该结论都成立，故 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\lim_{n \to \infty} P(|\underbrace{\frac{1}{n} \sum_{i=1}^n X_i^2 - \bar
X^2}_{S_n^2} - \sigma^2| \ge \epsilon) = 0 \iff \\
S_n^2 \stackrel{P}{\to} \sigma^2
\end{gathered}
\]&lt;/span&gt; 运用类似的&lt;span class=&#34;math inline&#34;&gt;\(\epsilon,
\delta\)&lt;/span&gt;语言，我们可以证明&lt;span class=&#34;math inline&#34;&gt;\(S^2 =
\frac{n}{n-1} S_n^2 \stackrel{P}{\to} \sigma^2\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id=&#34;次序统计量&#34;&gt;次序统计量&lt;/h2&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\((X_{(1)}, \dots,
X_{(n)})\)&lt;/span&gt;为样本&lt;span class=&#34;math inline&#34;&gt;\((X_1, \dots,
X_n)\)&lt;/span&gt;排序后的结果，则&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)} = \min
(X_1, \dots, X_n), X_{(n)} = \max (X_1, \dots,
X_n)\)&lt;/span&gt;亦是统计量。&lt;/p&gt;
&lt;p&gt;记&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)},
X_{(n)}\)&lt;/span&gt;的概率密度函数分别为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(1)}}, p_{X_{(n)}}\)&lt;/span&gt;，则 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
p_{X_{(1)}}(u) = n \big( 1 - P_X(u) \big)^{n-1} p_X(u) \\
p_{X_{(n)}}(u) = n \big( P_X(u) \big)^{n-1} p_X(u)
\end{gather}
\]&lt;/span&gt; 记&lt;span class=&#34;math inline&#34;&gt;\(X_{(k)}\)&lt;/span&gt;的概率密度函数为&lt;span class=&#34;math inline&#34;&gt;\(p_{X_{(k)}}\)&lt;/span&gt;，则…&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>参数估计</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Thu, 07 Jul 2022 11:06:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid>
      <description>

&lt;h2 id=&#34;点估计&#34;&gt;点估计&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim
p(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;为总体的一组样本，若用一个统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;来估计&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。&lt;/p&gt;
&lt;h3 id=&#34;矩估计&#34;&gt;矩估计&lt;/h3&gt;
&lt;p&gt;矩估计的思想就是就是替换思想，即用样本矩替换总体矩。设总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩（origin moment）&lt;span class=&#34;math inline&#34;&gt;\(\mu_k = \E[X^k]\)&lt;/span&gt;，总体的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;中心矩（central moment）&lt;span class=&#34;math inline&#34;&gt;\(\alpha_k = \E[(X - \mu)^k]\)&lt;/span&gt;；样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩为&lt;span class=&#34;math inline&#34;&gt;\(m_k = \frac 1 n \sum_{i=1}^n
X_i^k\)&lt;/span&gt;，样本的&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩为&lt;span class=&#34;math inline&#34;&gt;\(a_k = \frac 1 n \sum_{i=1}^n (X_i - \bar
X)^k\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;仅以中心矩为例，如果未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta =
\varphi(\mu_1, \dots, \mu_p)\)&lt;/span&gt;，则其估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \varphi(m_1, \dots,
m_p)\)&lt;/span&gt;，这种估计总体未知参数的方法叫作矩估计法。矩估计往往不唯一，如设&lt;span class=&#34;math inline&#34;&gt;\(X \sim P(\lambda)\)&lt;/span&gt;，则由于&lt;span class=&#34;math inline&#34;&gt;\(\E(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;；又&lt;span class=&#34;math inline&#34;&gt;\(\Var(X) = \lambda\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\hat \lambda\)&lt;/span&gt;可写作&lt;span class=&#34;math inline&#34;&gt;\(\frac 1 n \sum_{i=1}^n X_i^2 - \bar
X^2\)&lt;/span&gt;。此时往往采用较低阶的矩来估计未知参数。&lt;/p&gt;
&lt;h3 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;设总体有分布律&lt;span class=&#34;math inline&#34;&gt;\(X \sim
P(X=x;\theta)\)&lt;/span&gt;或密度函数&lt;span class=&#34;math inline&#34;&gt;\(X \sim
p(x;\theta)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots,
x_n\)&lt;/span&gt;为取自总体的一组样本观测值，将样本的联合分布律或联合密度函数看作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的函数： &lt;span class=&#34;math display&#34;&gt;\[
L(\theta) = \prod_{i=1}^n P(X=x_i;\theta)\ \text或 \ L(\theta) =
\prod_{i=1}^n p(x_i;\theta)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta)\)&lt;/span&gt;又称作&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的似然函数，似然函数满足关系式&lt;span class=&#34;math inline&#34;&gt;\(L(\hat \theta) = \max_{\theta}
L(\theta)\)&lt;/span&gt;的解&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计量。&lt;/p&gt;
&lt;p&gt;由于最大似然估计对样本使用较为充分，通常其方差较小。&lt;/p&gt;
&lt;h3 id=&#34;优良性评判&#34;&gt;优良性评判&lt;/h3&gt;
&lt;h4 id=&#34;无偏性unbiasedness&#34;&gt;无偏性（unbiasedness）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个无偏估计（量），否则则称作有偏估计（量）。如果有
&lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} \E [\hat \theta(X_1, \dots, X_n)] = \theta
\]&lt;/span&gt; 则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个渐进无偏估计（量）。渐进无偏亦记作&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta \stackrel{L_1}{\to}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值也许偏大，取另一些样本时估计量也许偏小，但多次取样本进行估计，平均来讲偏差为&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即&lt;strong&gt;系统误差&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;有趣的是，有一些估计虽然不满足无偏性，但满足一致性，所以我们依旧会采用这些估计。比如在估计正态总体的方差时，令&lt;span class=&#34;math inline&#34;&gt;\(S_n^2 \triangleq
a_2\)&lt;/span&gt;，则最大似然估计为&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;，该估计不满足无偏性，但满足一致性；&lt;span class=&#34;math inline&#34;&gt;\(S_n^2/k\)&lt;/span&gt;形式（&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;为待定系数）的最小均方差估计为&lt;span class=&#34;math inline&#34;&gt;\(n S_n^2/(n+1)\)&lt;/span&gt;（&lt;a href=&#34;https://www.wikiwand.com/en/Mean_squared_error#Variance&#34;&gt;参此&lt;/a&gt;），也不满足无偏性，但满足一致性。&lt;/p&gt;
&lt;h4 id=&#34;最小方差minimum-variance&#34;&gt;最小方差（minimum-variance）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1 = \hat
\theta_2\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的两个估计量，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的取值空间为&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;，若对任意的&lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat \theta_1) \le \Var(\hat
\theta_2)\)&lt;/span&gt;，且至少有一个&lt;span class=&#34;math inline&#34;&gt;\(\theta \in
\Theta\)&lt;/span&gt;使得该不等式严格成立，则称&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_1\)&lt;/span&gt;比&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta_2\)&lt;/span&gt;有效。&lt;/p&gt;
&lt;h4 id=&#34;一致性consistency&#34;&gt;一致性（consistency）&lt;/h4&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta = \hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个估计量，若对任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，有 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|\hat \theta - \theta| &amp;gt; \epsilon) = 0 \\
\equiv \\
\lim_{n \to \infty} P(|\hat \theta - \theta| \le \epsilon) = 1
\]&lt;/span&gt; 则称估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;具有一致性，一致性描述的是一个估计量&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/#依概率收敛（convergence in probability）&#34;&gt;依概率收敛&lt;/a&gt;到真实值的过程，一致性亦记作&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta \stackrel{P}{\to}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;一致性是一个很基本（“基本”不是指“一致性是其他两条性质的必要条件”）的要求：随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。&lt;/p&gt;
&lt;h3 id=&#34;cramer-rao不等式&#34;&gt;Cramer-Rao不等式&lt;/h3&gt;
&lt;p&gt;实际上，点估计量不仅仅可以估计未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;本身（假设为一元情况），更可以估计未知参数的某个函数&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，即给定总体的一组样本&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;，用统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g = \hat g(X_1, \dots, X_n)\)&lt;/span&gt;估计&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;。估计量最好的效果便是达到最小方差无偏（minimum-variance
unbiased &amp;lt;MVU&amp;gt;）估计，Cramer-Rao不等式给出了点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat g\)&lt;/span&gt;方差的一个下界。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{cr} \Var(\hat g) \ge (g&amp;#39;(\theta))^2 / (nI(\theta))
\end{equation}
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(I(\theta) = \int
[(\frac{\partial p(x;\theta)}{\partial \theta})^2 / p(x;\theta)] \d
x\)&lt;/span&gt;为Fisher Information。当&lt;span class=&#34;math inline&#34;&gt;\(g(\theta)
= \theta\)&lt;/span&gt;，即只估计未知参数本身时，有&lt;span class=&#34;math inline&#34;&gt;\(\Var(\hat g) \ge 1 / (nI(\theta))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;成立有一定的条件，其本身就暗含了&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial p(x;\theta)}{\partial
\theta}\)&lt;/span&gt;存在及&lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\theta)\)&lt;/span&gt;存在的&lt;strong&gt;条件&lt;/strong&gt;。记
&lt;span class=&#34;math display&#34;&gt;\[
S = S(X_1, \dots, X_n, \theta) = \sum_{i=1}^n \frac{\partial \ln
p(X_i;\theta)} {\partial \theta} = \sum_{i=1}^n [\frac{\partial
p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\int p(x;\theta)\ \d x =
1\)&lt;/span&gt;，此式两边同时对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导，并&lt;strong&gt;假定&lt;/strong&gt;此处求导可以移至积分号内部，可得到&lt;span class=&#34;math inline&#34;&gt;\(\int \frac{\partial p(x;\theta)}{\partial \theta}
\d x = 0\)&lt;/span&gt;。根据&lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/#Law of the Unconscious Statistician&#34;&gt;LOTUS&lt;/a&gt;，
&lt;span class=&#34;math display&#34;&gt;\[
\E [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
= \int [\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta)]
p(x;\theta)\ \d x
= \int \frac{\partial p(x;\theta)} {\partial \theta}\d x = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;的独立性，
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Var(S) &amp;amp;= \sum_{i=1}^n \Var [\frac{\partial p(X_i;\theta)}
{\partial \theta} / p(X_i;\theta)] \\
&amp;amp;= \sum_{i=1}^n \{ \E [\big (\frac{\partial p(X_i;\theta)} {\partial
\theta} / p(X_i;\theta) \big)^2] - \E^2 [\frac{\partial p(X_i;\theta)}
{\partial \theta} / p(X_i;\theta)] \} \\
&amp;amp;= \sum_{i=1}^n \E [\big (\frac{\partial p(X_i;\theta)} {\partial
\theta} / p(X_i;\theta) \big)^2] \\
&amp;amp;= n \int \big (\frac{\partial p(x;\theta)} {\partial \theta} /
p(x;\theta) \big)^2 p(x;\theta)\ \d x \\
&amp;amp;= n I(\theta)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根据协方差的性质， &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\label{cov_prop} [\Cov(\hat g, S)]^2 \le \Var(\hat g) \Var(S) =
\Var(\hat g) n I(\theta)
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又&lt;span class=&#34;math inline&#34;&gt;\(\E(S) = 0\)&lt;/span&gt;， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S) = \E (\hat g S) &amp;amp;= \int \dots \int \hat g(x_1,
\dots, x_n) \sum_{i=1}^n [\frac{\partial p(x_i;\theta)} {\partial
\theta} / p(x_i;\theta)] \prod_{i=1}^n p(x_1;\theta)\ \d x_1 \dots \d
x_n \\
&amp;amp;= \int \dots \int \hat g(x_1, \dots, x_n) \frac{\partial
p(x_1;\theta) \dots p(x_n;\theta)} {\partial \theta}\ \d x_1 \dots \d
x_n
\end{aligned}
\]&lt;/span&gt; &lt;strong&gt;假定&lt;/strong&gt;此处对&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;求导可以移至积分号外部， &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Cov(\hat g, S)
&amp;amp;= \frac \partial{\partial \theta} \int \dots \int \hat g(x_1,
\dots, x_n) p(x_1;\theta) \dots p(x_n;\theta)\ \d x_1 \dots \d x_n \\
&amp;amp;= \frac \partial{\partial \theta} g(\theta) = g&amp;#39;(\theta)
\end{aligned}
\]&lt;/span&gt; 将上式重新带入&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cov_prop}\)&lt;/span&gt;，从而得到&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cr}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/56411276/answer/204992057&#34;&gt;对Cramer-Rao不等式的理解&lt;/a&gt;
|| &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Wiki
(see the multi-variate case)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;区间估计&#34;&gt;区间估计&lt;/h2&gt;
&lt;p&gt;点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作&lt;strong&gt;置信区间&lt;/strong&gt;（confidence
interval）。&lt;/p&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X \sim
f(x;\theta)\)&lt;/span&gt;的分布形式已知，但其参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;未知。设&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots,
X_n\)&lt;/span&gt;为总体的一组样本，给定一个很小的数&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \alpha &amp;lt; 1\)&lt;/span&gt;，若有统计量&lt;span class=&#34;math inline&#34;&gt;\(\theta_l = \theta_l (X_1, \dots, X_n) \le
\theta_r(X_1, \dots, X_n) = \theta_r\)&lt;/span&gt;，使得 &lt;span class=&#34;math display&#34;&gt;\[
P(\theta_l \le \theta \le \theta_r) \ge 1 - \alpha
\]&lt;/span&gt; 我们称&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\([\theta_l,
\theta_r]\)&lt;/span&gt;的&lt;strong&gt;置信水平&lt;/strong&gt;（confidence level），&lt;span class=&#34;math inline&#34;&gt;\(\theta_l\)&lt;/span&gt;为&lt;strong&gt;置信下限&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\theta_r\)&lt;/span&gt;为&lt;strong&gt;置信上限&lt;/strong&gt;。一般来说置信水平不唯一，因为若&lt;span class=&#34;math inline&#34;&gt;\(1 -
\alpha\)&lt;/span&gt;是某个区间的置信水平，则对于任意&lt;span class=&#34;math inline&#34;&gt;\(\alpha &amp;lt; \tilde \alpha &amp;lt; 1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(1 - \tilde
\alpha\)&lt;/span&gt;亦是该区间的置信水平。故一般的“置信水平”是这一系列置信水平中的最大者。&lt;/p&gt;
&lt;h3 id=&#34;枢轴变量法&#34;&gt;枢轴变量法&lt;/h3&gt;
&lt;p&gt;区间估计一般采用枢轴变量法，枢轴变量法的一般步骤为：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个点估计&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;（如&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;构造&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;的一个函数&lt;span class=&#34;math inline&#34;&gt;\(G = G(\theta, \hat
\theta)\)&lt;/span&gt;（称作枢轴（pivot）函数），且&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;完全已知，且其分布与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关，&lt;/li&gt;
&lt;li&gt;对任何常数&lt;span class=&#34;math inline&#34;&gt;\(a &amp;lt; b\)&lt;/span&gt;，不等式&lt;span class=&#34;math inline&#34;&gt;\(a \le G(\theta, \hat \theta) \le
b\)&lt;/span&gt;能够改写成等价的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le
B\)&lt;/span&gt;，且&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;仅与&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta,a,b\)&lt;/span&gt;有关，与&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;取&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;的上&lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha / 2}) = 1 -
\alpha/2\)&lt;/span&gt;）及上&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt;分位点&lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(F(w_{1 - \alpha / 2}) = \alpha /
2\)&lt;/span&gt;），此时有&lt;span class=&#34;math inline&#34;&gt;\(F(w_{\alpha/2}) - F(w_{1
- \alpha / 2}) = 1 - \alpha\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
P(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}) = 1 -
\alpha
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(w_{1-\alpha/2} \le G(\theta, \hat
\theta) \le w_{\alpha/2}\)&lt;/span&gt;可改写为对应的&lt;span class=&#34;math inline&#34;&gt;\(A \le \theta \le B\)&lt;/span&gt;的形式，且&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;仅与估计量和两个分位点有关，&lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;就构成了&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在构造枢轴函数时，一般会使用一些现有结论，比如&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#中心极限定理&#34;&gt;中心极限定理&lt;/a&gt;的近似、&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/&#34;&gt;三大分布与正态总体的抽样分布&lt;/a&gt;等等。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Matrix Identity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</link>
      <pubDate>Wed, 22 Dec 2021 15:51:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</guid>
      <description>
&lt;p&gt;A useful matrix identity: &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; It can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} &amp;amp;= PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1} \\
\end{aligned}
\]&lt;/span&gt; Its reduced form: &lt;span class=&#34;math display&#34;&gt;\[
(I_N+AB)^{-1}A = A(I_M+BA)^{-1}
\]&lt;/span&gt; It can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(I_N+AB)^{-1}A &amp;amp;= A(I_M+BA)^{-1} \\
\iff A &amp;amp;= (I_N + AB)A(I_M + BA)^{-1} \\
\iff A &amp;amp;= (A + ABA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A(I_M + BA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>贝叶斯推断</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/</link>
      <pubDate>Sun, 14 May 2023 18:27:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/</guid>
      <description>

&lt;h2 id=&#34;贝叶斯公式&#34;&gt;贝叶斯公式&lt;/h2&gt;
&lt;p&gt;在全概率公式之下，有 &lt;span class=&#34;math display&#34;&gt;\[
P(B_i | A) = \frac{P(A B_i)}{P(A)} = \frac{P(B_i) P(A|B_i)}{P(A)} =
\frac{P(B_i) P(A|B_i)} {\sum_j P(B_j) P(A|B_j)}
\]&lt;/span&gt;
这便是贝叶斯公式。贝叶斯公式中的项目也有它们在贝叶斯学派中相应的称呼：
&lt;span class=&#34;math display&#34;&gt;\[
\text{posterior} = \frac{\text{prior} \times
\text{likelihood}}{\text{evidence}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯估计&#34;&gt;贝叶斯估计&lt;/h2&gt;
&lt;p&gt;在参数估计问题中，记&lt;span class=&#34;math inline&#34;&gt;\(D = \{ X_1, \dots,
X_n \}\)&lt;/span&gt;为样本、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;为参数，并用将&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;代入后验概率中的事件&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;代入后验概率中的&lt;span class=&#34;math inline&#34;&gt;\(B_i\)&lt;/span&gt;，我们得到：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(\theta | D) = \frac{P(\theta) P(D|\theta)} {\sum_j P(\theta_j)
P(D|\theta_j)} \\
\]&lt;/span&gt; 取决于&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;和单个样本的取值是连续型或是离散型，上式中的&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;可代表密度函数或分布律，而分母中的求和运算应当在&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;取值连续的时候被替换在&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;所有可行范围内的积分运算。比较值得注意的一点是，贝叶斯推断为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;引入了先验分布，而在频率学派中，参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;是不存在什么先验分布的。&lt;/p&gt;
&lt;p&gt;由于贝叶斯学派假设参数服从某个分布，在先验、似然已知的情况下，我们可以求得后验的解析解、近似解，或者通过Metropolis-Hastings等算法对后验直接采样。无论采取哪种方式，我们都可以获得后验的（近似）密度函数。如此一来，贝叶斯估计其实天然是一种区间估计。除开&lt;u&gt;直接获取后验密度函数&lt;/u&gt;，我们再讨论一些贝叶斯方法中常见的其他的估计方法。&lt;/p&gt;
&lt;h3 id=&#34;最大后验估计&#34;&gt;最大后验估计&lt;/h3&gt;
&lt;p&gt;最大后验估计（maximum a posteriori estimation）得到的点估计是以下：
&lt;span class=&#34;math display&#34;&gt;\[
\hat \theta = \arg \max_{\theta} \frac{P(\theta) P(D | \theta)}{\int
P(\theta&amp;#39;) P(D|\theta&amp;#39;) \d \theta&amp;#39;} = \arg \max_{\theta}
{P(\theta) P(D | \theta)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;最小均方差估计&#34;&gt;最小均方差估计&lt;/h3&gt;
&lt;p&gt;最小均方差估计（minimum mean squared error
estimation）得到的点估计是以下： &lt;span class=&#34;math display&#34;&gt;\[
\theta^\star = \arg \min_{\hat \theta} \E_{\theta \sim \text{posterior}}
[(\hat \theta - \theta)^2]
\]&lt;/span&gt; 换言之，此时的到的点估计&lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt;即为&lt;span class=&#34;math inline&#34;&gt;\(\E_{\theta \sim \text{posterior}}
\theta\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id=&#34;可信区间&#34;&gt;可信区间&lt;/h3&gt;
&lt;p&gt;可信区间（credible interval），或者叫最大后验密度（highest posterior
density）得到的是一个区间，该区间是使得随机变量落在该区间内的概率大于某一个数字（常用的有95%、98%）的最小区间。&lt;/p&gt;
&lt;h3 id=&#34;案例抛硬币&#34;&gt;案例：抛硬币&lt;/h3&gt;
&lt;p&gt;在具体的抛硬币案例中（抛&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;次硬币，其中&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次正面朝上，未知参数为正面朝上概率&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;）， &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{likelihood: } P(n|p) = {N \choose n} p^n (1-p)^{N-n} \\
\text{prior: } \rho(p) = 1 \\
\text{evidence: } \int_0^1 P(n|p) \rho(p) \d p \\
\text{posterior: } \rho(p|n)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; \rho(p|n) = \frac{P(n|p) \rho(p)}{\int_0^1 P(n|p) \rho(p) \d p} \\
&amp;amp;= \frac{{N \choose n} p^n (1-p)^{N-n} \rho(p)}{\int_0^1 {N \choose
n} x^n (1-x)^{N-n} \rho(x) \d x} \\
&amp;amp;= \frac{p^n (1-p)^{N-n} 1}{\int_0^1 x^n (1-x)^{N-n} 1 \d x} \\
&amp;amp;= \frac{p^{n+1-1} (1-p)^{N-n+1-1}} {\underbrace{\int_0^1 x^{n+1-1}
(1-x)^{N-n+1-1} \d x}_{\mathrm{Beta}(n+1, N-n+1)}} \\   
&amp;amp;= \mathrm{Beta}(p|n+1,N-n+1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(n+1,
N-n+1)\)&lt;/span&gt;表示&lt;strong&gt;&lt;em&gt;Beta函数&lt;/em&gt;&lt;/strong&gt;在&lt;span class=&#34;math inline&#34;&gt;\((n+1, N-n+1)\)&lt;/span&gt;处的取值；&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(p|\underbrace{n+1}_{a&amp;gt;0},
\underbrace{N-n+1}_{b&amp;gt;0})\)&lt;/span&gt;表示参数&lt;span class=&#34;math inline&#34;&gt;\(a =n+1,
b=N-n+1\)&lt;/span&gt;时的&lt;strong&gt;&lt;em&gt;Beta分布&lt;/em&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;需要注意的是&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Beta}(p|1,1)\)&lt;/span&gt;等价于&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;之间的均匀分布： &lt;span class=&#34;math display&#34;&gt;\[
\mathrm{Beta}(p|1,1) = \text{Uniform}(p|0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在prior为Beta分布、likelihood为二项分布时，得到的posterior依旧是Beta分布（不过该Beta分布的参数和prior中Beta分布的参数有所不同），此时的prior和likelihood称作&lt;strong&gt;conjugate
distributions&lt;/strong&gt;，此时的prior称作likelihood的&lt;strong&gt;conjugate
prior&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Beta分布是二项分布的conjugate prior；高斯分布是高斯分布的conjugate
prior。&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Quadratic Form</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</link>
      <pubDate>Fri, 05 May 2023 10:21:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</guid>
      <description>

&lt;h2 id=&#34;quadratic-form&#34;&gt;Quadratic Form&lt;/h2&gt;
&lt;p&gt;Quadratic form involves many concepts like real symmetric matrix,
positive definiteness and singular value decomposition. It can be quite
helpful to glue these things together.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;quadratic function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; variables, or say a vector &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, is the sum of second-order terms:
&lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \sum_{i=1}^n \sum_{j=1}^n c_{ij} x_i x_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above summation can be simplified as matrix product &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n \times
n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = c_{ij}\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; is called the
&lt;strong&gt;quadratic form&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In essence, there is a nicer formulation for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Firstly define the &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = \frac{1}{2} (c_{ij} + c_{ji})\)&lt;/span&gt;.
It suffices to show &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a real
symmetric matrix and &lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \x^T A \x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus the discussion of quadratic form usually encompasses the real
symmetric matrix.&lt;/p&gt;
&lt;h3 id=&#34;positive-definiteness&#34;&gt;Positive Definiteness&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be a real symmetric
matrix. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;strong&gt;positive
definite&lt;/strong&gt; if and only if the quadratic form of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive. Specifically, for every
&lt;span class=&#34;math inline&#34;&gt;\(\x \ne 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;A real symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
positive definite if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvalues are positive.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For every &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenpair &lt;span class=&#34;math inline&#34;&gt;\((\lambda, \v)\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(\v^T A \v = \lambda \v^T \v &amp;gt; 0 \Rightarrow
\lambda &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s spectral decomposition
as &lt;span class=&#34;math inline&#34;&gt;\(A = Q \Lambda Q^T\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Q Q^T = I\)&lt;/span&gt;. For every &lt;span class=&#34;math inline&#34;&gt;\(\x &amp;gt; 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\x^T A \x = \overbrace{\x^T Q}^{y^T} \Lambda \overbrace{Q^T \x}^{y} &amp;gt;
0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One possible reason why we would like to define positive definiteness
on real symmetric matrix is that, any real matrix can be easily
decomposed into the addition of a real symmetric matrix and a real
&lt;strong&gt;skew-symmetric&lt;/strong&gt; matrix whose quadratic form is zero:
&lt;span class=&#34;math display&#34;&gt;\[
A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\frac{A - A^T}{2}\)&lt;/span&gt;’s
quadratic form is zero and it makes no contribution to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s, the only component of interest will
be the real symmetric &lt;span class=&#34;math inline&#34;&gt;\(\frac{A +
A^T}{2}\)&lt;/span&gt;. So why not just focus on the real symmetric
matrix?&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>随机变量的收敛</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</link>
      <pubDate>Wed, 13 Jul 2022 14:41:05 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/</guid>
      <description>

&lt;h2 id=&#34;依概率收敛convergence-in-probability&#34;&gt;依概率收敛（convergence in
probability）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;随机变量序列&lt;/strong&gt;即是由随机变量构成的序列。对于一个普通数列&lt;span class=&#34;math inline&#34;&gt;\(\{x_n\}\)&lt;/span&gt;来说，若其收敛于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，则意味着当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;充分大时，&lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离可以达到任意小。而随机变量序列&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;的极限却不能按照这样定义，因为&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;取值不确定，不可能总和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;的距离任意小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n - c| &amp;lt; \epsilon) =
1\)&lt;/span&gt;，抑或是，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，都有&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} P(|X_n -
c| \ge \epsilon) =
0\)&lt;/span&gt;），则称该随机变量序列&lt;strong&gt;依概率收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{P}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon, \delta &amp;gt;
0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt;
0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有
&lt;span class=&#34;math display&#34;&gt;\[
1 - \delta &amp;lt; P(|X_n - c| &amp;lt; \epsilon) \le 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;依概率收敛的一个例子便是&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Bernoulli大数定律&#34;&gt;Bernoulli大数定律&lt;/a&gt;，即当试验次数足够多时，事件的频率会依概率收敛到该事件的概率。&lt;/p&gt;
&lt;h2 id=&#34;几乎必然收敛almost-sure-convergence&#34;&gt;几乎必然收敛（almost-sure
convergence）&lt;/h2&gt;
&lt;p&gt;在某些情况下，若随机变量序列能够和某个数字&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;几乎接近，我们说它几乎必然收敛。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(P(\lim_{n \to \infty} X_n = c) =
1\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;几乎必然收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{a.s.}{\to} c\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;换言之，对于任意&lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt;
0\)&lt;/span&gt;，都存在&lt;span class=&#34;math inline&#34;&gt;\(N &amp;gt;
0\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; N\)&lt;/span&gt;时，始终有
&lt;span class=&#34;math display&#34;&gt;\[
P(|X_n - c| &amp;lt; \epsilon) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，几乎必然收敛和依概率收敛是不等价的，因为&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty}
f(x_n)\)&lt;/span&gt;中的极限符号不总是能够交换到函数&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;内部，举个简单的例子： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\{ x_n \} = -\frac{1}{n}, \
f(x) = \begin{cases}
x^2 - 1, &amp;amp; -1 \le x &amp;lt; 0 \\
x, &amp;amp; x \ge 0
\end{cases} \\
\lim_{n \to \infty} f(x_n) = \lim_{n \to \infty}(\frac{1}{n^2}-1) = -1
\ne f(\lim_{n \to \infty} x_n) = f(0) = 0
\end{gathered}
\]&lt;/span&gt; 注意&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;是右连续的，这也意味着，我们可以找到类似的右连续的分布函数&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，使得极限符号不能被移至&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;内部。也就是说，几乎必然收敛和依概率收敛是不等价的，而显然，几乎必然收敛是强于依概率收敛的。&lt;/p&gt;
&lt;h2 id=&#34;l_p收敛convergence-in-l_p&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛（convergence in &lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，对于某个&lt;span class=&#34;math inline&#34;&gt;\(p
&amp;gt; 0\)&lt;/span&gt;，如果存在一个常数&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} \E(|| X_n - c||_p^p) =
0\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{L_p}{\to} c\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;均方收敛&#34;&gt;均方收敛&lt;/h3&gt;
&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt;收敛又称作均方收敛。根据&lt;a href=&#34;https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#Chebyshev不等式&#34;&gt;Chebyshev不等式&lt;/a&gt;， &lt;span class=&#34;math display&#34;&gt;\[
P(|X_n-\E(X_n)| \ge \epsilon) \le \frac{\Var(X_n)}{\epsilon^2} =
\frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2}
\]&lt;/span&gt; 在两边取&lt;span class=&#34;math inline&#34;&gt;\(n \to
\infty\)&lt;/span&gt;可以得到 &lt;span class=&#34;math display&#34;&gt;\[
\lim_{n \to \infty} P(|X_n-\E(X_n)| \ge \epsilon) \le \lim_{n \to
\infty} \frac{\E[(X_n - \E(X_n))^2]}{\epsilon^2} = 0
\]&lt;/span&gt;
即均方收敛成立时，依概率收敛也成立，反之则不必然，故均方收敛也强于依概率收敛；但均方收敛和几乎必然收敛之间并没有推导关系。&lt;/p&gt;
&lt;h2 id=&#34;依分布收敛convergence-in-distribution&#34;&gt;依分布收敛（convergence
in distribution）&lt;/h2&gt;
&lt;p&gt;前面三者描述的是随机变量序列取值的某种特性，而依分布收敛则不同，它描述的是随机变量序列分布函数的特性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2,
\dots\)&lt;/span&gt;是一个随机变量序列，让&lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;的分布函数，如果存在一个分布函数&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，使得&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} F_n(x) =
F(x)\)&lt;/span&gt;，则称该随机变量序列&lt;strong&gt;依分布收敛&lt;/strong&gt;于&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;，记作&lt;span class=&#34;math inline&#34;&gt;\(X_n
\stackrel{d}{\to} F\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;收敛到随机变量&#34;&gt;“收敛到随机变量”&lt;/h2&gt;
&lt;p&gt;除了上述讨论的收敛到值、收敛到函数的情况外，另外一个比较有趣的话题是“收敛到随机变量”，或者说“两个随机变量相等”是一个怎样的概念？&lt;/p&gt;
&lt;p&gt;我们讨论概率的时候，会涉及到两个函数：一个是概率函数，另一个是随机变量这一从事件到数字的映射。方便起见我们令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;为两个随机变量，随机变量相等，则意味着这两个从事件到数字的映射相等，进而&lt;span class=&#34;math inline&#34;&gt;\(P(X = Y) = 1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;映射相等，意味着定义域、值域、映射关系完全相等。如果我有两个骰子，令&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;表示第一个骰子掷出的点数、&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;表示第二个骰子掷出的点数，那么&lt;span class=&#34;math inline&#34;&gt;\(X =
Y\)&lt;/span&gt;吗？答案是不，因为这两个随机变量的定义域不相等：&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的定义域表示第一个骰子的所有可能事件，&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的定义域表示第二个骰子的所有可能事件；虽然两个骰子掷出的点数都只能是1、2、3、4、5、6，但这代表的仅是值域相同，而“第一个骰子掷出一”（注意这里避免使用任何数字，以表示它是一个事件）这个事件和“第二个骰子掷出一”是不一样的，因为&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;不会因为第二个骰子掷出一而取为1。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B&#34;&gt;随机变量的收敛&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>假设检验</title>
      <link>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link>
      <pubDate>Tue, 06 Dec 2022 19:02:13 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid>
      <description>

&lt;h2 id=&#34;假设检验&#34;&gt;假设检验&lt;/h2&gt;
&lt;p&gt;参数估计是根据样本值得出参数的一个估计量，并且在区间估计中，我们还能得到一个这个参数&lt;strong&gt;置信区间&lt;/strong&gt;和相应的&lt;strong&gt;置信水平&lt;/strong&gt;。而在假设检验中，我们要做的则是根据某个&lt;strong&gt;假设&lt;/strong&gt;（hypothesis）以及给定的样本，决定是否接受这个假设。注意我们特地将假设和估计（estimation）区分开，因为这个假设并不是由样本得到的一个估计量，而是一条已有的断言（assertion）；我们要做的，则是给出在什么样的情况下（对应置信区间），我们能够以多高的信心否定这个断言（对应置信水平）。&lt;/p&gt;
&lt;p&gt;至于假设检验和区间估计的关系，其实我们会发现，如果本身就能对未知参数做有效的区间估计，那其实对它的假设检验设计，自然迎刃而解。&lt;/p&gt;
&lt;h3 id=&#34;一般检验方法&#34;&gt;一般检验方法&lt;/h3&gt;
&lt;p&gt;一般检验方法指Neyman-Pearson方法。&lt;/p&gt;
&lt;h4 id=&#34;建立假设&#34;&gt;建立假设&lt;/h4&gt;
&lt;p&gt;对要检验的问题，一般有一个&lt;strong&gt;原假设&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;（也叫零假设，null
hypothesis）以及一个&lt;strong&gt;备择假设&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;（alternative
hypothesis）。原假设一般是总体的某个未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;等于某个具体值&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
H_0: \theta = \theta_0
\]&lt;/span&gt; 这种只包含一个假设值（即&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;）的原假设又叫作&lt;strong&gt;简单原假设&lt;/strong&gt;（simple
hypothesis
null）。而备择假设一般和原假设互斥，它通常有以下三种形式：&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta \ne
\theta_0\)&lt;/span&gt;，此时&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;为对立关系，我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;两侧的可能，这样的检测问题也称为双边检验（two-sided
test）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta &amp;gt;
\theta_0\)&lt;/span&gt;，此时我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;右侧的可能，这样的检测问题也称为右侧的单边检验；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta &amp;lt;
\theta_0\)&lt;/span&gt;，此时我们要检验&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;落在&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;左侧的可能，这样的检测问题也称为左侧的单边检验；&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;选择否定域形式&#34;&gt;选择否定域形式&lt;/h4&gt;
&lt;p&gt;根据已有的样本，我们能够给出未知参数的点估计量&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;（在假设检验中又称作&lt;strong&gt;检验统计量&lt;/strong&gt;&amp;lt;test
statistic&amp;gt;），如果&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;的距离小于某个&lt;strong&gt;临界值&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(c &amp;gt; 0\)&lt;/span&gt;（critical
value），我们就可以接受原假设（即便&lt;span class=&#34;math inline&#34;&gt;\(\hat
\theta\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;不完全相等），否则则否定原假设。使得原假设被接受的样本所在的区域就被称作&lt;strong&gt;接受域&lt;/strong&gt;（acceptance
region）；使得原假设被否定的样本所在的区域就被称作&lt;strong&gt;否定域&lt;/strong&gt;（也叫拒绝域，rejection
region）；一般我们习惯先构造否定域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;，则剩余区域就为接受域&lt;span class=&#34;math inline&#34;&gt;\(\overline W\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
W = \{ (x_1, \dots, x_n) \big | |\hat \theta(x_1, \dots, x_n) -
\theta_0| &amp;gt; c \} \\
\overline W = W^c
\end{gather}
\]&lt;/span&gt;
对于某些参数，可能本身越小越好（比如故障率），所以我们仅需要进行右侧的单边检测，此时对应的否定域为
&lt;span class=&#34;math display&#34;&gt;\[
W = \{ (x_1, \dots, x_n) \big | \hat \theta(x_1, \dots, x_n) - \theta_0
&amp;gt; c \} \\
\]&lt;/span&gt; 此时可以等价认为&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \le
\theta_0\)&lt;/span&gt;，这种情况下，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;是一个&lt;strong&gt;复合原假设&lt;/strong&gt;（composite
hypothesis null），因为其包含的假设值不止一个。&lt;/p&gt;
&lt;p&gt;对于另外一些参数，可能本身越大越好（比如身高均值），所以我们仅需要左侧的单边检测，此时对应的否定域为
&lt;span class=&#34;math display&#34;&gt;\[
W = \{ (x_1, \dots, x_n) \big | \hat \theta(x_1, \dots, x_n) - \theta_0
&amp;lt; c \} \\
\]&lt;/span&gt; 同理，此时可以等价认为&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta
\ge \theta_0\)&lt;/span&gt;，这种情况下，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;也是一个复合原假设。&lt;/p&gt;
&lt;h4 id=&#34;设定显著性水平&#34;&gt;设定显著性水平&lt;/h4&gt;
&lt;p&gt;给定假设，我们已经可以根据样本属于接受域还是否定域，做出接受或是否定假设的决策了。但和点估计中的问题一样，我们依然是基于样本提供的不完全信息做出的判断，所以我们的判断不总是正确的。这种判断会有四种结果：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;判断：接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;判断：否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;实际：&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立&lt;/td&gt;
&lt;td&gt;判断正确&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;第一类错误&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;实际：&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;成立&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;第二类错误&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;判断正确&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;通常较低的第一类错误风险&lt;span class=&#34;math inline&#34;&gt;\(P(\text{否定}H_0;
H_0\text{成立})\)&lt;/span&gt;和较低的第二类错误风险&lt;span class=&#34;math inline&#34;&gt;\(P(\text{接受}H_0;
H_1\text{成立})\)&lt;/span&gt;不可兼得，因为在检验统计量确定后，这两个概率主要是由临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;导出的否定域大小来控制的。而我们更希望降低第一类错误发生的风险。也就是说我们一旦否定，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;很大概率确实是不成立的；尽管这意味着我们在接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;有可能不成立——不过虚惊一场总好过后知后觉。所以实际应用中，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;往往对应了比较严重的结果，我们不希望在&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立时，我们却没有发现（即否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;）；或者&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;本身就对应了我们比较想否定的结果，这样我们否定时，它确实不成立的概率也更高。&lt;/p&gt;
&lt;p&gt;我们会将第一类错误发生的概率限制在&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;之内，这个&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;便是&lt;strong&gt;显著性水平&lt;/strong&gt;（significance
level）。显著性水平其实代表了我们对小概率事件的接受程度，即我们认为概率小于&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的事件应该是小概率事件，并且是不应该被正好碰上的；而此时在&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立的假设下，“给定的一组样本属于否定域”正是这样的一个小概率事件，如果碰上了这样的小概率事件，则有理由怀疑&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;不成立。&lt;/p&gt;
&lt;h4 id=&#34;确定临界值&#34;&gt;确定临界值&lt;/h4&gt;
&lt;p&gt;在确定显著性水平后，我们便可以进一步确定临界值，从而给出完整的否定域。此时我们调整临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，从而使得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
P(\text{否定}H_0; H_0\text{成立}) = P( (X_1, \dots, X_n) \in W;\theta =
\theta_0) \\
= P(|\hat \theta(X_1, \dots, X_n) - \theta_0| &amp;gt; c; \theta = \theta_0)
\le \alpha
\end{gathered}
\]&lt;/span&gt; 问题就变成了一个简单的分布问题。此处需要指出的是，&lt;span class=&#34;math inline&#34;&gt;\(P(\text{否定}H_0;
H_0\text{成立})\)&lt;/span&gt;不应写作&lt;span class=&#34;math inline&#34;&gt;\(P( (X_1,
\dots, X_n) \in W | \theta =
\theta_0)\)&lt;/span&gt;，因为此处讨论的是频率学派中的假设检验，频率学派中的未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;并没有先验分布。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P( (X_1, \dots, X_n) \in W;\theta =
\theta_0)\)&lt;/span&gt;又叫作&lt;strong&gt;功效函数&lt;/strong&gt;（power
function），记作&lt;span class=&#34;math inline&#34;&gt;\(\beta_W(\cdot)\)&lt;/span&gt;，它表示在未知参数取特定值时，一组随机样本属于否定域的概率。
前面我们之所以说单边检验等价于原假设对应某个形式的复合假设（比如&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \le \theta_0\)&lt;/span&gt;或&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta \ge
\theta_0\)&lt;/span&gt;），是因为这种情况下，有 &lt;span class=&#34;math display&#34;&gt;\[
\max_{h \in H_0} \beta_W(h) = \beta_W(\theta_0) \le \alpha
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;p值和p值检验法&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值和&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验法&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验法指的是数学家Fisher提出的检验方法。&lt;/p&gt;
&lt;p&gt;假设检验的&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值是在原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;成立的情况下，检验统计量&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta(X_1, \dots,
X_n)\)&lt;/span&gt;出现其具体观测值&lt;span class=&#34;math inline&#34;&gt;\(z = \hat
\theta(x_1,\dots,x_n)\)&lt;/span&gt;或者比之更极端的值的概率，即&lt;span class=&#34;math inline&#34;&gt;\(p = P(\hat \theta = z; \theta =
\theta_0)\)&lt;/span&gt;（类似likelihood）。&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值检验中，我们检验&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值是否足够小，如果&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值小到一定程度，我们还是会否定&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，即&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p \le
\alpha\)&lt;/span&gt;，则我们在显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;下否定原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(p &amp;gt;
\alpha\)&lt;/span&gt;，则我们在显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;下接受原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/291968/are-type-i-error-fwer-both-conditional-probabilities&#34;&gt;hypothesis
testing - Are type I error &amp;amp; FWER both conditional probabilities? -
Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/hezhijian/book/test.html&#34;&gt;第 3 章
假设检验 | 数理统计讲义 (bookdown.org)&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>特征函数</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 09 May 2022 11:51:08 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/</guid>
      <description>

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;h3 id=&#34;感性认知&#34;&gt;感性认知&lt;/h3&gt;
&lt;p&gt;根据泰勒级数我们可以得知，两个函数&lt;span class=&#34;math inline&#34;&gt;\(f(x),g(x)\)&lt;/span&gt;，如果它们各阶导数相等的越多，它们就越相似，换言之
&lt;span class=&#34;math display&#34;&gt;\[
\text{各阶导数都相同} \Rightarrow f(x) = g(x)
\]&lt;/span&gt; 可以说，函数的各阶导数即是它们的特征。&lt;/p&gt;
&lt;p&gt;对于随机变量来说，这样的“特征”也存在。随机变量的特征即是它的各阶矩，即
&lt;span class=&#34;math display&#34;&gt;\[
\text{各阶矩都相同} \Rightarrow \text{随机变量对应的分布相同}
\]&lt;/span&gt; 对于随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，其特征函数定义为 &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}]
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{itX}\)&lt;/span&gt;的泰勒级数为 &lt;span class=&#34;math display&#34;&gt;\[
e^{itX} = 1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots +
\frac{(itX)^n}{n!}
\]&lt;/span&gt; 代入特征函数可得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\varphi(t) &amp;amp;= \E[1 + \frac{itX}{1!} - \frac{t^2X^2}{2!} + \dots +
\frac{(itX)^n}{n!}] \\
&amp;amp;= \E[1] + \E[\frac{itX}{1!}] - \E[\frac{t^2X^2}{2!}] + \dots +
\E[\frac{(itX)^n}{n!}] \\
&amp;amp;= 1 + \frac{it \overbrace{\E[X]}^\text{一阶矩} }{1!} - \frac{t^2
\overbrace{\E[X^2]}^\text{二阶矩} }{2!} + \dots + \frac{(it)^n
\overbrace{\E[矩} }{n!} \\
\end{aligned}
\]&lt;/span&gt;
可见特征函数包含了随机变量的所有矩，亦即随机变量的所有“特征”，所以可以说特征函数是随机变量的另一种描述方式。&lt;/p&gt;
&lt;h3 id=&#34;理性认知&#34;&gt;理性认知&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \E[e^{itX}] = \int_{-\infty}^{+\infty} e^{itx} p(x)\; dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而对&lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;进行逆傅里叶变换可得
&lt;span class=&#34;math display&#34;&gt;\[
F(t) = \int_{-\infty}^{+\infty} p(x) e^{-itx} dx
\]&lt;/span&gt; 可见二者互为共轭关系： &lt;span class=&#34;math display&#34;&gt;\[
\varphi(t) = \overline{F(t)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;应用&#34;&gt;应用&lt;/h2&gt;
&lt;p&gt;通过求&lt;span class=&#34;math inline&#34;&gt;\(t =
0\)&lt;/span&gt;时的各阶导数，可以快速求得各阶矩： &lt;span class=&#34;math display&#34;&gt;\[
\varphi^{(k)}(0) = i^k \E[X^k]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/23686709&#34;&gt;特征函数的理解&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/energy-efficient-computing/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/energy-efficient-computing/notes/</guid>
      <description>&lt;h1 id=&#34;implementation-trickcomm-lower-bound&#34;&gt;Implementation Trick&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;h2 id=&#34;gemm&#34;&gt;GEMM&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ge&lt;/strong&gt;neral &lt;strong&gt;M&lt;/strong&gt;atrix &lt;strong&gt;M&lt;/strong&gt;ultiplication describes the implementation tricks that speeds up computation in neural network. Matrix multiplication is a classical, fundamental and established field in both math and computer science. And this is the reason why much effort and interest have been put into how to further speed up it and how to convert other kinds of operations into it.&lt;/p&gt;
&lt;h3 id=&#34;im2col&#34;&gt;Im2Col&lt;/h3&gt;
&lt;h4 id=&#34;single-feature-map-and-kernel&#34;&gt;Single Feature Map and Kernel&lt;/h4&gt;
&lt;p&gt;A normal convolution operation will slide a &lt;strong&gt;window&lt;/strong&gt; of the same size as &lt;strong&gt;kernel&lt;/strong&gt; ($F: k_h \times k_w$) through the &lt;strong&gt;feature map&lt;/strong&gt; ($I: i_h \times i_w$) in a row-major order (for simplicity, we will take that stride is $1$ and padding is $0$). This causes problem because numbers in a single convolution operation will span multiple columns, due to which the spatial locality cannot be exploited.&lt;/p&gt;
&lt;p&gt;Since the convolution operation is in essence doing the &amp;ldquo;sum of products&amp;rdquo;, we may just as well treat the convolution as dot product between two vectors.&lt;/p&gt;
&lt;img src=&#34;./Images/im2col.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To realize it, we can squeeze the kernel into a $k_h k_w \times 1$ column vector and each window on the feature map to a $1 \times k_h k_w$ row vector (in memory, a column vector &lt;code&gt;v[N][1]&lt;/code&gt; is no difference from a row vector &lt;code&gt;v[1][N]&lt;/code&gt;). Then we stack these row vectors vertically in the order as their original window would appear in the convolution. This newly synthesized matrix $L$ is usually called &lt;strong&gt;lowered matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As for implementation, we don&amp;rsquo;t do this &amp;ldquo;window by window&amp;rdquo;, because usually the feature map has a large width, which still introduce the same issue of not exploiting the spatial locality when accessing across rows. For each position in the feature map, we can identify all the positions that it will appear in the lowered matrix in one off. Since kernel size is usually small, accessing this lowered matrix across rows causes less cache misses than that in the input.&lt;/p&gt;
&lt;p&gt;Treating $L$ as $l_h \times l_w \times k_h \times k_w$, we fill up its entries in following way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;im2lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;normal-input-and-filter&#34;&gt;Normal Input and Filter&lt;/h4&gt;
&lt;p&gt;By far we only consider the case of a single convolution. More often that not, the real-world convolution is done in batch with multiple channels, meaning that the input $I$ is of shape $i_n \times i_c \times i_w \times i_w$ and filter $F$ is of shape $f_c \times i_c \times k_h \times k_w$. Each of $f_c$ output channels is obtained as the sum of the one-on-one convolution between each of the $i_c$ kernels and each of the $i_c$ channels (which is in accordance with PyTorch&amp;rsquo;s &lt;code&gt;Conv2d&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now we consider the case where there are multiple channels. We simplify a bit by still setting $i_n = 1$. As a result, the input $I$ is of shape $i_c \times i_h \times i_w$ and filter $F$ still remains $f_c \times i_c \times k_h \times k_w$. Suppose each feature map contains $d = o_h \times o_w$ unique kernel windows. Then each transformed channel should be of shape $d \times k_h k_w$ (the order of symbols in shortened multiplication matters too; in this case, $k_h k_w$ indicates we squeeze by row); each transformed kernel should be of shape $k_h k_w \times 1$.&lt;/p&gt;
&lt;p&gt;The input contains $i_c$ such transformed $d \times k_h k_w$ channels. Instead of doing matrix multiplication $i_c$ times, we can concatenate these $i_c$ matrices horizontally to give a single $d \times (i_c k_h k_w)$ lowered matrix $L$.&lt;/p&gt;
&lt;p&gt;The filter contains $f_c \times i_c$ such transformed $k_h k_w \times 1$ kernels. For each output channel, we can concatenate corresponding $i_c$ kernels vertically to facilitate the one-on-one convolution, which gives a single $(i_c k_h k_w) \times f_c$ transformed filter $F&amp;rsquo;$.&lt;/p&gt;
&lt;p&gt;Now $I \circledast F$ becomes $L \times F&amp;rsquo;$. The output shape is $d \times f_c$. Each input image becomes a $d \times 1$ column vector finally, which is exactly what &amp;ldquo;Im2Col&amp;rdquo; means. This $d \times 1$ vector can be transposed and then reshaped into $o_h \times o_w$ to recover the convolution result (no actual transformation has to be done, just to interpret it this way). Then by applying the Im2Col trick repeatedly, we can chain up and handle consecutive convolutional layers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;im2lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;ker2col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mecmec&#34;&gt;MEC&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Im2Col costs much extra space because most entries in the feature map appears $k^2$ times in the transformed lowered matrix. The &lt;strong&gt;m&lt;/strong&gt;emory-&lt;strong&gt;e&lt;/strong&gt;fficient &lt;strong&gt;c&lt;/strong&gt;omputation method improves on this by putting the entries of (say vertically) adjacent windows in one row so that entries can be reused. By doing so, the transformed kernel row vector will slide through each row of obtained matrix to compute convolution result.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/mec-basic.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Again in single feature map and kernel case, the filter $F: k_h \times k_w$ is squeezed to a $k_h k_w \times 1$ column vector. The input $I: i_h \times i_w$ is converted to the lowered matrix $L: l_h \times l_w$ where
$$
l_h = o_w \triangleq (i_w - k_w) / s_w + 1 \
l_w = i_h k_w, \text{assuming that $k_w \ge s_w$}\
$$
Note that a $k_w$-width bar in $I$ (like $A$) expands to a row in $L$, and finally a row in $O$. Treating $L$ as $o_w \times i_h \times k_w$, we fill up its entries in following way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// const int l_h = o_w, l_w = i_h * k_w;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// I can be transposed first for better spatial locality.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;memory-layout&#34;&gt;Memory Layout&lt;/h4&gt;
&lt;p&gt;Memory layout is not a standalone method to speed up matrix computation, but that it cornerstones most implementation tricks. Different methods may assume different memory layouts.&lt;/p&gt;
&lt;p&gt;In convention, the input is of shape $\mathrm{N \times C \times H \times W}$, which is the assumed layout of Im2Col. However, the preferred layout for convolution is usually $\mathrm{N \times H \times W \times C}$.&lt;/p&gt;
&lt;p&gt;The justification is that, we usually would parallelize by accessing a window of pixels across all the channels. Though the window spans multiple columns in both cases, $\mathrm{N \times C \times H \times W}$ would separate these $C$ windows apart but $\mathrm{N \times H \times W \times C}$ would otherwise bring all the $c$ channel values of a pixel in a row, in which case the spatial locality can be exploited.&lt;/p&gt;
&lt;p&gt;$\mathrm{N \times H \times W \times C}$ is the assumed layout by MEC. That is, $I$ is of shape $i_n \times i_h \times i_w \times i_c$ and $F$ is of shape $k_h \times k_w \times i_c \times f_c$. Given $i_n \times i_c$ number of $o_w \times i_h k_w$ matrices, $L$ is obtained by interleaving horizontally across channels, and stacked vertically across images. As a result, $L$ is of shape $i_n o_w \times i_h k_w i_c$. Accordingly, $F$ is squeezed to $F&amp;rsquo;: k_h k_w i_c \times f_c$.&lt;/p&gt;
&lt;p&gt;The example below shows a $3 \times 7 \times 7 \times 1$ input and $3 \times 3 \times 1 \times 1$ filter.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/mec-batch.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Given the transformed $L$ and $F&amp;rsquo;$, there are two methods for the remaining dot product (convolution). One is to do $L[0 : i_n o_w, s_w i_h k_w h : s_w i_h k_w h + k_h k_w c] \times F&amp;rsquo;$ for $h = 0, \dots, o_h$, resulting in a $\mathrm{H \times N \times W \times C}$ layout (note that each bar in $L$ expands to a row in $O$), illustrated at the upper right in the above figure. If we want to chain up convolutional layers, we need to convert this layout to $\mathrm{N \times H \times W \times C}$.&lt;/p&gt;
&lt;p&gt;Another is to do $L[o_w n : o_w (n+1), s_w i_h k_w h : s_w i_h k_w h + k_h k_w c]$ for $h = 0, \dots, o_h, n = 0,\dots,i_n$, resulting in a $\mathrm{N \times H \times W \times C}$ layout, illustrated at the lower right in the above figure (the tensor is not properly drawn though; it should have been of shape $3 \times 5 \times 5$).&lt;/p&gt;
&lt;h2 id=&#34;direct-convmmdirect-convdirect-mm-1direct-mm-2direct-mm-3&#34;&gt;Direct Conv/MM&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;GEMM usually requires extra time and space to do transformation. In-place convolution/matrix multiplication also have room for improvement.&lt;/p&gt;
&lt;h3 id=&#34;loop-reordering&#34;&gt;Loop Reordering&lt;/h3&gt;
&lt;p&gt;Nested loop is very common in computation. If the effective statements only appear in the innermost loop, the nesting level of loop indices can be usually be changed without causing side effect.&lt;/p&gt;
&lt;p&gt;One reason to shuffle the loop indices is to better exploit the spatial locality. Another is to exploit the temporal locality, or called input reuse. As an example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// before reordering
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;memset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// after reordering
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matmul_ikj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;memset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before reordering, when innermost loop traverses over &lt;code&gt;k&lt;/code&gt;, there will be one output reuse (&lt;code&gt;C[i][j]&lt;/code&gt;), one cache hit (&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache miss (&lt;code&gt;B[k][j]&lt;/code&gt;. After reordering, when innermost loop traverses over &lt;code&gt;j&lt;/code&gt;, there will be one input reuse (&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache hit (&lt;code&gt;B[k][j]&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;loop-unrolling&#34;&gt;Loop Unrolling&lt;/h3&gt;
&lt;p&gt;At the end of a loop, there is usually a branch checking to determine whether to exit the loop or not. Loop unrolling tries to reduce the number of branch checking in loop (see &lt;a href=&#34;https://www.wikiwand.com/en/Duff%27s_device&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duff&amp;rsquo;s device&lt;/a&gt;). If the number of loops is known in advance, as is usually the case in a &lt;code&gt;for&lt;/code&gt; loop, the number of branch checking can be reduced by repeating the loop statements several times:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;/* count &amp;gt; 0 and count % 8 == 0 assumed */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// before unrolling
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// after unrolling
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Manual loop unrolling makes little sense nowadays since modern CPU can smartly predict the correct branch and modern compiler can automatically optimize the loop code.&lt;/p&gt;
&lt;h3 id=&#34;write-caching&#34;&gt;Write Caching&lt;/h3&gt;
&lt;h3 id=&#34;tilingtiled-mmtiled-mm-multithreaded&#34;&gt;Tiling&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Tiling utilizes the matrix multiplication property that
$$
\begin{gather*}
A =
\begin{pmatrix}
A_{11} &amp;amp; A_{12} \
A_{21} &amp;amp; A_{22}
\end{pmatrix},
B =
\begin{pmatrix}
B_{11} &amp;amp; B_{12} \
B_{21} &amp;amp; B_{22}
\end{pmatrix} \ \
A B = 
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp;amp; A_{11} B_{12} + A_{12} B_{22} \
A_{21} B_{11} + A_{22} B_{21} &amp;amp; A_{21} B_{12} + A_{22} B_{22} \
\end{pmatrix}
\end{gather*}
$$&lt;/p&gt;
&lt;p&gt;Tiling divides the matrix into blocks that can better fit in the cache line, so that the temporal locality can be exploited.&lt;/p&gt;
&lt;h3 id=&#34;vectorization-simdvectorization&#34;&gt;Vectorization (SIMD)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;h3 id=&#34;array-packing&#34;&gt;Array Packing&lt;/h3&gt;
&lt;h2 id=&#34;dataflow-optimization&#34;&gt;Dataflow Optimization&lt;/h2&gt;
&lt;h3 id=&#34;systolic-arraysystolic-array&#34;&gt;Systolic Array&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Systolic array is dataflow trick implemented in hardware level to speed up compute-bound task. It is an analog to the heart beat: in systolic array setting, memory is the heart, which bumps data (blood) to (usually a regular array of) processing elements (cells) and then recycle (processing result).&lt;/p&gt;
&lt;p&gt;A whole bandwidth of data would certainly entail a bunch of PEs to digest. These PEs can have local memory and execution kernel, which means they can be any kind of computing devices. PEs also connect to each for passing data. All that&amp;rsquo;s left to do is to properly orchestrate the data flow.&lt;/p&gt;
&lt;p&gt;The crux is that, instead of bumping one piece of data to a single processing element (PE), bringing the memory bandwidth to the utmost utilization would be more efficient. Other than that, once data is brought out from memory, it and its intermediate result can be used effectively at each PE it passes through.&lt;/p&gt;
&lt;h2 id=&#34;elimination-of-multiplicationmnnfaster-mm&#34;&gt;Elimination of Multiplication&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;Multiplication is way more time-consuming than addition. Therefore generally, we are willing to trade off with more additions for less multiplications. The two algorithms below can reduce the number of multiplications in matrix computation.&lt;/p&gt;
&lt;h3 id=&#34;strassens-algorithmstrassen-implstrassen-analysis&#34;&gt;Strassen&amp;rsquo;s Algorithm&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;h1 id=&#34;endbmatrix&#34;&gt;Firstly we look at the &lt;strong&gt;Strassen&amp;rsquo;s algorithm&lt;/strong&gt; of matrix computation. Suppose we do the matrix multiplication on two square matrices $M$ and $N$ following the idea of blockwise multiplication. We split the two matrices into
$$
M = \begin{bmatrix}
A &amp;amp; B \
C &amp;amp; D
\end{bmatrix},
N = \begin{bmatrix}
E &amp;amp; F \
G &amp;amp; H
\end{bmatrix}
$$
Then we calculate the intermediate matrices
$$
\begin{align*}
S_1 &amp;amp;= (B-D)(G+H) \
S_2 &amp;amp;= (A+D)(E+H) \
S_3 &amp;amp;= (A-C)(E+F) \
S_4 &amp;amp;= (A+B)H \
S_5 &amp;amp;= A(F-H) \
S_6 &amp;amp;= D(G-E) \
S_7 &amp;amp;= (C+D)E \
\end{align*}
$$
And the final result will be
$$
\begin{bmatrix}
A &amp;amp; B \
C &amp;amp; D
\end{bmatrix}
\begin{bmatrix}
E &amp;amp; F \
G &amp;amp; H
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
S_1 + S_2 - S_4 + S_6 &amp;amp; S_4 + S_5 \
S_6 + S_7 &amp;amp; S_2 - S_3 + S_5 - S_7
\end{bmatrix}
$$
The derivation of matrix multiplication with Strassen&amp;rsquo;s algorithm can be formulated as
$$
T(n) = 
\begin{cases}
\Theta(1), &amp;amp; \text{if $n=1$;} \
7 \Theta(\frac{n}{2}) + \Theta(n^2), &amp;amp; \text{if $n&amp;gt;1$.}
\end{cases}
$$
The master theorem provides an asymptotic analysis for divide-and-conquer recurrence like this. Let $T(n)$ be a monotonically increasing function that satisfies
$$
T(n) = a T(\frac{n}{b}) + f(n) \
T(1) = c
$$
where $a \ge 1, b \ge 2, c &amp;gt; 0$. If $f(n) \in \Theta(n^d)$ where $d \ge 0$, then
$$
T(n) = \begin{cases}
\Theta(n^d) &amp;amp; \text{if $a &amp;lt; b^d$} \
\Theta(n^d \log n) &amp;amp; \text{if $a = b^d$} \
\Theta(n^{\log_b a}) &amp;amp; \text{if $a &amp;gt; b^d$} \
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;The time complexity of Strassen&amp;rsquo;s algorithm is $\Theta(n^{\log_2 7}) \approx \Theta(n^{2.8074})$. There are 7 multiplications and 18 additions (recall that subtraction is addition in computer arithmetics) in Strassen&amp;rsquo;s algorithm. In usual blockwise matrix multiplication, these numbers are 8 and 4. These extra 14 additions in Strassen&amp;rsquo;s algorithm may drag down its performance when input size is small. Other than that, the memory access pattern of Strassen&amp;rsquo;s algorithm is quite chaotic. There are many temp matrices of different shapes generated during the execution. Besides, floating-point errors will accumulate in Strassen&amp;rsquo;s large number of additions. These factors may constitute the reason why Stassen algorithm is not widely adopted.&lt;/p&gt;
&lt;h3 id=&#34;winograd-algorithmwinograd&#34;&gt;Winograd Algorithm&lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;In 1-D convolution, let $m$ be the length of the output vector and $r$ be the length of kernel. The baseline implementation would require $mr$ multiplications. But it is argued that the minimum number of required multiplications is $m + r - 1$ (denoted as $F(m,r)$ the corresponding algorithm).&lt;/p&gt;
&lt;p&gt;Similarly in 2-D convolution, let $m \times n$ be the output dimension and $r \times s$ be the kernel dimension. The baseline implementation would require $m n r s$ multiplications. But the minimum number of required multiplications is $(m + r - 1)(n + s - 1)$ (denoted as $F(m \times n, r \times s)$ the corresponding algorithm).&lt;/p&gt;
&lt;h1 id=&#34;endbmatrix-1&#34;&gt;The Winograd paper documents the following algorithm for $F(2,3)$:
$$
F(2,3) =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 \
d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}
\begin{bmatrix}
g_0 \
g_1 \
g_2
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
m_1 + m_2 + m_3 \
m_2 - m_3 - m_4
\end{bmatrix}
$$
where
$$
m_1 = (d_0 - d_2) g_0, m_2 = (d_1 + d_2) \frac{g_0 + g_1 + g_2}{2} \
m_4 = (d_1 - d_3) g_2, m_3 = (d_2 - d_1) \frac{g_0 - g_1 + g_2}{2}
$$
Actually, this can be written in matrix form as
$$
Y =A^T [(G g) \odot (B^T d)]
$$
where
$$
B^T =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; -1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; -1
\end{bmatrix},
G =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \
\frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} \
\frac{1}{2} &amp;amp; -\frac{1}{2} &amp;amp; \frac{1}{2} \
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} \
A^T =
\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; -1 &amp;amp; -1
\end{bmatrix}\
g =
\begin{bmatrix}
g_0 &amp;amp; g_1 &amp;amp; g_2
\end{bmatrix}^T \
d =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}^T
$$
$F(m \times m, r \times r)$ can be built upon $F(m, r)$. For example, $F(2 \times 2, 3 \times 3)$ is
$$
Y&amp;rsquo; = A^T \left[ [G g G^T] \odot [B^T d B] \right] A
$$
$F(m \times n, r \times s)$ can be built upon $F(m, r)$ and $F(n, s)$.&lt;/p&gt;
&lt;p&gt;Winograd algorithm is great. One problem is that the $A,B,G$ matrix are too specific. For input/kernel of different sizes, $A,B,G$ will be greatly different. For a convolutional neural network that involves inputs/kernels of varying sizes, Winograd algorithm is not suitable for acceleration on special-purpose hardware, which is usually dedicated to a fixed type of computation.&lt;/p&gt;
&lt;h2 id=&#34;sparse-computationsparse&#34;&gt;Sparse Computation[^sparse]&lt;/h2&gt;
&lt;p&gt;Structured sparsity can reduce computation when there is zero in the multiplicands. But this involves zero-check for each element of the filter, which might ruin the CPU pipeline however. To avoid zero-check in a sparse matrix, we may as well store the positions of all the nonzero elements. During computation, only involved nonzero elements will be multiplied and accumulated to obtain the final result.&lt;/p&gt;
&lt;h3 id=&#34;sparse-matrix-multiplication&#34;&gt;Sparse Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;For a sparse matrix, we either store it in &lt;strong&gt;compressed sparse row&lt;/strong&gt; (CSR) or &lt;strong&gt;compressed sparse column&lt;/strong&gt; (CSC) (or other formats&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;). That is, for every row/column, we maintain a linked list whose nodes store the nonzero value and its offset in this row/column.&lt;/p&gt;
&lt;p&gt;The compression is done after training. The choice of CSR or CSC in matrix multiplication seems arbitrary, so long as one matrix is CSR and the other is CSC. But in practice, convolution is done row-by-row. We need to access the feature map across rows more often than across columns. Thus, feature map is stored in CSR and the filter is stored in CSC.&lt;/p&gt;
&lt;h3 id=&#34;sparse-sparse-convolution&#34;&gt;Sparse-sparse Convolution&lt;/h3&gt;
&lt;p&gt;Sometimes not only the filter, but also the feature map is sparse, e.g. in point cloud case. We may as well apply the Im2Col and then apply the sparse matrix multiplication. But better still, we hope to directly apply the sparse convolution&lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;18&lt;/a&gt;&lt;/sup&gt; on the original feature map.&lt;/p&gt;
&lt;p&gt;But does sparse-sparse convolution always work? It depends. The crux is that we don&amp;rsquo;t just carry out a single sparse-sparse convolution. Modern models consist of multiple layers. We hope the output of each layer is sparse so that sparse-sparse convolution can be chained up. If the output of any intermediate layer is not sparse enough, the whole process becomes pointless.&lt;/p&gt;
&lt;h2 id=&#34;tensor-virtual-machine&#34;&gt;Tensor Virtual Machine&lt;/h2&gt;
&lt;p&gt;Current neural network frameworks translate models into a computation graph in ONNX format, which in turn is translated into hardware instructions by the compilers from different manufacturers (e.g. TensorRT for NVIDIA GPU, MNN for ARM Cortex-A CPU, OpenVINO for Intel CPU).&lt;/p&gt;
&lt;h2 id=&#34;cuda&#34;&gt;CUDA&lt;/h2&gt;
&lt;p&gt;Ideally, the computation can be parallelized for greater speedup. CUDA provides such API for parallel computation on GPU. One key concept of CUDA is its granularity of execution: grid -&amp;gt; block -&amp;gt; thread (from coarsest to finest).&lt;/p&gt;
&lt;h1 id=&#34;model-trick&#34;&gt;Model Trick&lt;/h1&gt;
&lt;p&gt;Other than implementation tricks, the matrix computation can be sped up by multiplication with zero. Block of zeros usually allows us to jump a series of block computation when using the implementation tricks mentioned before. Other than that, it saves space due to the sparse matrix storage model (a kind of &lt;strong&gt;model compression&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;It would be great if there are multiple blocks of zeros in the matrix. Better still, these zeros won&amp;rsquo;t undermine the model results much.&lt;/p&gt;
&lt;h2 id=&#34;sparsification&#34;&gt;Sparsification&lt;/h2&gt;
&lt;p&gt;Sparsification tries to zero parameters in block during training. It does so by adding special regularization term to the loss function. Typical sparsity analysis assumes the linear-regression-like problem. Denote the original loss as $\ell_w(X, Y)$ where $\ell$ is the MSE loss function, $X$ is the data, $Y$ is the target ($Y$ may be a feature map or label vector) and $w$ is the model parameters (interpreted as a vector). Then, the problem is formulated as
$$
\min_{w} \ell_w(X, Y) \triangleq ||Y - X w||&lt;em&gt;F
$$
We may force an extra $l_p$ norm term on $w$:
$$
\hat \ell_w(X, Y) \triangleq \ell_w(X, Y) + \lambda ||w||&lt;em&gt;p
$$
Note that
$$
||w||&lt;em&gt;0 = \sum&lt;/em&gt;{i} \mathbb{1}[w_i \ne 0] \
||w||&lt;em&gt;1 = \sum&lt;/em&gt;{i} |w_i| \
||w||&lt;em&gt;2 = \sqrt{\sum&lt;/em&gt;{i} x_i^2} \
||w||&lt;/em&gt;\infty = \max&lt;/em&gt;{i} |x_i|
$$
$l_0$ norm is a direct attempt to penalize nonzero parameters. However, $\lambda ||w||_0$ is not continuous at points when there is a zero entry in $w$; when $\forall i, w_i \ne 0$, $\lambda ||w||_0$ does not contribute gradient at all. There is no analytical way to determine $w_i$ should be zero or not. The only course open is to manually set each $w_i$ to zero. But this method is prohibitive in terms of the complexity: there are $2^{|w|}$ combinations to try (there are &lt;a href=&#34;https://www.wikiwand.com/en/Matching_pursuit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orthogonal matching pursuit&lt;/a&gt; &amp;lt;??&amp;gt; and &lt;a href=&#34;https://www.jmlr.org/papers/volume19/17-194/17-194.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other methods&lt;/a&gt; that try to approximate it though). Thus, the $l_0$ norm is ruled out for consideration.&lt;/p&gt;
&lt;p&gt;Then $l_1$ norm pops up. $l_1$ norm is good since there is an analytical solution to its gradient w.r.t. $w$. (why does $l_1$ norm add sparsity?)&lt;/p&gt;
&lt;h3 id=&#34;structured-sparsity&#34;&gt;Structured Sparsity&lt;/h3&gt;
&lt;p&gt;Better zero parameters is that zero parameters appear block-wise. Block-wise zero entries are the essence of sparsity. However, the Lasso term does not guarantee zero entries appear in block. To amend it, &lt;strong&gt;group Lasso&lt;/strong&gt; trick is invented and the regularized loss becomes
$$
\hat \ell_w(X, Y) = \ell_w(X, Y) + \sum_j \lambda_j ||\beta_j||_1
$$
where $\beta_j$ is the $l_2$ norm of $j$-th group of parameters that usually spatially near.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/structured-sparsity.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Structured sparsity precedes random sparsity, because random sparsity does not secure a regular memory access pattern, so that there will be a poor cache locality. The figure above illustrates from irregular structured sparsity to regular structured sparsity&lt;sup id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;. Obviously, regular structured sparsity is preferred so long as it won&amp;rsquo;t undermine the model performance much.&lt;/p&gt;
&lt;h3 id=&#34;nonlinearity-approximationnonlinear-approx&#34;&gt;Nonlinearity Approximation&lt;sup id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;As mentioned, typical sparsification analyzes the linear regression problem, which has two components: 1) the model as a linear function; 2) the MSE loss function. The MSE loss function does well, not among the best though, in most tasks. But mere linear model won&amp;rsquo;t do generally.&lt;/p&gt;
&lt;p&gt;Most nonlinearity in nonlinear model comes from the element-wise function mapping on the tensor obtained by linear transformation of input, e.g. the response on feature map obtained by convolution (in this case, $X,w,X w$ will be the transformed input, kernel and convolution result respectively, as shown in &lt;a href=&#34;#GEMM&#34;&gt;GEMM&lt;/a&gt; section). Our focus is still on sparsity of the linear component in the nonlineear model.&lt;/p&gt;
&lt;p&gt;The objective becomes
$$
\min_w ||Y - f(X w)||_F
$$
where $f$ is a nonlineear function like $\mathrm{ReLU}$.&lt;/p&gt;
&lt;h2 id=&#34;pruning&#34;&gt;Pruning&lt;/h2&gt;
&lt;p&gt;Pruning is a kind of post-processing trick, which happens after training, to make parameters more &amp;ldquo;zero&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;channel-pruning&#34;&gt;Channel Pruning&lt;/h3&gt;
&lt;p&gt;Channel pruning boils down to the following optimization problem:
$$
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i \right\Vert_F^2 \
\text{s.t.} \quad &amp;amp; ||\beta||_0 \le c&amp;rsquo;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note here that $X_i$ is not the feature map at $i$-th channel, but instead the sampled window of kernel size ($k_h \times k_h$) on $i$-th feature map. It is known that this problem is NP-hard because of the $l_0$ norm term in the constraint. In practice, it can be relaxed to (why and really?)
$$
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i \right\Vert_F^2 \
\text{s.t.} \quad &amp;amp; ||\beta||_1 \le c&amp;rsquo;&amp;rsquo; \and \forall i, ||W_i||_F = 1
\end{aligned}
$$
Note that kernels $W$ is also included for optimization. This is because after pruning, kernels may still be fine-tuned a bit to preserve the accuracy.&lt;/p&gt;
&lt;p&gt;The above formula can be optimized in an alternative fashion: i.e. $W$ is fixed and $\beta$ is to be optimized; then $\beta$ is fixed and $W$ is to be optimized. We also add a regularization term for each $W_i$. This is because optimizing $W$ is materially a linear regression task, which easily has infinitely many solution due to the dimension of $X_i$ unless there is an extra constraint.&lt;/p&gt;
&lt;h3 id=&#34;low-rank-decomposition&#34;&gt;Low-rank Decomposition&lt;/h3&gt;
&lt;p&gt;Take the matrix multiplication as an example to appreciate the idea of low-rank decomposition:
$$
X \times W \Rightarrow X \times U \times V
$$
We may decompose $W: m \times n$ into $U: m \times r$ and $N: r \times n$ where hopefully $r &amp;lt; n,m$ (note that when $r = \rank W$, there exists $U$ and $V$ that can fully recover $W$). In this way, storage cost can be saved and computation can be sped up.&lt;/p&gt;
&lt;p&gt;Note that our objective is not to reconstruct the matrix $W$ (called &lt;strong&gt;matrix approximation&lt;/strong&gt;) with some other lower-rank matrices. Or else simply the singular value decomposition would do the job. Instead, we are to reconstruct the model output (called &lt;strong&gt;matrix regression&lt;/strong&gt;). Therefore, the problem is formulated as
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  X W||_F \
\text{s.t.} \quad &amp;amp; W = U V \
&amp;amp; \rank U, \rank V \le r
\end{aligned}
$$&lt;/p&gt;
&lt;h4 id=&#34;combination-with-other-methods&#34;&gt;Combination with Other Methods&lt;/h4&gt;
&lt;p&gt;Low-rank decomposition can be combined with sparsity and nonlinearity approximation to give the following ultimate pruning problem:
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||_F \
\text{s.t.} \quad &amp;amp; W = A + B \
&amp;amp; ||A||_0 \le S \
&amp;amp; \rank B \le L \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $|| \cdot ||_0$ is the number of nonzero entries in the matrix.&lt;/p&gt;
&lt;p&gt;The sparsity comes from $A$ and low-rank decomposition comes from $B$. The above problem is NP-hard due to the constraints on $A$ and $B$. We can relax these constraints to other forms:
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||&lt;em&gt;F \
\text{s.t.} \quad &amp;amp; W = A + B \
&amp;amp; ||A||&lt;/em&gt;{21} \le S  \
&amp;amp; ||B||&lt;em&gt;* \le L \
\end{aligned}
$$
where $|| \cdot ||&lt;/em&gt;{21}$ takes the $l_1$ norm of the $l_2$ norms taken on each column of the matrix; and $|| \cdot ||_*$ is the nuclear norm that equals to the sum of singular values.&lt;/p&gt;
&lt;p&gt;To solve it, alternating direction method of multipliers (ADMM) can be adopted. The augmented Lagrangian function is (??)
$$
L(W,A,B,\Lambda) = ||Y -  f(X W)||&lt;em&gt;F + \lambda_1 ||A||&lt;/em&gt;{21} + \lambda_2 ||B||_* + \Lambda \odot (W - A - B) + \frac{\rho}{2}||W - A - B||_F^2
$$&lt;/p&gt;
&lt;h4 id=&#34;tensor-decomposition&#34;&gt;Tensor Decomposition&lt;/h4&gt;
&lt;p&gt;Tucker decomp, CP decomp, Tucker-2 (or TT) decomp&lt;/p&gt;
&lt;h2 id=&#34;quantizationquant-pytorch&#34;&gt;Quantization&lt;sup id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;21&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;Model parameters are natively floating-point numbers. During training, model parameters usually won&amp;rsquo;t be too large and won&amp;rsquo;t exceed the range of &lt;code&gt;int8&lt;/code&gt;.  One way to make the model smaller and run faster is to map model parameters to integers of smaller size (say from &lt;code&gt;fp32&lt;/code&gt; to &lt;code&gt;int16&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Other than the efficiency perspective, if the original values span a very narrow range (say from -10 to 10), by mapping them to a wider range of integers, better precision may even be obtained. This is mainly due to that most floating-point number arithmetic suffer from precision loss. Particularly, when two floating-point numbers of significant difference adds or subtracts, a great loss in precision can occur (called &lt;strong&gt;catastrophic cancellation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;After integer arithmetic, we can map these integer values back again. The process involved is called &lt;strong&gt;quantization/dequantization&lt;/strong&gt;. For a number $r$, its quantization is $Q(r) = \text{Int}(r/k) - b$ where $k$ is the scale and $b$ is the bias. To dequantize, $\hat r = k(Q(r) + b)$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/quant_sym.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/quant_asym.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The first question to consider &lt;strong&gt;symmetric quantization or asymmetric quantization&lt;/strong&gt; (the figure above). That is, should the zero in the original domain mapped to zero or $-b$ (because $\text{Int}(0/k)-b = -b$). In essence, this is a question of choice of bias. Preferring to computation efficiency, we hope bias is zero. To show it, consider the dequantization process of the matrix product:
$$
A = k_A \times A_Q + b_A \
B = k_B \times B_Q + b_B \
A B = k_A k_B A_Q B_Q + k_A b_B A_Q ＋ k_B b_A B_Q + b_A b_B
$$
It would have been cleaner if $b_A, b_B$ are zero. However, if the activations in the network are mostly non-negative, like in the case where ReLU is used, symmetric quantization would waste half of the quantization range.&lt;/p&gt;
&lt;p&gt;The second question is to consider using the &lt;strong&gt;restricted range or the full range&lt;/strong&gt; of the target integer type. Take &lt;code&gt;int8&lt;/code&gt; for an example, should we map numbers to $[-127, 127]$ or $[-128, 127]$? When symmetric quantization is used, the answer is the restricted range. The reason is that, the quantization of two numbers of the same magnitude but different signs, should be of the same magnitude but different signs too. But had the full range been used, supposing the floating-point range was $[-2.2, 2.2]$, $2.2$ and $-2.2$ would have been quantized into $2.2 \times \frac{127}{2.2} = 127$ and $-2.2 \times \frac{128}{2.2} = -128$ respectively. The problem is that the scaling factors for positive number and negative number are different due to the asymmetric range, in which case a small bias would be introduced and leads to precision loss.&lt;/p&gt;
&lt;p&gt;The third question is the timing of quantization. Should the quantization happen during training (&lt;strong&gt;quantization-aware training&lt;/strong&gt;) or after training (&lt;strong&gt;post-training quantization&lt;/strong&gt;)? Accompanying this question is what the best scale should be.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Post-training (static) quantization (PTQ)&lt;/p&gt;
&lt;img src=&#34;./Images/ptq.svg&#34; style=&#34;zoom: 25%;&#34; /&gt;
&lt;p&gt;In PTQ, model are trained before quantized. After training, a small subset of training data (called &lt;strong&gt;calibration data&lt;/strong&gt;) is used to determine the scale (magnitude) and the clipping range (quantization bit number). Notice that model parameters are fixed in this process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quantization-aware training (QAT)&lt;/p&gt;
&lt;img src=&#34;./Images/qat.svg&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;In QAT, model parameters are quantized and then trained. QAT brings a great save in computation during training. But this yields another question: should the quantization happen during forward pass or during back propagation?&lt;/p&gt;
&lt;p&gt;Quantization is used in forward pass but not in back propagation. The reason not to update gradient in quantized domain is that, numbers in quantized domain are integers but gradient is fractional. Another reason not to back-propagate in quantized domain is that, the gradient might be so large as to disturb the convergence of model, compared with that in dequantized domain.&lt;/p&gt;
&lt;p&gt;The error is measured between the dequantized output and the target output. Model parameters will update in dequantized format and will be re-quantized for next round of training. There are arguments on both sides for this approach. One good aspect is that it takes the quantization error into consideration. But this causes the gradient mismatch because the gradient of the parameters are computed with quantized values but updated in dequantized format. In worst case, this may cause the model to diverge.&lt;/p&gt;
&lt;p&gt;This training method reconciles with the idea of stochastic neuron, where back propagation is done by &lt;strong&gt;straight-through estimator&lt;/strong&gt; (STE)&lt;sup id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;. There is another method based on STE and called &lt;strong&gt;parameterized clipping activation&lt;/strong&gt; (PACT)&lt;sup id=&#34;fnref:23&#34;&gt;&lt;a href=&#34;#fn:23&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;23&lt;/a&gt;&lt;/sup&gt;. The meat of PACT is a learnable ReLU function:
$$
y = \mathrm{PACT}(x, \alpha) = 0.5 (|x| - |x-\alpha| + \alpha) =
\begin{cases}
0, &amp;amp; x \le 0 \
x, &amp;amp; 0 &amp;lt; x &amp;lt; \alpha \
\alpha, &amp;amp; x \ge \alpha
\end{cases}
$$
Back-propagation is done w.r.t. the dequantized value $y_{dq} \triangleq \lfloor y \cdot \frac{2^k - 1}{\alpha} \rfloor \frac{\alpha}{2^k - 1}$. $\frac{\partial y_{dq}}{\partial y}$ is set to $\frac{\text{range before quant.}}{\text{range of quant. domain}}$ as would be in STE. $\alpha$ is learnable so that clipping range can be dynamically adjusted:
$$
\frac{\partial y}{\partial \alpha} =
\begin{cases}
0, &amp;amp; x &amp;lt; \alpha \
1, &amp;amp; x \ge \alpha
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The fourth question is the granularity of quantization. There are usually three kinds of choices, namely channel-wise, layer-wise and group-wise quantization.&lt;/p&gt;
&lt;h3 id=&#34;bnntnnbnn&#34;&gt;BNN/TNN&lt;sup id=&#34;fnref:24&#34;&gt;&lt;a href=&#34;#fn:24&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;24&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Binarized and ternary neural network bring quantization to an extreme. The reason to particularly split them out under &lt;a href=&#34;#quantization&#34;&gt;quantization&lt;/a&gt; is that, their multiplication and addition logic are quite different. They use 1 bit and 2 bits for clipping: BNN maps values to -1 and 1; TNN maps values to -1, 0 and 1. Then the multiplication and addition only involve Boolean operations, which will be much faster. Other than that, the training process of BNN/TNN resembles that of quantization.&lt;/p&gt;
&lt;p&gt;The motivation behind is that model parameters are usually within $[-1, 1]$. So why not try just using -1, 0 and 1? Given a weight matrix $W$ (squeezed into an $\R^n$ vector), there is an analytical solution to the best scaling factor $k$ and the quantized value $B$ (squeezed into an $\R^n$ vector) for BNN:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
B^* &amp;amp;= \arg \min_{B} | W - kB |&lt;em&gt;2 = \arg \min&lt;/em&gt;{B} | W - kB |&lt;em&gt;2^2 \
&amp;amp;= \arg \min&lt;/em&gt;{B} W^T W - 2 k W^T B + k^2 B^T B \
\end{aligned}
$$
Since $W$ is known and $B \in { -1, 1 }^n$, $W^T W$ is a constant $c$ and $B^T B$ is $n$. Thus,
$$
\begin{gathered}
B^* = \arg \min_{B} k^2 n + c - 2 k W^T B = \mathrm{sign}(W) \
k = \frac{W^T B^*}{n}
\end{gathered}
$$&lt;/p&gt;
&lt;h2 id=&#34;knowledge-distillation&#34;&gt;Knowledge Distillation&lt;/h2&gt;
&lt;p&gt;Knowledge distillation is a model compression method in which a smaller model is trained to mimic the pre-trained larger model. The larger and the smaller model are referred to as &amp;ldquo;teacher&amp;rdquo; and &amp;ldquo;student&amp;rdquo; respectively.&lt;/p&gt;
&lt;p&gt;There are three kinds of distillation methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Response-based knowledge&lt;/p&gt;
&lt;p&gt;This is perhaps the easiest one to think of: given the same input, the output (usually a categorical distribution) should be the same.&lt;/p&gt;
&lt;p&gt;In this case, the distillation loss is set to be the cross entropy between the distributions output by the teacher and the student. Minimizing the cross entropy between the output distributions equivalently minimizes the their KL-divergence, given that teacher&amp;rsquo;s distribution is fixed.&lt;/p&gt;
&lt;p&gt;On the other hand, the student model can further be rectified by the ground-truth loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature-based knowledge&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relation-based knowledge&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;network-architecture-search&#34;&gt;Network Architecture Search&lt;/h2&gt;
&lt;p&gt;There are mainly two indexes for a network: one is the &lt;strong&gt;latency&lt;/strong&gt; and the other is &lt;strong&gt;accuracy&lt;/strong&gt;. In NAS, latency is cheaper to check upon than accuracy, since the training is more time-consuming than a single pass of input.&lt;/p&gt;
&lt;p&gt;NAS is mostly based on heuristics. During training, we can save time by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;early stop&lt;/li&gt;
&lt;li&gt;warm restart (parameter reuse)&lt;/li&gt;
&lt;li&gt;use the arrogate target like FLOPs (which is usually proportional to latency)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for searching in the solution space, we can do with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grid search&lt;/li&gt;
&lt;li&gt;random sampling&lt;/li&gt;
&lt;li&gt;reinforcement learning&lt;/li&gt;
&lt;li&gt;evolutional algorithm&lt;/li&gt;
&lt;li&gt;Bayesian optimization like Gaussian process&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h1&gt;
&lt;p&gt;Related courses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.cs.ucsb.edu/~tyang/class/240a17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS240A - Applied Parallel Computing (ucsb.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hanlab.mit.edu/courses/2023-fall-65940&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT 6.5940 Fall 2023 TinyML and Efficient Deep Learning Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.05662&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1911.05662&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.06873&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1706.06873&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10144741&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/10144741&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/how_to/optimize_operators/opt_gemm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tvm.apache.org/docs/how_to/optimize_operators/opt_gemm.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/6877334&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/6877334&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://spatial-lang.org/dotprod&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spatial-lang.org/dotprod&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://csapp.cs.cmu.edu/3e/waside/waside-blocking.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://csapp.cs.cmu.edu/3e/waside/waside-blocking.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://penny-xu.github.io/blog/tiled-matrix-multiplication&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://penny-xu.github.io/blog/tiled-matrix-multiplication&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.juliahub.com/LoopVectorization/4TogI/0.9.8/examples/matrix_multiplication/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.juliahub.com/LoopVectorization/4TogI/0.9.8/examples/matrix_multiplication/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8zbh4gWGa7I&amp;amp;t=424s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=8zbh4gWGa7I&amp;t=424s&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.12418&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2002.12418&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DruwS2_cVys&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=DruwS2_cVys&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://martin-thoma.com/strassen-algorithm-in-python-java-cpp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://martin-thoma.com/strassen-algorithm-in-python-java-cpp/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.diva-portal.org/smash/get/diva2:1219121/FULLTEXT01.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.diva-portal.org/smash/get/diva2:1219121/FULLTEXT01.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1509.09308&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1509.09308&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ceca.pku.edu.cn/docs/20191126112405101722.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ceca.pku.edu.cn/docs/20191126112405101722.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How does sparse convolution work? | by Zhiliang Zhou | Towards Data Science&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:18&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://rancheng.github.io/Sparse-Convolution-Explained/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Convolution explained with code – Ran Cheng – Robotics, Vision, Learning.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:18&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:19&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.08922.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1705.08922.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:19&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:20&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.4229&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1411.4229&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:20&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:21&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://pytorch.org/blog/quantization-in-practice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pytorch.org/blog/quantization-in-practice/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:21&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:22&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1308.3432&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1308.3432&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:23&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.06085&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1805.06085&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:23&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:24&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.05279&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1603.05279&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:24&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</guid>
      <description>&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h2&gt;
&lt;p&gt;The standard optimization problem will be in the form:
$$
\inf_{x \in X} f(x) \
$$
where $X$ is the &lt;strong&gt;feasible/constraint  region/set&lt;/strong&gt;, $f: \R^n \mapsto \R$ is the objective function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $x^&lt;em&gt;$ is an &lt;strong&gt;optimal solution&lt;/strong&gt; to the problem if $v^&lt;/em&gt; = f(x^&lt;em&gt;)$. In this case, we also say that $x^&lt;/em&gt;$ attains optimal value $v^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $x&amp;rsquo;$ is a &lt;strong&gt;local minimizer&lt;/strong&gt; if $\exists \epsilon &amp;gt; 0, \forall x \in B(x&amp;rsquo;, \epsilon), f(x) \ge f(x&amp;rsquo;)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;types-of-problems&#34;&gt;Types of Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Unconstrained: $X = \R^n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discrete programming&lt;/p&gt;
&lt;p&gt;$X$ is a discrete set, which means $\forall x \in X, \exists \epsilon &amp;gt; 0, X \cap B(x, \epsilon) = { x }$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Every feasible solution to discrete optimization problem is a local minimizer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linear programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$X = { x \in \R^n | (a^i)^T x \le c_i, i=1,2,\dots,m }$ is a set defined by a &lt;strong&gt;finite&lt;/strong&gt; number of linear inequalities.&lt;/p&gt;
&lt;p&gt;$f = b_1 x_1 + b_2 x_2 + \dots + b_n x_n = b^T x$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quadratic programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$X$ is the same as that in linear programming.&lt;/p&gt;
&lt;p&gt;$f(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j = x^T A x$, where $A = [a_{ij}] \in R^{n \times n}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: This form does not include any linear term. Generally, a quadratic function takes on the form $f(x) = x^T A x + b^T x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: We may assume $A$ is symmetric, since even it is not, we can have $A&amp;rsquo; = \frac{A + A^T}{2}$ and
$$
x^T A x = x^T A^T x \to x^T A x = x^T A&amp;rsquo; x
$$&lt;/p&gt;
&lt;p&gt;The right hand side is obvious because $x^T A^T x$ is a number, and it equals to its transpose $x^T A x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-definite programming&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Consider $Q \in \mathcal{S}^{n}$, where $\mathcal{S}^n$ is the set of $n \times n$ symmetric matrix. The following is equivalent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Q$ is &lt;strong&gt;positive semi-definite&lt;/strong&gt; (short as PSD, denoted as $Q \succcurlyeq 0$).&lt;/li&gt;
&lt;li&gt;$\forall x \in \R^n, x^T Q x \ge 0$.&lt;/li&gt;
&lt;li&gt;All eigenvalues of $Q$ are non-negative.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $C, A_1, \dots, A_m \in S^n$ and $b \in \R^m$ be given, the semi-definite programming is
$$
\begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; C - \underbrace{\sum_{i=1}^m x_i A_i}_{M(x)} \succcurlyeq 0
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The constraint is called a linear matrix inequality. Observe that $M: \R^m \mapsto S^n$ satisfies
$$
M(\alpha x + \beta y) = \alpha M(x) + \beta M(y)
$$
$M$ is a linear map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: Compare between linear programming and positive semi-definite programming:
$$
\begin{gathered}&lt;/p&gt;
&lt;p&gt;\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; (a^i)^T x \le c_i, \
&amp;amp; i = 1,\dots,m
\end{aligned}&lt;/p&gt;
&lt;p&gt;\quad \quad&lt;/p&gt;
&lt;p&gt;\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; C - \sum_{i=1}^m x_i A_i \succcurlyeq 0
\end{aligned}&lt;/p&gt;
&lt;p&gt;\end{gathered}
$$
In linear programming, construct matrices:
$$
C&amp;rsquo; =
\begin{pmatrix}
c_1 &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; c_2 &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp; \
&amp;amp; &amp;amp; &amp;amp; c_m
\end{pmatrix},&lt;/p&gt;
&lt;p&gt;A_i&amp;rsquo; =
\begin{pmatrix} 
a_1^i &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; a_2^i &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp; \
&amp;amp; &amp;amp; &amp;amp; a_m^i \
\end{pmatrix}
$$
Then
$$
C&amp;rsquo; - \sum_{i=1}^m x_i A_i&amp;rsquo; =
\begin{pmatrix}
c_1 - \sum_{j=1}^m x_j a_1^j &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; c_2 - \sum_{j=1}^m x_j a_2^j &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp;  \
&amp;amp; &amp;amp; &amp;amp; c_m - \sum_{j=1}^m x_j a_m^j &amp;amp;  \
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\succcurlyeq 0
$$
because a diagonal matrix is PSD if and only if all of its diagonal entries are non-negative.&lt;/p&gt;
&lt;p&gt;In this sense, linear programming is special case of semi-definite programming where $C$ and $A_i$&amp;rsquo;s are all diagonal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples-of-problems&#34;&gt;Examples of Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Air traffic control&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$n$ airplanes are arriving.&lt;/li&gt;
&lt;li&gt;$i$-th airplane arrives within $[a_i, b_i]$.&lt;/li&gt;
&lt;li&gt;Assume airplanes arrive and land in order.&lt;/li&gt;
&lt;li&gt;Let $t_i$ be the landing time assigned to $i$-th airplane $i$.&lt;/li&gt;
&lt;li&gt;The metering time is defined to be the time difference between two consecutive airplane landings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implicit constraints derived from above conditions is that $a_i \le t_i \le b_i$ and $ t_i \le t_{i+1}$. For safety, we want to the minimum metering time to be maximized. That is,
$$
\begin{aligned}
\max_{t} \quad &amp;amp; f(t) \triangleq \min_{1 \le i \le n-1} t_{i+1} - t_i \
\textrm{s.t.} \quad &amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
The constraints are all linear. The &lt;code&gt;min&lt;/code&gt; operation in $f$, however, doesn&amp;rsquo;t comfort us. We can introduce a new variable $z$ and convert the original problem to an equivalent one:
$$
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \
\textrm{s.t.} \quad &amp;amp; z = \min_{1 \le i \le n-1} t_{i+1} - t_i \
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
Further, since the objective is to maximize and $z$&amp;rsquo;s coefficient is positive, the problem can be converted to
$$
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \
\textrm{s.t.} \quad &amp;amp; z \le t_{i+1} - t_i, i=1,\dots,n-1\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
Now the problem becomes a linear one and is easy to solve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data fitting problem&lt;/p&gt;
&lt;p&gt;Given data points $(a_i, b_i) \in \R^n \times \R$, a typical choice to fit those data would be an affine function $f(x) = y^T x + t$. Other than the choice of function, the choice of objective function matters too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Least squares&lt;/p&gt;
&lt;p&gt;The objective minimizes the sum of the squares of errors for all data points:
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} (y^T a_i - b_i)^2 \
\end{aligned}
$$
This is a quadratic programming problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum absolute deviation
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} |y^T a_i - b_i| \
\end{aligned}
$$
Using the same trick of reparameterization, the problem is equivalent to
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \
\textrm{s.t.} \quad &amp;amp; z_i = |y^T a_i - b_i|, i=1,\dots,m \
\end{aligned}
$$
Further, the trick of relaxation applies:
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \
\textrm{s.t.} \quad &amp;amp; y^T a_i - b_i \le z_i, i=1,\dots,m \
&amp;amp; y^T a_i - b_i \ge -z_i, i=1,\dots,m \
\end{aligned}
$$
This becomes a linear programming problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key takeaway from these examples are the &lt;strong&gt;problem equivalence&lt;/strong&gt; trick, which includes introducing new variables and relaxing $\max$ or $\min$ to inequalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</guid>
      <description>&lt;h2 id=&#34;convex-set&#34;&gt;Convex Set&lt;/h2&gt;
&lt;p&gt;Given $x^1, \dots, x^k \in \R^n$, we say that $y = \sum_{i=1}^k \alpha_i x^{i}$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;linear combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\alpha_1, \dots, \alpha_k \in \R$;&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;affine combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\sum_{i=1}^k \alpha_i = 1$;&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;convex combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\sum_{i=1}^k \alpha_i = 1$ and $0 \le \alpha_1, \dots, \alpha_k$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let $S \in \R^n$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is a &lt;strong&gt;linear subspace&lt;/strong&gt; if $\forall x,y \in S, \alpha,\beta \in \R, \alpha x + \beta y \in S$;&lt;/li&gt;
&lt;li&gt;$S$ is an &lt;strong&gt;affine subspace&lt;/strong&gt; if $\forall x,y \in S, \alpha \in \R, \alpha x + (1-\alpha) y \in S$;&lt;/li&gt;
&lt;li&gt;$S$ is a &lt;strong&gt;convex set&lt;/strong&gt; if $\forall x,y \in S, 0 \le \alpha \le 1, \alpha x + (1 - \alpha) y \in S$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is affine.&lt;/li&gt;
&lt;li&gt;Any affine combination of a finite number of points in $S$ belongs to $S$.&lt;/li&gt;
&lt;li&gt;$S$ can be written as $S = {x} + V \triangleq { x + v: v \in V }$. Note that though $V$ is unique, $x$ is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is convex.&lt;/li&gt;
&lt;li&gt;Any convex combination of a finite number of points in $S$ belongs to $S$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;examples-of-convex-set&#34;&gt;Examples of Convex Set&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Nonnegative orthant (in 2-D, an orthant is called a quadrant): $\R^n_+ \triangleq { x \in \R^n: \forall i, x_i \ge 0 }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyperplane&lt;/p&gt;
&lt;p&gt;Given $w \in \R^n, b \in R$, the hyperplane $H(s, c)$ is the set ${ x \in \R^n: s^T x = c }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Half-space&lt;/p&gt;
&lt;p&gt;Given $w \in \R^n, b \in R$, the upper half-space $H^+(s, c)$ is the set ${ x \in \R^n: s^T x \ge c }$; the lower half-space $H^-(s, c)$ is the set ${ x \in \R^n | s^T x \le c }$.&lt;/p&gt;
&lt;p&gt;Note that hyperplane $H(s, c)$ is the intersection of $H^+(s, c)$ and $H^-(s, c)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Euclidean ball&lt;/p&gt;
&lt;p&gt;Given the center $\bar x \in \R^n$ and the radius $r &amp;gt; 0$, the Euclidean ball $B(\bar x, r)$ is the set ${ x \in \R^n: ||x - \bar x||_2 \le r }$.&lt;/p&gt;
&lt;p&gt;A generalization of Euclidean ball would be to extend the norm to other numbers that are larger than 1. For $q \ge 1$
$$
B_q(\bar x, r) = { x \in \R^n: ||x - \bar x||&lt;em&gt;q \le r }
$$
is also a convex set. Note that $||z||&lt;/em&gt;\infty = \lim_{q \to \infty} (\sum_i z_i^q)^{1/q} = \max_i z_i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convex cone&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A set $K \in \R^n$ is called a &lt;strong&gt;cone&lt;/strong&gt; if $\forall x \in K, \alpha &amp;gt; 0, \alpha x \in K$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Linear subspace is a cone. Affine subspace is not necessarily so.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;convex cone&lt;/strong&gt; is a cone that is convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some examples of convex cone include $\R^n_+$, and $\mathcal{S}_+^n$, which is the set of $n \times n$ PSD matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving Operations&lt;/h3&gt;
&lt;p&gt;For any two convex sets $S_1$ and $S_2$, there are some binary operators that will preserve the convexity after being applied.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set operations&lt;/p&gt;
&lt;p&gt;$S_1 \cup S_2$ is not necessarily convex. $S_1 \cap S_2$ is always convex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Affine transformations&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $A: \R^n \mapsto \R^m$ is &lt;strong&gt;affine&lt;/strong&gt; if $\forall x,y \in \R^n, \alpha \in \R$,
$$
A(\alpha x + (1 - \alpha) y) = \alpha A(x) + (1 - \alpha) A(y)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $A: \R^n \mapsto \R^m$ be affine, $S \subseteq \R^n$ be convex. Then $A(S) \triangleq { A(x): x \in S }$ is convex.&lt;/p&gt;
&lt;p&gt;There are two types of affine transformation worth noting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt;: $A(x) = U x$ where $U \in \R^{n \times n}$ and $U U^T = U^T U= I$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Projection&lt;/strong&gt;: $A(x) = P x$ where $P \in \R^{n \times n}$ and $P^2 = P$.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;orthogonal projection&lt;/strong&gt;, its projection matrix further satisfies $P^T = P$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an example of affine transformation, given center $\bar x$ and axes $Q$ which is positive definite, the &lt;strong&gt;ellipsoid&lt;/strong&gt; $E(\bar x, Q)$ is the set ${ x \in \R^n | (x - \bar x)^T Q (x - \bar x) \le 1 }$. Note that $B(\bar x, r) = E(\bar x, I/r^2)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: When $Q$ is positive semi-definite, it might occur that ${ x \in \R^n | (x - \bar x)^T Q (x - \bar x) \le 1 }$ will degenerate into two parallel lines. Just consider the case when $Q = [1,2] [1,2]^T$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: There always exists an affine transformation $A: \R^n \mapsto \R^n$ such that $A(B(0, 1)) = E(\bar x, Q)$. Note that $B(\bar x, r) = E(\bar x, I/r^2)$. See &lt;a href=&#34;../2-cvxanal.pdf&#34;&gt;this handout&lt;/a&gt; for the construction of such transformation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, ellipsoid is also a convex set (Problem 2 of Homework 1 proves this from the first principle).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;topological-preparation&#34;&gt;Topological Preparation&lt;/h3&gt;
&lt;h4 id=&#34;basic-topology&#34;&gt;Basic Topology&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Given a set $S \in \R^n, S \ne \emptyset$ and a point $x \notin S$, we want to find a point in $S$ that is closest (in terms of Euclidean distance) to $x$. Formally, $\hat x = \arg\min_{z \in S} ||z - x||_2$ is called the &lt;strong&gt;projection&lt;/strong&gt; of $x$ onto $S$, denoted as $\hat x = \Pi_S(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that this projection does not necessarily exist. Neither the projection is unique. Under what conditions can we guarantee the existence and uniqueness of projection? Before that, some concepts are needed. Let $S \subseteq \R^n$ be a set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: $x$ is an &lt;strong&gt;interior point&lt;/strong&gt; of $S$ if $\exists \epsilon &amp;gt; 0, B(x, \epsilon) \subseteq S$. The collection of all interior points of $S$ is called the &lt;strong&gt;interior&lt;/strong&gt; of $S$, denoted as $\mathop{\mathrm{int}} S$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We say that $S$ is &lt;strong&gt;open&lt;/strong&gt; if $S = \mathop{\mathrm{int}} S$. We say that $S$ is &lt;strong&gt;closed&lt;/strong&gt; if $\R^n \setminus S$ is open. Note that it can be the case that a set is neither open nor closed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The intersection of any family of closed sets is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be continuous and $c \in \R$ be a constant number. Then $S = { x \in \R^n: f(x) \le c }$ is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: $S$ is closed if and only if for every convergent sequence ${ x_n }$ in $S$, its limit is in $S$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $S \subseteq \R^n$ is &lt;strong&gt;compact&lt;/strong&gt; if it is closed and bounded ($\exists M &amp;gt; 0$ such that $S \subseteq B(0, M)$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;projection&#34;&gt;Projection&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Weierstrass theorem&lt;/strong&gt;: Let $f: \R^n \mapsto \R$ be continuous and $S \subseteq \R^n$ be compact. Then $\inf_{x \in S} f(x)$ always has a solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $S \subseteq \R^n$ be non-empty, closed, and convex. Then for every $x \in \R^n$, there exists a unique $\hat x$ such that $\hat x = \Pi_S(x)$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Existence&lt;/p&gt;
&lt;p&gt;We may assume that $x \notin S$. Consider any $x&amp;rsquo; \in S$ and define $T \triangleq S \cap B(x, ||x - x&amp;rsquo;||_2)$. Observe that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\arg \min_{z \in S} ||x - z||&lt;em&gt;2 = \arg \min&lt;/em&gt;{z \in T} ||x - z||_2$.&lt;/li&gt;
&lt;li&gt;$T$ is closed and bounded.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then by &lt;em&gt;Weierstrass&amp;rsquo; theorem&lt;/em&gt;, $\hat x$ that solves $\min_{z \in T} ||x - z||_2$ exists. And by Point 1 above, $\hat x = \Pi_S(x)$. Note that above argument does not need convexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uniqueness&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that $\hat x_1 = \Pi_S(x), \hat x_2 = \Pi_S(x)$ and $\hat x_1 \ne \hat x_2$. Let $z = (\hat x_1 + \hat x_2) / 2$. By convexity, $z \in S$. Then $x$&amp;rsquo;s distance to $z$ can be calculated as
$$
\begin{aligned}
|| x - (\hat x_1 + \hat x_2)/2 ||_2 &amp;amp;= || (x - \hat x_1)/2 + (x - \hat x_2)/2 ||_2 \
&amp;amp;\le || (x - \hat x_1)/2 ||_2 + || (x - \hat x_2)/2 ||&lt;em&gt;2 \
&amp;amp;= \min&lt;/em&gt;{z \in S} ||x - z||_2
\end{aligned}
$$
If the $\le$ is strict, the fact that $\hat x_1$ and $\hat x_2$ are the projection will be contradicted; else $(x - \hat x_1)$ and $(x - \hat x_2)$ are collinear, which means $x, \hat x_1, \hat x_2$ are collinear, in which case the only possible way for $(x - \hat x_1)$ and $(x - \hat x_2)$ to be equal is $\hat x_1 = \hat x_2$, contradicting the assumption $\hat x_1 \ne \hat x_2$.&lt;/p&gt;
&lt;p&gt;As a result, $\hat x_1 = \hat x_2$, which means the projection of $x$ is unique.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $S \subseteq \R^n$ be non-empty, closed, and convex. Then for any $x \in \R^n$, we have
$$
\hat x = \Pi_{S}(x) \iff \forall z \in S, (z - \hat x)^T(x - \hat x) \le 0
$$
Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For any $z \in S$ and $0 \le \alpha \le 1$, define $z(\alpha) = \alpha z + (1 - \alpha) \hat x$. By convexity, $z(\alpha) \in S$. Then,
$$
||\hat x - x||_2^2 \le ||z(\alpha) - x||_2^2
$$
Note on the other hand that
$$
\begin{aligned}
&amp;amp;\quad ||z(\alpha) - x||_2^2 = [z(\alpha) - x]^T [z(\alpha) - x] \
&amp;amp;= [\hat x + \alpha (z - \hat x) - x]^T [\hat x + \alpha (z - \hat x) - x] \
&amp;amp;= [(\hat x - x) + \alpha (z - \hat x)]^T [(\hat x - x) + \alpha (z - \hat x)] \
&amp;amp;= ||\hat x - x||_2^2 + 2\alpha (z - \hat x)^T (\hat x - x) + \alpha^2 ||z - \hat x||_2^2
\end{aligned}
$$
To make the above hold for any $\alpha \in [0,1]$, it must be the case that $(z - \hat x)^T (\hat x - x) \ge 0$ or equivalently
$$
(z - \hat x)^T (x - \hat x) \le 0
$$
Convexity is crucial in this property. Just consider the following case:&lt;/p&gt;
&lt;img src=&#34;./convexity-is-important.png&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that there exists some $x&amp;rsquo; \ne \Pi_S(x)$ such that for every $z \in S$,
$$
(z - x&amp;rsquo;)^T (x - x&amp;rsquo;) \le 0
$$
Then for $\Pi_S(x) \in S$, we have
$$
\begin{equation}
(\Pi_S(x) - x&amp;rsquo;)^T (x - x&amp;rsquo;) \le 0 \label{eq1}
\end{equation}
$$
From the proof of sufficiency we know that for this specific $x&amp;rsquo;$,
$$
\begin{equation}
(x&amp;rsquo; - \Pi_S(x))^T (x - \Pi_S(x)) \le 0 \label{eq2}
\end{equation}
$$
Add up together $\eqref{eq1}$ and $\eqref{eq2}$ to give
$$
(\Pi_S(x) - x&amp;rsquo;)^T (\Pi_S(x) - x&amp;rsquo;) \le 0
$$
This indicates that $\Pi_S(x) = x&amp;rsquo;$, which concludes the proof.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;separation&#34;&gt;Separation&lt;/h4&gt;
&lt;p&gt;Given $S_1, S_2 \subseteq \R^n$, it is easy to certify that $S_1 \cap S_2 \ne \emptyset$ so long as there is a $x \in S_1$ such that $x \in S_2$. But how can one certify that $S_1 \cap S_2 = \emptyset$?&lt;/p&gt;
&lt;p&gt;One geometric intuition is to find a hyperplane $H(s,c)$ such that $S_1 \subseteq H^+(s,c) \setminus H(s,c)$ and $S_2 \subseteq H^-(s,c) \setminus H(s,c)$. Obviously a hyperplane separation won&amp;rsquo;t always work. But it helps in the discussion of convex set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;point-set separation&lt;/strong&gt;. Let $S \subseteq \R^n$ be non-empty, closed and convex. Let $x \notin S$. Then there exists $y \in \R^n$ such that
$$
\left( \max_{z \in S} y^T z \right) &amp;lt; y^T x
$$
Proof:&lt;/p&gt;
&lt;p&gt;By the projection theorem, $\hat x \triangleq \Pi_S(x)$ exists and is unique. Take $y$ as $(x - \hat x)$. $y \ne \vec 0$ because $x \notin S$. Then according the projection&amp;rsquo;s property, for every $z \in S$,
$$
\begin{gather}
\begin{aligned}
(z - \hat x)^T \underbrace{(x - \hat x)}_{y} &amp;amp;\le 0 \
z^T y - \hat x^T y &amp;amp;\le 0 \
\end{aligned} \quad \Rightarrow \quad
\begin{aligned}
y^T z &amp;amp;\le y^T (x  - y) \
&amp;amp;\le y^T x - ||y||&lt;em&gt;2^2 \
&amp;amp;&amp;lt; y^T x
\end{aligned}
\end{gather}
$$
$\max&lt;/em&gt;{z \in S} y^T z$ is obtained at $z = \hat x$, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The theorem above is an algebraic description of point-set separation and in essence reveals a hyperplane separation. $y$ is exactly the normal vector of this hyperplane. $(\max_{z \in S} y^T z) &amp;lt; y^T x$ actually says that $x$ is more distant in the direction of $y$ than every point $z$ in $S$.&lt;/p&gt;
&lt;p&gt;A direct result of the theorem above is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A closed convex set $S \subseteq \R^n$ is the intersection of all half-spaces containing $S$, i.e.
$$
S = \bigcap_{\substack{\text{$S \subseteq H$ and} \ \text{$H$ is a halfspace}}} H
$$
Proof:&lt;/p&gt;
&lt;p&gt;Without loss of generality, due to that $H^+(s,c) = H^-(-s,-c)$, we claim that $S$ is the intersection of all the lower half-spaces containing $S$:
$$
S = \bigcap_{H^-(s, c) \supseteq S} H^-(s,c)
$$
We begin with two special cases: $\emptyset$ and $\R^n$. We show that $\emptyset$ and $\R^n$ satisfies the above because
$$
\emptyset = H^-(0, -1) \cap \text{any lower halfspace} \
\R^n = \bigcap_{c \ge 0} H^-(0, c)
$$
Now consider any set $\emptyset \subsetneq S \subsetneq \R^n$. Let $x \notin S$. Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, there exists $y_x \in \R^n$ such that
$$
c_x \triangleq \max_{z \in S} y_x^T z &amp;lt; y_x^T x
$$
Therefore, $S \subseteq H^-_x \triangleq H^-(y_x, c_x)$. Note that $x \notin H^-_x$. $S \subseteq H^-&lt;em&gt;x$ holds for any $x \notin S$. Therefore,
$$
S \subseteq \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x
$$
On the other hand, $\bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x \subseteq S$. Suppose on the contrary there exists $z \in \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x$ but $z \notin S$. However,
$$
z \in \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x = \bigcap&lt;/em&gt;{{ x \notin S: x \ne z } \cup { z } } H^-_x \subseteq H^-_z
$$
which contradicts the fact that $z \notin H^-&lt;em&gt;z$. Therefore, $\bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x \subseteq S$ and consequently
$$
S = \bigcap&lt;/em&gt;{x \notin S} H^-_x
$$
Notice that ${ H^-&lt;em&gt;x: x \notin S }$ contains all the lower half-spaces that superset $S$. Because for every lower half-space $H^-(s, c) \supseteq S$, there exists $x \in H^+(s, c) \setminus H(s, c)$ such that $x \notin S$. For this specific $x$, there exists $s$ such that
$$
\max&lt;/em&gt;{z \in S} s^T z \le c &amp;lt; s^T x
$$
Therefore, every $H^-(s, c) \supseteq S$ can be written as $H^-_x$ for some $x \notin S$, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By far, the separation between a point (or a set of a single point) and a set is settled. Now we consider the set-set separation. It is easy to conjecture the geometric intuition derived above as the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conjecture: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let $S_1, S_2 \subseteq \R^n$ be two non-empty, closed and convex sets with $S_1 \cap S_2 = \emptyset$. Then there exists $y \in \R^n$ such that
$$
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
$$
Disproof:&lt;/p&gt;
&lt;p&gt;Just consider $S_1 = { (u, v): u \ge 1/v, v \ge 1 }$ and $S_2 = { (u, 0): u \ge 1 }$. The only possible separation hyperplane is the $x$-axis, with the corresponding normal vector $y = [0, -1]^T$. However in this case, $\max_{z \in S_1} y^T z$ does not exist at all and thus $\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z$ does not hold.&lt;/p&gt;
&lt;p&gt;Trivially changing $\max$ to $\sup$ won&amp;rsquo;t help, since $\sup_{z \in S_1} [0, -1] z = 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lack of boundedness forbids us to take the advantage of &lt;em&gt;Weierstrass&amp;rsquo; theorem&lt;/em&gt; to show the existence of maxima. It would be cheering if the predicate of the conjecture can be relaxed so that boundedness and thus compactness applies, yielding the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let $S_1, S_2 \subseteq \R^n$ be two non-empty, closed and convex sets, with $S_1 \cap S_2 = \emptyset$ and either $S_1$ or $S_2$ being bounded. Then there exists $y \in \R^n$ such that
$$
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
$$
Hint of proof:&lt;/p&gt;
&lt;p&gt;We only show the proof when $S_2$ is bounded. The case when $S_1$ is bounded can be handled similarly.&lt;/p&gt;
&lt;p&gt;Consider the set $S \triangleq { x - y: x \in S_1, y \in S_2 }$ (which will be ${ x - y: x \in S_2, y \in S_1 }$ in the other case). Since $S_1 \cap S_2 = \emptyset$, $0 \notin S$. $S$ is non-empty obviously. $S$ can be further verified to be closed and convex (??). Then the property of point-set separation can be applied to proceed with the proof.&lt;/p&gt;
&lt;p&gt;That is, there exists $u \in \R^n$ such that
$$
\left( \max_{z \in S} u^T z \right) &amp;lt; u^T \cdot 0 = 0 \
\Downarrow \
\max_{x \in S_1, y \in S_2} u^T (x - y) &amp;lt; 0
$$&lt;/p&gt;
&lt;p&gt;Set $y$ as $y^* = \min_{y \in S_2} u^T y$ (such $y^&lt;em&gt;$ exists because the compactness of $S_2$) to give
$$
\begin{aligned}
\max_{x \in S_1} u^T (x - y^&lt;/em&gt;) &amp;amp;&amp;lt; 0 \
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; u^T y^* \
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; \min_{y \in S_2} u^T y \
\end{aligned}
$$
which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a richer discussion of convex set separation, please refer &lt;a href=&#34;https://www.wikiwand.com/en/Hyperplane_separation_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</guid>
      <description>&lt;h2 id=&#34;convex-function&#34;&gt;Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $f: \R^n \mapsto \R_+$, where $\R_+ = \R \cup { +\infty }$ (in convex discussion, usually only $+\infty$ is included) be an extended real-valued function. Note that $f$ shouldn&amp;rsquo;t trivially be $+\infty$ everywhere. We say that $f$ is &lt;strong&gt;convex&lt;/strong&gt; if $\forall x_1, x_2 \in \R^n, \alpha \in [0,1]$,
$$
f(\alpha x_1 + (1-\alpha) x_2) \le \alpha f(x_1) + (1-\alpha) f(x_2)
$$
Note that for $x \in \R$, $x &amp;lt; +\infty, x + \infty = \infty + x = \infty, +\infty \le +\infty$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interestingly, convex functions on &lt;u&gt;an open domain&lt;/u&gt; are always continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The &lt;strong&gt;epigraph&lt;/strong&gt; of a function $f: \R^n \mapsto \R$ is the set $\epi f \triangleq { (x, t) \in \R^n \times \R: f(x) \le t }$.&lt;/p&gt;
&lt;p&gt;Definition: The &lt;strong&gt;effective domain&lt;/strong&gt; of $f$ is the set $\dom f \triangleq { x \in \R^n: f(x) &amp;lt; \infty }$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that the real line does not contain $\infty$. Therefore, the effective domain of $f(x) = x$ is still $\R$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $S \subseteq \R^n$ be a set. The &lt;strong&gt;indicator&lt;/strong&gt; of $S$ is the function
$$
\mathbb 1_S (x) = 
\begin{cases}
0, &amp;amp; x \in S \
+\infty, &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the indicator, we have
$$
\underset{\text{constrained}}{\inf_{x \in S} f(x)} \iff \underset{\text{unconstrained}}{\inf_{x \in \R^n} f(x) + \mathbb 1_S(x)}
$$
That is, we convert a constrained problem to a unconstrained one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition (verify it): Let $f: \R^n \mapsto \R_+$. Then, $f$ is convex (as a function) if and only if $\epi f$ is convex (as a set). Moreover, let $S \subseteq \R^n$ be a set. Then $S$ is convex (as a set) if and only if $\mathbb 1_S$ is convex (as a function).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proposition above associates the convexity of a function with that of its epigraph; and the convexity of a set with that of its indicator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Jensen&amp;rsquo;s inequality&lt;/strong&gt;. Let $f: \R^n \mapsto \R_+$. Then $f$ is convex if and only if $f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)$ for any $m \in \N^+$ and any $x_1, \dots, x_m \in \R^n$ and any $\alpha_1, \dots, \alpha_m \ge 0$ such that $\sum_{i=1}^m \alpha_i = 1$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Sufficiency is easy to show by interpreting the definition of convex function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For $i=1,\dots,m$, we have
$$
\big( x_i, f(x_i) \big) \in \epi f
$$
Since $\epi f$ is convex,
$$
\sum_{i=1}^m \alpha_i \big( x_i, f(x_i) \big) =  \big( \sum_{i=1}^m \alpha_i x_i, \sum_{i=1}^m \alpha_i f(x_i) \big) \in \epi f
$$
which means
$$
f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-negative combination&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $f_1, \dots, f_m$ be convex functions, $\alpha_1, \dots, \alpha_m \ge 0$ be non-negative scalars. Then,
$$
f \triangleq \sum_{i=1}^m \alpha_i f_i \text{ is convex.}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pointwise supremum&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $I$ be an index set (either finite or infinite) and ${ f_i: i \in I }$ be a collection of convex functions. Then
$$
f \triangleq \sup_{i \in I} f_i \text{ is convex.}
$$
To show it, let $x_1, x_2 \in \R^n$ and $\alpha \in [0, 1]$. Then,
$$
\begin{aligned}
&amp;amp;f(\alpha x_1 + (1-\alpha) x_2) = \sup_{i \in I} f_i(\alpha x_1 + (1-\alpha) x_2) \
&amp;amp;\le \sup_{i \in I} \big( \alpha f_i(x_1) + (1-\alpha) f_i(x_2) \big) \
&amp;amp;\le [\alpha \sup_{i \in I} f_i(x_1)] + [(1-\alpha) \sup_{i \in I} f_i(x_2)] \
&amp;amp;= \alpha f(x_1) + (1-\alpha) f(x_2)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Geometrically, pointwise supremum is intersecting the epigraphs of $f_i$&amp;rsquo;s.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Consider the mapping $f: \R^{m \times n} \supseteq X \to ||X||$ where $||X||$ is the largest singular value of $X$. Show that $f$ is convex.&lt;/p&gt;
&lt;p&gt;By the &lt;em&gt;Courant-Fischer theorem&lt;/em&gt;,
$$
\begin{aligned}
||X|| &amp;amp;= \max_{u \in \R^m, v \in \R^n} u^T X v \
\text{s.t.} &amp;amp;\quad ||u||&lt;em&gt;2 = 1, ||v||&lt;em&gt;2 = 1
\end{aligned}
$$
Let $f&lt;/em&gt;{u,v}(X) = u^T X v$ and $I = { (u,v) \in \R^m \times \R^n: ||u||&lt;em&gt;2 = 1, ||v||&lt;em&gt;2 = 1 }$. Then,
$$
f(X) = \max&lt;/em&gt;{(u,v) \in I} f&lt;/em&gt;{u,v}(X)
$$
Note that $f&lt;/em&gt;{u,v}(X)$ is linear and thus convex in $X$. It follows directly from the pointwise supremum principle that $f$ is convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Composition with increasing function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $g: \R^n \mapsto \R$ be convex, $h: \R \mapsto \R$ be convex. $h \circ g$ is not generally convex. To verify it, take
$$
h(x) = -x, g(x) = x^2
$$
But if $h$ is convex as well as increasing, then $h \circ g$ is convex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Restriction on lines&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a point $x_0 \in \R^n$ and a direction $h \in \R^n \setminus { 0 }$, we call the set
$$
{ x_0 + t h: t \in \R }
$$
a line through $x_0$ in the direction $h$. Let $f: \R^n \mapsto \R$ be a function. Define&lt;/p&gt;
&lt;p&gt;$$
\tilde f_{x_0, h}(t) \triangleq f(x_0 + t h)
$$
as the &lt;strong&gt;restriction of $f$ on the line&lt;/strong&gt; ${ x_0 + t h: t \in \R }$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, $f$ is convex if and only if $\tilde f_{x_0, h}$ is convex for any $x_0 \in \R^n$ and any $h \in \R^n \setminus { 0 }$.&lt;/p&gt;
&lt;h2 id=&#34;differentiable-convex-function&#34;&gt;Differentiable Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f: \R^n \mapsto \R$ be differentiable, i.e. $\nabla f$ exists. Then, $f$ is convex if and only if for every $x, y \in \R^n$,
$$
f(x) \ge f(y) + (\nabla f(y))^T (x - y) \tag{gradient inequality}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a fixed $y$, the right-hand side of the gradient inequality is in essence an affine function of $x$. The graph of this affine function is
$$
t(x) = f(y) + (\nabla f(y))^T (x - y) \
\Updownarrow \
\underbrace{[(\nabla f(y))^T, -1]}&lt;em&gt;{s^T} \underbrace{\begin{bmatrix}
x \
t
\end{bmatrix}}&lt;/em&gt;{z} - \underbrace{[(\nabla f(y))^T y - f(y)]}_{c} = 0
$$&lt;/p&gt;
&lt;p&gt;Interestingly, the normal vector $s$ always points downwards (due to the $-1$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f$ be twice continuously differentiable, i.e. $\nabla f, \nabla^2 f$ exist and $\nabla^2 f$ is continuous. Then, $f$ is convex if and only if
$$
\nabla^2 f(x) \succcurlyeq 0, \forall x \in \R^n
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $f: \mathcal{S}&lt;em&gt;{++}^n \mapsto \R$ be given by $f(X) = -\ln \det (X)$ where $\mathcal{S}&lt;/em&gt;{++}^n$ is the set of $n \times n$ positive definite matrix. Show that $f$ is convex.&lt;/p&gt;
&lt;p&gt;This problem usually emerges when calculating the capacity of channel in information theory. To show it, we construct $f$&amp;rsquo;s restriction on all the lines. Let $X_0 \in \mathcal{S}&lt;em&gt;{++}^n$ and $H \in \mathcal{S}^n \setminus { 0 }$. Define
$$
g(t) \triangleq \tilde f(X_0 + t H) = -\ln \det(X_0 + t H)
$$
Since $X_0 \in \mathcal{S}&lt;/em&gt;{++}^n$, we can write $X_0 = X_0^{1/2} X_0^{1/2}$ where $X_0^{1/2} \in \mathcal{S}_{++}^n$. Let $X_0^{-1/2} = (X_0^{1/2})^{-1}$. Then,
$$
\begin{align*}
g(t) &amp;amp;= -\ln \det(X_0^{1/2} (I + t X_0^{-1/2} H X_0^{-1/2}) X_0^{1/2}) \
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2})
\end{align*}
$$
Note that $X_0^{-1/2} H X_0^{-1/2}$ is real symmetric. Let $\lambda_1, \dots, \lambda_n$ be its eigenvalues. Then $(I + t X_0^{-1/2} H X_0^{-1/2})$&amp;rsquo;s eigenvalues are $t \lambda_1 + 1, \dots, t \lambda_n + 1$.
$$
\begin{align*}
g(t) &amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2}) \
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \sum_i \ln(t\lambda_i + 1)
\end{align*}
$$
Now it is easy to verify that $-\ln(t\lambda_i + 1)$&amp;rsquo;s are convex, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;differentiability-agnostic-convex-function&#34;&gt;Differentiability-agnostic Convex Function&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;Differentiability-agnostic&amp;rdquo; means that no premise on the differentiability of the convex function is assumed. In this section, we discuss several &amp;ldquo;differentiability-agnostic&amp;rdquo; topics.&lt;/p&gt;
&lt;h3 id=&#34;subgradient&#34;&gt;Subgradient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $f: \R^n \mapsto \R$ be a function. A vector $s \in \R^n$ is called a &lt;strong&gt;subgradient&lt;/strong&gt; of $f$ at $x_0$ if
$$
f(x) \ge f(x_0) + s^T (x - x_0), \forall x \in \R^n
$$
The set
$$
\partial f(x_0) \triangleq { s \in \R^n: \text{$s$ is a subgradient of $f$ at $x_0$} }
$$
is called the &lt;strong&gt;sub-differential&lt;/strong&gt; of $f$ at $x_0$. Sub-differential is always convex and closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of subgradient is quite inspired from the gradient inequality. The geometric interpretation of subgradient resembles that of gradient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f: \R^n \mapsto \R$ be convex. Then, $\partial f(x)$ is non-empty, convex and compact (non-emptiness is due to the continuity (which in turn is due to the openness of $\R^n$) and boundedness is due to convexity). Moreover,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is differentiable at $x_0$ if and only if $\partial f(x_0) = { \nabla f(x_0) }$;&lt;/li&gt;
&lt;li&gt;$x_0$ is the global minima if and only if $0 \in \partial f(x_0)$;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The topological properties that the sub-differential possesses imply that we can do projection on the sub-differential; besides we may apply Weierstrass theorem to solve some optimization problem w.r.t. the sub-differential (such as what is the minimal-norm subgradient).&lt;/p&gt;
&lt;h3 id=&#34;conjugate-function&#34;&gt;Conjugate Function&lt;/h3&gt;
&lt;p&gt;If two functions have the identical epigraphs, then these two functions are identical. That&amp;rsquo;s the geometric view of a function.&lt;/p&gt;
&lt;p&gt;Let $f: \R^n \mapsto \R$ be convex. $\epi f$ is known to be convex. Suppose in addition that $\epi f$ is non-empty and closed. Recall that a non-empty, closed and convex set can be written as the intersection of lower halfspaces that contain it:
$$
\epi f = \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T, y_0]^T, c)
$$&lt;/p&gt;
&lt;p&gt;where $s \in \R^n$ and $y_0 \in \R$. We argue that $y_0 \le 0$ because the last entry of some $z \in \epi f$ can be arbitrarily large. We further argue that $y_0 &amp;lt; 0$ or else $f(x) = -\infty$ everywhere. Without loss of generality let $y_0 = -1$, since $H^-([s^T, y_0]^T,c) = H^-([-s^T/y_0, -1]^T,c)$.&lt;/p&gt;
&lt;p&gt;Notice that this lower half-space $H^-([s^T, -1]^T, c)$ is exactly the epigraph of the affine function
$$
h_{s, c}(x) = s^T x - c
$$
because for any $(x, y) \in \R^n \times \R$ that belongs to $H^-([s^T, -1]^T, c)$, we have
$$
\begin{aligned}
s^T x - y &amp;amp;\le c \
y &amp;amp;\ge s^T x - c
\end{aligned}
$$
On the other hand, $\epi f \subseteq H^-([s^T, -1]^T, c) = \epi{h_{s, c}}$ indicates that $\forall x \in \R^n, f(x) \ge h_{s, c}(x)$. Consequently, $\epi f$ can be re-written as
$$
\begin{aligned}
\epi f &amp;amp;= \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T, -1]^T,c) \
&amp;amp;= \bigcap_{\epi{h_{s, c}} \supseteq \epi f} \epi{h_{s, c}} \
&amp;amp;= \bigcap_{f \ge h_{s, c}} \epi{h_{s, c}}
\end{aligned}
$$
Therefore, $f$ can be written as the pointwise supremum of the affine functions who are less than $f$:
$$
f = \sup_{h \le f} h
$$
The normal vectors of those affine functions that are less than $f$ and that pass through $(x_0, f(x_0))$ essentially forms the sub-differential of $f$ at $x_0$.&lt;/p&gt;
&lt;p&gt;Consider the set
$$
\begin{aligned}
S_f &amp;amp;= { (y, t) \in \R^n \times \R: y^T x - t \le f(x), \forall x \in \R^n } \
&amp;amp;= { (y, t) \in \R^n \times \R: \sup_{x}(y^T x - f(x)) \le t } \
&amp;amp;\triangleq \epi{f^&lt;em&gt;}
\end{aligned}
$$
where
$$
f^&lt;/em&gt;(y) \triangleq \sup_{x}(y^T x - f(x))
$$
$f^*(y)$ is called the &lt;strong&gt;conjugate function&lt;/strong&gt; of $f$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</guid>
      <description>&lt;h2 id=&#34;linear-programming&#34;&gt;Linear Programming&lt;/h2&gt;
&lt;p&gt;Recall that the linear programming problem is
$$
\label{lp} \begin{aligned}
\min_{x} \quad&amp;amp; c^T x \
\text{s.t.} \quad&amp;amp; a_i^T x \le b_i, i=1,\dots,m
\end{aligned} \tag{LP}
$$
where $c \in \R^n$ and $a_i \in \R^{n}, b_i \in \R, i=1,\dots,m$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;polyhedron&lt;/strong&gt; is the intersection of a finite set of halfspaces. A bounded polyhedron is called a &lt;strong&gt;polytope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the feasible set of LP is a polyhedron. Interestingly, all polyhedrons are convex. That is, the feasible set of LP is convex.&lt;/p&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;The standard form of LP is
$$
\label{standard-lp} \begin{aligned}
\min_{x \in \R_+^n} \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; A x = b \
\end{aligned} \tag{Standard LP}
$$
where $c \in \R^n, A \in \R^{m \times n}, b \in \R^m$. Standard LP has standard solutions. Thus, the next question is how to convert an ordinary LP problem to a standard one.&lt;/p&gt;
&lt;p&gt;Now write $x = x^+ - x^-$, where $x^+, x^- \in \R_+^n$. Introduce another slack variable $s \in \R_+^m$. $\eqref{lp}$ is equivalent to
$$
\begin{aligned}
\min_{x^+, x^- \ge 0, s \ge 0} \quad &amp;amp; c^T (x^+ - x^-) \
\text{s.t.} \quad &amp;amp; A (x^+ - x^-) + s = b \
\end{aligned}
$$
Let
$$
A&amp;rsquo; = \left[\begin{array}{c:c}
A &amp;amp; -A &amp;amp; I
\end{array}\right] \in \R^{m \times (2n+m)} \
x&amp;rsquo; = \begin{bmatrix}
x^+ \
\hdashline
x^- \
\hdashline
s
\end{bmatrix} \in \R_+^{2n+m},
c&amp;rsquo; = \begin{bmatrix}
c \
\hdashline
-c \
\end{bmatrix} \in \R_+^{2n}
$$
The problem becomes
$$
\begin{aligned}
\min_{x&amp;rsquo; \in \R_+^{2n+m}} \quad &amp;amp; (c&amp;rsquo;)^T x&amp;rsquo; \
\text{s.t.} \quad &amp;amp; A&amp;rsquo; x&amp;rsquo; = b \
\end{aligned}
$$
which observes the formality of standard LP. Here lays down again the standard LP problem:&lt;/p&gt;
&lt;p&gt;$$
\label{primal} \begin{aligned}
v_p^* = \min_{x \in \R_+^n} \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; A x = b \
\end{aligned} \tag{P}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The first question&lt;/strong&gt; to ask is whether there is a optimal solution to it. The answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If $\eqref{primal}$ is feasible, then either 1) the optimal value $v^* = -\infty$ (no optimal solution),or 2) it has an optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas&amp;rsquo; Lemma&lt;/h3&gt;
&lt;p&gt;But &lt;strong&gt;the second question&lt;/strong&gt; arises: how to certify that $\eqref{primal}$ is infeasible? We meet the similar dilemma to that when dealing with set-set separation: it is easy to test the feasibility of any $x$; but it is prohibitive to test the feasibility of all $x$&amp;rsquo;s just to show that the original problem is infeasible.&lt;/p&gt;
&lt;p&gt;The idea is that, given the feasible polyhedron $P \triangleq { x \in \R_+^n: Ax = b }$ of the original problem, we construct another auxiliary polyhedron $Q$ such that exactly one of the following holds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P \ne \emptyset, Q = \emptyset$;&lt;/li&gt;
&lt;li&gt;$P = \emptyset, Q \ne \emptyset$.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Farkas&amp;rsquo; lemma&lt;/strong&gt;. Let $A \in \R^{m \times n}$ and $b \in \R^m$ be given. Then exactly one of the following systems is solvable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A x = b, x \ge 0$;&lt;/li&gt;
&lt;li&gt;$A^T y \le 0, b^T y &amp;gt; 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;$A x = b, x \ge 0$ means that $b$ is a non-negative linear combinations of $a_1, \dots, a_n$. $A^T y \le 0$ means that $y$ forms an obtuse angle to $a_1, \dots, a_n$; $b^T y &amp;gt; 0$ means $y$ forms an acute angle with $b$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that the above two statements cannot be solvable at the same time. or else this gives rise to the contradiction for some $x_0 \ge 0, y_0 \in \R^m$:
$$
\underbrace{x_0^T}&lt;em&gt;{\ge 0} \underbrace{A^T y_0}&lt;/em&gt;{\le 0} = y_0^T A x = \underbrace{y_0^T b}_{&amp;gt; 0}
$$
We then claim that the above two statements cannot be unsolvable at the same time. Suppose $1$ is unsolvable.&lt;/p&gt;
&lt;p&gt;In this case, $b \notin A^+ \triangleq { Ax: x \ge 0 }$. It can be verified that $A^+$ is non-empty ($0 \in A^+$), closed (closeness is not generally preserved under affine transformation; but in this case it is &amp;lt;refer to &lt;a href=&#34;../3-lp.pdf&#34;&gt;this handout&lt;/a&gt;&amp;gt;) and convex (because convexity is preserved under affine transformation $A^+ = A \R_+^n$). Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, for any $x \ge 0$, there exists a $y_0 \in \R^m$ such that
$$
\max_{x \ge 0} y_0^T A x &amp;lt; y_0^T b
$$
Take $x = 0$, we have
$$
y_0^T b &amp;gt; 0
$$
We claim that $A^T y_0 \le 0$. Suppose on the contrary that there exists an $i$ such that $(A^T y_0)&lt;em&gt;i$ is positive. Then for any $\lambda \ge 0$, take $x = \lambda e_i$ and give
$$
y_0^T A x = x^T A^T y_0 = \underbrace{\lambda}&lt;/em&gt;{\ge 0} \underbrace{(A^T y_0)&lt;em&gt;i}&lt;/em&gt;{&amp;gt; 0} &amp;lt; y_0^T b
$$
which is impossible when $\lambda \to \infty$.&lt;/p&gt;
&lt;p&gt;Therefore, $2$ is solvable when $1$ is unsolvable. $1$ and $2$ cannot be unsolvable at the same time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We construct $Q&amp;rsquo;$ as ${ y \in \R^m: A^T y \le 0, b^T y &amp;gt; 0 }$. Note that $Q&amp;rsquo;$ is not a polyhedron yet, because ${ y \in \R^m: b^T y &amp;gt; 0 }$ is open and is not a half-space. However, observe that (verify it)
$$
A^T y \le 0, b^T y &amp;gt; 0 \text{ is solvable} \iff A^T y \le 0, b^T y = 1 \text{ is solvable}
$$
${ y \in \R^m: b^T y = 1 }$ can be rewritten as ${ y \in \R^m: b^T y \ge 1, b^T y \le 1 }$ which is an intersection of halfspaces. Therefore, $Q \triangleq { y \in \R^m: A^T y \le 0, b^T y \ge 1, b^T \le 1 }$ is non-empty if and only if $Q&amp;rsquo;$ is non-empty. Moreover, $Q$ is an intersection of half-spaces as desired.&lt;/p&gt;
&lt;p&gt;Now, we convert the infeasibility of $P$ into the feasibility of $Q$. &lt;strong&gt;The third question&lt;/strong&gt; is, suppose we verify that $P$ is feasible, then given a solution to $\eqref{primal}$, how to certify its optimality? The idea is to establish a lower bound on the optimal value $v^*_p$.&lt;/p&gt;
&lt;h3 id=&#34;duality&#34;&gt;Duality&lt;/h3&gt;
&lt;p&gt;Consider a $y \in \R^m$ such that $A^T y \le c$, then for any $x \in \R_+^n$ such that $b \triangleq A x$, we have
$$
b^T y = x^T A^T y \le c^T x
$$
The above holds for $x^&lt;em&gt;$ of $\eqref{primal}$ as well. Therefore,
$$
b^T y \le c^T x_p^&lt;/em&gt; = v_p^*
$$
We can try to find the largest lower bound w.r.t. $y$:
$$
\label{dual} \begin{aligned}
v_d^* = \max_y &amp;amp;\quad b^T y \
\text{s.t.} &amp;amp;\quad A^T y \le c
\end{aligned} \tag{D}
$$
Automatically, $v_d^* \le v_p^&lt;em&gt;$. Note that $\eqref{primal}$ is the also the dual of $\eqref{dual}$ in that $v_p^&lt;/em&gt; \ge v_d^*$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt; of LP. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then,
$$
c^T \bar x \ge b^T \bar y
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If $v_p^* = -\infty$, then $\eqref{dual}$ is infeasible.&lt;/li&gt;
&lt;li&gt;If $v_d^* = +\infty$, then $\eqref{primal}$ is infeasible.&lt;/li&gt;
&lt;li&gt;If $\bar x$ is feasible for $\eqref{primal}$, $\bar y$ is feasible for $\eqref{dual}$, and the &lt;strong&gt;duality gap&lt;/strong&gt; $\Delta(\bar x, \bar y) \triangleq c^T \bar x - b^T \bar y$ is zero, then $\bar x$ is optimal for $\eqref{primal}$ and $\bar y$ is optimal for $\eqref{dual}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What about the converses of conclusions above?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $\eqref{dual}$ is infeasible, then either $v_p^* = -\infty$ or $\eqref{primal}$ is infeasible.&lt;/p&gt;
&lt;p&gt;It is rather easy to construct a problem such that both the $\eqref{primal}$ and $\eqref{dual}$ are infeasible, since the constraints $Ax = b, x \ge 0$ and $A^T y \le c$ are quite independent. The following will give an example:
$$
A =
\begin{bmatrix}
-1 &amp;amp; -1 \
1 &amp;amp; 1
\end{bmatrix},
b = [1, 1]^T,
c = [-1, -1]^T
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\eqref{primal}$ is infeasible, then either $v_d^* = +\infty$ or $\eqref{dual}$ is infeasible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality&lt;/strong&gt; for LP. Suppose $\eqref{primal}$ has an optimal solution $x_p^&lt;em&gt;$. Then $\eqref{dual}$ has an optimal solution $y_d^&lt;/em&gt;$ and $v_p^* = c^T x_p^* = b^T y_d^* = v_d^*$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that
$$
Ax = b, x \ge 0, c^T x - v_p^* &amp;lt; 0 \tag{I}
$$
is unsolvable. Consider &lt;strong&gt;homogenizing&lt;/strong&gt; the above system:
$$
Ax - bt = 0, c^T x - v_p^* t &amp;lt; 0, x \ge 0, t \ge 0 \tag{II}
$$
We argue that $\textrm{(I)}$ is solvable if and only if $\textrm{(II)}$ is solvable. If $\bar x$ solves $\textrm{(I)}$, then $(\bar x, 1)$ solves $\textrm{(II)}$ too. If $(\bar x, \bar t)$ solves $\textrm{(II)}$, we discuss by case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar t &amp;gt; 0$. In this case, $\bar x / \bar t$ solves $\textrm{(I)}$.&lt;/li&gt;
&lt;li&gt;$\bar t = 0$. In this case, $x^* + \bar x$ solves $\textrm{(I)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\textrm{(II)}$ can be rewritten as
$$
\begin{bmatrix}
A &amp;amp; -b \
-A &amp;amp; b \
-I &amp;amp; 0 (n \times 1) \
0 (1 \times m) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
x \
t
\end{bmatrix} \le 0,&lt;/p&gt;
&lt;p&gt;[-c^T, v_p^&lt;em&gt;]
\begin{bmatrix}
x \
t
\end{bmatrix} &amp;gt; 0
$$
By &lt;em&gt;Farkas&amp;rsquo; lemma&lt;/em&gt;, the following system is solvable:
$$
\begin{bmatrix}
A^T &amp;amp; -A^T &amp;amp; -I &amp;amp; 0 (m\times 1) \
-b^T &amp;amp; b^T &amp;amp; 0 (1 \times n) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
z_1 \
z_2 \
z_3 \
z_4
\end{bmatrix} =
\begin{bmatrix}
-c \
v_p^&lt;/em&gt;
\end{bmatrix},&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
z_1 \
z_2 \
z_3 \
z_4
\end{bmatrix}
\ge 0
$$
This means
$$
A^T (z_1 - z_2) - z_3 = -c \Rightarrow \
A^T (z_2 - z_1) \le c \
$$&lt;/p&gt;
&lt;p&gt;$$
-b^T (z_1 - z_2) - z_4 = v_p^* \Rightarrow \
b^T (z_2 - z_1) \ge v_p^*
$$&lt;/p&gt;
&lt;p&gt;$y^* \triangleq z_2 - z_1$ is a feasible solution to the $\eqref{dual}$. Plus the weak duality which states $b^T y^* \le v_p^&lt;em&gt;$, $b^T y^&lt;/em&gt; = v_p^&lt;em&gt;$. Therefore, $y_d^&lt;/em&gt; = y^&lt;em&gt;$ and $c^T x_p^&lt;/em&gt; = b^T y_d^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;complementary slackness&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then, they are optimal for their respective problems if and only if
$$
\underbrace{\bar x_i}_{\substack{\text{$i$-th} \ \text{primal variable}}}\ \underbrace{(c - A^T \bar y)&lt;em&gt;i}&lt;/em&gt;{\substack{\text{$i$-th } \ \text{dual constraint}}} = 0
$$
Proof:
$$
\begin{aligned}
c^T \bar x - b^T \bar y &amp;amp;= c^T x - (A \bar x)^T \bar y \
0 &amp;amp;= \bar x^T (c - A^T \bar y)
\end{aligned}
$$
Plus that $\bar x \ge 0, c - A^T \bar y \ge 0$, we can conclude with the complementary slackness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After establishing the lower bound of $\eqref{primal}$&amp;rsquo;s optimal value with $\eqref{dual}$&amp;rsquo;s optimal value, by strong duality, to find optimal solutions to $\eqref{primal}$ and $\eqref{dual}$ is equivalent to find a feasible solution to
$$
\begin{gather}
Ax = b, x \ge 0 \tag{primal constraint} \
A^T y \le c \tag{dual constraint} \
c^T x = b^T y \tag{zero duality gap}
\end{gather}
$$
Note that the zero duality gap is in essence equivalent to
$$
x^T (c - A^T y) = 0 \tag{complementary slackness}
$$
An optimization problem is converted to feasibility problem. Particularly, an LP optimization problem is no harder than an LP feasibility problem.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-lp-problem&#34;&gt;Examples of LP Problem&lt;/h2&gt;
&lt;h3 id=&#34;vertex-cover&#34;&gt;Vertex Cover&lt;/h3&gt;
&lt;p&gt;Given a graph $G = (V, E)$ and a cost function $c: V \mapsto \R^+$, find a vertex cover that minimizes overall cost. Here we say that $S \subseteq V$ is vertex cover if every edge has at least one endpoint in $S$. The cost of a vertex cover the sum of costs of the vertices contained.&lt;/p&gt;
&lt;p&gt;Let $x_i \in { 0,1 }$ be an indicator variable defined as
$$
x_i =
\begin{cases}
1, &amp;amp; \text{if vertex $i$ is in the cover} \
0, &amp;amp; \text{otherwise}
\end{cases}
$$
Then we have the following integer programming problem:
$$
\label{vc} \begin{aligned}
v^* = \min_x \quad &amp;amp;\sum_i x_i c_i \
\text{s.t.} \quad &amp;amp;x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp;x_i \in { 0,1 }, \forall i=1,\dots,|V|
\end{aligned} \tag{Vertex Cover}
$$
The problem above is NP-hard. One typical way to tackle it is to relax the constraint to make it easier.
$$
\label{vc-lp-I} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp; 0 \le x_i \le 1, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC I}
$$
Note that we can further drop the $x_i \le 1$ in $\eqref{vc-lp-I}$ because we can always let $x_i&amp;rsquo; = 1$ for $x_i &amp;gt; 1$ without violating the constraints but with smaller objective (because $c \ge 0$):
$$
\label{vc-lp-II} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp; 0 \le x_i, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC II}
$$
Once $x_\text{lp}^&lt;em&gt;$ solves $\eqref{vc-lp-II}$, we need to round $x_\text{lp}^&lt;/em&gt;$ to give a feasible $x_\text{rd}$ to $\eqref{vc}$ such that $v_\text{rd} = c^T x_\text{rd} \approx v^&lt;em&gt;$. Obviously, $v^&lt;/em&gt; \ge v_\text{lp}^&lt;em&gt;$ and $v^&lt;/em&gt; \le v_\text{rd}$. The question is if we can upper-bound $v_\text{rd}$ by $\alpha v^*$ for some $\alpha \ge 1$. Here $\alpha$ is called the &lt;strong&gt;approximation ratio&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $P = { x \in \R^n: a_i^T x \le b, i=1,\dots,m }$ be a polyhedron and let $\bar x \in P$. Let $I(\bar x) = { i: a_i^T x = b }$ be the active index set. We say that $\bar x$ is a &lt;strong&gt;vertex&lt;/strong&gt; of $P$ if ${ a_i: i \in I(\bar x) }$ has $n$ linearly-independent vectors. Alternatively, the system
$$
a_i^T x = b_i, i \in I(\bar x)
$$
has a unique solution (which is $\bar x$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The equality constraint ensures that the solution is on the boundary of the feasible set, or rather, on a hyperplane. $n$ linearly-independent vectors ensure that the solution is the intersection of $n$ hyperplanes, which is why it is called &lt;u&gt;vertex solution&lt;/u&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If an LP has a vertex feasible solution and is bounded (which means the optimal value is finite), then it has a vertex optimal solution (??).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $Q = { x \in \R^{|V|}: x_i + x_j \ge 1, \forall (i,j) \in E; x \ge 0 }$, which is exactly the feasible set of $\eqref{vc-lp-II}$. Then $Q$ has a vertex solution (??). Let $\bar x$ be one of such vertex solutions. Then,
$$
\forall i, \bar x_i \in { 0, 1/2, 1 }
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the theorem and proposition above, and because $\eqref{vc-lp-II}$ is bounded (verify it), we can have a vertex optimal solution $x_\text{lp}^&lt;em&gt;$ to $\eqref{vc-lp-II}$. Next, we choose to round the optimal vertex solution $x_\text{lp}^&lt;/em&gt;$ in this way: if $x_{\text{lp}&lt;em&gt;i}^*$ is $0$, $x&lt;/em&gt;{\text{rd}&lt;em&gt;i} = 0$; else $x&lt;/em&gt;{\text{rd}&lt;em&gt;i} = 1$. Note that this rounding method satisfies the constraint of $\eqref{vc}$. The largest divergence of $v&lt;/em&gt;\text{rd}$ from $v_\text{lp}^&lt;em&gt;$ happens when $x_\text{lp}^&lt;/em&gt;$ is full of $1/2$ or $0$, in which case $v_\text{rd} = 2 v_\text{lp}^&lt;em&gt;$. Thus, we can conclude that
$$
v_\text{lp}^&lt;/em&gt; \le v^* \le c^T x_\text{rd} \le 2 v_\text{lp}^* \le 2 v^*
$$&lt;/p&gt;
&lt;p&gt;The approximation ratio in this problem is $\alpha = 2$.&lt;/p&gt;
&lt;h3 id=&#34;max-flow&#34;&gt;Max Flow&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;cooperative-game-theory&#34;&gt;Cooperative Game Theory&lt;/h3&gt;
&lt;p&gt;Let $\mathcal{N} = { 1,\dots,n }$ be the set of players and $v: 2^N \mapsto \R^+$ be the worth function, which satisfies $v(\emptyset) = 0$. The subset of $\mathcal{N}$ is called &lt;strong&gt;coalition&lt;/strong&gt;. Let $x \ge 0$ be the &lt;strong&gt;allocation vector&lt;/strong&gt; and $x_i$ be the payoff assigned to player $x_i$. Let $x(\emptyset) = 0$ and $x(S) \triangleq \sum_{i \in S} x_i$ for non-empty coalition $S$. We say that coalition $S$ &lt;strong&gt;improves upon&lt;/strong&gt; $x$ if $v(S) &amp;gt; x(S)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The allocation vector $x$ is said to be in the &lt;strong&gt;core&lt;/strong&gt; if
$$
\begin{gather}
x(\mathcal{N}) = v(\mathcal{N}) \
\forall S \subseteq \mathcal{N}, x(S) \ge v(S) \label{core-inequality}
\end{gather}
$$
which means no $S$ improves upon $x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is essential to understand &lt;strong&gt;the implication behind the game&lt;/strong&gt;. This &amp;ldquo;cooperative game&amp;rdquo; is actually preventing players from cooperation, or collusion, or called forming a coalition. The allocation vector is part of this &amp;ldquo;evil scheme&amp;rdquo;. The fact of matter is, the solver of this problem is trying to pay off players (in total the amount is no more than the value he thinks the whole coalition $\mathcal{N}$ deserves) such that for every possible coalition, there will be some member who thinks it unfair because the share (equal share for everyone!) he gets from the coalition is no more than the amount he would otherwise earns himself.
$$
x(S) = \sum_{i \in S} x_i \ge v(S)
$$
$x(S)$ is additive w.r.t. $S$, which exactly represents the payoff every player grabs (snout in the trough) covertly and separately. There must be some $i$ in $S$ such that $x_i \ge v(S)/|S|$. The situation worsens when the $\ge$ relation is strict.&lt;/p&gt;
&lt;p&gt;The question is, is the core non-empty for a given $(\mathcal{N}, v)$, or rather, can such scheme exist?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Treasure hunt. $n$ people discovered treasures in the mountain. 2 people are needed to bring one piece out and thus $v(S) = \lfloor \frac{|S|}{2} \rfloor$. Is the core empty?&lt;/p&gt;
&lt;p&gt;First consider the case when $n$ is even. $x = [1/2, \dots, 1/2]$ is in the core.&lt;/p&gt;
&lt;p&gt;Now simply consider the case when $n$ is odd. The fact is that the core is empty in this case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;. The cooperative game $(\mathcal{N}, v)$ has a non-empty core if and only if for every set of numbers ${ y_S }_{S \subseteq \mathcal{N}}$, whose elements are indexed by $\mathcal{N}$&amp;rsquo;s subset, such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\forall S \subseteq \mathcal{N}, y_S \ge 0$ and&lt;/li&gt;
&lt;li&gt;$\forall i \in \mathcal{N}, \sum_{S: i \in S} = 1$ and&lt;/li&gt;
&lt;li&gt;$\sum_{S \subseteq \mathcal{N}} y_S v(S) \le v(\mathcal{N})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;$y_S$ can be interpreted as the amount of time each player in $S$ spent on the coalition $S$, justifying the non-negativity constraint. The total amount of time player $i$ spent on different coalitions is $1$. $y_S v(S)$ can be understood as the proportional outcome from partial-commitment.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Consider the following optimization problem:
$$
\label{cgp} \tag{Coop. Game} \begin{aligned}
\min \quad &amp;amp; [x(\mathcal{N}) \triangleq \sum_{i \in N} x_i] \
\text{s.t.} \quad &amp;amp; x(S) \ge v(S), \forall S \subseteq \mathcal{N}
\end{aligned}
$$
Observe that the constraints naturally imply the inequality conditions $\eqref{core-inequality}$ for an allocation to be in the core. $x \ge 0$ is automatically embedded into $x({ i }) \ge v({ i }) \ge 0, i=1,\dots,n$. Other than that, the minimizing objective together with the constraint $x(\mathcal{N}) \ge v(\mathcal{N})$ implies $\eqref{cgp}$ is lower-bounded by $v(\mathcal{N})$. Also note that $\eqref{cgp}$ is always feasible because we can assign each $x(S)$ sufficiently large to surpass $v(S)$. Therefore, $\eqref{cgp}$ always has an optimal solution.&lt;/p&gt;
&lt;p&gt;To rewrite $\eqref{cgp}$ to LP form, let
$$
e = [1,\dots,1]^T,
A =
\begin{bmatrix}
\mathbb{1}_n(\emptyset)^T \
\vdots \
\mathbb{1}&lt;em&gt;n(S)^T \
\vdots \
\mathbb{1}&lt;em&gt;n(\mathcal{N})^T
\end{bmatrix},
b =
\begin{bmatrix}
v(\emptyset) \
\vdots \
v(S) \
\vdots \
v(\mathcal{N})
\end{bmatrix}
$$
where $\mathbb{1}&lt;/em&gt;\mathcal{N}(S)$ is the indicator function that returns the vector in $\R^n$ whose $i$-th entry indicates if $i \in S$. Then $\eqref{cgp}$ becomes
$$
\label{cgp-lp} \tag{Coop. Game LP} \begin{aligned}
v^* = \min \quad &amp;amp; e^T x \
\text{s.t.} \quad &amp;amp; A x \ge b
\end{aligned}
$$
We don&amp;rsquo;t rush to convert it to standard LP form yet; otherwise we need to introduce a slack variable. On the other hand, we formulate the following LP problem, whose optimal value is &lt;strong&gt;of the same magnitude as but of different sign to&lt;/strong&gt; $\eqref{cgp-lp}$:
$$
\label{cgp-primal} \tag{Coop. Game P} \begin{aligned}
v_p^* = \max \quad &amp;amp; -e^T x \
\text{s.t.} \quad &amp;amp; -A x \le -b
\end{aligned}
$$
That is, $v^* = -v_p^&lt;em&gt;$. Recall that the primal and the dual are relative. The above problem has the dual
$$
\label{cgp-dual} \tag{Coop. Game D} \begin{aligned}
v_d^&lt;/em&gt; = \min&lt;/em&gt;{y \ge 0} \quad &amp;amp; -b^T y \
\text{s.t.} \quad &amp;amp; A^T y = e
\end{aligned}
$$
which is in the standard LP form.&lt;/p&gt;
&lt;p&gt;Note that columns of $A^T$ are exactly those indicator vectors. Thus, $A^T$&amp;rsquo;s columns and $y$&amp;rsquo;s entries can both be indexed by sets. Also, rows of $A^T$ are indexed by elements. Since every entry of $A^T y$ is $1$, interpreting $A^T y$ as the row-column dot-product, the constraint in $\eqref{cgp-dual}$ is exactly
$$
\forall i \in \mathcal{N}, \sum_{S: i \in S} y_S = 1
$$
By strong duality, we have
$$
v^* = x^&lt;em&gt;(\mathcal{N}) = \sum_{S} y_S^&lt;/em&gt; v(S) = -v_d^* = -v_p^*
$$
$\forall S \subseteq \mathcal{N}, x^&lt;em&gt;(S) \ge v(S)$ by the constraint of $\eqref{cgp-lp}$. By &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;&amp;rsquo;s condition, we have $x^&lt;/em&gt;(\mathcal{N}) = \sum_{S} y_S^* v(S) \le v(\mathcal{N})$. Therefore, we conclude that $x^&lt;em&gt;(\mathcal{N}) = v(\mathcal{N})$. $x^&lt;/em&gt;$ is in the core.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be inferred that there are lots of duality in game theory, given its mini-max nature.&lt;/p&gt;
&lt;h2 id=&#34;solving-lp&#34;&gt;Solving LP&lt;/h2&gt;
&lt;p&gt;The algorithm for solving LP problems generally falls into two methods: simplex method and interior point method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</guid>
      <description>&lt;p&gt;Though some problems can be relaxed to accommodate the linear programming form, applications of LP are still limited due to its linear constraints. To extend, a natural idea is to gradually allow for other kinds of non-linear constraints. But that would go too far away from the established theories for linear programming.&lt;/p&gt;
&lt;p&gt;In this post, we study the conic linear programming, which is a phased open-up to non-linear problems.&lt;/p&gt;
&lt;h2 id=&#34;concept-preparation&#34;&gt;Concept Preparation&lt;/h2&gt;
&lt;h3 id=&#34;good-order-and-proper-cone&#34;&gt;&amp;ldquo;Good&amp;rdquo; Order and &amp;ldquo;Proper&amp;rdquo; Cone&lt;/h3&gt;
&lt;p&gt;Before exploring other non-linear problems, we first study the properties of a &amp;ldquo;good&amp;rdquo; order $\succeq$. between vectors. We expect it to have the following basic three properties of a &lt;strong&gt;partial-order&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;: $\forall u \in \R^n, u \succeq u$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anti-symmetry&lt;/strong&gt;
$$
\forall u,v \in \R^n, u \succeq v, v \succeq u \to u = v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;
$$
\forall u,v,w \in \R^n, u \succeq v, v \succeq w \to u \succeq w
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than them, we expect $\succeq$ to further possess the following two arithmetic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt;
$$
\forall u,v \in \R^n, u \succeq v, \alpha \succeq 0, \alpha u \succeq \alpha v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Additivity&lt;/strong&gt;
$$
\forall u,v,w,z \in \R^n, u \succeq v, w \succeq z \to u + w \succeq v + z
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say the $\succeq$ order is &amp;ldquo;good&amp;rdquo; if it satisfies the above five properties. The above five is from the algebraic perspective. To illustrate it geometrically, consider the set $K \triangleq { x \in \R^n: x \ge 0 }$. We say $K$ is a &lt;strong&gt;&amp;ldquo;proper&amp;rdquo; cone&lt;/strong&gt; in that it is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;non-empty&lt;/strong&gt; and &lt;strong&gt;closed under addition&lt;/strong&gt;
$$
\forall u, v \in K, u + v \in K
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;conic&lt;/strong&gt;
$$
\forall u \in K, \alpha &amp;gt; 0 \to \alpha u \in K \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;pointed&lt;/strong&gt;
$$
u, -u \in K \to u = 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We further claim that a proper cone is &lt;strong&gt;convex&lt;/strong&gt; and contains the zero vector (verify it).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The pointedness plus closeness (not closeness under addition) implies that there is no complete line in this cone, which equivalently means there is no non-trivial subspace (i.e., except ${ 0 }$ and the universe) inside this cone. To show it, let $K$ be a closed pointed cone, suppose on the contrary there exists $u, v \in K$ such that for every $\alpha \in \R$, we have $u + \alpha(v - u) \in K$. For $t &amp;gt; 1$, consider the sequence ${w_+^t}&lt;em&gt;{t=1}^{\infty}$ and ${w&lt;/em&gt;-^t}&lt;em&gt;{t=1}^{\infty}$ where
$$
w&lt;/em&gt;+^t = \frac{u + t(v-u)}{|u + t(v-u)|&lt;em&gt;2}, w&lt;/em&gt;-^t = \frac{u - t(v-u)}{|u - t(v-u)|&lt;em&gt;2}
$$
Now $w&lt;/em&gt;+^t, w_-^t \in K$ for all $t \ge 1$. But $w_+^t \to w \triangleq \frac{v-u}{|v-u|&lt;em&gt;2}$ and $w&lt;/em&gt;-^t \to -w$ . Since $K$ is closed, we have $w, -w \in K$. However, since $w$ has unit norm and is not zero vector, it follows that $K$ is not pointed, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: At times, people also refer to this as the &lt;strong&gt;salient&lt;/strong&gt; property of a cone. A &lt;u&gt;proper cone&lt;/u&gt; is variously defined on a subset of these properties (closeness, closeness under addition, pointed, salient, and essentially, conic) depending on the context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, the algebraic properties and the geometric properties can derive each other. A good order and a proper cone have a one-to-one relationship. Now we ask, in an arbitrary $n$-d universe (or a finite-dimensional Euclidean space), is $\ge$ the only good order, or equivalently, is $\R_+^n$ the only proper cone? The answer is no.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-proper-cone&#34;&gt;Examples of Proper Cone&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lorentz cone / second-order cone / ice cream cone
$$
\mathcal{Q}^{n+1} \triangleq { (t, x) \in \R \times \R^n: t \ge ||x||_2 }
$$
$\mathcal{Q}^{n+1}$ is a closed proper cone. Then what is the good order associated with this proper cone?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Semi-definite cone
$$
\mathcal{S}&lt;em&gt;+^n \triangleq { X \in \mathcal{S}^n: u^T X u \ge 0, \forall u \in \R^n }
$$
$\mathcal{S}&lt;/em&gt;+^n$ is a closed proper cone. Then what is the good order associated with this proper cone?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zero cone: ${ 0 }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New cones from old ones by Cartesian product&lt;/p&gt;
&lt;p&gt;Let $K_1, \dots, K_m$ be closed proper cones (with non-empty interior). Then
$$
K \triangleq K_1 \times \dots \times K_m = { (x_1, \dots, x_m): x_i \in K_i, \forall i=1,\dots,m }
$$
is a closed proper cone with non-empty interior (with non-empty interior).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: $\R_+^n, \mathcal{Q}^{n+1}, \mathcal{S}&lt;em&gt;+^n$ have non-empty interior:
$$
\intr(\R&lt;/em&gt;+^n) = \R_{++}^n \
\intr(\mathcal{Q}^{n+1}) = { (t, x) \in \R \times \R^n: t &amp;gt; ||x||&lt;em&gt;2 } \
\intr(\mathcal{S}&lt;/em&gt;+^n) = \mathcal{S}_{++}^n \
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conic-linear-programming&#34;&gt;Conic Linear Programming&lt;/h2&gt;
&lt;h3 id=&#34;formulation&#34;&gt;Formulation&lt;/h3&gt;
&lt;p&gt;Recall that linear programming is&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\min &amp;amp; \quad c^T x \
\text{s.t.} &amp;amp; \quad b - A x \ge 0
\end{align*}
$$
Now replacing $\ge$ with the good order $\succeq$ gives the conic linear programming.
$$
\begin{align*}
\tag{CLP}
\min &amp;amp; \quad c^T x \
\text{s.t.} &amp;amp; \quad b - A x \succeq 0
\end{align*}
$$
The next question is, can we recover Farkas&amp;rsquo; lemma and strong duality in this setting? The answer is yes. And the good aspect of the good order is that we can recover the conclusions in linear programming verbatim. Next we generalize the LP problem and show the same result applies.&lt;/p&gt;
&lt;p&gt;Let $E$ be a finite-dimensional Euclidean space (e.g. $\R^n, \mathcal{S}^n$) and $\langle \cdot, \cdot \rangle$ be the inner product on $E$ (e.g., on $\R^n$, $\langle x, y \rangle = x^T y$; on $\mathcal{S}^n$, $\langle X, Y \rangle = \tr(X^T Y)$). Let $K \subseteq E$ be a &lt;strong&gt;closed proper cone&lt;/strong&gt;. Then we can have a good order $\succeq$ on $E$. Consider the &lt;strong&gt;standard form of CLP&lt;/strong&gt; as well as the primal problem:
$$
\begin{equation}
\tag{P} \label{primal}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; \langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall i=1,\dots,m \
&amp;amp; x \in K \subseteq E
\end{aligned}
\end{equation}
$$
What is its dual? We mimic the procedure when we build the lower bound for LP. Let $y \in \R^m$. By &lt;u&gt;the linearity of inner product&lt;/u&gt;,
$$
\langle b, y \rangle = b^T y = 
\sum_{i=1}^m \langle a_i, x \rangle y_i =
\sum_{i=1}^m \langle y_i a_i, x \rangle =
\langle \sum_{i=1}^m y_i a_i, x \rangle
$$
If we impose $c \succeq \sum_{i=1}^m y_i a_i$ like $c \ge A^T y$, can we draw $\langle c - \sum_{i=1}^m y_i a_i, x \rangle \ge 0$ like $x^T(c - A^T y) \ge 0$? The answer is that, this constraint is not enough in general. To reinforce the constraint, construct the set
$$
K^* = { w \in E: \langle w, x \rangle \ge 0, \forall x \in K }
$$&lt;/p&gt;
&lt;p&gt;We can draw some properties for $K^*$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$K^&lt;em&gt;$ is non-empty because $0 \in K^&lt;/em&gt;$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$K^&lt;em&gt;$ is always closed and convex (no matter what $K$ is, because $K^&lt;/em&gt;$ is the intersection of hyperplanes which are closed and convex).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$K^*$ is conic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $K$ is a closed proper cone with non-empty interior, then so is $K^&lt;em&gt;$. That is, $K$ and $K^&lt;/em&gt;$ are heterogenous on the premise that $K$ is a closed proper cone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have (verify it)
$$
(\R_+^n)^* = \R_+^n, (\mathcal{Q}^{n+1})^* = \mathcal{Q}^{n+1}, (\mathcal{S}&lt;em&gt;+^n)^* = \mathcal{S}&lt;/em&gt;+^n
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, $K^&lt;em&gt;$ is called the &lt;strong&gt;dual cone&lt;/strong&gt;. We impose that $c - \sum_{i=1}^m y_i a_i \in K^&lt;/em&gt;$. Then, the dual problem can be written as
$$
\begin{equation}
\tag{D} \label{dual}
\begin{aligned}
v_d^* = \sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in K^* \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;The constraint of CLP&amp;rsquo;s dual in essence describes that &lt;u&gt;an affine map of the variable $y$ belongs to a cone&lt;/u&gt;. This is a good indicator on whether we are dealing with a CLP.&lt;/p&gt;
&lt;h4 id=&#34;second-order-cone-programming&#34;&gt;Second-order Cone Programming&lt;/h4&gt;
&lt;h1 id=&#34;endbmatrix&#34;&gt;Consider the second-order cone programming:
$$
\begin{equation}
\tag{SOCP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{Q}^{n+1} \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$
Let $a_i = [u_i, \underbrace{a_{i,1}, \dots, a_{i, n}}&lt;em&gt;{\bar a_i^T}]^T$ and $c = [v, \underbrace{c_1, \dots, c_n}&lt;/em&gt;{d^T}]^T$. The dual constraint becomes
$$
\begin{gathered}
c - \sum_{i=1}^m y_i a_i =
\begin{bmatrix}
v \
d
\end{bmatrix} -
\sum_{i=1}^m y_i
\begin{bmatrix}
u_i \
\bar a_i
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
v - u^T y \
d - A^T y
\end{bmatrix} \succeq 0, \
\text{where }
\begin{aligned}[t]
A &amp;amp;= [\bar a_1, \dots, \bar a_n]^T, \
u &amp;amp;= [u_1, \dots, u_n]^T, \
\end{aligned}
\end{gathered}
$$
That is
$$
v - u^T y \ge |d - A^T y|&lt;em&gt;2
$$
The left-hand and the right-hand side of the above are both an affine function in $y$. The above is equivalent to
$$
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \
(d - A^T y)^T &amp;amp; v - u^T y 
\end{bmatrix}
\in \mathcal{S}&lt;/em&gt;+^{n+1}
$$
To show it, firstly the case when $v - u^T y = 0$ trivially holds. On the other hand, when $v - u^T y &amp;gt; 0$, we have
$$
\begin{aligned}
(v - u^T y)^2 &amp;amp;\ge (d - A^T y)^T (d - A^T y) \
\underbrace{(v - u^T y)}&lt;em&gt;{C} &amp;amp;\ge \underbrace{(d - A^T y)^T}&lt;/em&gt;{B^T} \underbrace{\frac{I_n}{v - u^T y}}&lt;em&gt;{A^{-1}} \underbrace{(d - A^T y)}&lt;/em&gt;{B}
\end{aligned}
$$
Consider the &lt;em&gt;Schur complement&lt;/em&gt;:
$$
\begin{bmatrix}
A &amp;amp; B \
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}&lt;em&gt;+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}&lt;/em&gt;+^m, C \in \mathcal{S}&lt;em&gt;+^n, \
A - B C^{-1} B^T \in \mathcal{S}&lt;/em&gt;+^m \text{ or } C - B^T A^{-1} B \in \mathcal{S}&lt;em&gt;+^n
\end{gathered}
$$
We have,
$$
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \
(d - A^T y)^T &amp;amp; v - u^T y 
\end{bmatrix}
\in \mathcal{S}&lt;/em&gt;+^{n+1}
$$
The converse similarly follows from &lt;em&gt;Schur complement&lt;/em&gt;. Based on this result, we further claim that an SOCP is equivalent to an semi-definite programming (SDP). But it is never necessary to solve SOCP by converting it to SDP, which only increases the complexity.&lt;/p&gt;
&lt;h4 id=&#34;semi-definite-programming&#34;&gt;Semi-definite Programming&lt;/h4&gt;
&lt;p&gt;$$
\begin{equation}
\tag{SDP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{S}_+^n \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;In terms of inclusion (and thus difficulty), LP =&amp;gt; QCQP =&amp;gt; SOCP =&amp;gt; SDP =&amp;gt; CLP.&lt;/p&gt;
&lt;h3 id=&#34;weak-duality&#34;&gt;Weak Duality&lt;/h3&gt;
&lt;p&gt;Our previous derivation of $\eqref{primal}$ and $\eqref{dual}$ implies the weak duality of CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality of CLP&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then, $\langle c, \bar x \rangle \ge b^T \bar y$.&lt;/p&gt;
&lt;p&gt;Proof:
$$
\begin{gathered}
c - \sum_{i=1}^m y_i a_i \in K^* \Rightarrow \langle c - \sum_{i=1}^m y_i a_i, x \rangle \ge 0 \
\Downarrow \
\begin{aligned}
\langle c, x \rangle &amp;amp;\ge \langle \sum_{i=1}^m y_i a_i, x \rangle \
&amp;amp;= \sum_{i=1}^m \langle a_i, x \rangle y_i  \
&amp;amp;= b^T y
\end{aligned}
\end{gathered}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We next investigate another method to convert between the primal and dual in CLP. For a standard CLP problem, we have
$$
\begin{gathered}
\begin{aligned}
\inf \quad &amp;amp; \langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall i=1,\dots,m \
&amp;amp; x \in K \subseteq E
\end{aligned} \quad
\substack{\langle a_i, x \rangle = b \iff b - \langle a_i, x \rangle \in { 0 } \ \equiv} \quad
\begin{aligned}
-\sup \quad &amp;amp; -\langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \begin{bmatrix}
b \
0
\end{bmatrix} -
\begin{bmatrix}
A \
-I
\end{bmatrix} x
\in { 0 } \times K
\end{aligned}
\end{gathered}
$$
whose dual is
$$
\begin{equation*}
\begin{aligned}
-\inf \quad &amp;amp; [b^T, 0] [y^T, \tilde y^T]^T \
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
A^T &amp;amp; -I
\end{bmatrix}
\begin{bmatrix}
y \
\tilde y
\end{bmatrix} = -c \
&amp;amp;
\begin{bmatrix}
y \
\tilde y
\end{bmatrix} \in ({ 0 } \times K)^*
\end{aligned}
\end{equation*}
$$
Note that $({ 0 } \times K)^* = { 0 }^* \times K^* = \R \times K^*$. And interestingly, from above, we observe that the equality constraint is in essence a cone constraint.&lt;/p&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas&amp;rsquo; Lemma&lt;/h3&gt;
&lt;p&gt;Recall the Farkas&amp;rsquo; lemma for linear systems. Exactly one of the following two systems is solvable:
$$
\begin{gather*}
Ax = b, x \ge 0 \
A^T y \le 0, b^T y &amp;gt; 0
\end{gather*}
$$
Farkas&amp;rsquo; lemma secures a strong duality for LP. We would like to do the same to CLP. First we mimic the two systems in CLP:
$$
\begin{gather*}
\langle a_i, x \rangle = b_i, \forall i=1,\dots,m; x \in K \tag{I} \
-\sum_{i=1}^m y_i a_i \in K^&lt;em&gt;; b^T y &amp;gt; 0 \tag{II}
\end{gather&lt;/em&gt;}
$$
Is it true that exactly one of the above two systems is solvable? We first claim that they can&amp;rsquo;t be solvable at the same time. Suppose on the contrary they both hold. By $\mathrm{(II)}$, we have
$$
\begin{aligned}
b^T y =\sum_{i=1}^m \langle a_i, x \rangle y_i =\langle \sum_{i=1}^m  y_i a_i, x \rangle &amp;amp;&amp;gt; 0 \
\end{aligned}
$$
But that $-\sum_{i=1}^m y_i a_i \in K^*$ implies $\langle \sum_{i=1}^m  y_i a_i, x \rangle = -\langle -\sum_{i=1}^m  y_i a_i, x \rangle \le 0$, which is a contradiction. On the other hand, for CLP, it is possible that neither $\mathrm{(I)}$ nor $\mathrm{(II)}$ is solvable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $E = \mathcal{S}^2, K = \mathcal{S}&lt;em&gt;+^2$. Let
$$
A_1 = \begin{bmatrix}
1 &amp;amp; 0 \
0 &amp;amp; 0
\end{bmatrix},
A_2 = \begin{bmatrix}
0 &amp;amp; 1 \
1 &amp;amp; 0
\end{bmatrix},
b = \begin{bmatrix}
0 \
2
\end{bmatrix}
$$
Then
$$
\begin{gathered}
\begin{cases}
\langle A_1, X \rangle = 0 \
\langle A_2, X \rangle = 2 \
X \in \mathcal{S}&lt;/em&gt;+^2
\end{cases}
\iff
\begin{cases}
X_{11} = 0 \
2 X_{12} = 2 \
X \in \mathcal{S}&lt;em&gt;+^2
\end{cases}
\iff
\begin{bmatrix}
0 &amp;amp; 1 \
1 &amp;amp; X&lt;/em&gt;{22}
\end{bmatrix} \in \mathcal{S}_+^2
\text{, which is unsolvable} \&lt;/p&gt;
&lt;p&gt;\begin{cases}
-(y_1 A_1 + y_2 A_2) \in \mathcal{S}&lt;em&gt;+^2 \
2 y_2 &amp;gt; 0
\end{cases}
\iff
\begin{cases}
\begin{bmatrix}
-y_1 &amp;amp; -y_2 \
-y_2 &amp;amp; 0 \
\end{bmatrix} \in \mathcal{S}&lt;/em&gt;+^2 \
y_2 &amp;gt; 0
\end{cases}
\text{, which is unsolvable}
\end{gathered}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is what goes wrong? Recall in the proof of the original Farkas&amp;rsquo; lemma, we apply the separation theorem to the setting
$$
b \notin { Ax: x \in \R_+^n }
$$
The right-hand set is always closed. However, for an arbitrary closed proper cone $K$, the set
$$
{ (\langle a_1, x \rangle, \dots, \langle a_m, x \rangle): x \in K }
$$
is not always closed. Without closeness, we can&amp;rsquo;t properly apply the &lt;em&gt;point-set separation theorem&lt;/em&gt;. For the same reason, the optimum may not be attained in CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Continue with the $A_1, A_2, b$ in the previous example. Consider the set
$$
S = { (\langle A_1, X \rangle, \langle A_2, X \rangle): X \in \mathcal{S}&lt;em&gt;+^2 }
$$
$(\langle A_1, X \rangle, \langle A_2, X \rangle) = (X&lt;/em&gt;{11}, 2 X_{12})$ basically describes the trajectory of $X$ as $X$ varies in $\mathcal{S}&lt;em&gt;+^2$.
$$
X = \begin{bmatrix}
X&lt;/em&gt;{11} &amp;amp; X_{12} \
X_{12} &amp;amp; X_{22}
\end{bmatrix}
$$
If $X_{11} = 0$, the only way to make $X$ PSD is to make $X_{12} = 0$; if $X_{11} &amp;gt; 0$, $X_{12}$ can be arbitrary because we can take $X_{22}$ sufficiently large such that $X_{11} X_{22} \ge X_{12}^2$. Therefore,
$$
S = { (0, 0) } \cup { (x, y): x &amp;gt; 0, y \in \R }
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If only we can guarantee the closeness of transformation of a proper cone!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;conic Farkas&amp;rsquo; lemma&lt;/strong&gt;. Suppose there exists a $\bar y \in \R^m$ satisfying
$$
-\sum_{i=1}^m \bar y_i a_i \in \intr(K^*) \tag{Slater condition}
$$
Then exactly one of the $\mathrm{(I)}$ and $\mathrm{(II)}$ is solvable.&lt;/p&gt;
&lt;p&gt;The spirit of the Slater condition (and many its variants) is that &amp;ldquo;some vector is in the interior of some set&amp;rdquo;. For this case, it guarantees the closeness of $K^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;Recall the LP strong duality theorem: suppose that the primal is feasible and is bounded below (alternatively the dual is feasible and is bounded above); then, $v_p^* = v_d^*$ and both the primal and the dual have optimal solutions. The question is what about &lt;strong&gt;the duality gap and the attainment&lt;/strong&gt; of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;nonzero duality gap&lt;/strong&gt;. Consider the SDP:
$$
\begin{aligned}
v_p^* = \inf \quad &amp;amp; y_1\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
0 &amp;amp; y_1 &amp;amp; 0 \
y_1 &amp;amp; y_2 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 1+y_1
\end{bmatrix} \in \mathcal{S}&lt;em&gt;3^+
\end{aligned}
$$
$y_1 = 0, y_2 = 0$ is the only solution to it. Thus, $v_p^* = 0$. It can be rewritten as
$$
\begin{aligned}
v_p^* = - \sup \quad &amp;amp; \underbrace{[-1, 0]}&lt;/em&gt;{b^T} [y_1, y_2]^T \
\text{s.t.} \quad &amp;amp;
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 1 \
\end{bmatrix}
}&lt;em&gt;{C} -
y_1 
\underbrace{
\begin{bmatrix} 
0 &amp;amp; -1 &amp;amp; 0 \
-1 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; -1
\end{bmatrix}
}&lt;/em&gt;{A_1} -
y_2 
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; -1 \
\end{bmatrix}
}_{A_2}&lt;/p&gt;
&lt;p&gt;\in \mathcal{S}&lt;em&gt;3^+
\end{aligned}
$$
Its dual is
$$
\begin{aligned}
v_d^* = -\inf \quad &amp;amp; X&lt;/em&gt;{33} \
\text{s.t.} \quad &amp;amp; -2 X_{12} - X_{33} = -1 \
&amp;amp; -X_{22} = 0 \
&amp;amp; X \in \mathcal{S}&lt;em&gt;+^3
\end{aligned}
$$
The dual is feasible and $X&lt;/em&gt;{33}$ can only be $1$. Thus, $v_d^* = -1$.&lt;/p&gt;
&lt;p&gt;Note that the duality gap is nonzero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;non-attainment&lt;/strong&gt;. Consider the SOCP:
$$
\begin{aligned}
v_p^* = \sup \quad &amp;amp; -y_1 \
\text{s.t.} \quad &amp;amp; (y_1 + y_2, 1, y_1 - y_2) \in \mathcal{Q}^3
\end{aligned}
$$
The constraint is
$$
y_1 + y_2 \ge \sqrt{1 + (y_1 - y_2)^2} \iff 4 y_1 y_2 \ge 1
$$
This implies that $y_1, y_2 &amp;gt; 0$. Obviously, $v_p^* = 0$ but cannot be attained. It can be rewritten as
$$
\begin{aligned}
v_p^* = \sup \quad &amp;amp; [-1, 0] [y_1, y_2]^T \
\text{s.t. }\quad &amp;amp;
\begin{bmatrix}
0 \
1 \
0
\end{bmatrix} - 
y_1 \begin{bmatrix}
-1 \
0 \
-1
\end{bmatrix} -
y_2 \begin{bmatrix}
-1 \
0 \
1
\end{bmatrix}
\in \mathcal{Q}^3
\end{aligned}
$$
Its dual is
$$
\begin{aligned}
v_d^* = \inf \quad &amp;amp; z_2 \
\text{s.t.} \quad &amp;amp; -z_1 - z_3 = -1 \
&amp;amp; -z_1 + z_3 = 0 \
&amp;amp; (z_1, z_2, z_3) \in \mathcal{Q}^3
\end{aligned}
$$
The only solution is $(1/2, 0, 1/2)$ and the $v_d^* = 0$ is attained at this point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite the above, can we draw anything about the duality gap and the attainment of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality of CLP&lt;/strong&gt;. Suppose $\eqref{primal}$ is bounded and satisfies Slater&amp;rsquo;s condition, i.e. there exists a feasible $\bar x$ such that $\bar x \in \intr(K)$. Then, $v_p^* = v_d^&lt;em&gt;$ and there exists an $y^&lt;/em&gt;$ that is optimal to $\eqref{dual}$.&lt;/p&gt;
&lt;p&gt;On the other hand, suppose $\eqref{dual}$ is bounded and satisfies Slater&amp;rsquo;s condition, i.e. there exists a feasible $\bar y$ such that $c - \sum_{i=1}^m \bar y_i a_i \in \intr(K^&lt;em&gt;)$. Then, $v_p^&lt;/em&gt; = v_d^&lt;em&gt;$ and there exists an $x^&lt;/em&gt;$ that is optimal to $\eqref{primal}$.&lt;/p&gt;
&lt;p&gt;With Slater&amp;rsquo;s condition on one side, we can guarantee the zero duality gap and the attainment only on the other side. The CLP strong duality is much &amp;ldquo;weaker&amp;rdquo; than the LP strong duality. Refer to the &lt;u&gt;non-attainment&lt;/u&gt; example.&lt;/p&gt;
&lt;p&gt;Pay attention what CLP strong duality does not say as well. Even though the Slater&amp;rsquo;s condition is not satisfied, zero duality gap and attainment can hold. Refer to the &lt;u&gt;nonzero duality gap&lt;/u&gt; example.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</guid>
      <description>&lt;p&gt;Firstly consider that in the previous (conic) linear programming discussion,
$$
\tag{Problem} \begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
$$
The coefficients $\hat a_i, \hat b, \hat c$ are given data. They correspond to the measurements in the real world. But it is highly likely that these measurements are not 100% accurate. How do we take into account these uncertainties?&lt;/p&gt;
&lt;p&gt;Without loss of generality, we may assume that $\hat c$ is deterministic, since the above is equivalent to
$$
\begin{aligned}
\min \quad &amp;amp; t \
\text{s.t.} \quad &amp;amp; \hat c^T x \le t \
&amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
$$
In this new problem, we bring all uncertainties into the constraint and the coefficient $[0,\dots,0,1]$ for the variable $[x^T, t]$ is deterministic.&lt;/p&gt;
&lt;p&gt;The problem remains how to handle the uncertainty in the constraint in $\text{(Problem)}$. There are several viable methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stochastic optimization&lt;/p&gt;
&lt;p&gt;Stochastic optimization assumes that data follow a probability distribution $\mathbb{P}$ (unknown). The constraint becomes
$$
\Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta
$$
for some $\delta &amp;gt; 0$. This method is non-trivial, due to the integral of multi-dimensional probability distribution function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Robust optimization&lt;/p&gt;
&lt;p&gt;The assumption is that data is drawn from a &lt;strong&gt;ambiguity set&lt;/strong&gt; $\mathcal{U}$. We require that
$$
\hat a_i^T x \le \hat b_i, \forall i=1,2,\dots, \forall (\hat a_i, \hat b_i) \in \mathcal{U}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distributionally-robust optimization&lt;/p&gt;
&lt;p&gt;This is kind of the combination of the above two methods. The assumption is that data follow a probability distribution $\mathbb{P}$, which in turn belongs to some ambiguity set $\mathcal{U}$. The constraint becomes
$$
\inf_{\mathbb{P} \in \mathcal{U}} \Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta, \forall i=1,2,\dots
$$
for some $\delta &amp;gt; 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;robust-linear-programming&#34;&gt;Robust Linear Programming&lt;/h2&gt;
&lt;p&gt;Consider the problem
$$
\label{rp} \tag{R} \begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; \underbrace{[\hat \alpha_i^T, \hat b_i]}_{\hat a_i^T} \underbrace{[x^T, x&amp;rsquo;]^T}&lt;em&gt;z \le 0, \
&amp;amp; \quad \hat a_i \in \mathcal{U}&lt;em&gt;i, i=1,\dots,m \
&amp;amp; \underbrace{x&amp;rsquo;}&lt;/em&gt;{z&lt;/em&gt;{n+1}} = -1
\end{aligned}
$$
where $\mathcal{U}&lt;em&gt;i \triangleq { y \in \R^{n+1}: y = u_i + B_i v, B_i \in \mathcal{S}&lt;/em&gt;{++}^{n+1}, |v| \le 1 }$ is an ellipsoid.&lt;/p&gt;
&lt;p&gt;Note that $\eqref{rp}$ is not LP actually, as it may contain infinitely-many linear constraints. This is usually hard. Just imagine its dual problem. For a primal that has infinitely-many constraints, its dual has infinitely-many variables.&lt;/p&gt;
&lt;p&gt;The key question now is how to tackle $\hat a_i^T z \le 0, \forall \hat a_i \in \mathcal{U}&lt;em&gt;i$ (ignoring $z&lt;/em&gt;{n+1} = -1$ for now). In essence, it is equivalent to
$$
\begin{aligned}
\left[ \sup_{\hat a_i \in \mathcal{U}&lt;em&gt;i} \hat a_i^T z \right] &amp;amp;\le 0 \iff \
\left[ \sup&lt;/em&gt;{|v| \le 1} (u_i + B_i v)^T z \right] &amp;amp;\le 0 \iff \
u_i^T z + \left[ \sup_{|v| \le 1} v^T B_i z \right] &amp;amp;\le 0 \iff \
y_i^T z + |B_i z|_2 &amp;amp;\le 0
\end{aligned}
$$
This is exactly an SOCP constraint.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</guid>
      <description>&lt;h2 id=&#34;quadratically-constrained-quadratic-programming&#34;&gt;Quadratically Constrained Quadratic Programming&lt;/h2&gt;
&lt;p&gt;Consider the quadratically constrained quadratic programming:
$$
\label{qcqp} \tag{QCQP} \begin{aligned}
\inf \quad &amp;amp; x^T Q x \
\text{s.t.} \quad &amp;amp; x^T A_i x \ge b_i, \forall i=1,\dots,m
\end{aligned}
$$
where $Q, A_1, \dots, A_m \in \mathcal{S}^n$. By far, no convexity is not assumed. But on the other hand, we can apply the &lt;strong&gt;semi-definite relaxation&lt;/strong&gt; technique to transform the $\eqref{qcqp}$.&lt;/p&gt;
&lt;p&gt;Observe that $x^T Q x = \tr(x^T Q x) = \tr(Q x x^T) = Q \bullet x x^T$, which is linear in $x x^T$. We can apply the same trick to the constraints so that $\eqref{qcqp}$ is equivalent to
$$
\begin{aligned}
v^* = \inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; \exists x \in \R^n, X = x x^T \
\text{ non-convex??}
\end{aligned}
$$
Note that $\exists x \in \R^n, X = x x^T \iff X \in \mathcal{S}_+^n, \rank(X) \le 1$. That is.
$$
\begin{aligned}
\inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; X \succeq 0 \
&amp;amp; \rank(X) \le 1
\end{aligned}
$$
The $\rank$ function is not convex. We may as well drop the rank constraint so that the semi-definite relaxation of $\eqref{qcqp}$ is
$$
\label{relaxed-qcqp} \tag{Relaxed QP} \begin{aligned}
v_R^* = \inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; X \succeq 0
\end{aligned}
$$
Observe that $v^* \ge v_R^*$ and $\eqref{relaxed-qcqp}$ is a SDP.&lt;/p&gt;
&lt;h3 id=&#34;max-cut-problem&#34;&gt;Max-cut Problem&lt;/h3&gt;
&lt;p&gt;Let $G = (V,E)$ be an undirected graph and $w: E \mapsto \R_+$ be a weight function on $E$. A subset $S \subseteq V$ defines a cut and the value of a cut is
$$
w(S) \triangleq \sum_{(i,j) \in E, i \in S, j \notin S} w_{ij}
$$
The goal is to find $S \subseteq V$ such that $w(S)$ is maximized. The minimization problem is trivial, simply choosing $S$ as $V$ or $\emptyset$ gives the minimum value $0$. Let $x_i \in { -1, +1 }$ be a binary variable indicating whether vertex $i$ is in the cut ($+1$) or not ($-1$). Then,
$$
\label{mc} \tag{Max-cut} \begin{aligned}
v^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - x_i x_j) \
\text{s.t.} \quad &amp;amp; x_i^2 = 1, \forall i=1,\dots,n \
\end{aligned}
$$
Apply the SDR technique (there is a middle step to convert the above to a QP) to get
$$
\label{relaxed-mc} \tag{Relaxed Max-cut} \begin{aligned}
v_\text{sdr}^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}) \
\text{s.t.} \quad &amp;amp; X_{ii} = 1, \forall i=1,\dots,n \
&amp;amp; X \succeq 0
\end{aligned}
$$
Note that $v^* \le v_\text{sdr}^&lt;em&gt;$. If $\eqref{relaxed-mc}$ is solved with a rank-one matrix $X^&lt;/em&gt;$, we can automatically decompose it to give the optimal solution to $\eqref{mc}$. The crux is how to preserve the optimality when $X^*$ is of rank higher than one. Here is an algorithm for it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve $\eqref{relaxed-mc}$ to get an optimal solution $X^&lt;em&gt;$. Let $X^&lt;/em&gt; = U^T U$ where $U \in \R^{n \times n}$. Let $u_i \in \R^n$ be the $i$-th column of $U$. We have $|u_i|&lt;em&gt;2^2 = u_i^T u_i = X&lt;/em&gt;{ii}^* = 1$.&lt;/li&gt;
&lt;li&gt;Let $r \in \R^n$ be a random vector uniformly distributed on the sphere $S^n = { x \in \R^n: |x|_2 = 1 }$ (this can be done by normalizing the samples from standard Gaussian in $\R^n$).&lt;/li&gt;
&lt;li&gt;Let $x_i&amp;rsquo; = \sign(u_i^T r)$ where $\sign(z)$ is $+1$ if $z \ge 0$ or $-1$ otherwise. Return ${ x_i&amp;rsquo;: i=1,\dots,n }$ as a feasible solution to $\eqref{mc}$. This is also referred to as the &lt;strong&gt;hyperplane rounding&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the first place, $x_i&amp;rsquo;$s are feasible. Let $v&amp;rsquo;$ be the objective value associated with the cut ${ x_i&amp;rsquo;: i=1,\dots,n }$. Clearly, $v&amp;rsquo; \le v^&lt;em&gt;$. To analyze its approximation bound, firstly note that ${ x_i&amp;rsquo;: i=1,\dots,n }$ is random. We can only consider the $\E[v&amp;rsquo;]$.
$$
\begin{aligned}
&amp;amp;\E[v&amp;rsquo;] = \frac{1}{2} \E[\sum_{(i,j) \in E} w_{ij} (1 - x_i&amp;rsquo; x_j&amp;rsquo;)] \
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \E[\frac{1 - x_i&amp;rsquo; x_j&amp;rsquo;}{2}] \
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \Pr[\sign(u_i^T r) \ne \sign(u_j^T r)] \
\end{aligned}
$$
Let $u, v \in S^n$ be arbitrary, $r$ be uniformly distributed on $S^{n-1}$. Then,
$$
\Pr[\sign(u^T r) \ne \sign(v^T r)] = \frac{\arccos(u^T v)}{\pi}
$$
Under the above setting, for any $z \in [-1, 1]$ and $\theta$ such that $\cos \theta = z$,
$$
\frac{\arccos z}{\pi} = \frac{2 \theta}{\pi(1 - \cos \theta)} \frac{1}{2} (1 - z) \
\ge \alpha \cdot \frac{1}{2} (1-z)
$$
where $\alpha = \min_{0 \le \theta \le \pi} \frac{2 \theta}{\pi (1 - \cos \theta)} &amp;gt; 0.878$. As a result,
$$
\begin{aligned}
&amp;amp;\E[v&amp;rsquo;] = \sum_{(i,j) \in E} w_{ij} \frac{\arccos u_i^T u_j}{\pi} \
&amp;amp;\ge \sum_{(i,j) \in E} w_{ij} \alpha \cdot \frac{1}{2} (1 - \underbrace{u_i^T u_j}&lt;em&gt;{X&lt;/em&gt;{ij}^&lt;/em&gt;}) \
&amp;amp;= \alpha \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}^&lt;em&gt;) \
&amp;amp;= \alpha v_\text{sdr}^&lt;/em&gt; \ge \alpha v^*
\end{aligned}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</guid>
      <description>&lt;h2 id=&#34;nonlinear-programming&#34;&gt;Nonlinear Programming&lt;/h2&gt;
&lt;p&gt;Recall the unconstrained optimization problem:
$$
\inf_{x \in \R^n} \quad f(x)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be a continuously differentiable function, $\bar x \in \R^n$ be an arbitrary point. If there exists s &lt;strong&gt;direction&lt;/strong&gt; $d \in \R^n \setminus { 0 }$ such that $\nabla f(\bar x)^T d &amp;lt; 0$, then there exists $\alpha_0 &amp;gt; 0$ such that
$$
f(\bar x + \alpha d) &amp;lt; f(\bar x), \forall \alpha \in (0, \alpha_0]
$$
Here, $d$ is called a &lt;strong&gt;descent direction&lt;/strong&gt; of $f$ at $\bar x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a necessary condition for $\bar x$ to be a local minima is that $\nabla f(\bar x) = 0$ (&lt;strong&gt;first-order necessary condition&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be a convex and continuously differentiable function. Then, $\bar x$ is a global minima of $f$ if and only if $\nabla f(\bar x) = 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;second-order sufficient condition&lt;/strong&gt;. Let $f: \R^n \mapsto \R$ be a twice continuously differentiable function. If $\nabla f(\bar x) = 0$ and $\nabla^2 f(\bar x) \succ 0$, then $\bar x$ is a local minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;In constrained case, simple conditions does not apply, e.g. $\inf_{x \ge 1} x^2$ and $\inf_{x \ge -1} x^2$. Consider the following constrained problem:
$$
\label{primal} \tag{P} \begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; g_i(x) \le 0, i=1,\dots,r \
&amp;amp; h_j(x) = 0, j=1,\dots,s
\end{aligned}
$$
where $f, g_i, h_j$ are continuously differentiable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Fritz John necessary condition&lt;/strong&gt;. Let $\bar x$ be a local minimum of $\eqref{primal}$. Then there exist &lt;strong&gt;multipliers&lt;/strong&gt; $u \in \R$, $v_1, \dots, v_r \in \R$, $w_1, \dots, w_s \in \R$ such that
$$
\begin{gather}
u \nabla f(\bar x) + \sum_{i=1}^r v_i \nabla g_i(\bar x) + \sum_{j=1}^s w_j \nabla h_j(x) = 0 \tag{vanishing gradient} \
[u, v_1, \dots, v_r, w_1, \dots, w_s] \ne 0 \tag{non-trivial solution} \
u, v_i \ge 0, i=1,\dots,r \tag{non-negativity} \
v_i g_i(\bar x) = 0, i=1,\dots,r \tag{complementarity} \
\end{gather}
$$
$v_i$ tells the importance of the inequality constraint $g_i(x)$; $v_i = 0$ implies that $g_i(x) \le 0$ is well-fulfilled (strictly less than zero).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The implication is that, at a local minima, we should not be able to find a direction that decreases the objective value as well as maintains the feasibility. For simplicity of discussion, we drop the equality constraints below.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Linear non-independence&lt;/p&gt;
&lt;p&gt;The vanishing gradient together with the non-trivial solution in essence rules out the possibility of linear independence among the gradients. This is easy to interpret. If $\nabla f(\bar x), \nabla g_1(\bar x), \dots, \nabla g_r(\bar x)$ are linearly independent, it is easy to find a direction $d$ in the $\Col^\perp(\nabla g_1(\bar x), \dots, \nabla g_r(\bar x))$ that is acute to $-\nabla f(\bar x)$. Then moving along $d$ at $\bar x$ will decrease the objective function value but maintain the inequality constraints, which contradicts that $\bar x$ is a local minima.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non-negativity + complementarity&lt;/p&gt;
&lt;p&gt;These two components should be discussed together and are a bit intriguing. Please refer to &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0022247X67901631&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surely, the premise is that $u$ is nonzero. When the only solution to $u$ is zero, the corresponding solution $\bar x$ to $x$ will be a garbage point: it will not be a local minima. In this case, $\nabla f(\bar x)$ must have a component that is orthogonal to $\nabla g_i(\bar x)$&amp;rsquo;s. We can walk along this component (or its opposite) to decrease $f$ without compromising inequality constraints. This motivates the study of &lt;strong&gt;constraint qualification&lt;/strong&gt;, which aims to ensure a nonzero $u$.&lt;/p&gt;
&lt;p&gt;Note that in the above discussion, we ignore the effect of $\nabla h_j(\bar x)$&amp;rsquo;s. They only make the choice of direction stricter, since the direction has to be orthogonal to them.&lt;/p&gt;
&lt;h3 id=&#34;kkt-conditions-and-constraint-qualification&#34;&gt;KKT Conditions and Constraint Qualification&lt;/h3&gt;
&lt;p&gt;One observation is that, if the constraint gradients are linearly independent (though required dependent), there is no way to have $u = 0$ or otherwise $v_1, \dots, v_r, u_1, \dots, u_s$ has to be zero due to the linear independence, which in turn violates the nonzero multiplier condition. How to &amp;ldquo;obtain&amp;rdquo; the contradicting linear independence expectation and nonzero multiplier condition at the same time?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Karush-Kuhn-Tucker conditions&lt;/strong&gt;. Let $\bar x$ be a local minima of $\eqref{primal}$. Let
$$
I(\bar x) = { i: g_i(\bar x) = 0 }
$$
be the index set on the &lt;strong&gt;active inequality constraint&lt;/strong&gt;. Suppose that ${ \nabla g_i(\bar x) }&lt;em&gt;{i \in I} \cup { \nabla h_j(\bar x) }&lt;/em&gt;{j=1}^s$ are linearly independent (&lt;strong&gt;linear-independence constraint qualification&lt;/strong&gt;). Then, there exists $v \in \R^r$ and $w \in \R^s$ such that
$$
\nabla f(\bar x) + \sum_{i=1}^r \nabla g_i(x) + \sum_{j=1}^s \nabla h_j(x) = 0 \
v_i \ge 0, i=1,\dots,r \
v_i g_i(\bar x) = 0, i=1,\dots,r
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: importance of CQ. Consider the problem
$$
\begin{aligned}
\inf \quad &amp;amp; f(x_1, x_2) = x_1 \
\text{s.t.} \quad &amp;amp; g_1(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - 1 \le 0 \
&amp;amp; g_2(x_1, x_2) = (x_1 - 1)^2 + (x_2 + 1)^2 - 1 \le 0 \
\end{aligned}
$$
The only feasible and thus optimal solution is $\bar x = (1, 0)$. In this case, the active inequality constraint gradient is
$$
\begin{bmatrix}
0 \
-2
\end{bmatrix},
\begin{bmatrix}
0 \
2
\end{bmatrix}
$$
They are linearly dependent. Therefore, KKT conditions doesn&amp;rsquo;t hold. Here is the reason why we intentionally separate the equality constraints from inequality constraints. Though $h(x) = 0 \iff h(x) \le 0 \land -h(x) \le 0$, if we lay down the equality constraint as inequality constraints, the gradient of $h(x)$ is always linearly dependent to that of $-h(x)$.&lt;/p&gt;
&lt;p&gt;One takeaway is that, even in this convex optimization problem, KKT may not hold.&lt;/p&gt;
&lt;p&gt;Another takeaway is that, it is very convenient to &amp;ldquo;draw circles&amp;rdquo; when finding counter examples related to constraint qualification. Better still, leave only one feasible point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem with LICQ is that it is tedious to check for every solution of $\bar x$. We may prefer some kind of &lt;strong&gt;&amp;ldquo;looser&amp;rdquo; constraint qualifications&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Slater constraint qualification&lt;/strong&gt;. Suppose that $g_1, \dots, g_r$ are convex and $h_1, \dots, h_s$ are affine. Let $\bar x$ be a local minima. Denote the feasible region as $S$. Suppose that there exists $x&amp;rsquo; \in S$ such that $g_i(x&amp;rsquo;) &amp;lt; 0$ for $i=1,\dots,r$. Then, the KKT conditions are necessary for optimality.&lt;/p&gt;
&lt;p&gt;This Slater condition quite resembles that in the conic Farkas&amp;rsquo; lemma.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Suppose that $g_1, \dots, g_r$ are concave and $h_1, \dots, h_s$ are affine. Then, the KKT conditions are necessary for optimality.&lt;/p&gt;
&lt;p&gt;This theorem is especially useful when all the constraint functions are affine (since affine functions are both convex and concave).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $A \in \R{m \times n}, b \in \R^m, c \in \R^n$ be given. Consider
$$
\begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; Ax = b \
&amp;amp; x \ge 0
\end{aligned}
$$
Beware of the dimension of constraint functions when converting this LP problem to nonlinear programming problem:
$$
\begin{aligned}
\min \quad &amp;amp; c^Tx \
\text{s.t.} \quad &amp;amp; g_i(x) = -x_i = -e_i^T x \le 0, i=1,\dots,n \
&amp;amp; h_i(x) = b_j - a_j^T x = 0, j=1,\dots,m
\end{aligned}
$$
This is a linearly constrained problem; thus KKT conditions are necessary for optimality:
$$
\begin{gather}
c - \sum_{i=1}^r v_i e_i - \sum_{j=1}^s w_i a_j = 0 \label{grad} \
v_i \ge 0 \label{dual} \
v_i x_i = 0, i=1,\dots,r \label{compl}
\end{gather}
$$
From $\eqref{grad}$,
$$
c - v - A^T w = 0
$$
Given $v \ge 0$ from $\eqref{dual}$ and the complementarity from $\eqref{compl}$, we have $c \ge A^T w$ and $(c - A^T w)_i x_i = 0$. This essentially recovers the sufficient and necessary conditions for the optimality of LP. But KKT only tells the condition is necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is when are KKT conditions sufficient?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;KKT sufficient conditions&lt;/strong&gt;. Suppose that $f, g_1, \dots, g_r$ are convex and $h_1, \dots, h_s$ are affine. Suppose further that there exists $(\bar x, \bar v, \bar w)$ satisfying the KKT conditions:
$$
\begin{gather}
g_i(\bar x) \le 0, h_j(\bar x) = 0, i=1,\dots,r, j=1,\dots,s \tag{primal feasibility} \
\tag{dual feasibility} \left. \begin{gathered}
\nabla f(\bar x) + \sum_{i=1}^r \bar v_i \nabla g_i(\bar x) + \sum_{j=1}^s \bar w_i \nabla h_i(x) = 0 \
\bar v_i \ge 0, i=1,\dots,r
\end{gathered} \right} \
\bar v_i g_i(x) = 0, i=1,\dots,r \tag{complentary slackness}
\end{gather}
$$
Then $\bar x$ is a global minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h3&gt;
&lt;p&gt;For simplicity, we rewrite the constrained problem as follows:
$$
\label{primalp} \tag{P} \begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; G(x) \le 0 \
&amp;amp; H(x) = 0
\end{aligned}
$$
where $G(x) = [g_1(x), \dots, g_r(x)]$ and $H(x) = [h_1(x), \dots, h_s(x)]$.&lt;/p&gt;
&lt;p&gt;Observe that
$$
\eqref{primalp} \equiv \inf_{x \in \R^n} \sup_{v \in \R_+^r, w \in \R^s} \underbrace{f(x) + v^T G(x) + w^T H(x)}&lt;em&gt;{L(x, v, w)}
$$
The dual of $\eqref{primalp}$ is then
$$
\begin{equation}
v_d^* = \sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} \inf_{x \in \R^n} L(x, v, w) \label{dualp} \tag{D}
\end{equation}
$$
Observe that
$$
\underbrace{\inf_{x \in \R^n}L(x, \bar v, \bar w)}&lt;em&gt;{\theta(\bar v, \bar w)} \le L(\bar x, \bar v, \bar w) \le \underbrace{\sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} L(\bar x, v, w)}&lt;em&gt;{\gamma(\bar x)}
$$
This implies that
$$
v_d^* = \sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} \theta(v, w) \le \inf_{x \in \R^n} \gamma(x) = v_p^*
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primalp}$ and $(\bar v, \bar w)$ be feasible for $\eqref{dualp}$. Then,
$$
f(\bar x) = \gamma(\bar x) \ge \theta(\bar v, \bar w)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: illustration of weak duality. Consider a simple case:
$$
\begin{gathered}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; g(x) \le 0
\end{aligned} \
\rule{6cm}{0.4pt} \
\begin{aligned}
v_d^* = \sup_{v \ge 0} &amp;amp; \inf_{x \in \R^n} [f(x) + v g(x)] \
\end{aligned}
\end{gathered}
$$
Let $\mathcal{G} = { (y,z): y = g(x), z = f(x), x \in \R^n }$. Then,
$$
\begin{aligned}
\theta(v) &amp;amp;= \inf_{x \in \R^n} [f(x) + v g(x)] \
&amp;amp;= \inf_{(y, z) \in \mathcal{G}} [z + v y]
\end{aligned}
$$
This set is quite related to the proof of the &lt;strong&gt;strong duality&lt;/strong&gt; under KKT sufficient condition together with Slater condition.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/mean-average-precision/</guid>
      <description>&lt;h2 id=&#34;the-precision&#34;&gt;The Precision&lt;/h2&gt;
&lt;p&gt;The typical &lt;strong&gt;precision&lt;/strong&gt; and &lt;strong&gt;recall&lt;/strong&gt; definition in a binary classification is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}, \text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Precision determines, among all the samples that are identified as positive, how many are really positive. Recall determines, among all the samples that are positive, how many are successfully identified.&lt;/p&gt;
&lt;p&gt;Binary classification task will assign each case a score that shows how confident the model is in the case is indeed positive. We can arrange all the samples in descending order of this score. Then beginning with an empty set, we add the sample one by one into this set. Every time we add in a new sample, we can calculate the precision and the recall within this set.&lt;/p&gt;
&lt;p&gt;During this process, recall will increase monotonically; but precision may go up and down. We can draw a plot with regard to this two numbers and this is the precision-recall curve (PRC). The area under PRC usually indicates the goodness of the model, as that of a perfect model will be 1.&lt;/p&gt;
&lt;h2 id=&#34;average-precision&#34;&gt;Average Precision&lt;/h2&gt;
&lt;p&gt;The &amp;ldquo;average precision&amp;rdquo; term usually appears in document retrieval and object detection scenario. For document retrieval task,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\text{precision} = \frac{{ \text{relevant documents} } \cap { \text{retrieved documents} } } {{ \text{retrieved documents} }}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The question is where the &amp;ldquo;average&amp;rdquo; comes from. Similar to the plotting of PRC, we may rank the retrieved documents according to the relevance. Starting from an empty set of documents predicted as relevant, we add these retrieved documents one by one. Then we can have a series of precisions to average upon.&lt;/p&gt;
&lt;p&gt;On the other hand, in object detection task, predicted anchors are firstly ranked according to its predicted objectness score. After that, each anchor will be assigned a objectness label. The &amp;ldquo;positivity&amp;rdquo; of a anchor is mainly determined by its intersection over union (IoU) with the ground-truth bounding boxes. The rules for determining positivity is complex. But as a result, we will have positive anchors, negative anchors and those neither positive nor negative. During training, only positive and negative anchors will contribute gradient. But in precision, non-positive anchors are treated as &amp;ldquo;negative&amp;rdquo;. For a detailed discussion of positivity in object detection, please refer &lt;a href=&#34;https://vignesh943628.medium.com/metrics-on-object-detection-b9fe3f1bac59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can rank the anchors according to the objectness score. Then similarly beginning with an empty set, we add the anchor one by one into this set. Every time we add in a new anchor, we can calculate the precision. By doing so, we obtain a series of precisions to average.&lt;/p&gt;
&lt;h3 id=&#34;mean-average-precision&#34;&gt;Mean Average Precision&lt;/h3&gt;
&lt;p&gt;We can also obtain a series of average precisions for different classes of objects, yielding the concept of &lt;strong&gt;mean average precision&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;average-mean-average-precision&#34;&gt;&amp;ldquo;Average Mean Average Precision&amp;rdquo;&lt;/h3&gt;
&lt;p&gt;Moreover, we may change the IoU threshold to obtain a series of mean average precisions to average upon; in some sense this is the &amp;ldquo;average mean average precision&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid>
      <description>&lt;h2 id=&#34;贝叶斯分类器&#34;&gt;贝叶斯分类器&lt;/h2&gt;
&lt;p&gt;贝叶斯分类器（Bayes classifier）基于“能够根据标签$y$预测特征$\x$”的思想，在训练环节，它学习$p(\x|y)$和$p(y)$；在预测环节，它给出$\arg \max_y p(y|\x) = \arg \max_y p(\x|y) p(y)$。可以看出，贝叶斯分类器的两个关键部分便是“似然”和“先验”，其预测环节采用了最大后验估计的思想，从这个角度或许能够理解为什么它叫做“贝叶斯”分类器。&lt;/p&gt;
&lt;h3 id=&#34;朴素贝叶斯分类器&#34;&gt;朴素贝叶斯分类器&lt;/h3&gt;
&lt;p&gt;贝叶斯分类器在训练环节学习$p(\x|y)$和$p(y)$，$p(y)$可以直接由频率估计得来，而$p(\x|y)$这个条件概率则不太好求，尤其当$\x$为多元随机变量时，其各个成分之间的关系难以捕捉。&lt;/p&gt;
&lt;p&gt;为简化问题，朴素贝叶斯分类器（naive Bayes classifier）做出这样的假设：$\x$的各个成分在$Y=y$给定的情况下，是相互独立的，此时有$p(\x|y) = p(x_1|y) \dots p(x_n|y)$。进一步，为了求解各个成分在$Y=y$给定时的条件概率分布，我们还可以对条件概率的分布形式作出假设，比如假设$p(x_i|y)$服从伯努利分布、多项分布、高斯分布（分别对应伯努利朴素贝叶斯分类器&amp;lt;Bernoulli naive Bayes classifier&amp;gt;、多项分布朴素贝叶斯分类器&amp;lt;multinomial naive Bayes classifier&amp;gt;、高斯分布朴素贝叶斯&amp;lt;Gaussian naive Bayes classifier&amp;gt;）等等。&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯最优分类器&#34;&gt;贝叶斯最优分类器&lt;/h2&gt;
&lt;p&gt;给定贝叶斯分类器$f \in \mathcal{H}$，给定样本$(\x,y)$，0-1损失函数$l$定义为，
$$
\newcommand{\I}{\mathbb{I}} l(f(\x), y) = \I[f(\x) \ne y] = 1 - \I[f(\x) = y]
$$
我们的目标是最小化$l(f) \triangleq \E_{\x,y} l(f(\x), y)$：
$$
\begin{aligned}
&amp;amp;l(f) = \sum_{\x} \sum_{y} p(\x,y) l(f(\x), y) \
&amp;amp;= \sum_x p(\x) [\sum_y p(y|\x) l(f(\x), y)] \
&amp;amp;= \E_\x [\sum_y p(y|\x) l(f(\x), y)]
\end{aligned}
$$
用$f^\star$表示贝叶斯最优分类器（Bayes optimal classifier），则$f^\star = \arg \min_{f \in \mathcal{H}} \E_\x [\sum_y p(y|\x) l(f(\x), y)]$；故对于每一个样本$\x$，都应该有：
$$
\begin{aligned}
&amp;amp;f^\star(\x) = \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x) l(f(\x), y)] \
&amp;amp;= \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x) (1 - \I[f(\x) = y])] \
&amp;amp;= \arg \min_{f \in \mathcal{H}} [\sum_y p(y|\x) - \sum_y p(y|\x) \I[f(\x) = y]] \
&amp;amp;= \arg \min_{f \in \mathcal{H}} [1 - \sum_y p(y|\x) \I[f(\x) = y]] \
&amp;amp;= \arg \min_{f \in \mathcal{H}} [- \sum_y p(y|\x) \I[f(\x) = y]] \
&amp;amp;= \arg \max_{f \in \mathcal{H}} [\sum_y p(y|\x) \I[f(\x) = y]] \
&amp;amp;= \arg \max_{y \in \mathcal{Y}} p(y|\x)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;以$Y$仅有两种取值举例：
$$
\begin{aligned}
f^\star(x) &amp;amp;= \arg \max_{f \in \mathcal{H}} [p(y_1|\x) \I[f(\x) = y_1] + p(y_2|\x) \I[f(\x) = y_2] ] \
&amp;amp;= \arg \max_{y \in \mathcal{Y}} p(y|\x)
\end{aligned}
$$
以上结论在$\x$为连续型随机变量或者$y$为连续型随机变量时也成立。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://artint.info/html1e/ArtInt_181.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial Intelligence - foundations of computational agents &amp;ndash; 7.3.3 Bayesian Classifiers (artint.info)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/567299/what-does-it-mean-for-the-bayes-classifier-to-be-optimal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;machine learning - What does it mean for the Bayes Classifier to be optimal? - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/machine-learning/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;p&gt;z&amp;mdash;
title: 隐马尔可夫模型
linktitle: 隐马尔可夫模型
type: book
lastmod: &amp;lsquo;2023-05-22T09:43:15&amp;rsquo;&lt;/p&gt;
&lt;h1 id=&#34;prevnext-pager-order-if-docs_section_pager-enabled-in-paramstoml&#34;&gt;Prev/next pager order (if &lt;code&gt;docs_section_pager&lt;/code&gt; enabled in &lt;code&gt;params.toml&lt;/code&gt;)&lt;/h1&gt;
&lt;h2 id=&#34;markup-pandoc&#34;&gt;weight: 40
markup: pandoc&lt;/h2&gt;
&lt;h2 id=&#34;隐马尔可夫模型&#34;&gt;隐马尔可夫模型&lt;/h2&gt;
&lt;p&gt;隐马尔可夫模型（Hidden Markov Model）的对象是序列类型的数据，其基本假设为：该序列的观测值实际上是来自于完全关于当前时序下的状态（state，也可叫作隐变量&amp;lt;latent variable&amp;gt;）取值的一个条件概率分布，而状态取值亦会随着时序发生变化，且其状态变化遵从马尔可夫过程。&lt;/p&gt;
&lt;p&gt;令$Y_1, \dots, Y_T$表示$T$个时序下的观测值，$X_1, \dots, X_T$表示$T$个时序下的状态取值，对于任意$i = 1, \dots, T$，有$Y_i \sim p(\cdot | X_i)$；一般来说，隐马尔科夫模型中假设状态取值和观测取值都是离散的，假设共有$N$种状态取值$x_1, \dots, x_N$和$M$种观测取值$y_1, \dots, y_M$，令状态转移矩阵（transition matrix）为$N \times N$的方阵$A$，令观测概率矩阵（emission matrix）为$N \times M$的矩阵$B$；令$\pi$为初始状态概率向量。&lt;/p&gt;
&lt;p&gt;隐马尔可夫模型即是由$\pi, A, B$三者决定，所以其可以用三元组表示：
$$
\lambda = (\pi, A, B)
$$&lt;/p&gt;
&lt;p&gt;隐马尔可夫模型有三类基本问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;概率问题。给定模型$\lambda = (\pi, A, B)$和观测序列$\Y = (y_{o_1}, \dots, y_{o_T})$，计算模型$\lambda$产生观测序列$\Y$的概率。&lt;/li&gt;
&lt;li&gt;学习问题。给定观测序列$\Y = (y_{o_1}, \dots, y_{o_T})$，估计模型参数$\lambda = (\pi, A, B)$。&lt;/li&gt;
&lt;li&gt;预测问题（也叫解码&amp;lt;decoding&amp;gt;问题）。给定模型$\lambda = (\pi, A, B)$和观测序列$\Y = (y_{o_1}, \dots, y_{o_T})$，求使得条件概率$P(\X | \Y)$最大的状态序列$\X$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;概率问题&#34;&gt;概率问题&lt;/h3&gt;
&lt;h4 id=&#34;暴力解法&#34;&gt;暴力解法&lt;/h4&gt;
&lt;p&gt;在预测问题中，最直接的想法便是首先枚举出所有的状态序列，然后根据状态序列计算观测序列的概率。某个状态序列$\X = (x_{i_1}, \dots, x_{i_T})$出现的概率为
$$
P(\X) = \pi_{i_1} a_{i_1, i_2} a_{i_2, i_3} \dots a_{i_{T-1}, i_T}
$$
给定这个状态序列$\X$的情况下，观测序列$\Y$出现的概率为
$$
P(\Y | \X) = b_{i_1, o_1} b_{i_2, o_2} \dots b_{i_T, o_T}
$$
两者同时出现的联合概率为
$$
P(\X, \Y) = P(\X) \times P(\Y | \X) = \pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \dots a_{i_{T-1}, i_T} b_{i_T, o_T}
$$
然后对上式关于所有可能的状态序列求和，得到
$$
P(\Y) = \sum_\X P(\X, \Y) = \sum_{i_1, i_2, \dots, i_T} \pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \dots a_{i_{T-1}, i_T} b_{i_T, o_T}
$$
该式共有$N^T$项，每一项有$O(T)$次相乘，故整体复杂度为$O(T N^T)$，难以接受。&lt;/p&gt;
&lt;h4 id=&#34;前向算法&#34;&gt;前向算法&lt;/h4&gt;
&lt;p&gt;首先引入前向概率的概念。给定隐马尔可夫模型$\lambda$及观测序列$\Y = (y_{o_1}, \dots, y_{o_T})$，定义到时刻$t$的观测序列为给定的$(y_{o_1}, \dots, y_{o_t})$且此时状态为$x_{i}$的概率为前向概率，记作
$$
\alpha_t(i) = P(Y_1 = y_{o_1}, \dots, Y_T = y_{o_t}, X_T = x_i; \lambda)
$$
初始情况下，即$t=1$时，有
$$
\alpha_1(i) = \pi_i a_{i, o_1}
$$
对$t = 2, \dots, T$，有
$$
\alpha_t(i) = \left[ \sum_{j=1}^N \alpha_{t-1}(j) a_{j, i} \right] b_{i, o_t}
$$
最终，
$$
P(\Y) = \sum_{i=1}^N \alpha_T(i)
$$
前向算法主要利用了状态变化的序列结构，记录了状态变化的中间过程，从而节省了计算时间。整个算法最耗时的步骤为递推这一步，$\alpha_t(i)$对应的加和表达式包含了$O(N)$个项，而对于每一个时刻$t$，有$N$个这样的加和表达式，故$T$个时刻整体耗时为$O(N^2 T)$。&lt;/p&gt;
&lt;h3 id=&#34;学习问题&#34;&gt;学习问题&lt;/h3&gt;
&lt;h4 id=&#34;有监督学习&#34;&gt;有监督学习&lt;/h4&gt;
&lt;p&gt;有监督学习是较好处理的一种问题，在这种情况下，$\X, \Y$都已知，需要我们估计模型参数$\pi, A, B$。&lt;/p&gt;
&lt;p&gt;设样本中状态由$i$转移到$j$的频数为$A_{i, j}$，则$a_{i, j}$的估计为
$$
\hat a_{i, j} = \frac{A_{i, j}} {\sum_{k=1}^N A_{i, k}}
$$
设样本中状态为$i$且观测为$j$的频数是$B_{i, j}$，则$b_{i, j}$的估计为
$$
\hat b_{i, j} = \frac{B_{i, j}} {\sum_{k=1}^N B_{i, k}}
$$
至于初始状态向量$\pi$，若样本中由多条时序链，$\pi$亦可由相应频次估计而来。&lt;/p&gt;
&lt;h4 id=&#34;无监督学习&#34;&gt;无监督学习&lt;/h4&gt;
&lt;p&gt;多数情况下，状态——或者说隐变量$\X$——是没有办法观测到的，而这时就可以采用&lt;a href=&#34;https://chunxy.github.io/notes/articles/optimization/expectation-maximization/&#34;&gt;EM算法&lt;/a&gt;，通过优化似然函数的下界，找出最好的模型参数。EM算法在隐马尔科夫模型中具体实现由Baum和Welch提出，故实际求解算法被称作Baum-Welch算法。&lt;/p&gt;
&lt;p&gt;此问题的似然函数以及对数似然函数分别为：
$$
\begin{aligned}
P(\Y; \lambda) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda) \
\log P(\Y; \lambda) &amp;amp;= \log \sum_{\X} P(\X, \Y; \lambda)
\end{aligned}
$$
根据EM算法，
$$
\begin{aligned}
&amp;amp;\log P(\Y; \lambda) = \log \E_{\X \sim P(\cdot | \Y; \lambda^t)} P(\X, \Y; \lambda) \
&amp;amp;\ge \E_{\X \sim P(\cdot | \Y; \lambda^t)} \log P(\X, \Y; \lambda) \triangleq Q(\lambda^t, \lambda) \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;而下一轮迭代中，新的估计$\lambda^{t+1}$为：
$$
\lambda^{t+1} = \arg \max_\lambda Q(\lambda^t, \lambda)
$$&lt;/p&gt;
&lt;p&gt;又由于
$$
\begin{aligned}
&amp;amp;Q(\lambda^t, \lambda) = \E_{\X \sim P(\cdot | \Y; \lambda^t)} \log P(\X, \Y; \lambda) \
&amp;amp;= \sum_\X P(\X | \Y; \lambda^t) \log P(\X, \Y; \lambda) \
&amp;amp;= \sum_\X \frac{P(\X, \Y; \lambda^t)}{P(\Y; \lambda^t)} \log P(\X, \Y; \lambda) \
&amp;amp;= \frac{\sum_\X P(\X, \Y; \lambda^t) \log P(\X, \Y; \lambda)}{P(\Y; \lambda^t)} \
\end{aligned}
$$
其中的$\frac{1}{P(\Y; \lambda^t)}$与$\lambda$无关，所以
$$
\lambda^{t+1} = \arg \max_\lambda \sum_\X P(\X, \Y; \lambda^t) \log P(\X, \Y; \lambda)
$$&lt;/p&gt;
&lt;p&gt;注意在接下来的讨论中，我们会令$\X = (x_{i_1}, \dots, x_{i_T})$，但为了整体简洁，我们不会在每一个$\X$出现的地方将此式展开。对上式做进一步展开，
$$
\begin{aligned}
&amp;amp;\sum_\X P(\X, \Y; \lambda^t) \log P(\X, \Y; \lambda) = \sum_\X P(\X, \Y; \lambda^t) \log \pi_{i_1} a_{i_1, i_2} b_{i_1, o_1} \dots a_{i_{T-1}, i_T} b_{i_T, o_T} \
&amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \log \pi_{i_1} + \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^{T-1} a_{i_t, i_{t+1}} + \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^T \log b_{i_t, o_t}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;我们可以对其中的三项分别最大化，以达到整体最大化。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;注意到$\pi$满足约束条件$\sum_{j=1}^N \pi_j = 1$，写出其拉格朗日函数：
$$
\begin{aligned}
L_\pi (\pi, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \log \pi_{i_1} + \gamma (\sum_{j=1}^N \pi_j - 1) \
&amp;amp;= \sum_{j=1}^N P(\Y, i_1 = j; \lambda^t) \log \pi_{j} + \gamma (\sum_{j=1}^N \pi_j - 1)
\end{aligned}
$$
对上式关于$\pi$求导并令其为$0$，得到对于任意$j = 1, \dots, N$，
$$
\begin{align}
\frac{P(\Y, i_1 = j; \lambda^t)}{\pi_j} + \gamma &amp;amp;= 0 \
P(\Y, i_1 = j; \lambda^t) + \pi_j \gamma &amp;amp;= 0 \label{pi-eq}
\end{align}
$$
对上式关于$j$求和，得到
$$
\sum_{j=1}^N P(\Y, i_1 = j; \lambda^t) + \sum_{j=1}^N \pi_j \gamma = 0 \
\gamma = -P(\Y; \lambda^t)
$$
重新代入$\eqref{pi-eq}$得到
$$
\pi_j = \frac{P(\Y, i_1=j; \lambda^t)}{P(\Y; \lambda^t)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;注意到$A$满足约束条件$\forall j = 1, \dots, N, \sum_{k=1}^N a_{j, k} = 1$，写出其拉格朗日函数：
$$
\begin{aligned}
L_A (A, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^{T-1} \log a_{i_t, i_{t+1}} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^N a_{j, k} - 1) \
&amp;amp;= \sum_{j=1}^N \sum_{k=1}^N \sum_{t=1}^{T-1} P(\Y, i_1=j, i_2=k; \lambda^t) \log a_{j, k} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^N a_{j, k} - 1)
\end{aligned}
$$
类似对$\pi$的求解，我们可以得到
$$
a_{j, k} = \frac{\sum_{t=1}^{T-1} P(\Y, i_1=j, i_2=k; \lambda^t)}{\sum_{t=1}^{T-1} P(\Y, i_1=j; \lambda^t)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;注意到$B$满足约束条件$\forall j = 1, \dots, N, \sum_{k=1}^M b_{j, k} = 1$，写出其拉格朗日函数：
$$
\begin{aligned}
L_B (B, \gamma) &amp;amp;= \sum_{\X} P(\X, \Y; \lambda^t) \sum_{t=1}^T \log b_{i_t, o_t} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^M b_{j, k} - 1) \
&amp;amp;= \sum_{j=1}^N \sum_{t=1}^T P(\Y, i_t=j; \lambda^t) \log b_{j, o_t} + \sum_{j=1}^N \gamma_j (\sum_{k=1}^M b_{j, k} - 1)
\end{aligned}
$$
类似对$\pi$的求解，我们可以得到
$$
b_{j, k} = \frac{\sum_{t=1}^T P(\Y, i_t=j; \lambda^t) \mathbb{I}[o_t = y_k]}{\sum_{t=1}^T P(\Y, i_t=j; \lambda^t)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;预测问题&#34;&gt;预测问题&lt;/h3&gt;
&lt;p&gt;预测问题常用的算法是Viterbi算法，它是通过动态规划求解出一条最优路径。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/positive-semi-definite-matrix/</guid>
      <description>&lt;p&gt;Positive semi-definite matrix involves many concepts like quadratic form, &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/&#34;&gt;real symmetric matrix&lt;/a&gt; and &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/&#34;&gt;singular value decomposition&lt;/a&gt;. It can be quite helpful to glue these things together here.&lt;/p&gt;
&lt;h2 id=&#34;quadratic-form&#34;&gt;Quadratic Form&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;quadratic function&lt;/strong&gt; $f$ of $n$ variables, or say a vector $\x$ of length $n$, is the sum of second-order terms:
$$
f(\x) = \sum_{i=1}^n \sum_{j=1}^n c_{ij} x_i x_j
$$&lt;/p&gt;
&lt;p&gt;The above summation can be simplified as matrix product $\x^T A \x$ where $A$ is $n \times n$ and $a_{ij} = c_{ij}$. $\x^T A \x$ is called the &lt;strong&gt;quadratic form&lt;/strong&gt; of $A$.&lt;/p&gt;
&lt;p&gt;In essence, there is a nicer formulation for $A$. Firstly define the $n \times n$ matrix $A$ such that $a_{ij} = \frac{1}{2} (c_{ij} + c_{ji})$. It suffices to show $A$ is a real symmetric matrix and
$$
f(\x) = \x^T A \x
$$&lt;/p&gt;
&lt;p&gt;Thus the discussion of quadratic form usually encompasses the real symmetric matrix.&lt;/p&gt;
&lt;h2 id=&#34;positive-semi-definiteness&#34;&gt;Positive Semi-definiteness&lt;/h2&gt;
&lt;p&gt;Let $A$ be a real symmetric matrix. $A$ is &lt;strong&gt;positive definite&lt;/strong&gt; if and only if the quadratic form of $A$ is positive. Specifically, for every $\x \ne 0$, $\x^T A \x &amp;gt; 0$. $A$ is &lt;strong&gt;positive semi-definite&lt;/strong&gt; if and only if the quadratic form of $A$ is non-negative. Specifically, for every $\x \ne 0$, $\x^T A \x \ge 0$.&lt;/p&gt;
&lt;p&gt;One possible reason why we would like to define positive definiteness on real symmetric matrix is that, any real matrix can be easily decomposed into the addition of a real symmetric matrix and a real &lt;strong&gt;skew-symmetric&lt;/strong&gt; matrix whose quadratic form is zero:
$$
A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
$$
Since $\frac{A - A^T}{2}$&amp;rsquo;s quadratic form is zero and it makes no contribution to $A$&amp;rsquo;s, the only component of interest will be the real symmetric $\frac{A + A^T}{2}$. So why not just focus on the real symmetric matrix?&lt;/p&gt;
&lt;p&gt;Note that there is also &amp;ldquo;PSD&amp;rdquo; matrix that is not symmetric. It is not very easy to find a such matrix. As a guideline, drop the idea to find such a matrix whose eigenvalues are all real. But for an example,
$$
\x^T \begin{bmatrix}
1 &amp;amp; 1 \
-1 &amp;amp; 1
\end{bmatrix}
\x = x_1^2 + x_2^2 \ge 0
$$&lt;/p&gt;
&lt;p&gt;Interestingly, the real part of such matrix&amp;rsquo;s eigenvalues must be positive. Refer to &lt;a href=&#34;https://math.stackexchange.com/a/325412&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;psd-and-eigenvalues&#34;&gt;PSD and Eigenvalues&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A real symmetric matrix $A$ is positive semi-definite if and only if $A$&amp;rsquo;s eigenvalues are non-negative.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For every $A$&amp;rsquo;s eigenpair $(\lambda, \v)$, we have $\v^T A \v = \lambda \v^T \v \ge 0 \Rightarrow \lambda \ge 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Take $A$&amp;rsquo;s spectral decomposition as $A = Q \Lambda Q^T$ where $Q Q^T = I$. For every $\x &amp;gt; 0$, we have
$$
\x^T A \x = \overbrace{\x^T Q}^{y^T} \Lambda \overbrace{Q^T \x}^{y} \ge 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similarly we have that a real symmetric matrix $A$ is positive definite if and only if $A$&amp;rsquo;s eigenvalues are positive. Then it follows that a positive definite matrix is invertible, because its eigenvalues are positive and thus its determinant (product of eigenvalues) is positive.&lt;/p&gt;
&lt;h3 id=&#34;non-negative-diagonals&#34;&gt;Non-negative Diagonals&lt;/h3&gt;
&lt;p&gt;The diagonal entries of a PSD matrix $A$ must be non-negative. This is because for the basis vector $e_i = [0, \dots, \underset{i\text{-th}}{1}, \dots, 0]$, we have $e_i^T A e_i = a_{ii} \ge 0$. Furthermore, a PSD matrix is PD if and only if its main diagonal entries are positive.&lt;/p&gt;
&lt;p&gt;Specifically, if one diagonal entry of $A$ is zero, then the row and the column to which this diagonal entry belongs to must be zero.&lt;/p&gt;
&lt;p&gt;To show it, we firstly argue that $a_{ij}^2 \le a_{ii} a_{jj}$. For every $\lambda \in \R$, we have
$$
\begin{gathered}
(e_i + \lambda e_j)^T A (e_i + \lambda e_j) = e_i^T A e_i + \lambda (e_i^T A e_j + e_j^T A e_i) + \lambda^2 e_j^T A e_j \
= a_{ii} + 2 a_{ij} \lambda + a_{jj} \lambda^2 \ge 0
\end{gathered}
$$
Note the formula above is a quadratic function in $\lambda$. This quadratic form has at most one real root. Hence, $4 a_{ij}^2 \le 4 a_{ii} a_{jj} \Rightarrow a_{ij}^2 \le a_{ii} a_{jj}$. Therefore, if $j$-th diagonal entry of $A$ is zero, for any $i = 1,\dots,n$, we have $a_{ij}^2 \le 0 \Rightarrow a_{ij} = 0$.&lt;/p&gt;
&lt;p&gt;Another implication is that any $2 \times 2$ submatrix $\begin{bmatrix} a_{ii} &amp;amp; a_{ij} \ a_{ji} &amp;amp; a_{jj} \end{bmatrix}$ obtained from $A$ is always PSD. In fact, any submatrix obtained by removing the row and column of one of the main diagonal entry of a PSD matrix is PSD. Let $A_{kk}$ be the matrix obtained by removing $A$&amp;rsquo;s $k$-th row and column. To show it, let $\x \in \R^n$ and let $\bar \x = \x$ except that $\bar \x_k = 0$. $A_{kk}$&amp;rsquo;s quadratic form can be written as
$$
\sum_{i=1, i \ne k}^{n} \sum_{j=1, j \ne k}^n a_{ij} x_i x_j = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j \mathbb{1}[i,j \ne k] = \bar \x^T A \bar x \ge 0
$$&lt;/p&gt;
&lt;h3 id=&#34;decomposition&#34;&gt;Decomposition&lt;/h3&gt;
&lt;h4 id=&#34;definitive-decomposition&#34;&gt;Definitive Decomposition&lt;/h4&gt;
&lt;p&gt;A positive semi-definite matrix $A$ can be decomposed into the product of a square matrix $Q$ and this matrix&amp;rsquo;s transpose $Q^T$. In fact, a real symmetric matrix is positive semi-definite if and only if it can be decomposed this way.&lt;/p&gt;
&lt;p&gt;Consider that a real symmetric matrix can be orthogonally diagonalized:
$$
A = P \Lambda P^T
$$
When $A$ is PSD, its eigenvalues are all non-negative. Thus, $\Lambda$ can be written as $\Lambda^{1/2} \Lambda^{1/2}$ and
$$
\begin{aligned}
&amp;amp;A = P (\Lambda^{1/2} \Lambda^{1/2}) P^T \
&amp;amp;= P \Lambda^{1/2} (\Lambda^{1/2})^T P^T \
&amp;amp;= \underbrace{(P \Lambda^{1/2})}&lt;em&gt;{Q} \underbrace{(P \Lambda^{1/2})^T}&lt;/em&gt;{Q^T}
\end{aligned}
$$
$A$ is positive definite if and only if $Q$ is invertible.&lt;/p&gt;
&lt;h4 id=&#34;square-root&#34;&gt;Square Root&lt;/h4&gt;
&lt;p&gt;A real symmetric matrix $A$ is PSD if and only if there is a PSD matrix $B$ satisfying that $A = BB$. This $B$ is unique and is called &lt;strong&gt;non-negative square root&lt;/strong&gt; of $A$ (there are non-PSD matrix whose square also equals $A$). $B$ is usually denoted as $A^{1/2}$.&lt;/p&gt;
&lt;p&gt;Consider that a real symmetric matrix $A$ can be orthogonally diagonalized as $A = P \Lambda P^T$. Setting $B = P \Lambda^{1/2} P^T$ would give the non-negative square root of $A$.&lt;/p&gt;
&lt;p&gt;Note that this cannot be naively extrapolated to that &amp;ldquo;a real symmetric matrix can be decomposed (not necessarily PSD) into two identical symmetric matrices (not necessarily PSD)&amp;rdquo;. Just consider that, the product of two identical symmetric matrices are positive semi-definite; but not all real symmetric matrices are positive semi-definite.&lt;/p&gt;
&lt;p&gt;In fact, any &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt; matrix can have its square root, though not necessarily a PSD one.&lt;/p&gt;
&lt;h4 id=&#34;cholesky-decomposition&#34;&gt;Cholesky Decomposition&lt;/h4&gt;
&lt;p&gt;A PSD matrix $A$ can be written as $A = L L^T$, where $L$ is a lower-triangular matrix with non-negative diagonal. If $A$ is positive definite, then the diagonal of $L$ is positive and the Cholesky decomposition is unique.&lt;/p&gt;
&lt;h3 id=&#34;schur-complement&#34;&gt;Schur Complement&lt;/h3&gt;
&lt;p&gt;By applying Schur complement in PSD matrix, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix}
A &amp;amp; B \
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}&lt;em&gt;+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}&lt;/em&gt;+^m, C \in \mathcal{S}&lt;em&gt;+^n, \
A - B C^{-1} B^T \in \mathcal{S}&lt;/em&gt;+^m \text{ or } C - B^T A^{-1} B \in \mathcal{S}_+^n
\end{gathered}
$$&lt;/p&gt;
&lt;h3 id=&#34;sylvesters-condition&#34;&gt;Sylvester&amp;rsquo;s Condition&lt;/h3&gt;
&lt;p&gt;Sylvester&amp;rsquo;s criterion states that a $n \times n$ real symmetric matrix $M$ is positive-definite if and only if all the following matrices have a positive determinant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the upper left 1-by-1 corner of $M$,&lt;/li&gt;
&lt;li&gt;the upper left 2-by-2 corner of $M$,&lt;/li&gt;
&lt;li&gt;the upper left 3-by-3 corner of $M$,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;$M$ itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://adityam.github.io/stochastic-control/linear-algebra/postive-definite-matrix.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course Notes - 29  Positive definite matrices (adityam.github.io)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/optimization/convex-conjugate/</guid>
      <description>&lt;h2 id=&#34;convex-conjugate&#34;&gt;Convex Conjugate&lt;/h2&gt;
&lt;p&gt;For a convex function $f$, its &lt;strong&gt;convex conjugate&lt;/strong&gt; $f^&lt;em&gt;$ is defined as
$$
f^&lt;/em&gt;(t) = \sup_x [x^T \cdot t - f(x)]
$$
By definition, a Convex Conjugate pair $(f,f^&lt;em&gt;)$ has the following property:
$$
f(x) + f^&lt;/em&gt;(t) \ge x^T \cdot t
$$
As a conjugate, $f^{&lt;strong&gt;} = f$:
$$
\begin{aligned}
f^{&lt;/strong&gt;}(t) &amp;amp;= \sup_x [x^T \cdot t - f^*(x)] \
&amp;amp;= \sup_x [x^T \cdot t - \sup_y [y^T \cdot x - f(y)]] \
&amp;amp;= \sup_x [x^T \cdot t + \inf_y [f(y) - y^T \cdot x]] \
&amp;amp;= \sup_x \inf_y [x^T (t-y) + f(y)]	\
&amp;amp;\Downarrow_\text{Mini-max Theorem} \
&amp;amp;= \inf_y \sup_x [x^T (t-y) + f(y)]
\end{aligned}
$$
The above reaches the infimum only if $y=t$. Otherwise, $\sup_x [x^T (t-y) + f(y)]$ can make it to infinity. Therefore,
$$
f^{**}(t) = \inf_y \sup_x [f(t)] = f(t)
$$
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_conjugate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wiki&lt;/a&gt; || &lt;a href=&#34;https://zhuanlan.zhihu.com/p/188702379&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;凸优化-凸共轭&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/introduction/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point (Claude Shannon, 1948).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similar to that stated above, the fundamental subject of machine learning is about with best effort recovering the original data using the so-called model or algorithm.&lt;/p&gt;
&lt;p&gt;The recovery can be done by memoizing. In communication system, this is exactly what we want: we need to try our best to convey the message with the achievable highest fidelity. But it may be overwhelmingly costly when the volume of data becomes larger, such as in the case faced by machine learning. Thus, a better way would be to summarizing, i.e. using a function to reveal the correlation (here I don&amp;rsquo;t mean the mathematical term correlation) among data.&lt;/p&gt;
&lt;p&gt;This book reviews the classic concepts and methods of the recovery process in communication system field. But it would also be interesting for machine learning folks, as the two fields meet the same capacity limit of the recovery.&lt;/p&gt;
&lt;h2 id=&#34;a-system-view&#34;&gt;A System View&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make the life easier by introducing a few concepts. In communication, we have &lt;strong&gt;source&lt;/strong&gt; message to convey. After being passed to and encoded by the &lt;strong&gt;encoder&lt;/strong&gt;, the message travels along the &lt;strong&gt;channel&lt;/strong&gt; to be finally decoded by the &lt;strong&gt;decoder&lt;/strong&gt; and received by the receiver.&lt;/p&gt;
&lt;p&gt;The encoding process includes transforming the data into digital form and introducing redundant information for the actual transmission. The redundancy is necessary to keep the underlying data from the &lt;strong&gt;noisy&lt;/strong&gt; channel, which may alter bits, or add/drop bits (these two cases are very rare though) to the bit flow transmitting on it.&lt;/p&gt;
&lt;p&gt;Information theory is concerned with the theoretical limit and potential for such systems. The coding theory is concerned with designing such a coding theme as to reach the limit/potential.&lt;/p&gt;
&lt;h2 id=&#34;error-correcting-codes&#34;&gt;Error-correcting Codes&lt;/h2&gt;
&lt;p&gt;By the name of &amp;ldquo;error-correcting&amp;rdquo;, we mean that we want to be able to detect and correct errors; the retransmission is not an option.&lt;/p&gt;
&lt;p&gt;The way we do this, as stated above, is to add redundancy. But that the redundancy needs to be cost-effective. The &lt;strong&gt;rate&lt;/strong&gt; is the ratio between the amount of source data and that of the transmitted data; the higher the better.&lt;/p&gt;
&lt;p&gt;There are some examples of coding scheme in this genre:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repetition code&lt;/li&gt;
&lt;li&gt;block code
&lt;ul&gt;
&lt;li&gt;Hamming code&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-best-performance&#34;&gt;The Best Performance?&lt;/h2&gt;
&lt;p&gt;There seems to be a tradeoff between the error probability (which we want to minimize) and the rate (which we want to maximize), which can be seen mathematically in the case of repetition code and Hamming code.&lt;/p&gt;
&lt;p&gt;It seems that the error-rate curve, no matter for which kind of coding scheme, would pass the $(0, 0)$ point: in order to achieve a vanishingly small error probability, one would have to reduce the rate correspondingly to zero.&lt;/p&gt;
&lt;p&gt;However, Shannon proved that this curve may intersect with the rate axis at some nonzero point! And this maximum rate at which the communication is possible with arbitrarily small error probability is called the &lt;strong&gt;capacity&lt;/strong&gt; of the channel. The capacity is only related to the noise level of this channel. For any noisy binary symmetric channel with noise level of $f$ (i.e., the probability that a bit is flipped), its capacity is
$$
C(f) = 1 - H_2(f) = 1 - \left[ f \log \frac{1}{f} + (1-f) \log \frac{1}{1-f} \right]
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/symbol-code/</guid>
      <description>&lt;p&gt;Other than block code where symbols are encoded in chunks, symbol code will assign each symbol a unique codeword. Among the codeword schemes, we prefer those where no codeword is a prefix of any other codeword. These are called &lt;strong&gt;prefix code&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem is how to give such scheme, or does it really exist?&lt;/p&gt;
&lt;h2 id=&#34;kraft-mcmillan-inequality&#34;&gt;Kraft-McMillan Inequality&lt;/h2&gt;
&lt;p&gt;Kraft-McMillan inequality reveals the relation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Denote the length of each symbol code as $l_i$ and suppose there are $I$ symbols. If $\sum_{i=1}^I 2^{-l_i} \le 1$, then there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes&amp;rsquo; lengths.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The proof is done by construction. The number of codes of length $l$ should be less than $2^{l}$, or else the inequality will be violated. Therefore $\forall l = 0,1,\dots$, we can loosely arrange all the codes of length $l$ to be unique (since there are $2^l$ many distinct bit strings of length $l$). Then the uniqueness condition is checked.&lt;/p&gt;
&lt;p&gt;Denote the number of codes of length $l$ by $C_l$. For any two consecutive lengths $l$ and $(l+1)$, we have
$$
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;amp;\le\sum_{i=1}^I 2^{-l_i}\le 1 \
C_{l+1} &amp;amp;\le 2^{l+1} - 2 C_l \
C_{l+1} &amp;amp;\le 2(2^l - C_l) \
\end{aligned}
$$
This means we can append these unused $(2^l - C_l)$ codes of length $l$ with $0$ and $1$ to suit the number of codes of length $l+1$. Construction completes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose we have a set of uniquely-decodable prefix coding. Denote the length of each symbol code as $l_i$ and there are $I$ symbols. Then,
$$
\sum_{i=1}^I 2^{-l_i} \le 1
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let $S = \sum_{i=1}^I 2^{-l_i} \le 1$, then,
$$
\begin{aligned}
S^N &amp;amp;= (\sum_{i=1}^I 2^{-l_i})^N \
&amp;amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots + l_{i_N})}
\end{aligned}
$$
The $(l_{i_1} + \dots + l_{i_N})$ term can be treated as the length of encoding of $a_{i_1} \dots a_{i_N}$ of arbitrary length $N$. Let $l_\min = \min_i l_i, l_\max = \max_i l_i$, the above can be re-written as
$$
S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
$$
where $C_l$ represents the number of symbol codes of length $l$. Since the coding is uniquely-decodable, $C_l \le 2^l$. Therefore,
$$
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
$$
If $S &amp;gt; 1$, the above cannot hold for arbitrary $N$. Therefore $S \le 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;source-coding-theorem-for-symbol-code&#34;&gt;Source Coding Theorem for Symbol Code&lt;/h2&gt;
&lt;p&gt;For an ensemble $X$, there exists a prefix code $C$ with expected length satisfying
$$
H(X) \le L(C,X) &amp;lt; H(X) + 1
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We define the implicit probabilities $q_i = 2^{-l_i} / z$ where $z = \sum_i 2^{-l_i}$. Then,
$$
\begin{aligned}
L(C,X) &amp;amp;= \sum_i p_i l_i = -\sum_i [p_i \log (q_iz)] \
&amp;amp;=\sum_i [p_i \log 1/q_i] - \log z \
&amp;amp;\ge H(X)
\end{aligned}
$$
The equality holds when $z = 1$ (the code is complete) and $q = p$ ($l_i = \log 1/p_i$).&lt;/p&gt;
&lt;p&gt;From another perspective, suppose the coding is complete but not optimal,
$$
\begin{aligned}
L(C,X) &amp;amp;= -\sum_i [p_i \log (q_iz)] = -\sum_i [p_i \log (q_i)] \
&amp;amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i \log (q_i)] \
&amp;amp;= H(X) + D_{KL}(p || q)
\end{aligned}
$$
where the cost is the extra $D_{KL}(p || q)$ bits, which is brought by instead treating $q$ as the real distribution. $D_{KL(p||q)}$ is termed as relative entropy or the &lt;a href=&#34;https://chunxy.github.io/notes/articles/information-theory/kl-divergence/&#34;&gt;KL-divergence&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
