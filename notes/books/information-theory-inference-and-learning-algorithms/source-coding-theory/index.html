<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="Notations and Concepts An ensemble \(X\) is a triple \((X, \newcommand{A}{\mathcal A} \newcommand{P}{\mathcal P} \A_X, \P_X)\), where \(x\) is the outcome of a random variable, which takes on values from" />

  
  <link rel="alternate" hreflang="en-us" href="https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.84ebe1e3608d6fadc06cb4d7207008ff.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=G-J44SJXJTFD"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-J44SJXJTFD', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huc0707d156b6b3b9945e544e63d06d5e5_16450_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huc0707d156b6b3b9945e544e63d06d5e5_16450_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Chunxy&#39; Website" />
  <meta property="og:url" content="https://chunxy.github.io/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/" />
  <meta property="og:title" content="Source Coding Theory | Chunxy&#39; Website" />
  <meta property="og:description" content="Notations and Concepts An ensemble \(X\) is a triple \((X, \newcommand{A}{\mathcal A} \newcommand{P}{\mathcal P} \A_X, \P_X)\), where \(x\) is the outcome of a random variable, which takes on values from" /><meta property="og:image" content="https://chunxy.github.io/media/sharing.png" />
    <meta property="twitter:image" content="https://chunxy.github.io/media/sharing.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="article:published_time" content="2022-07-07T11:06:13&#43;00:00" />
    
    <meta property="article:modified_time" content="2022-07-07T11:06:13&#43;00:00">
  

  



  

  

  

  <title>Source Coding Theory | Chunxy&#39; Website</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="fd99477a66067050df822acb9bb2f88b" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Chunxy&#39; Website</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Chunxy&#39; Website</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/blogs/"><span>Blogs</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link  active" href="/notes/"><span>Notes</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    




<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      <form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <div class="d-flex">
      <span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">
        
        
          Information Theory, Inference and Learning Algorithms
        
      </span>
      <span><i class="fas fa-chevron-down"></i></span>
    </div>
  </button>

  
  <button class="form-control sidebar-search js-search d-none d-md-flex">
    <i class="fas fa-search pr-2"></i>
    <span class="sidebar-search-text">Search...</span>
    <span class="sidebar-search-shortcut">/</span>
  </button>
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      


  
    
    
    
    
      
    
    

    
      <ul class="nav docs-sidenav">
        <li class=""><a href="/notes/">Notes</a></li>
    
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/"><i class="far fa-file-lines pr-1"></i>Articles</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/information-theory/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">Information Theory</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/information-theory/entropy/">Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/conditional-entropy/">Conditional Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/cross-entropy/">Cross Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/mutual-information/">Mutual Information</a></li>



  <li class=""><a href="/notes/articles/information-theory/kl-divergence/">KL-divergence</a></li>



  <li class=""><a href="/notes/articles/information-theory/f-divergence/">f-divergence</a></li>



  <li class=""><a href="/notes/articles/information-theory/jenson-shannon-divergence/">Jenson-Shannon Divergence</a></li>



  <li class=""><a href="/notes/articles/information-theory/overview/">Overview</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/machine-learning/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">Machine Learning</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/machine-learning/machine-learning-bullet-points/">Machine Learning Bullet Points</a></li>



  <li class=""><a href="/notes/articles/machine-learning/linear-discriminant-analysis/">Linear Discriminant Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/logistic-regression/">Logistic Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/support-vector-machine/">Support Vector Machine</a></li>



  <li class=""><a href="/notes/articles/machine-learning/linear-regression/">Linear Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/non-linear-regression/">Non-linear Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/clustering/">Clustering</a></li>



  <li class=""><a href="/notes/articles/machine-learning/dimensionality-reduction/">Dimension Reduction</a></li>



  <li class=""><a href="/notes/articles/machine-learning/principal-component-analysis/">Principal Component Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/eckart-young-mirsky-theorem/">Eckart-Young-Mirsky Theorem</a></li>



  <li class=""><a href="/notes/articles/machine-learning/independent-component-analysis/">Independent Component Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/ransac/">RANSAC</a></li>



  <li class=""><a href="/notes/articles/machine-learning/fishers-linear-discriminant/">Fisher&#39;s Linear Discriminant</a></li>



  <li class=""><a href="/notes/articles/machine-learning/bias-variance-decomposition/">Bias-variance Decomposition</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">Mathematics</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/calculus/"><img src="/media/icons/header2.png" alt="header2.png" class="svg-icon svg-baseline pr-1">Calculus</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/calculus/jacobian-matrix/">Jacobian Matrix</a></li>



  <li class=""><a href="/notes/articles/mathematics/calculus/spherical-coordinates/">Spherical Coordinates</a></li>



  <li class=""><a href="/notes/articles/mathematics/calculus/lipschitz-continuity/">Lipschitz Continuity</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/linear-algebra/"><img src="/media/icons/header2.png" alt="header2.png" class="svg-icon svg-baseline pr-1">Linear Algebra</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/">Eigenvectors and Eigenvalues</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/">Real Symmetric Matrix</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/singular-value-decomposition/">Singular Value Decomposition</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/matrix-identity/">Matrix Identity</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/difference-equation/">Difference Equation</a></li>



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/linear-algebra/metrics/"><img src="/media/icons/header3.png" alt="header3.png" class="svg-icon svg-baseline pr-1">Metrics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/">Spectral Normalization</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/">Frobenius Normalization</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/metrics/chebyshev-distance/">Chebyshev Distance</a></li>

      
        </ul>
      
    

    
      </div>
    



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/laplace-expansion/"></a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/numerical-analysis/"><img src="/media/icons/header2.png" alt="header2.png" class="svg-icon svg-baseline pr-1">Numerical Analysis</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/numerical-analysis/fourier-transform/">Fourier Transform</a></li>



  <li class=""><a href="/notes/articles/mathematics/numerical-analysis/stirlings-approximation/">Stirling&#39;s Approximation</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/probability-and-statistics/"><img src="/media/icons/header2.png" alt="header2.png" class="svg-icon svg-baseline pr-1">Probability and Statistics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/probability-and-statistics/gaussian-distribution/">Gaussian Distribution</a></li>



  <li class=""><a href="/notes/articles/mathematics/probability-and-statistics/unconscious-statistics/">Unconscious Statistics</a></li>



  <li class=""><a href="/notes/articles/mathematics/probability-and-statistics/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B/">随机变量的收敛</a></li>



  <li class=""><a href="/notes/articles/mathematics/probability-and-statistics/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/">特征函数</a></li>



  <li class=""><a href="/notes/articles/mathematics/probability-and-statistics/whitening/">Whitening</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/optimization/"><img src="/media/icons/header2.png" alt="header2.png" class="svg-icon svg-baseline pr-1">Optimization</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/optimization/lagrange-multiplier/">Lagrange Multiplier</a></li>



  <li class=""><a href="/notes/articles/optimization/convex-optimization/">Convex Optimization</a></li>



  <li class=""><a href="/notes/articles/optimization/gradient-descent/">Gradient Descent</a></li>



  <li class=""><a href="/notes/articles/optimization/coordinate-descent/">Coordinate Descent</a></li>



  <li class=""><a href="/notes/articles/optimization/expectation-maximization/">Expectation Maximization</a></li>



  <li class=""><a href="/notes/articles/optimization/subgradient/">Subgradient</a></li>



  <li class=""><a href="/notes/articles/optimization/least-angle-regression/">Least Angle Regression</a></li>



  <li class=""><a href="/notes/articles/optimization/convex-conjugate/"></a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/"><i class="fas fa-book pr-1"></i>Books</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/information-theory-inference-and-learning-algorithms/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">Information Theory, Inference and Learning Algorithms</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class="active"><a href="/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/">Source Coding Theory</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/linear-algebra-and-its-applications/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">Linear Algebra and Its Applications</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/gram-schmidt-orthogonalization/">Gram-Schmidt Orthogonalization</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/least-squares/">Least Squares</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/">Orthogonality and Projection</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/">Coordinate System and Change of Basis</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"><img src="/media/icons/header1.png" alt="header1.png" class="svg-icon svg-baseline pr-1">概率论与数理统计</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E6%80%BB%E8%A7%88/">总览</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/">事件与概率</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83/">常见分布</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/">协方差与相关系数</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0/">随机变量的函数</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/">大数定律和中心极限定理</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/">三大分布与正态总体的抽样分布</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/">统计量</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/">参数估计</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/">假设检验</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/">贝叶斯推断</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/papers/"><i class="fas fa-paperclip pr-1"></i>Papers</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/papers/noise-contrastive-estimation/">Noise Contrastive Estimation</a></li>



  <li class=""><a href="/notes/papers/contrastive-predictive-coding/">Contrastive Predictive Coding</a></li>



  <li class=""><a href="/notes/papers/bounding-mutual-information/">Bounding Mutual Information</a></li>



  <li class=""><a href="/notes/papers/flatnce/">FlatNCE</a></li>

      
        </ul>
      
    

    
      </div>
    

      
    

    
      </ul>
    

  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#notations-and-concepts">Notations and Concepts</a></li>
    <li><a href="#shannons-source-coding-theorem">Shannon’s source coding theorem</a></li>
    <li><a href="#source-coding-theorem-for-symbol-codes">Source coding theorem for symbol codes</a>
      <ul>
        <li><a href="#kraft-mcmillan-inequality">Kraft-McMillan inequality</a></li>
        <li><a href="#the-source-coding-theorem">The source coding theorem</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          
            
  <nav class="d-none d-md-flex" aria-label="breadcrumb">
    <ol class="breadcrumb">
      
  
    
  
    
  
    
  
    
  

    <li class="breadcrumb-item">
      <a href="/">
        
          Home
        
      </a>
    </li>
  

    <li class="breadcrumb-item">
      <a href="/notes/">
        
          Notes
        
      </a>
    </li>
  

    <li class="breadcrumb-item">
      <a href="/notes/books/">
        
          Books
        
      </a>
    </li>
  

    <li class="breadcrumb-item">
      <a href="/notes/books/information-theory-inference-and-learning-algorithms/">
        
          Information Theory, Inference and Learning Algorithms
        
      </a>
    </li>
  

      <li class="breadcrumb-item active" aria-current="page">
        Source Coding Theory
      </li>
    </ol>
  </nav>




          
        </div>

        
        

        <div class="docs-article-container">
          <h1>Source Coding Theory</h1>

          <div class="article-style">
            

<h2 id="notations-and-concepts">Notations and Concepts</h2>
<p>An <strong>ensemble <span class="math inline">\(X\)</span></strong> is a triple <span class="math inline">\((X, \newcommand{A}{\mathcal A} \newcommand{P}{\mathcal P} \A_X, \P_X)\)</span>, where <span class="math inline">\(x\)</span> is the outcome of a random variable, which takes on values from <span class="math inline">\(\A_X = \{a_1, a_1, \dots \}\)</span>, that has probability <span class="math inline">\(\P_X = \{p_1, p_2, \dots \}\)</span>.</p>
<p><strong>The raw bit content</strong> of <span class="math inline">\(X\)</span> is <span class="math display">\[
H_0(X) = \log |\mathcal A_X|
\]</span> <strong>The smallest <span class="math inline">\(\delta\)</span>-sufficient subset</strong> <span class="math inline">\(S_\delta\)</span> is the smallest subset of <span class="math inline">\(\mathcal A_X\)</span> satisfying <span class="math display">\[
P(X \in S_\delta) \ge 1 - \delta
\]</span> <strong>The essential bit content</strong> of <span class="math inline">\(X\)</span> is <span class="math display">\[
H_\delta(X) = \log |S_\delta|
\]</span></p>
<h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be an ensemble with entropy <span class="math inline">\(H(X) = H\)</span> bits, and <span class="math inline">\(X^N\)</span> be the ensemble composed of <span class="math inline">\(N\)</span> i.i.d. such random variables. Given <span class="math inline">\(\epsilon &gt; 0\)</span> and <span class="math inline">\(0 &lt; \delta &lt; 1\)</span>, there exists a positive integer <span class="math inline">\(N_0\)</span> such that for <span class="math inline">\(N &gt; N_0\)</span>, <span class="math display">\[
|\frac{1}{N} H_\delta (X^N) - H| &lt; \epsilon
\]</span></p>
</blockquote>
<p>Or put it in a verbal way,</p>
<blockquote>
<p><span class="math inline">\(N\)</span> i.i.d. random variables each with entropy <span class="math inline">\(H(X) = H\)</span> can be compressed into more than <span class="math inline">\(NH\)</span> bits with negligible information loss as <span class="math inline">\(N \to \infty\)</span>; conversely if they are compressed fewer than <span class="math inline">\(NH\)</span> bits, it is virtually certain that there is information loss.</p>
</blockquote>
<ul>
<li><p><strong>Proof</strong>: The random variable <span class="math inline">\(\frac 1 N \log \frac 1 {P(Y)}\)</span> is defined for the ensemble <span class="math inline">\(Y = X^N\)</span> which composes of <span class="math inline">\(N\)</span> i.i.d. <span class="math inline">\(X_1 \dots X_N\)</span>. <span class="math inline">\(\frac 1 N \log \frac 1 {P(Y)}\)</span> can be re-written as the average of <span class="math inline">\(N\)</span> information contents <span class="math inline">\(h_i = \log \frac 1 {P(X_i)}\)</span>: <span class="math display">\[
\frac 1 N \log \frac 1 {P(Y)} = \frac 1 N \log \frac 1 {P(X_1) \dots P(X_N)} = \frac 1 N (\log \frac{1}{P(X_1)} + \dots + \log \frac{1}{P(X_N)})
\]</span> Each of these information contents is in turn a random variable with mean <span class="math inline">\(\bar h_i = H(X) = H\)</span> and variance <span class="math inline">\(\sigma_{h_i}^2 = \sigma^2\)</span>. The <strong>typical set</strong> with parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(\beta\)</span> is defined as <span class="math display">\[
T_{N \beta} = \{y \in \mathcal A_X^N: [\frac 1 N \log \frac{1}{P(y)} - H]^2 &lt; \beta^2 \}
\]</span> By the <a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/#相互独立同分布大数定律">Weak Law of Large Numbers</a>, <span class="math inline">\(P((\frac 1 N \log \frac{1}{P(y)} - H)^2 \ge \beta^2) = \frac{\sigma^2}{\beta^2 N}\)</span>, and thus <span class="math display">\[
P(y \in T_{N \beta}) \ge 1 - \frac{\sigma^2}{\beta^2 N}
\]</span> As <span class="math inline">\(N\)</span> increases, the probability that <span class="math inline">\(y\)</span> falls in <span class="math inline">\(T_{N \beta}\)</span> draws near to <span class="math inline">\(1\)</span>. We need to relate this to the theorem that for any given <span class="math inline">\(\epsilon, \delta\)</span>, there is a sufficiently-large <span class="math inline">\(N\)</span> such that <span class="math inline">\(H_\delta(X^N) \simeq NH\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(H_\delta(X^N) &lt; N(H + \epsilon)\)</span></p>
<p>The set <span class="math inline">\(T_{N \beta}\)</span> is not the best sufficient subset for compression. Therefore, <span class="math inline">\(\log |T_{N \beta}|\)</span> upper-bounds the <span class="math inline">\(H_\delta(X^N)\)</span>. On the other hand, for all <span class="math inline">\(y \in T_{N \beta}\)</span>, <span class="math inline">\(2^{-N(H - \beta)} &lt; P(y) &lt; 2^{-N(H + \beta)}\)</span>, <span class="math display">\[
\begin{align*}
|T_{N \beta}| 2^{-N(H + \beta)} &amp;&lt; 1 \\
|T_{N \beta}| &amp;&lt; 2^{N(H + \beta)} \\
\end{align*}
\]</span> If we set <span class="math inline">\(\beta = \epsilon\)</span> and <span class="math inline">\(N = N_0\)</span> in a way such that <span class="math inline">\(\frac{\sigma^2}{\epsilon^2 N_0} \le \delta\)</span> and thus <span class="math inline">\(P(y \in T_{N_0 \epsilon}) \ge 1 - \delta\)</span>, <span class="math inline">\(T_{N_0 \epsilon}\)</span> is a <span class="math inline">\(\delta\)</span>-sufficient subset. Then, <span class="math inline">\(H_\delta(X^N) \le \log |T_{N_0 \epsilon}| \le N_0(H + \epsilon)\)</span> holds for any <span class="math inline">\(N \ge N_0\)</span>.</p></li>
<li><p><span class="math inline">\(H_\delta(X^N) &gt; N(H - \epsilon)\)</span></p>
<p>This part is reached by contradiction. Suppose instead there exists a <span class="math inline">\(\delta&#39;\)</span> such that there exists a sufficiently large <span class="math inline">\(N_0\)</span> which results in <span class="math inline">\(H_\delta(X^{N_0}) \le N_0(H - \epsilon)\)</span> for arbitrary <span class="math inline">\(\beta\)</span>. When <span class="math inline">\(\beta = \epsilon/2\)</span>, we have <span class="math display">\[
H_\delta(X^N) \le N_0(H - 2\beta)
\]</span> Denote the associated subset by <span class="math inline">\(S&#39;\)</span>. We are to disprove <span class="math inline">\(S&#39;\)</span> with <span class="math inline">\(|S&#39;| \le 2^{N(H - 2\beta)}\)</span> can achieve <span class="math inline">\(P(y \in S&#39;) \ge 1 - \delta\)</span>. <span class="math display">\[
P(y \in S&#39;) = P(y \in S&#39; \cap T_{N \beta}) + P(y \in S&#39; \cap \overline{T_{N \beta}})
\]</span> <span class="math inline">\(|S&#39; \cap T_{N \beta}| \le |S&#39;| \le 2^{N(H - 2\beta)}\)</span>. The maximum value of the first term is obtained when <span class="math inline">\(S&#39; \cap T_{N \beta}\)</span> contains <span class="math inline">\(2^{N(H - 2\beta)}\)</span> outcomes all with probability <span class="math inline">\(2^{-N(H - \beta)}\)</span>. <span class="math inline">\(S&#39; \cap \overline{T_{N \beta}} \subseteq \overline{T_{N \beta}}\)</span>, <span class="math inline">\(P(y \in S&#39; \cap \overline{T_{N \beta}}) \le P(y \in \overline{T_{N \beta}}) &lt; \frac{\sigma^2}{\beta^2 N}\)</span>. Therefore, <span class="math display">\[
P(y \in S&#39;) \le 2^{N(H - 2\beta)} 2^{-N(H - \beta)} + \frac{\sigma^2}{\beta^2 N} = 2^{-N \beta} + \frac{\sigma^2}{\beta^2 N}
\]</span> For arbitrary <span class="math inline">\(\delta\)</span> and a sufficiently-large <span class="math inline">\(N_0\)</span>, we can have <span class="math inline">\(P(y \in S&#39;) \le 1 - \delta\)</span> instead of <span class="math inline">\(P(y \in S&#39;) \ge 1 - \delta\)</span>. We shall conclude that <span class="math inline">\(S&#39;\)</span> with <span class="math inline">\(|S&#39;| \le 2^{N(H - 2\beta)}\)</span> cannot achieve <span class="math inline">\(P(y \in S&#39;) \ge 1 - \delta\)</span> and thus <span class="math inline">\(H_\delta(X^N) &gt; N(H - \epsilon)\)</span>.</p></li>
</ol></li>
</ul>
<h2 id="source-coding-theorem-for-symbol-codes">Source coding theorem for symbol codes</h2>
<h3 id="kraft-mcmillan-inequality">Kraft-McMillan inequality</h3>
<ol type="1">
<li><p>Denote the length of each symbol code as <span class="math inline">\(l_i\)</span> and there are <span class="math inline">\(I\)</span> symbols. If <span class="math display">\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]</span> there exists a set of uniquely-decodable prefix coding with these lengths as their symbol codes’ lengths.</p>
<ul>
<li><p><strong>Proof</strong>: The proof is done by construction. The number of codes of length <span class="math inline">\(l\)</span> should be less than <span class="math inline">\(2^{l+1}\)</span>, or else the above condition will be violated. Therefore we can loosely arrange all the codes of length <span class="math inline">\(l\)</span> to be unique. Then the uniqueness condition is checked.</p>
<p>Denote the number of codes of length <span class="math inline">\(l\)</span> by <span class="math inline">\(C_l\)</span>. For any two consecutive lengths <span class="math inline">\(l\)</span> and <span class="math inline">\(l+1\)</span>, we have <span class="math display">\[
\begin{aligned}
2^{-l} C_l + 2^{-(l+1)} C_{l+1} &amp;\le 1 \\
C_{l+1} &amp;\le 2^{l+1} - 2 C_l \\
C_{l+1} &amp;\le 2(2^l - C_l) \\
\end{aligned}
\]</span> This means we can append these unused <span class="math inline">\(2^l - C_l\)</span> codes of length <span class="math inline">\(l\)</span> with <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> to meet the number of codes of length <span class="math inline">\(l+1\)</span>. Construction completes.</p></li>
</ul></li>
<li><p>Suppose we have a set of uniquely-decodable prefix coding. Denote the length of each symbol code as <span class="math inline">\(l_i\)</span> and there are <span class="math inline">\(I\)</span> symbols. Then, <span class="math display">\[
\sum_{i=1}^I 2^{-l_i} \le 1
\]</span></p>
<ul>
<li><strong>Proof</strong>: Let <span class="math inline">\(S = \sum_{i=1}^I 2^{-l_i} \le 1\)</span>, then, <span class="math display">\[
\begin{aligned}
  S^N &amp;= (\sum_{i=1}^I 2^{-l_i})^N \\
  &amp;= \sum_{i_1=1}^I \dots \sum_{i_N=1}^I 2^{-(l_{i_1} + \dots + l_{i_N})}
  \end{aligned}
\]</span> The <span class="math inline">\((l_{i_1} + \dots + l_{i_N})\)</span> term can be treated as the length of encoding of <span class="math inline">\(a_{i_1} \dots a_{i_N}\)</span> of arbitrary length <span class="math inline">\(N\)</span>. Let <span class="math inline">\(l_\min = \min_i l_i, l_\max = \max_i l_i\)</span>, the above can be re-written as <span class="math display">\[
  S^N = \sum_{l = Nl_\min}^{Nl_\max} 2^{-l}C_l
\]</span> where <span class="math inline">\(C_l\)</span> represents the number of symbol codes of length <span class="math inline">\(l\)</span>. Since the coding is uniquely-decodable, <span class="math inline">\(C_l \le 2^l\)</span>. Therefore, <span class="math display">\[
S^N \le \sum_{l = Nl_\min}^{Nl_\max} 2^{-l} 2^l \le Nl_\max
\]</span> If <span class="math inline">\(S &gt; 1\)</span>, the above cannot hold for arbitrary <span class="math inline">\(N\)</span>. Therefore <span class="math inline">\(S \le 1\)</span>.</li>
</ul></li>
</ol>
<h3 id="the-source-coding-theorem">The source coding theorem</h3>
<p>For an ensemble <span class="math inline">\(X\)</span>, there exists a prefix code <span class="math inline">\(C\)</span> with expected length satisfying <span class="math display">\[
H(X) \le L(C,X) &lt; H(X) + 1
\]</span></p>
<ul>
<li><p><strong>Proof</strong>: We define the implicit probabilities <span class="math inline">\(q_i = 2^{-l_i} / z\)</span> where <span class="math inline">\(z = \sum_i 2^{-l_i}\)</span>. Then, <span class="math display">\[
\begin{aligned}
L(C,X) = \sum_i p_i l_i &amp;= -\sum_i [p_i \log (q_iz)] \\
&amp;=\sum_i [p_i \log 1/q_i] - \log z \\
&amp;\ge H(X)
\end{aligned}
\]</span> The equality holds when <span class="math inline">\(z = 1\)</span> (the code is complete) and <span class="math inline">\(q = p\)</span> (<span class="math inline">\(l_i = \log 1/p_i\)</span>).</p>
<p>From another perspective, suppose the coding is complete but not optimal, <span class="math display">\[
\begin{aligned}
&amp;L(C,X) = -\sum_i [p_i \log (q_iz)] = -\sum_i [p_i \log (q_i)] \\
&amp;= -\sum_i [p_i \log (p_i)] + \sum_i [p_i \log (p_i)] -\sum_i [p_i \log (q_i)] \\
&amp;= H(X) + D_{KL}(p || q)
\end{aligned}
\]</span> where the cost is the extra <span class="math inline">\(D_{KL}(p || q)\)</span> bits, which is brought by instead treating <span class="math inline">\(q\)</span> as the real distribution. <span class="math inline">\(D_{KL(p||q)}\)</span> is termed as relative entropy or the <a href="/notes/articles/information-theory/kl-divergence/">KL-divergence</a>.</p></li>
</ul>



          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on Jul 7, 2022</p>

          



          




          


        </div>

      </article>

      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Chunxy. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.53d67dc2cb1ebceb89d5e2aba2f86112.js"></script>

    
    
    
      

      
      

      

    

    
    
    

    
    
    <script src="https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":false}</script>

    
    
    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d14f720598b8ad98ae8105a0a502bab6.js"></script>

    
    
    
    
    
    






</body>
</html>
