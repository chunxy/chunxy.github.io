<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Algebra | Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/</link>
      <atom:link href="https://chunxy.github.io/notes/articles/mathematics/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <description>Linear Algebra</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Jul 2022 12:58:49 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Linear Algebra</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/</link>
    </image>
    
    <item>
      <title>Determinant</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</link>
      <pubDate>Tue, 16 May 2023 11:42:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/determinant/</guid>
      <description>

&lt;h2 id=&#34;derivation-of-determinant&#34;&gt;Derivation of Determinant&lt;/h2&gt;
&lt;p&gt;Determinant may be the most infamous concept in linear algebra, in terms of its odd definition and computation. Sometimes, we may wonder why there has to be a determinant.&lt;/p&gt;
&lt;p&gt;So instead of giving its definition directly, we first lay down some properties we expect the determinant to have. Then we try to construct the determinant from the ground up and prove its existence and uniqueness.&lt;/p&gt;
&lt;p&gt;Determinant in essence captures the volume of the parallelepiped formed by vectors of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix. When &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, it is the length; when &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, it is the area of parallelogram; when &lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt;, it is the volume of parallelepiped. What about when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; goes larger?&lt;/p&gt;
&lt;p&gt;As said, there are some basic properties that we expect the determinant (the volume) to have, and that indeed hold for cases &lt;span class=&#34;math inline&#34;&gt;\(n = 1,2,3\)&lt;/span&gt;. In the following, let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix and denote &lt;span class=&#34;math inline&#34;&gt;\(\det(\v_1, \dots, \v_n)\)&lt;/span&gt; as the determinant of the matrix formed by a system of vectors &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;basic-properties&#34;&gt;Basic properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Linearity in each argument: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\alpha \v_k + \beta \u_k}_k, \dots, \v_n) = \alpha \det(\v_1, \dots, \underset{k}{\v_k}, \dots, \v_n) + \beta \det(\v_1, \dots, \underset{k}{\u_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anti-symmetry: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underset{j}{\v_j}, \dots, \underset{k}{\v_k}, \dots, \v_n) = -\det(\v_1, \dots, \underset{j}{\v_k}, \dots, \underset{k}{\v_j}, \dots, \v_n)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normalization: &lt;span class=&#34;math display&#34;&gt;\[
\det(I) = 1
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these basic properties, we can derive some advanced properties for determinant.&lt;/p&gt;
&lt;h3 id=&#34;derived-properties&#34;&gt;Derived properties&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Preservation under column replacement: &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \underbrace{\v_j + \alpha \v_k}_{j}, \dots, \underset{k}{\v_k}, \dots, \v_n) = \det(\v_1, \dots, \underset{j}{\v_j}, \dots, \underset{k}{\v_k}, \dots, \v_n)
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;zero-determinants&#34;&gt;Zero determinants&lt;/h4&gt;
&lt;ol start=&#34;2&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has a zero column, then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has two equal columns, then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If one column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the multiple of another, then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If columns of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are linearly dependent, then &lt;span class=&#34;math inline&#34;&gt;\(\det A = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;diagonal-matrices-and-triangular-matrices&#34;&gt;Diagonal matrices and triangular matrices&lt;/h4&gt;
&lt;ol start=&#34;6&#34; type=&#34;1&#34;&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;diagonal matrix&lt;/strong&gt; equal the product of the diagonal entries.&lt;/li&gt;
&lt;li&gt;Determinant of a &lt;strong&gt;triangular matrix&lt;/strong&gt; equal the product of the diagonal entries.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;transpose-and-product&#34;&gt;Transpose and product&lt;/h4&gt;
&lt;ol start=&#34;8&#34; type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det A^T = \det A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This property implies that, all the statements above about columns, can be applied to rows. Thus, to compute the determinant of a matrix, we can apply row operations to transform it to reduced row echelon form first, and then obtain the result by computing the product of the diagonal entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\det (AB) = (\det A)(\det B)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;construction&#34;&gt;Construction&lt;/h3&gt;
&lt;p&gt;Now with these properties on hand, how can we find the definition of the determinant and how can we know that the definition is unique over these properties?&lt;/p&gt;
&lt;p&gt;Consider an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A = \{ a_{jk} \}_{j,k=1}^n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_n\)&lt;/span&gt; be its columns and &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\e}{\mathrm{e}} \e_1, \dots, \e_n\)&lt;/span&gt; be the unit vectors. We have &lt;span class=&#34;math display&#34;&gt;\[
\v_k = a_{1, k} \e_1 + a_{2, k} \e_2 + \dots + a_{n, k} \e_{n} = \sum_{j=1}^n a_{j, k} \e_j
\]&lt;/span&gt; By the linearity in each argument, expand the first column to give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \det(\sum_{j=1}^n a_{j, 1} \e_i, \v_2, \dots, \v_n) = \sum_{j=1}^n a_{j, 1} \det(\e_j, \v_2, \dots, \v_n)
\]&lt;/span&gt; Further expand the second column to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1}, \v_2, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \det(\e_{j_1}, \sum_{j_2=1}^n a_{j_2, 2} e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n a_{j_1, 1} \sum_{j_2=1}^n a_{j_2, 2} \det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n) \\
&amp;amp;= \sum_{j_1=1}^n \sum_{j_2=1}^n a_{j_1, 1} a_{j_2, 2} \det(\e_{j_1}, e_{j_2}, \v_3, \dots, \v_n)
\end{aligned}
\]&lt;/span&gt; Expand the remaining columns to give &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{j_1=1}^n \sum_{j_2=1}^n \dots \sum_{j_n=1}^n a_{j_1, 1} a_{j_2, 2} \dots a_{j_n, n} \det(\e_{j_1}, e_{j_2}, \dots, \e_{j_n})
\]&lt;/span&gt; This yields &lt;span class=&#34;math inline&#34;&gt;\(n^n\)&lt;/span&gt; terms! But luckily, many terms are zero, as long as any two of &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots, j_n\)&lt;/span&gt; coincides. To eliminate zero terms, consider the permutation of &lt;span class=&#34;math inline&#34;&gt;\(\{ 1, \dots, n \}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(j_1, \dots, j_n\)&lt;/span&gt; are chosen to be a permutation, &lt;span class=&#34;math inline&#34;&gt;\(\det(\e_{j_1}, e_{j_2}, \dots, \e_{j_n})\)&lt;/span&gt; is nonzero. We may use a function &lt;span class=&#34;math inline&#34;&gt;\(\sigma: \{ 1, \dots, n \} \to \{ 1, \dots, n \}\)&lt;/span&gt; to denote a permutation. Let the set of all permutations of set &lt;span class=&#34;math inline&#34;&gt;\(\{ 1, \dots n \}\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Perm}(n)\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\det(\v_1, \dots, \v_n) = \sum_{\sigma \in \mathrm{Perm}(n)} a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n} \det(e_{\sigma(1)}, e_{\sigma(2)}, \dots, e_{\sigma(n)})
\]&lt;/span&gt; The matrix with columns &lt;span class=&#34;math inline&#34;&gt;\(e_{\sigma(1)}, e_{\sigma(2)}, \dots, e_{\sigma(n)}\)&lt;/span&gt; can be obtained from identity matrix by finitely many column exchanges. So &lt;span class=&#34;math inline&#34;&gt;\(\det(e_{\sigma(1)}, e_{\sigma(2)}, \dots, e_{\sigma(n)})\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; depending on the number of column exchanges. We informally define the sign of function &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; if an even number of column exchanges are needed to permute &lt;span class=&#34;math inline&#34;&gt;\(1, \dots, n\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sigma(1), \dots, \sigma(n)\)&lt;/span&gt;; and the signa of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; if the number of exchanges is odd.&lt;/p&gt;
&lt;p&gt;The necessary condition of the definition of determinant requires us define it like &lt;span class=&#34;math display&#34;&gt;\[
\det A \coloneq \sum_{\sigma \in \mathrm{Perm}(n)} a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n} \mathrm{sign}(\sigma)
\]&lt;/span&gt; If we define it in this way, we can verify that it indeed satisfies the basic properties, concluding the construction of determinant.&lt;/p&gt;
&lt;h2 id=&#34;cofactor-expansion&#34;&gt;Cofactor Expansion&lt;/h2&gt;
&lt;p&gt;For an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(A_{j,k}\)&lt;/span&gt; denote &lt;span class=&#34;math inline&#34;&gt;\((n-1) \times (n-1)\)&lt;/span&gt; matrix obtained by crossing out the row &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The determinant of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be expanded in the row number &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
\det A = a_{j, 1} (-1)^{j+1} \det A_{j, 1} + a_{j, 2} (-1)^{j+2} \det A_{j, 2} + \dots + a_{j, n} (-1)^{j+n} \det A_{j, n}
\]&lt;/span&gt; The numbers &lt;span class=&#34;math inline&#34;&gt;\(C_{j,k} = (-1)^{j+k} \det A_{j,k}\)&lt;/span&gt; are called the &lt;strong&gt;cofactors&lt;/strong&gt; of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The matrix &lt;span class=&#34;math inline&#34;&gt;\(C = \{ C_{j,k} \}_{j=1,k=1}^n\)&lt;/span&gt; is called the &lt;strong&gt;cofactor matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an invertible matrix, then &lt;span class=&#34;math display&#34;&gt;\[
A^{-1} = \frac{1}{\det A} C^T
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(C^T\)&lt;/span&gt; is sometimes denoted as &lt;span class=&#34;math inline&#34;&gt;\(A^*\)&lt;/span&gt;, called the &lt;strong&gt;adjugate matrix&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cramers-rule&#34;&gt;Cramer’s Rule&lt;/h3&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and an equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt;, there is a unique solution that &lt;span class=&#34;math display&#34;&gt;\[
x = A^{-1} b = \frac{1}{\det A} C^T b
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; is obtained by multiplying the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of the cofactor matrix with &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which is equivalent to the determinant of the matrix obtained by replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. This property is known as &lt;strong&gt;Cramer’s rule&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, the entry &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the solution of the equation &lt;span class=&#34;math inline&#34;&gt;\(Ax = b\)&lt;/span&gt; is given by the formula &lt;span class=&#34;math display&#34;&gt;\[
x_k = \frac{\det B_k}{\det A}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt; is obtained by replacing &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with the vector &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>Eigenvectors and Eigenvalues</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/</guid>
      <description>

&lt;h2 id=&#34;eigenvectors-and-eigenvalues&#34;&gt;Eigenvectors and Eigenvalues&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;eigenvector&lt;/strong&gt; of a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a &lt;strong&gt;nonzero&lt;/strong&gt; vector &lt;span class=&#34;math inline&#34;&gt;\(\rm x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\rm x = \lambda \rm x\)&lt;/span&gt; for some scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. This scalar &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt;. Note that by definition, &lt;span class=&#34;math inline&#34;&gt;\((\lambda, \vec 0)\)&lt;/span&gt; is a pair of eigenvalue and eigenvector of any square matrix. However, &lt;span class=&#34;math inline&#34;&gt;\(\vec 0\)&lt;/span&gt; is just too trivial an eigenvector that people exclude it from the eigen discussion.&lt;/p&gt;
&lt;p&gt;Note, though, &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; can be an eigenvalue. Also note that if &lt;span class=&#34;math inline&#34;&gt;\((\lambda, v)\)&lt;/span&gt; is a pair of eigen of matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\((\lambda, kv)\)&lt;/span&gt; is also a pair of eigen.&lt;/p&gt;
&lt;h3 id=&#34;similarity&#34;&gt;Similarity&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrices, then &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is similar to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; if there is an invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(PAP^{-1} = B\)&lt;/span&gt;, or equivalently &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}BP = A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are similar, then they have the same characteristic polynomial and hence then the same eigenvalues: &lt;span class=&#34;math display&#34;&gt;\[
B - \lambda I = PAP^{-1} - \lambda PP^{-1} = P(A - \lambda I)P^{-1} \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\det(B - \lambda I) = \det(P(A - \lambda I)P^{-1}) \\
&amp;amp;= \det(P) \cdot \det(A - \lambda I) \cdot \det(P^{-1}) \\
&amp;amp;= \det(A - \lambda I)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;independence-between-eigenvectors&#34;&gt;Independence between Eigenvectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are the eigenvectors corresponding to the distinct eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, ..., \lambda_r\)&lt;/span&gt; of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ... , v_r\)&lt;/span&gt; are also called eigenvectors from different &lt;strong&gt;eigenspaces&lt;/strong&gt;), then &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_r\)&lt;/span&gt; are linearly independent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;p&gt;Suppose instead these vectors are dependent. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the least index such that &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is a linear combination of previous vectors. Then there exists scalars &lt;span class=&#34;math inline&#34;&gt;\(c_1, c_2, ..., c_p\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
  c_1v_1 + c_2v_2 + \cdots + c_pv_p = v_{p+1} \label{lincom} 
  \]&lt;/span&gt; Multiply both sides with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to obtain &lt;span class=&#34;math display&#34;&gt;\[
  c_1\lambda_1v_1 + c_2\lambda_2v_2 + \cdots + c_p\lambda_pv_p = \lambda_{p+1}v_{p+1} \label{eq1}
  \]&lt;/span&gt; Multiply both sides of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{p+1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  c_1\lambda_{p+1}v_1 + c_2\lambda_{p+1}v_2 + \cdots + c_p\lambda_{p+1}v_p = \lambda_{p+1}v_{p+1} \label{eq2}
  \]&lt;/span&gt; Subtract &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
  c_1(\lambda_1 - \lambda_{p+1})v_1 + c_2(\lambda_2 - \lambda_{p+1})v_2 + \cdots + c_p(\lambda_p - \lambda_{p+1})v_p = 0 \label{diff} 
  \]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2, ..., v_p\)&lt;/span&gt; are independent, the weights in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{diff}\)&lt;/span&gt; are all zeros. None of &lt;span class=&#34;math inline&#34;&gt;\((\lambda_i - \lambda_{p+1})\)&lt;/span&gt; is zero, so &lt;span class=&#34;math inline&#34;&gt;\(c_i = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lincom}\)&lt;/span&gt; says &lt;span class=&#34;math inline&#34;&gt;\(v_{p+1}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, which is impossible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, as the eigenvector, is nonzero so that the conclusion can hold even for &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diagonalization&#34;&gt;Diagonalization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; linearly independent eigenvectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A = PDP^{-1}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(AP = PD\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\p}{\mathrm{p}} P = [\p_1, \p_2, ..., \p_n]\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix, &lt;span class=&#34;math display&#34;&gt;\[
AP = [A\p_1, A\p_2, ..., A\p_n] = [D_{11} \p_1, D_{22} \p_2, ..., D_{nn} \p_n]
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is invertible, &lt;span class=&#34;math inline&#34;&gt;\(\p_i\)&lt;/span&gt;’s are linearly independent, which indicates that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm p_i\)&lt;/span&gt;’s are &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors.&lt;/p&gt;
&lt;p&gt;From this, we could also see that if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is diagonalizable, then &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; must be the matrix concatenated with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvectors and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; must be the diagonal matrix filled with corresponding eigenvalues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Easy to show.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a counter-example, the matrix &lt;span class=&#34;math inline&#34;&gt;\(\begin{bmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 1 \end{bmatrix}\)&lt;/span&gt; is not diagonalizable.&lt;/p&gt;
&lt;p&gt;The diagonalization of a square matrix is also referred to as &lt;strong&gt;eigen decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-rank-trace-and-determinant&#34;&gt;Eigenvalues: Rank, Trace and Determinant&lt;/h3&gt;
&lt;p&gt;Since the characteristic equation of an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix is a polynomial of degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the equation always has exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; roots, counting multiplicities, including complex roots. There are some relations between eigenvalues and matrix’s rank, trace and determinant: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\rank = \text{the number of nonzero real eigenvalues, including multiplicities} \\
\tr = \text{sum of the eigenvalues} \\
\det = \text{product of the eigenvalues}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External Materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Eigen%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Eigen Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</link>
      <pubDate>Fri, 07 Jan 2022 12:55:32 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/singular-value-decomposition/</guid>
      <description>

&lt;h3 id=&#34;theorem&#34;&gt;Theorem&lt;/h3&gt;
&lt;p&gt;Any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be decomposed into &lt;span class=&#34;math inline&#34;&gt;\(U\Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\text{$U$ is $m \times m$, $V$ is $n \times n$} \\
U U^T = I \\
V V^T = I \\
\Sigma = \diag_{m \times n} (\sigma_1, \sigma_2, ..., \sigma_r) \\
\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_r \ge 0 \\
r = \rank A
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;This is shown by construction.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;constructing &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real symmetric matrix, so it can be diagonalized by an orthonormal matrix. Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; be this matrix: &lt;span class=&#34;math display&#34;&gt;\[
V^{-1} (A^T A) V = \Lambda \text{, where $\Lambda$ is the diagonal matrix consisting of $A^TA$&amp;#39;s eigenvalues}
\]&lt;/span&gt; Because &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; consists of orthonormal basis, we have &lt;span class=&#34;math inline&#34;&gt;\(V^T V = I\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
V^T (A^T A) V = \Lambda
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;’s columns be arranged such that their corresponding eigenvalues are decreasing. Because &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(A^T A\)&lt;/span&gt; is in the form of a covariance matrix, it has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; non-negative eigenvalues and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the number of its positive eigenvalues. Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i = \sqrt{\lambda_i}, i = 1, 2, ..., n\)&lt;/span&gt; (which are defined as &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s the &lt;strong&gt;singular values&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\lambda_1, \lambda_2, ..., \lambda_r \ge 0, \lambda_r, \lambda_{r+1}, ..., \lambda_{n} = 0 \\
\sigma_1, \sigma_2, ..., \sigma_r \ge 0, \sigma_r, \sigma_{r+1}, ..., \sigma_{n} = 0
\end{gather}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(V_1 = [\v_1, \v_2, ..., \v_r], V_2 = [\v_{r+1}, \v_{r+2}, ..., \v_n], V = [V_1, V_2]\)&lt;/span&gt;. Also let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_1 = 
\begin{bmatrix}
\sigma_1 \\
&amp;amp; \sigma_2 \\
&amp;amp; &amp;amp; \ddots \\
&amp;amp; &amp;amp; &amp;amp; \sigma_r
\end{bmatrix}
\]&lt;/span&gt; Complement &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to get the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
\Sigma =
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\rank(A^T A) = r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T A)\)&lt;/span&gt; is of dimension &lt;span class=&#34;math inline&#34;&gt;\(n-r\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; comprises of &lt;span class=&#34;math inline&#34;&gt;\(A^TA\)&lt;/span&gt;’s eigenvectors whose eigenvalues are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n - r\)&lt;/span&gt; independent columns and &lt;span class=&#34;math inline&#34;&gt;\(A^T A V_2 = 0\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(V_2\)&lt;/span&gt; spans &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^TA)\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
I = VV^T = [V_1, V_2][V_1, V_2]^T = V_1 V_1^T + V_2 V_2^T \\
A = A I = A V_1 V_1^T + A V_2 V_2^T = A V_1 V_1^T
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;constructing &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\u_i \coloneq \frac{1}{\sigma_i} A \v_i, i = 1, 2, ..., r \\
U_1 \coloneq [\u_1, \u_2, ..., \u_r]
\end{gather}
\]&lt;/span&gt; Thus &lt;span class=&#34;math inline&#34;&gt;\(AV_1 = U_1\Sigma_1\)&lt;/span&gt;. Also, &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s columns are orthonormal: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\u_i^T \u_j &amp;amp;= (\frac{1}{\sigma_i} A \v_i)^T (\frac{1}{\sigma_j} A \v_j) \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} \v_i^T A^T A \v_j \\
&amp;amp;= \frac{1}{\sigma_i\sigma_j} \v_i^T \lambda_j \v_j \\
&amp;amp;= \frac{\sigma_j}{\sigma_i} \v_i^T \v_j \\
&amp;amp;= 
\begin{cases}
0 &amp;amp; i \ne j \\
1 &amp;amp; i = j
\end{cases}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s are within &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt; and are orthonormal as shown, plus that &lt;span class=&#34;math inline&#34;&gt;\(\rank(A) = r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_i\)&lt;/span&gt;’s span the &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp\)&lt;/span&gt; be &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)\)&lt;/span&gt;’s complement, we have &lt;span class=&#34;math inline&#34;&gt;\(\Col(A)^\perp = \Nul(A^T)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\{ \u_{r+1}, \u_{r+2}, \dots, \u_{m} \}\)&lt;/span&gt; be an orthonormal basis of &lt;span class=&#34;math inline&#34;&gt;\(\Nul(A^T)\)&lt;/span&gt; such that they are orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt;’s column vectors. We construct &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
U_2 \coloneq [\u_{r+1}, \u_{r+2}, ..., \u_{m}] \\
U \coloneq [U_1, U_2]
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proving that &lt;span class=&#34;math inline&#34;&gt;\(U \Sigma V^T = A\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
U \Sigma V^T &amp;amp;= [U_1, U_2] 
\begin{bmatrix}
\Sigma_1 &amp;amp; 0 \\
0 &amp;amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= [U_1 \Sigma_1, 0]
\begin{bmatrix}
V_1^T \\
V_2^T \\ 
\end{bmatrix}
\\
&amp;amp;= U_1 \Sigma_1 V_1^T \\
&amp;amp;= A V_1 V_1^T \\
&amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;p&gt;Note that SVD is essentially the addition of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; rank-&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; matrix: &lt;span class=&#34;math display&#34;&gt;\[
U \Sigma V^T = \sum_{i=1}^r \sigma_i \u_i \v_i^T
\]&lt;/span&gt; There is another view on &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A A^T U = [A A^T U_1, A A^T U_2]
\]&lt;/span&gt; Breaking it into two parts, &lt;span class=&#34;math inline&#34;&gt;\(\forall i = 1, 2, ..., r\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;A A^T u_i = A A^T \frac{1}{\sigma_i} A \v_i \\
&amp;amp;= \frac{1}{\sigma_i} A (A^T A) \v_i \\
&amp;amp;= \frac{1}{\sigma_i} A \lambda_i \v_i \\
&amp;amp;= \lambda_i(\frac{1}{\sigma_i} A \v_i) \\
&amp;amp;= \lambda_i \u_i
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(U_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;’s concatenated eigenvectors, ordered by the magnitude of eigenvalues. &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(AA^T\)&lt;/span&gt;’s concatenated eigenvectors with eigenvalues being &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A A^T U_2 = 0 = 0 U_2
\]&lt;/span&gt; In all, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the concatenated orthonormal eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A A^T\)&lt;/span&gt;. It doesn’t matter how &lt;span class=&#34;math inline&#34;&gt;\(U_2\)&lt;/span&gt; is permuted.&lt;/p&gt;
&lt;h2 id=&#34;eckart-young-mirsky-theorem&#34;&gt;Eckart-Young-Mirsky Theorem&lt;/h2&gt;
&lt;p&gt;Given an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(r \le \min(m,n)\)&lt;/span&gt; and its SVD &lt;span class=&#34;math inline&#34;&gt;\(X = U \Sigma V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = \diag(\sigma_1, \sigma_2, ..., \sigma_r)\)&lt;/span&gt;, among all the &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrices of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;, the best approximation to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y^\star = U \Sigma_k V^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k = \diag(\sigma_1, \sigma_2, ..., \sigma_k)\)&lt;/span&gt;, when distance between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is defined as &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/frobenius-normalization/&#34;&gt;Frobenius norm&lt;/a&gt;: &lt;span class=&#34;math display&#34;&gt;\[
||X - Y||_F = \sqrt{\sum_{ij}(X - Y)_{ij}^2}
\]&lt;/span&gt; Firstly we prove that, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal, then &lt;span class=&#34;math inline&#34;&gt;\(||U A||_F = ||A U||_F = ||A||_F\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
||UA||_F^2 &amp;amp;= \tr((U A)^T U A) \\
&amp;amp;= \tr(A^T U U A) \\
&amp;amp;= \tr(A^T A) \\
&amp;amp;= ||A||_F^2
\end{aligned}
\]&lt;/span&gt; The same can be derived for &lt;span class=&#34;math inline&#34;&gt;\(||A U||_F\)&lt;/span&gt;. Then for any &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; of rank &lt;span class=&#34;math inline&#34;&gt;\(k \le r\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||U \Sigma V^T - Y||_F^2 \\
&amp;amp;= ||U^T U \Sigma V^T V - U^T Y V||_F^2 \\
&amp;amp;= ||\Sigma - U^T Y V||_F^2 \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(Z = U^T Y V\)&lt;/span&gt;, which is also of rank &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;||X - Y||_F^2 = ||\Sigma - Z||_F^2 = \sum_{ij}(\Sigma_{ij} - Z_{ij})^2 \\
&amp;amp;= \sum_{i=1}^r (\sigma_{i} - Z_{ii})^2 + \sum_{i&amp;gt;r}^{\min(m,n)} Z_{ii}^2 + \sum_{i \ne j} Z_{ij}^2 \\
\end{aligned}
\]&lt;/span&gt; The minimum is achieved if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Z_{ii} = \begin{cases}
\sigma_i, &amp;amp; 1 \le i \le r \\
0, &amp;amp; r \le i \le \min(m,n)
\end{cases} \\
Z_{ij} = 0, 1 \le i \le M, 1 \le j \le N, i \ne j
\end{gather}
\]&lt;/span&gt; Such &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; exists when &lt;span class=&#34;math inline&#34;&gt;\(Y = U \Sigma_k V^T\)&lt;/span&gt;. Q.E.D.&lt;/p&gt;
&lt;p&gt;Note that Eckart-Young-Mirsky theorem also holds for &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/metrics/spectral-normalization/&#34;&gt;spectral norm&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;svd-and-eigen-decomposition&#34;&gt;SVD and Eigen Decomposition&lt;/h2&gt;
&lt;p&gt;It is easy to mix up SVD with &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;eigen decomposition&lt;/a&gt;. Notably, eigen decomposition factorizes square matrix while SVD factorizes any matrix. Also, not all square matrices can be applied with eigen decomposition but all matrices can be applied with SVD. Finally, SVD can be interpreted as rotation-scaling-rotation because &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V^T\)&lt;/span&gt; are orthonormal. But in eigen decomposition, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}\)&lt;/span&gt; are not necessarily orthonormal, unless in the case of real symmetric matrix.&lt;/p&gt;
&lt;h2 id=&#34;external-materials&#34;&gt;External materials&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/6251584.html&#34;&gt;奇异值分解(SVD)原理与在降维中的应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/22237507&#34;&gt;奇异值的物理意义是什么？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chunxy.github.io/uploads/Singular%20Value%20Decomposition.pdf&#34; target=&#34;_blank&#34;&gt;Singular Value Decomposition.pdf&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Real Symmetric Matrix</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</link>
      <pubDate>Sun, 30 Jan 2022 13:46:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/</guid>
      <description>

&lt;h2 id=&#34;real-symmetric-matrix&#34;&gt;Real Symmetric Matrix&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; real-valued symmetric matrix. We have its properties as follows.&lt;/p&gt;
&lt;h3 id=&#34;real-valued-eigenvalues-and-eigenvectors&#34;&gt;Real-valued Eigenvalues and Eigenvectors&lt;/h3&gt;
&lt;p&gt;Its eigenvalues and thus eigenvectors are real-valued. Suppose by contradiction that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; has some imaginary eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and the corresponding imaginary eigenvector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Ax &amp;amp;= \lambda x \\
A(x_{real} + x_{img}) &amp;amp;= (\lambda_{real} + \lambda_{img})(x_{real} + x_{img}) \\
Ax_{real} + Ax_{img} &amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) + (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
\end{aligned}
\]&lt;/span&gt; Denoting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s complex conjugate by &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; respectively, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\left.
\begin{aligned}
&amp;amp;A\bar x = Ax_{real} - Ax_{img} \\
&amp;amp;= (\lambda_{real}x_{real} + \lambda_{img}x_{img}) \\
&amp;amp;\quad- (\lambda_{real}x_{real} + \lambda_{img}x_{real}) \\
&amp;amp;= \bar \lambda \bar x 
\end{aligned}
\right\} \Rightarrow
\begin{aligned}
(A\bar x)^T &amp;amp;= (\bar \lambda \bar x)^T \\
\bar x^TA^T &amp;amp;= \bar \lambda \bar x^T \\
\bar x^TA &amp;amp;= \bar \lambda \bar x^T
\end{aligned}
\end{gather}
\]&lt;/span&gt; Left-multiply &lt;span class=&#34;math inline&#34;&gt;\(\bar x^T\)&lt;/span&gt; on both sides of &lt;span class=&#34;math inline&#34;&gt;\(Ax = \lambda x\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar x^T \lambda x = \lambda \bar x^T x
\]&lt;/span&gt; Right-multiply &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on both side of &lt;span class=&#34;math inline&#34;&gt;\(\bar x^TA = \bar \lambda \bar x^T\)&lt;/span&gt; to give: &lt;span class=&#34;math display&#34;&gt;\[
\bar x^TAx = \bar \lambda \bar x^T x
\]&lt;/span&gt; Therefore &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda \bar x^T x = \lambda \bar x^T x\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\bar x^Tx\)&lt;/span&gt; is real-value, &lt;span class=&#34;math inline&#34;&gt;\(\bar \lambda = \lambda\)&lt;/span&gt;. In other words, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is real-valued and thus so is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;orthogonal-eigenvectors&#34;&gt;Orthogonal Eigenvectors&lt;/h3&gt;
&lt;p&gt;Its eigenvectors corresponding to different eigenvalues are orthogonal. Arbitrarily taking &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’ s two eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(v_1, v_2\)&lt;/span&gt; and their eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \lambda_2, \lambda_1 \ne \lambda_2\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align} 
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= v_1^TA^Tv_2 \\
&amp;amp;= v_1^TAv_2 \\
&amp;amp;= v_1^T \lambda_2v_2 \\
&amp;amp;= \lambda_2v_1^Tv_2 \\
\end{aligned} \\
&amp;amp;\begin{aligned}
(Av_1)^Tv_2 &amp;amp;= \lambda_1v_1^Tv_2
\end{aligned}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda_1v_1^Tv_2 &amp;amp;= \lambda_2v_1^Tv_2 \\
(\lambda_1 - \lambda_2)v_1^Tv_2 &amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ne \lambda_2\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(v_1^Tv_2 = 0\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; are orthogonal.&lt;/p&gt;
&lt;h3 id=&#34;diagonalizable&#34;&gt;Diagonalizable&lt;/h3&gt;
&lt;p&gt;It has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent eigenvectors and thus &lt;a href=&#34;https://chunxy.github.io/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/#Diagonalization&#34;&gt;diagonalizable&lt;/a&gt;. To show it, eigenvectors in different eigenspaces are orthogonal and thus linearly independent; and eigenvectors in the same eigenspace are also linearly independent because they form the basis of this eigenspace.&lt;/p&gt;
&lt;h4 id=&#34;easily-invertible&#34;&gt;“Easily Invertible”&lt;/h4&gt;
&lt;p&gt;Further on the diagonalizable property, suppose &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^{-1}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is not invertible, by &lt;a href=&#34;Eigenvectors%20and%20Eigenvalues.md#Eigenvalues:%20Rank,%20Trace%20and%20Determinant&#34;&gt;the relation between the matrix rank and the eigenvalues&lt;/a&gt;, some of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;’s entries on the diagonal are zero. By adding &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
A^\prime &amp;amp;= P\Lambda P^{-1} + \lambda PIP^{-1} \\
&amp;amp;= P(\Lambda + \lambda I)P^{-1}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt; complements all the zero entries on the diagonal of &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(A^\prime\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; nonzero eigenvalues and is invertible. A singular symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; becomes invertible by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda I\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;orthonormally-diagonalizable&#34;&gt;Orthonormally Diagonalizable&lt;/h4&gt;
&lt;p&gt;Its diagonalization can be in the form of &lt;span class=&#34;math inline&#34;&gt;\(A = P\Lambda P^T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(P^TP = I\)&lt;/span&gt;, by properly selecting the orthonormal eigenvectors.&lt;/p&gt;
&lt;p&gt;Eigenvectors from different eigenspaces are already orthogonal. Eigenvectors from the same eigenspace are independent but not necessarily orthogonal. However, the linear combination of these homo-spatial independent eigenvectors is still an eigenvector. Thus we can apply the Gram-Schmidt process to these eigenvectors and obtain the orthogonal basis for this eigenspace.&lt;/p&gt;
&lt;p&gt;Finally, we pull together all the orthogonal eigenvectors, normalize them to unit vector, and get the orthonormal matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In fact, an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is orthogonally diagonalizable if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a symmetric matrix. Such orthogonal diagonalization is also referred to as &lt;strong&gt;spectral decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-and-positive-definiteness&#34;&gt;Eigenvalues and Positive Definiteness&lt;/h3&gt;
&lt;p&gt;A real symmetric matrix is positive (semi-)definite if and only if its eigenvalues are (non-)negative.&lt;/p&gt;
&lt;h3 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h3&gt;
&lt;p&gt;Covariance matrix is a special kind of real symmetric matrix. It is positive semi-definite and thus its eigenvalues are non-negative.&lt;/p&gt;
&lt;h2 id=&#34;external-material&#34;&gt;External Material&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zybuluo.com/fsfzp888/note/1112344&#34;&gt;real-valued eigenvalues and orthogonal eigenvectors&lt;/a&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Difference Equation</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</link>
      <pubDate>Sat, 25 Dec 2021 20:08:33 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/difference-equation/</guid>
      <description>

&lt;h2 id=&#34;difference-equation&#34;&gt;Difference Equation&lt;/h2&gt;
&lt;p&gt;To solve difference equation like &lt;span class=&#34;math inline&#34;&gt;\(x_t = a_{t-1} x_{t-1} + a_{t-2} x_{t-2} + \dots + a_0\)&lt;/span&gt;, we first rewrite it into the matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left[ \begin{array} \\
x_t \\
x_{t-1} \\
\vdots \\
x_2 \\
x_1
\end{array} \right] =
\underbrace{
\left[
\begin{array} \\
a_{t-1} &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; \vdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0
\end{array}
\right]
}_{A}
\left[ \begin{array} \\
x_{t-1} \\
x_{t-2} \\
\vdots \\
x_1 \\
x_0
\end{array} \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To solve it, we try to find the eigenvalues and eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
A - \lambda I = \left[
\begin{array} \\
a_{t-1} - \lambda &amp;amp; a_{t-2} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots  &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{array}
\right]
\]&lt;/span&gt; Apply Laplacian expansion along the first column to get its determinant as &lt;span class=&#34;math display&#34;&gt;\[
\det (A - \lambda I) = (a_{t-1} -\lambda) (-\lambda)^{t-1} - \det B_{t-1}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
B_{t-1} = 
\underbrace{
\begin{bmatrix}
a_{t-2} &amp;amp; a_{t-3} &amp;amp; \cdots &amp;amp; a_1 &amp;amp; a_0 \\
1 &amp;amp; -\lambda &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; \vdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; -\lambda &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; -\lambda
\end{bmatrix}
}_\text{$t-1$ columns}
\]&lt;/span&gt; Apply Laplacian expansion along the first column of &lt;span class=&#34;math inline&#34;&gt;\(B_{n-1}\)&lt;/span&gt; to give the following recurrence relation: &lt;span class=&#34;math display&#34;&gt;\[
\left.
\begin{array} \\
\det B_{t-1} = a_{t-2} (-\lambda)^{t-2} - \det B_{t-2} \\
\det B_1 = a_0
\end{array}
\right\} \Rightarrow
\det B_{t-1} = (-1)^t (a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} + \dots + a_0)
\]&lt;/span&gt; In all, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det (A - \lambda I) &amp;amp;= (a_{t-1} - \lambda)(-\lambda)^{t-1} - (-1)^t (a_{t-2} \lambda^{t-2} + a_{t-3} \lambda^{t-3} + \dots + a_0) \\
&amp;amp;= (-1)^t (\lambda^t - a_{t-1} \lambda^{t-1} - a_{t-2} \lambda^{t-2} - \dots - a_0)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After solving the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 \ge \dots \ge \lambda_t\)&lt;/span&gt; and corresponding eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\newcommand{\v}{\mathrm{v}} \v_1, \dots, \v_t\)&lt;/span&gt; from above equation, we can rewrite the vector formed by the initial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; terms as the linear combination of &lt;span class=&#34;math inline&#34;&gt;\(\v_1, \dots, \v_t\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\x_0 = \left[ 
x_{t-1}, \cdots, x_0
\right]^T
= c_1 \v_1 + \dots c_t \v_t
\]&lt;/span&gt; Then for every &lt;span class=&#34;math inline&#34;&gt;\(n \ge t\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
x_n = \left[ A^{n-t+1} \x_0 \right]_0 = \left[ \lambda_1^{n-t+1} c_1 \v_1 + \dots + \lambda_t^{n-t+1} c_t \v_t\right]_0
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_n\)&lt;/span&gt; will be asymptotic to &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1^{n}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Matrix Identity</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</link>
      <pubDate>Wed, 22 Dec 2021 15:51:42 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/matrix-identity/</guid>
      <description>
&lt;p&gt;A useful matrix identity &lt;span class=&#34;math display&#34;&gt;\[
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} = PB^T(BPB^T+R)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1} &amp;amp;= PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}+B^TR^{-1}B)PB^T(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (P^{-1}PB^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^T+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= (B^TR^{-1}R+B^TR^{-1}BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1}(R+BPB^T)(BPB^T+R)^{-1} \\
\iff B^TR^{-1} &amp;amp;= B^TR^{-1} \\
\end{aligned}
\]&lt;/span&gt; Its reduced form &lt;span class=&#34;math display&#34;&gt;\[
(I_N+AB)^{-1}A = A(I_M+BA)^{-1}
\]&lt;/span&gt; can be proved with &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(I_N+AB)^{-1}A &amp;amp;= A(I_M+BA)^{-1} \\
\iff A &amp;amp;= (I_N + AB)A(I_M + BA)^{-1} \\
\iff A &amp;amp;= (A + ABA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A(I_M + BA)(I_M + BA)^{-1} \\
\iff A &amp;amp;= A
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Quadratic Form</title>
      <link>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</link>
      <pubDate>Fri, 05 May 2023 10:21:12 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/mathematics/linear-algebra/quadratic-form/</guid>
      <description>

&lt;h2 id=&#34;quadratic-form&#34;&gt;Quadratic Form&lt;/h2&gt;
&lt;p&gt;Quadratic form involves many concepts like real symmetric matrix, positive definiteness and singular value decomposition. It can be quite helpful to glue these things together.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;quadratic function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; variables, or say a vector &lt;span class=&#34;math inline&#34;&gt;\(\x\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, is the sum of second-order terms: &lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \sum_{i=1}^n \sum_{j=1}^n c_{ij} x_i x_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above summation can be simplified as matrix product &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = c_{ij}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x\)&lt;/span&gt; is called the &lt;strong&gt;quadratic form&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In essence, there is a nicer formulation for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Firstly define the &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} = \frac{1}{2} (c_{ij} + c_{ji})\)&lt;/span&gt;. It suffices to show &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a real symmetric matrix and &lt;span class=&#34;math display&#34;&gt;\[
f(\x) = \x^T A \x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus the discussion of quadratic form usually encompasses the real symmetric matrix.&lt;/p&gt;
&lt;h3 id=&#34;positive-definiteness&#34;&gt;Positive Definiteness&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be a real symmetric matrix. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;strong&gt;positive definite&lt;/strong&gt; if and only if the quadratic form of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive. Specifically, for every &lt;span class=&#34;math inline&#34;&gt;\(\x \ne 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\x^T A \x &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Theorem&lt;/p&gt;
&lt;p&gt;A real symmetric matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive definite if and only if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenvalues are positive.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Proof&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For every &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s eigenpair &lt;span class=&#34;math inline&#34;&gt;\((\lambda, \v)\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(\v^T A \v = \lambda \v^T \v &amp;gt; 0 \Rightarrow \lambda &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s spectral decomposition as &lt;span class=&#34;math inline&#34;&gt;\(A = Q \Lambda Q^T\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Q Q^T = I\)&lt;/span&gt;. For every &lt;span class=&#34;math inline&#34;&gt;\(\x &amp;gt; 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\x^T A \x = \overbrace{\x^T Q}^{y^T} \Lambda \overbrace{Q^T \x}^{y} &amp;gt; 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One possible reason why we would like to define positive definiteness on real symmetric matrix is that, any real matrix can be easily decomposed into the addition of a real symmetric matrix and a real &lt;strong&gt;skew-symmetric&lt;/strong&gt; matrix whose quadratic form is zero: &lt;span class=&#34;math display&#34;&gt;\[
A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\frac{A - A^T}{2}\)&lt;/span&gt;’s quadratic form is zero and it makes no contribution to &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;’s, the only component of interest will be the real symmetric &lt;span class=&#34;math inline&#34;&gt;\(\frac{A + A^T}{2}\)&lt;/span&gt;. So why not just focus on the real symmetric matrix?&lt;/p&gt;


</description>
    </item>
    
  </channel>
</rss>
