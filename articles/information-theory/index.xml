<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Information Theory | My Notes</title>
    <link>https://chunxy.github.io/notes/articles/information-theory/</link>
      <atom:link href="https://chunxy.github.io/notes/articles/information-theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Information Theory</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language>
    <image>
      <url>https://chunxy.github.io/notes/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Information Theory</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/</link>
    </image>
    
    <item>
      <title>Conditional Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/conditional-entropy/</guid>
      <description>

&lt;h2 id=&#34;conditional-entropy&#34;&gt;Conditional Entropy&lt;/h2&gt;
&lt;p&gt;The Conditional Entropy measures the the amount of information needed to describe the outcome of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that the value of another random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is known. The entropy of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; conditioned on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(Y|X) = -\sum_{(x,y) \in X \times Y} p_{(X,Y)}(x,y) \log \frac{p_{(X,Y)}(x,y)}{p_X(x)}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Cross Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/cross-entropy/</guid>
      <description>

&lt;h2 id=&#34;cross-entropy&#34;&gt;Cross Entropy&lt;/h2&gt;
&lt;p&gt;The Cross Entropy between two distributions over the same underlying set of events measures the average number of bits to identify the event drawn from the set if a coding scheme is used for the set is optimized for probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, instead of the true distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Cross Entropy of distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; relative to a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) = -\mathrm{E}_{x \sim p} \log q(x)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Differentiation</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/differentiation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/differentiation/</guid>
      <description>

&lt;p&gt;Both [[Cross Entropy]] and [[KL-divergence]] describe the relationship between &lt;strong&gt;two distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both [[Conditional Entropy]] and [[Mutual Information]] describe the relationship between &lt;strong&gt;two random variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Both [[Conditional Entropy]] and [[Cross Entropy]] would better be applied on discrete random variables.&lt;/p&gt;
&lt;p&gt;Both [[Mutual Information]] and [[KL-divergence]] can be applied on either discrete or continuous random variables&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-entropy&#34;&gt;KL-divergence and Entropy&lt;/h2&gt;
&lt;p&gt;By definition, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= -\mathrm{E}_{x \sim p} \log \frac{q(x)}{p(x)} \\
&amp;amp;= -\mathrm{E}_{x \sim p} \log q(x) - (-\mathrm{E}_{x \sim p} \log {p(x)}) \\
&amp;amp;= H(p||q) - H(p)
\end{align}
\]&lt;/span&gt; By the convexity of &lt;span class=&#34;math inline&#34;&gt;\(-\log\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
D_{KL}(p||q) &amp;amp;= \mathrm{E}_{x \sim p} [-\log \frac{q(x)}{p(x)}] \\
&amp;amp;\ge -\log(\mathrm{E}_{x \sim p} \frac{q(x)}{p(x)}) \\
&amp;amp;= 0
\end{align}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
H(p||q) \ge H(p)
\]&lt;/span&gt; The equality holds if and only if &lt;span class=&#34;math inline&#34;&gt;\(q = p\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kl-divergence-and-mutual-information&#34;&gt;KL-divergence and Mutual Information&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I(X;Y) &amp;amp;= \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&amp;amp;= \sum_{(x,y) \in X \times Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
&amp;amp;= \sum_{x \in X} p(x) D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{x \in X} D_{KL}[p(y|x), p(y)] \\
&amp;amp;= \E_{y \in Y} D_{KL}[p(x|y), p(x)] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/entropy/</guid>
      <description>

&lt;h2 id=&#34;entropy&#34;&gt;Entropy&lt;/h2&gt;
&lt;p&gt;The Entropy of discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (probability mass function) is defined as &lt;span class=&#34;math display&#34;&gt;\[
H(p) = -\mathrm{E}_{x \sim p}\log p(x)
\]&lt;/span&gt; The Entropy of continuous distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; (probability density function) is usually similarly defined as &lt;span class=&#34;math display&#34;&gt;\[
H(q) = -\E_{x\sim q}\log q(x) = -\int q(x)\log q(x)dx
\]&lt;/span&gt; This is actually the &lt;strong&gt;Differential Entropy&lt;/strong&gt; introduced by Shannon. In fact, it is not a good continuous analog of discrete entropy and it was not rigorously derived. For example, this formula can be negative. Therefore, in the case of Entropy, the random variable had better be discrete, although the differential entropy is widely used.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>f-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/f-divergence/</guid>
      <description>

&lt;h2 id=&#34;f-divergence&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence can be treated as the generalization of the KL-divergence. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \int f(\frac{p(x)}{q(x)}) q(x)\ \d x
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; has to satisfy that &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a convex function. The reason for these two constraints is that we hope &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
D_f (p||q) = 0 \text{ when $p=q$} \\
\forall p, q, D_f (p||q) \ge 0
\end{gather}
\]&lt;/span&gt; When &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \log x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence becomes KL-divergence.&lt;/p&gt;
&lt;h2 id=&#34;variational-f-divergence&#34;&gt;Variational &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence&lt;/h2&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; have no closed-form expression, it is difficult to compute the &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence. Therefore in practice &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;-divergence is computed with a variational expression: &lt;span class=&#34;math display&#34;&gt;\[
D_f (p||q) = \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f^*\)&lt;/span&gt; is the convex conjugate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. The derivation is as follows: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_f (p||q) &amp;amp;= \int f(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int f^{**}(\frac{p(x)}{q(x)}) q(x)\ \d x \\
&amp;amp;= \int \sup_t [\frac{p(x)}{q(x)} t - f^*(t)] q(x)\ \d x \\
&amp;amp;= \int \sup_t[p(x) t - f^*(t) q(x)]\ \d x \\
&amp;amp;\Downarrow_{T(x) = \arg \sup_t[p(x) t - f^*(t) q(x)]} \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \int [p(x) T(x) - f^*(T(x)) q(x)]\ \d x \\
&amp;amp;= \sup_{T:\mathcal X \to \R} \{ \E_p[f(x)] + \E_q[f^* \circ T(x)] \}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>KL-divergence</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/kl-divergence/</guid>
      <description>

&lt;h2 id=&#34;kl-divergence&#34;&gt;KL-divergence&lt;/h2&gt;
&lt;p&gt;KL-divergence, &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|q)\)&lt;/span&gt; is statistical distance, measuring how the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is different from the reference probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, both defined on the probability space &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathcal{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In information theory, it measures the &lt;strong&gt;relative entropy&lt;/strong&gt; from &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which is the average number of extra bits required to represent a message with &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In discrete form, &lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(p \| q) = -\sum_{x \in \mathcal{X}} p(x)\log \frac{q(x)}{p(x)} = \sum_{x \in \mathcal{X}} p(x)\log \frac{p(x)}{q(x)}
\]&lt;/span&gt; In continuous form, &lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(p \| q) = -\int_{x \in \mathcal{X}} p(x) \log \frac{q(x)}{p(x)}dx = \int_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}dx
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>Mutual Information</title>
      <link>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/notes/articles/information-theory/mutual-information/</guid>
      <description>

&lt;h2 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;Mutual Information of &lt;strong&gt;two random variables&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a measure of the mutual independence between them. It quantifies the amount of information obtained about one random variable by observing the other random variable. It is defined as &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = \sum_{(x,y) \in X \times Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]&lt;/span&gt; By its definition, &lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = I(Y;X)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = D_{KL}(p_{(X, Y)}|| p_X \times p_Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
  </channel>
</rss>
