<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="点估计 设总体\(X \sim p(x;\theta)\)的分布形式已知，但其参数\(\theta\)未知。设\(X_1, \dots, X_n\)为总体的一个样本，若用一个统计量\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)来估计\(\theta\)，则称\(\hat \theta\)为参数\(\theta\)的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。
矩估计 矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的\(k\)阶原点矩\(\mu_k = \E(X^k)\)，样本的\(k\)阶原点矩为\(A_k = \frac 1 n \sum_{i=1}^n X_i^k\)，如果未知参数\(\theta = \varphi(\mu_1, \dots, \mu_m)\)，则其估计量\(\hat \theta = \varphi(A_1, \dots, A_m)\)，这种估计总体未知参数的方法叫作矩估计法。" />

  
  <link rel="alternate" hreflang="en-us" href="https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/notes/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/notes/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/notes/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/notes/css/wowchemy.832f4349361bf479dece6e8aacc6bfa5.css" />

  



  


  


  




  
  
  

  

  
    <link rel="manifest" href="/notes/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/notes/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/notes/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="My Notes" />
  <meta property="og:url" content="https://chunxy.github.io/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" />
  <meta property="og:title" content="参数估计 | My Notes" />
  <meta property="og:description" content="点估计 设总体\(X \sim p(x;\theta)\)的分布形式已知，但其参数\(\theta\)未知。设\(X_1, \dots, X_n\)为总体的一个样本，若用一个统计量\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)来估计\(\theta\)，则称\(\hat \theta\)为参数\(\theta\)的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。
矩估计 矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的\(k\)阶原点矩\(\mu_k = \E(X^k)\)，样本的\(k\)阶原点矩为\(A_k = \frac 1 n \sum_{i=1}^n X_i^k\)，如果未知参数\(\theta = \varphi(\mu_1, \dots, \mu_m)\)，则其估计量\(\hat \theta = \varphi(A_1, \dots, A_m)\)，这种估计总体未知参数的方法叫作矩估计法。" /><meta property="og:image" content="https://chunxy.github.io/notes/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://chunxy.github.io/notes/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
    
  

  



  

  

  <head>

<script type="text/javascript" async>
    window.MathJax = {
		startup: {
			typeset: true,
		},
		tex: {
			macros: {
				

				
				R: "\\mathbb{R}", E: "\\mathrm{E}", x: "\\mathrm{x}", y: "\\mathrm{y}", 
				d: "\\mathrm{d}", Var: "\\mathrm{Var}", Cov: "\\mathrm{Cov}",
			},
			maxBuffer: 10*1024,
			processEscapes: false,
			
        }
    }
</script>

<script id="MathJax-script" defer="defer"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


  <title>参数估计 | My Notes</title>
</head>


<body id="top" data-spy="scroll"  data-target="#TableOfContents" class="page-wrapper   no-navbar" data-wc-page-id="94e05dbe1cc96526a5b0964b9680b082" >

  
  
  
  
  
  
  
  
  
  <script src="/notes/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    


  </div>

  <div class="page-body">
    
    
    

    




<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      <form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <div class="d-flex">
      <span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">
        
        
          概率论与数理统计
        
      </span>
      <span><i class="fas fa-chevron-down"></i></span>
    </div>
  </button>

  
  <button class="form-control sidebar-search js-search d-none d-md-flex">
    <i class="fas fa-search pr-2"></i>
    <span class="sidebar-search-text">Search...</span>
    <span class="sidebar-search-shortcut">/</span>
  </button>
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    
    <ul class="nav docs-sidenav">
      <li class=""><a href="/notes/">Chunxy&#39; notes</a></li>
    </ul>


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/"><i class="far fa-file-lines pr-1"></i>Articles</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/information-theory/">Information Theory</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/information-theory/conditional-entropy/">Conditional Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/cross-entropy/">Cross Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/differentiation/">Differentiation</a></li>



  <li class=""><a href="/notes/articles/information-theory/entropy/">Entropy</a></li>



  <li class=""><a href="/notes/articles/information-theory/f-divergence/">f-divergence</a></li>



  <li class=""><a href="/notes/articles/information-theory/kl-divergence/">KL-divergence</a></li>



  <li class=""><a href="/notes/articles/information-theory/mutual-information/">Mutual Information</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/machine-learning/">Machine Learning</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/machine-learning/bias-variance-decomposition/">Bias-variance Decomposition</a></li>



  <li class=""><a href="/notes/articles/machine-learning/clustering/">Clustering</a></li>



  <li class=""><a href="/notes/articles/machine-learning/dimension-reduction/">Dimension Reduction</a></li>



  <li class=""><a href="/notes/articles/machine-learning/eckart-young-mirsky-theorem/">Eckart-Young-Mirsky Theorem</a></li>



  <li class=""><a href="/notes/articles/machine-learning/fishers-linear-discriminant/">Fisher&#39;s Linear Discriminant</a></li>



  <li class=""><a href="/notes/articles/machine-learning/independent-component-analysis/">Independent Component Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/linear-discriminant-analysis/">Linear Discriminant Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/linear-regression/">Linear Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/logistic-regression/">Logistic Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/machine-learning-bullet-points/">Machine Learning Bullet Points</a></li>



  <li class=""><a href="/notes/articles/machine-learning/non-linear-regression/">Non-linear Regression</a></li>



  <li class=""><a href="/notes/articles/machine-learning/principal-component-analysis/">Principal Component Analysis</a></li>



  <li class=""><a href="/notes/articles/machine-learning/ransac/">RANSAC</a></li>



  <li class=""><a href="/notes/articles/machine-learning/support-vector-machine/">Support Vector Machine</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/">Mathematics</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/calculus/">Calculus</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/calculus/jacobian-matrix/">Jacobian Matrix</a></li>



  <li class=""><a href="/notes/articles/mathematics/calculus/lipschitz-continuity/">Lipschitz Continuity</a></li>



  <li class=""><a href="/notes/articles/mathematics/calculus/spherical-coordinates/">Spherical Coordinates</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/linear-algebra/">Linear Algebra</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/linear-algebra/eigenvectors-and-eigenvalues/">Eigenvectors and Eigenvalues</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/laplace-expansion/">Laplace Expansion</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/matrix-identity/">Matrix Identity</a></li>



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/linear-algebra/measures/">Measures</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/linear-algebra/measures/chebyshev-distance/">Chebyshev Distance</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/measures/frobenius-normalization/">Frobenius Normalization</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/measures/spectral-normalization/">Spectral Normalization</a></li>

      
        </ul>
      
    

    
      </div>
    



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/real-symmetric-matrix/">Real Symmetric Matrix</a></li>



  <li class=""><a href="/notes/articles/mathematics/linear-algebra/singular-value-decomposition/">Singular Value Decomposition</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/numerical-analysis/">Numerical Analysis</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/numerical-analysis/fourier-transform/">Fourier Transform</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/mathematics/statistics/">Statistics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/mathematics/statistics/probability-estimation/">Probability Estimation</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/articles/optimization/">Optimization</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/articles/optimization/convex-optimization/">Convex Optimization</a></li>



  <li class=""><a href="/notes/articles/optimization/coordinate-descent/">Coordinate Descent</a></li>



  <li class=""><a href="/notes/articles/optimization/expectation-maximization/">Expectation Maximization</a></li>



  <li class=""><a href="/notes/articles/optimization/gradient-descent/">Gradient Descent</a></li>



  <li class=""><a href="/notes/articles/optimization/lagrange-multiplier/">Lagrange Multiplier</a></li>



  <li class=""><a href="/notes/articles/optimization/least-angle-regression/">Least Angle Regression</a></li>



  <li class=""><a href="/notes/articles/optimization/subgradient/">Subgradient</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/"><i class="fas fa-book pr-1"></i>Books</a>
    
      
        <ul class="nav docs-sidenav">
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/information-theory-inference-and-learning-algorithms/">Information Theory, Inference and Learning Algorithms</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/information-theory-inference-and-learning-algorithms/source-coding-theory/">Source Coding Theory</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/linear-algebra-and-its-applications/">Linear Algebra and Its Applications</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/coordinate-system-and-change-of-basis/">Coordinate System and Change of Basis</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/least-squares/">Least Squares</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/orthogonality-and-projection/">Orthogonality and Projection</a></li>



  <li class=""><a href="/notes/books/linear-algebra-and-its-applications/schmit-gram-orthogonalization/">Schmit-Gram Orthogonalization</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">概率论与数理统计</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/common-distributions/">Common Distributions</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/function-of-random-variable/">Function of Random Variable</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/gaussian-distribution/">Gaussian Distribution</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/unconscious-statistics/">Unconscious Statistics</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%E4%B8%8E%E6%AD%A3%E6%80%81%E6%80%BB%E4%BD%93%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/">三大分布与正态总体的抽样分布</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/">协方差与相关系数</a></li>



  <li class="active"><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/">参数估计</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/">大数定律和中心极限定理</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0/">特征函数</a></li>



  <li class=""><a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E7%BB%9F%E8%AE%A1%E9%87%8F/">统计量</a></li>

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/">高等数学</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/books/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/limit-computing-tricks/">Limit Computing Tricks</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    



  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/notes/papers/"><i class="fas fa-paperclip pr-1"></i>Papers</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/notes/papers/bounding-mutual-information/">Bounding Mutual Information</a></li>



  <li class=""><a href="/notes/papers/contrastive-predictive-coding/">Contrastive Predictive Coding</a></li>



  <li class=""><a href="/notes/papers/noise-contrastive-estimation/">Noise Contrastive Estimation</a></li>

      
        </ul>
      
    

    
      </div>
    

</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#点估计">点估计</a>
      <ul>
        <li><a href="#矩估计">矩估计</a></li>
        <li><a href="#最大似然估计">最大似然估计</a></li>
        <li><a href="#优良性评判">优良性评判</a></li>
        <li><a href="#cramer-rao不等式">Cramer-Rao不等式</a></li>
      </ul>
    </li>
    <li><a href="#区间估计">区间估计</a></li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          
            
  <nav class="d-none d-md-flex" aria-label="breadcrumb">
    <ol class="breadcrumb">
      
  
    
  
    
  
    
  

    <li class="breadcrumb-item">
      <a href="/notes/">
        
          Home
        
      </a>
    </li>
  

    <li class="breadcrumb-item">
      <a href="/notes/books/">
        
          Books
        
      </a>
    </li>
  

    <li class="breadcrumb-item">
      <a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">
        
          概率论与数理统计
        
      </a>
    </li>
  

      <li class="breadcrumb-item active" aria-current="page">
        参数估计
      </li>
    </ol>
  </nav>




          
        </div>

        
        

        <div class="docs-article-container">
          <h1>参数估计</h1>

          <div class="article-style">
            

<h2 id="点估计">点估计</h2>
<p>设总体<span class="math inline">\(X \sim p(x;\theta)\)</span>的分布形式已知，但其参数<span class="math inline">\(\theta\)</span>未知。设<span class="math inline">\(X_1, \dots, X_n\)</span>为总体的一个样本，若用一个统计量<span class="math inline">\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)</span>来估计<span class="math inline">\(\theta\)</span>，则称<span class="math inline">\(\hat \theta\)</span>为参数<span class="math inline">\(\theta\)</span>的一个点估计量。构造点估计量的常用方式有两种：矩估计法和最大似然估计法。</p>
<h3 id="矩估计">矩估计</h3>
<p>矩估计的思想就是就是替换思想，即用样本原点矩替换总体原点矩，设总体的<span class="math inline">\(k\)</span>阶原点矩<span class="math inline">\(\mu_k = \E(X^k)\)</span>，样本的<span class="math inline">\(k\)</span>阶原点矩为<span class="math inline">\(A_k = \frac 1 n \sum_{i=1}^n X_i^k\)</span>，如果未知参数<span class="math inline">\(\theta = \varphi(\mu_1, \dots, \mu_m)\)</span>，则其估计量<span class="math inline">\(\hat \theta = \varphi(A_1, \dots, A_m)\)</span>，这种估计总体未知参数的方法叫作矩估计法。</p>
<p>矩估计法往往不唯一，如设<span class="math inline">\(X \sim P(\lambda)\)</span>，则由于<span class="math inline">\(\E(X) = \lambda\)</span>，<span class="math inline">\(\hat \lambda\)</span>可写作<span class="math inline">\(\bar X\)</span>；又<span class="math inline">\(\Var(X) = \lambda\)</span>，<span class="math inline">\(\hat \lambda\)</span>可写作<span class="math inline">\(\frac 1 n \sum_{i=1}^n X_i^2 - \bar X^2\)</span>。此时往往采用较低阶的矩来估计未知参数。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<p>设总体有分布律<span class="math inline">\(X \sim P(X=x;\theta)\)</span>或密度函数<span class="math inline">\(X \sim p(x;\theta)\)</span>，<span class="math inline">\(x_1, \dots, x_n\)</span>为取自总体的一个样本的观测值，将样本的联合分布律或联合密度函数看作<span class="math inline">\(\theta\)</span>的函数： <span class="math display">\[
L(\theta) = \prod_{i=1}^n P(X=x_i;\theta)\ \text或 \ L(\theta) = \prod_{i=1}^n p(x_i;\theta)
\]</span> <span class="math inline">\(L(\theta)\)</span>又称作<span class="math inline">\(\theta\)</span>的似然函数，似然函数满足关系式<span class="math inline">\(L(\hat \theta) = \max_{\theta} L(\theta)\)</span>的解<span class="math inline">\(\hat \theta\)</span>为<span class="math inline">\(\theta\)</span>的最大似然估计量</p>
<h3 id="优良性评判">优良性评判</h3>
<h4 id="无偏性unbiased">无偏性（Unbiased）</h4>
<p>设<span class="math inline">\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)</span>是<span class="math inline">\(\theta\)</span>的一个估计量，<span class="math inline">\(\theta\)</span>的取值空间为<span class="math inline">\(\Theta\)</span>，若对任意的<span class="math inline">\(\theta \in \Theta\)</span>，有 <span class="math display">\[
\E [\hat \theta(X_1, \dots, X_n)] = \theta
\]</span> 则称<span class="math inline">\(\hat \theta\)</span>是<span class="math inline">\(\theta\)</span>的一个无偏估计（量），否则则称作有偏估计（量）。如果有 <span class="math display">\[
\lim_{n \to \infty} \E [\hat \theta(X_1, \dots, X_n)] = \theta
\]</span> 则称<span class="math inline">\(\hat \theta\)</span>是<span class="math inline">\(\theta\)</span>的一个渐进无偏估计（量）。</p>
<p>估计的无偏性是指，估计量相对于未知参数真值来说，取某些样本时估计值偏大，取另一些样本时估计量偏小，多次取样本进行估计，平均来讲偏差为<span class="math inline">\(0\)</span>。如果估计量不具有无偏性，则无论取多少次样本，其平均值与真值也有偏差，亦即系统误差。</p>
<h4 id="最小方差minimum-variance">最小方差（Minimum-variance）</h4>
<p>设<span class="math inline">\(\hat \theta_1 = \hat \theta_2\)</span>是<span class="math inline">\(\theta\)</span>的两个估计量，<span class="math inline">\(\theta\)</span>的取值空间为<span class="math inline">\(\Theta\)</span>，若对任意的<span class="math inline">\(\theta \in \Theta\)</span>，有<span class="math inline">\(\Var(\hat \theta_1) \le \Var(\hat \theta_2)\)</span>，且至少有一个<span class="math inline">\(\theta \in \Theta\)</span>使得该不等式严格成立，则称<span class="math inline">\(\hat \theta_1\)</span>比<span class="math inline">\(\hat \theta_2\)</span>有效。</p>
<h4 id="一致性consistent">一致性（Consistent）</h4>
<p>设<span class="math inline">\(\hat \theta = \hat \theta(X_1, \dots, X_n)\)</span>是<span class="math inline">\(\theta\)</span>的一个估计量，若对任意<span class="math inline">\(\epsilon &gt; 0\)</span>，有 <span class="math display">\[
\lim_{n \to \infty} P(|\hat \theta - \theta| \ge \epsilon) = 0, \text{亦即} \hat \theta \stackrel{P}{\to} \theta
\]</span> 则称估计量<span class="math inline">\(\hat \theta\)</span>具有一致性。一致性是一个很基本的要求，随着样本数量增加，如果估计量不能够将偏差缩小到任意指定精度，那么这个估计通常是不好的。不满足一致性的估计量一般不予考虑。</p>
<h3 id="cramer-rao不等式">Cramer-Rao不等式</h3>
<p>实际上，点估计量不仅仅可以估计未知参数<span class="math inline">\(\theta\)</span>本身（假设为一元情况），更可以估计未知参数的某个函数<span class="math inline">\(g(\theta)\)</span>，即给定总体的一个样本<span class="math inline">\(X_1, \dots, X_n\)</span>，用统计量<span class="math inline">\(\hat g = \hat g(X_1, \dots, X_n)\)</span>估计<span class="math inline">\(g(\theta)\)</span>。估计量最好的效果便是达到最小方差无偏（minimum-variance unbiased &lt;MVU&gt;）估计，Cramer-Rao不等式给出了点估计量<span class="math inline">\(\hat g\)</span>方差的一个下界。 <span class="math display">\[
\label{cr} \Var(\hat g) \ge (g&#39;(\theta))^2 / (nI(\theta))
\]</span> 其中，<span class="math inline">\(I(\theta) = \int [(\frac{\partial p(x;\theta)}{\partial \theta})^2 / p(x;\theta)] \d x\)</span>为Fisher Information。当<span class="math inline">\(g(\theta) = \theta\)</span>，即只估计未知参数本身时，有<span class="math inline">\(\Var(\hat g) \ge 1 / (nI(\theta))\)</span>。</p>
<p><span class="math inline">\(\eqref{cr}\)</span>成立有一定的条件，其本身就暗含了<span class="math inline">\(\frac{\partial p(x;\theta)}{\partial \theta}\)</span>存在及<span class="math inline">\(g&#39;(\theta)\)</span>存在的<strong>条件</strong>。记 <span class="math display">\[
S = S(X_1, \dots, X_n, \theta) = \sum_{i=1}^n \frac{\partial \ln p(X_i;\theta)} {\partial \theta} = \sum_{i=1}^n [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)]
\]</span> <span class="math inline">\(\int p(x;\theta)\ \d x = 1\)</span>，此式两边同时对<span class="math inline">\(\theta\)</span>求导，并<strong>假定</strong>此处求导可以移至积分号内部，可得到<span class="math inline">\(\int \frac{\partial p(x;\theta)}{\partial \theta} \d x = 0\)</span>。根据[[Unconscious Statistics#Law of the Unconscious Statistician|LOTUS]]， <span class="math display">\[
\E [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] 
= \int [\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta)] p(x;\theta)\ \d x
= \int \frac{\partial p(x;\theta)} {\partial \theta}\d x = 0
\]</span></p>
<p>由于<span class="math inline">\(X_1, \dots, X_n\)</span>的独立性， <span class="math display">\[
\begin{aligned}
\Var(S) &amp;= \sum_{i=1}^n \Var [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \\
&amp;= \sum_{i=1}^n \{ \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] - \E^2 [\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta)] \} \\
&amp;= \sum_{i=1}^n \E [\big (\frac{\partial p(X_i;\theta)} {\partial \theta} / p(X_i;\theta) \big)^2] \\
&amp;= n \int \big (\frac{\partial p(x;\theta)} {\partial \theta} / p(x;\theta) \big)^2 p(x;\theta)\ \d x \\
&amp;= n I(\theta)
\end{aligned}
\]</span></p>
<p>根据协方差的性质， <span class="math display">\[
\label{cov_prop} [\Cov(\hat g, S)]^2 \le \Var(\hat g) \Var(S) = \Var(\hat g) n I(\theta)
\]</span></p>
<p>又<span class="math inline">\(\E(S) = 0\)</span>， <span class="math display">\[
\begin{aligned}
\Cov(\hat g, S) = \E (\hat g S) &amp;= \int \dots \int \hat g(x_1, \dots, x_n) \sum_{i=1}^n [\frac{\partial p(x_i;\theta)} {\partial \theta} / p(x_i;\theta)] \prod_{i=1}^n p(x_1;\theta)\ \d x_1 \dots \d x_n \\
&amp;= \int \dots \int \hat g(x_1, \dots, x_n) \frac{\partial p(x_1;\theta) \dots p(x_n;\theta)} {\partial \theta}\ \d x_1 \dots \d x_n 
\end{aligned}
\]</span> <strong>假定</strong>此处对<span class="math inline">\(\theta\)</span>求导可以移至积分号外部， <span class="math display">\[
\begin{aligned}
\Cov(\hat g, S) 
&amp;= \frac \partial{\partial \theta} \int \dots \int \hat g(x_1, \dots, x_n) p(x_1;\theta) \dots p(x_n;\theta)\ \d x_1 \dots \d x_n \\
&amp;= \frac \partial{\partial \theta} g(\theta) = g&#39;(\theta)
\end{aligned}
\]</span> 将上式重新带入<span class="math inline">\(\eqref{cov_prop}\)</span>，从而得到<span class="math inline">\(\eqref{cr}\)</span>。</p>
<h4 id="参考">参考</h4>
<p><a href="https://www.zhihu.com/question/56411276/answer/204992057">对Cramer-Rao不等式的理解</a>|<a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Wiki (see the multi-variate case)</a></p>
<h2 id="区间估计">区间估计</h2>
<p>点估计得到是未知参数的某个特定值，然而实际上由于点估计的方差因素，我们不可能得到完全准确的估计值。如果我们能够给出一个区间，使得我们有较大把握参数的真实值落在这个区间范围内，则显得我们的估计更加有效、可信，这个区间也叫作<strong>置信区间</strong>（confidence interval）。</p>
<p>设总体<span class="math inline">\(X \sim f(x;\theta)\)</span>的分布形式已知，但其参数<span class="math inline">\(\theta\)</span>未知。设<span class="math inline">\(X_1, \dots, X_n\)</span>为总体的一个样本，给定一个很小的数<span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>，若有统计量<span class="math inline">\(\theta_l = \theta_l (X_1, \dots, X_n) \le \theta_r(X_1, \dots, X_n) = \theta_r\)</span>，使得 <span class="math display">\[
P(\theta_l \le \theta \le \theta_r) \ge 1 - \alpha
\]</span> 我们称<span class="math inline">\(1 - \alpha\)</span>为<span class="math inline">\([\theta_;, \theta_r]\)</span>的<strong>置信水平</strong>（confidence level），<span class="math inline">\(\theta_l\)</span>为<strong>置信下限</strong>，<span class="math inline">\(\theta_r\)</span>为<strong>置信上限</strong>。一般来说置信水平不唯一，因为若<span class="math inline">\(1 - \alpha\)</span>是某个区间的置信水平，则对于任意<span class="math inline">\(\alpha &lt; \tilde \alpha &lt; 1\)</span>，<span class="math inline">\(1 - \tilde \alpha\)</span>亦是该区间的置信水平。故一般的“置信水平”是这一系列置信水平中的最大者。</p>
<p>区间估计的一般步骤为：</p>
<ol type="1">
<li><p>构造<span class="math inline">\(\theta\)</span>的一个点估计<span class="math inline">\(\hat \theta\)</span>（如<span class="math inline">\(\bar X\)</span>）</p></li>
<li><p>构造<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\hat \theta\)</span>的一个函数<span class="math inline">\(G = G(\theta, \hat \theta)\)</span>（称作主元（pivot）函数），且</p>
<ul>
<li><span class="math inline">\(G\)</span>的分布函数<span class="math inline">\(F\)</span>完全已知，且其分布与<span class="math inline">\(\theta\)</span>无关，</li>
<li>对任何常数<span class="math inline">\(a &lt; b\)</span>，不等式<span class="math inline">\(a \le G(\theta, \hat \theta) \le b\)</span>能够改写成等价的<span class="math inline">\(A \le \theta \le B\)</span>，且<span class="math inline">\(A,B\)</span>仅与<span class="math inline">\(\hat \theta,a,b\)</span>有关，与<span class="math inline">\(\theta\)</span>无关。</li>
</ul></li>
<li><p>取<span class="math inline">\(F\)</span>的上<span class="math inline">\(\alpha/2\)</span>分位点<span class="math inline">\(w_{\alpha/2}\)</span>及上<span class="math inline">\(1-\alpha/2\)</span>分位点<span class="math inline">\(w_{1-\alpha/2}\)</span>，此时有<span class="math inline">\(F(w_{\alpha/2}) - F(w_{1-w_{\alpha/2}}) = 1 - \alpha\)</span>，即 <span class="math display">\[
P(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}) = 1 - \alpha
\]</span> <span class="math inline">\(w_{1-\alpha/2} \le G(\theta, \hat \theta) \le w_{\alpha/2}\)</span>可改写为对应的<span class="math inline">\(A \le \theta \le B\)</span>的形式，且<span class="math inline">\(A, B\)</span>仅与估计量和两个分位点有关，<span class="math inline">\(A,B\)</span>就构成了<span class="math inline">\(\theta\)</span>的一个置信水平为<span class="math inline">\(1-\alpha\)</span>的置信区间。</p></li>
</ol>



          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/" rel="next">协方差与相关系数</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/notes/books/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/" rel="prev">大数定律和中心极限定理</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on Jan 1, 0001</p>

          





  

<p class="edit-page">
  <a href="https://github.com/wowchemy/hugo-notes-theme/edit/main/content/Books/%e6%a6%82%e7%8e%87%e8%ae%ba%e4%b8%8e%e6%95%b0%e7%90%86%e7%bb%9f%e8%ae%a1/%e5%8f%82%e6%95%b0%e4%bc%b0%e8%ae%a1.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>



          




          


        </div>

      </article>

      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2022 Chunxy. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

      

    
    <script src="/notes/js/vendor-bundle.min.53d67dc2cb1ebceb89d5e2aba2f86112.js"></script>

    
    
    
      

      
      

      

    

    
    
    

    
    
    <script src="https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":false}</script>

    
    
    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/notes/en/js/wowchemy.min.b7b574f4c1e92427575caf3142842f4a.js"></script>

    
    
    
    
    
    






</body>
</html>
