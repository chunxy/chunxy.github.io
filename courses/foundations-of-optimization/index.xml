<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Foundations of Optimization | Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/courses/foundations-of-optimization/</link>
      <atom:link href="https://chunxy.github.io/courses/foundations-of-optimization/index.xml" rel="self" type="application/rss+xml" />
    <description>Foundations of Optimization</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Jul 2022 12:58:49 +0000</lastBuildDate>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Foundations of Optimization</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/</link>
    </image>
    
    <item>
      <title>1-optimization-problem</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</guid>
      <description>

&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h2&gt;
&lt;p&gt;The standard optimization problem will be in the form: &lt;span class=&#34;math display&#34;&gt;\[
\inf_{x \in X} f(x) \\
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the
&lt;strong&gt;feasible/constraint region/set&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; is the objective
function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; is
an &lt;strong&gt;optimal solution&lt;/strong&gt; to the problem if &lt;span class=&#34;math inline&#34;&gt;\(v^* = f(x^*)\)&lt;/span&gt;. In this case, we also say
that &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; attains optimal value
&lt;span class=&#34;math inline&#34;&gt;\(v^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;
is a &lt;strong&gt;local minimizer&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\exists \epsilon &amp;gt; 0, \forall x \in B(x&amp;#39;,
\epsilon), f(x) \ge f(x&amp;#39;)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;types-of-problems&#34;&gt;Types of Problems&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Unconstrained: &lt;span class=&#34;math inline&#34;&gt;\(X =
\R^n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discrete programming&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a discrete set, which means
&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in X, \exists \epsilon &amp;gt; 0, X
\cap B(x, \epsilon) = \{ x \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Every feasible solution to discrete optimization problem is a
local minimizer.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X = \{ x \in \R^n | (a^i)^T x \le c_i,
i=1,2,\dots,m \}\)&lt;/span&gt; is a set defined by a &lt;strong&gt;finite&lt;/strong&gt;
number of linear inequalities.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f = b_1 x_1 + b_2 x_2 + \dots + b_n x_n =
b^T x\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quadratic programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same as that in linear
programming.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}
x_i x_j = x^T A x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A =
[a_{ij}] \in R^{n \times n}\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: This form does not include any linear term. Generally, a
quadratic function takes on the form &lt;span class=&#34;math inline&#34;&gt;\(f(x) =
x^T A x + b^T x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: We may assume &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is
symmetric, since even it is not, we can have &lt;span class=&#34;math inline&#34;&gt;\(A&amp;#39; = \frac{A + A^T}{2}\)&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
x^T A x = x^T A^T x \to x^T A x = x^T A&amp;#39; x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The right hand side is obvious because &lt;span class=&#34;math inline&#34;&gt;\(x^T A^T x\)&lt;/span&gt; is a number, and it equals to
its transpose &lt;span class=&#34;math inline&#34;&gt;\(x^T A x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Semi-definite programming&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Consider &lt;span class=&#34;math inline&#34;&gt;\(Q \in
\mathcal{S}^{n}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}^n\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; symmetric matrix. The
following is equivalent:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is &lt;strong&gt;positive
semi-definite&lt;/strong&gt; (short as PSD, denoted as &lt;span class=&#34;math inline&#34;&gt;\(Q \succcurlyeq 0\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in \R^n, x^T Q x \ge
0\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;All eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are
non-negative.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(C, A_1, \dots, A_m \in S^n\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R^m\)&lt;/span&gt; be given, the
semi-definite programming is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; C - \underbrace{\sum_{i=1}^m x_i A_i}_{M(x)}
\succcurlyeq 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The constraint is called a linear matrix inequality. Observe
that &lt;span class=&#34;math inline&#34;&gt;\(M: \R^m \mapsto S^n\)&lt;/span&gt; satisfies
&lt;span class=&#34;math display&#34;&gt;\[
M(\alpha x + \beta y) = \alpha M(x) + \beta M(y)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is a linear map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: Compare between linear programming and positive semi-definite
programming:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; (a^i)^T x \le c_i, \\
&amp;amp; i = 1,\dots,m
\end{aligned}
\quad \quad
\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \\
\textrm{s.t.} \quad &amp;amp; C - \sum_{i=1}^m x_i A_i \succcurlyeq 0
\end{aligned}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In linear programming, construct matrices:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[
C&amp;#39; =
\begin{pmatrix}
c_1 &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; c_2 &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp; \\
&amp;amp; &amp;amp; &amp;amp; c_m
\end{pmatrix},
A_i&amp;#39; =
\begin{pmatrix}
a_1^i &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; a_2^i &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp; \\
&amp;amp; &amp;amp; &amp;amp; a_m^i \\
\end{pmatrix}
\]&lt;/span&gt; Then $$ C’ - _{i=1}^m x_i A_i’ =
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
c_1 - \sum_{j=1}^m x_j a_1^j &amp;amp; &amp;amp; &amp;amp; \\
&amp;amp; c_2 - \sum_{j=1}^m x_j a_2^j &amp;amp; &amp;amp; \\
&amp;amp; &amp;amp; \ddots &amp;amp;  \\
&amp;amp; &amp;amp; &amp;amp; c_m - \sum_{j=1}^m x_j a_m^j &amp;amp;  \\
\end{pmatrix}\]&lt;/span&gt;
&lt;p&gt;$$ because a diagonal matrix is PSD if and only if all of its
diagonal entries are non-negative.&lt;/p&gt;
&lt;p&gt;In this sense, linear programming is special case of semi-definite
programming where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt;’s are all diagonal.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples-of-problems&#34;&gt;Examples of Problems&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Air traffic control&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; airplanes are arriving.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th airplane arrives within
&lt;span class=&#34;math inline&#34;&gt;\([a_i, b_i]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Assume airplanes arrive and land in order.&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; be the landing time
assigned to &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th airplane &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The metering time is defined to be the time difference between two
consecutive airplane landings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implicit constraints derived from above conditions is that &lt;span class=&#34;math inline&#34;&gt;\(a_i \le t_i \le b_i\)&lt;/span&gt; and $ t_i t_{i+1}$.
For safety, we want to the minimum metering time to be maximized. That
is, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t} \quad &amp;amp; f(t) \triangleq \min_{1 \le i \le n-1} t_{i+1} -
t_i \\
\textrm{s.t.} \quad &amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; The constraints are all linear. The &lt;code&gt;min&lt;/code&gt; operation
in &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, however, doesn’t comfort us.
We can introduce a new variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;
and convert the original problem to an equivalent one: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \\
\textrm{s.t.} \quad &amp;amp; z = \min_{1 \le i \le n-1} t_{i+1} - t_i \\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; Further, since the objective is to maximize and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;’s coefficient is positive, the problem
can be converted to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \\
\textrm{s.t.} \quad &amp;amp; z \le t_{i+1} - t_i, i=1,\dots,n-1\\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \\
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
\]&lt;/span&gt; Now the problem becomes a linear one and is easy to
solve.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data fitting problem&lt;/p&gt;
&lt;p&gt;Given data points &lt;span class=&#34;math inline&#34;&gt;\((a_i, b_i) \in \R^n
\times \R\)&lt;/span&gt;, a typical choice to fit those data would be an
affine function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = y^T x + t\)&lt;/span&gt;.
Other than the choice of function, the choice of objective function
matters too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Least squares&lt;/p&gt;
&lt;p&gt;The objective minimizes the sum of the squares of errors for all data
points: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} (y^T a_i - b_i)^2 \\
\end{aligned}
\]&lt;/span&gt; This is a quadratic programming problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimum absolute deviation &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} |y^T a_i - b_i| \\
\end{aligned}
\]&lt;/span&gt; Using the same trick of reparameterization, the problem is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \\
\textrm{s.t.} \quad &amp;amp; z_i = |y^T a_i - b_i|, i=1,\dots,m \\
\end{aligned}
\]&lt;/span&gt; Further, the trick of relaxation applies: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \\
\textrm{s.t.} \quad &amp;amp; y^T a_i - b_i \le z_i, i=1,\dots,m \\
&amp;amp; y^T a_i - b_i \ge -z_i, i=1,\dots,m \\
\end{aligned}
\]&lt;/span&gt; This becomes a linear programming problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key takeaway from these examples are the &lt;strong&gt;problem
equivalence&lt;/strong&gt; trick, which includes introducing new variables and
relaxing &lt;span class=&#34;math inline&#34;&gt;\(\max\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\min\)&lt;/span&gt; to inequalities.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>2-convex-set</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</guid>
      <description>

&lt;h2 id=&#34;convex-set&#34;&gt;Convex Set&lt;/h2&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k \in \R^n\)&lt;/span&gt;,
we say that &lt;span class=&#34;math inline&#34;&gt;\(y = \sum_{i=1}^k \alpha_i
x^{i}\)&lt;/span&gt; is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;linear combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots, \alpha_k \in \R\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;affine combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = 1\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;convex combination&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x^1, \dots, x^k\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^k \alpha_i = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 \le \alpha_1, \dots, \alpha_k\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(S \in \R^n\)&lt;/span&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;linear
subspace&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S,
\alpha,\beta \in \R, \alpha x + \beta y \in S\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is an &lt;strong&gt;affine
subspace&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S,
\alpha \in \R, \alpha x + (1-\alpha) y \in S\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;convex
set&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in S, 0 \le
\alpha \le 1, \alpha x + (1 - \alpha) y \in S\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is affine.&lt;/li&gt;
&lt;li&gt;Any affine combination of a finite number of points in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; belongs to &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\(S = {x} + V \triangleq \{ x + v: v \in V
\}\)&lt;/span&gt;. Note that though &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is
unique, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is convex.&lt;/li&gt;
&lt;li&gt;Any convex combination of a finite number of points in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; belongs to &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;examples-of-convex-set&#34;&gt;Examples of Convex Set&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Nonnegative orthant (in 2-D, an orthant is called a quadrant):
&lt;span class=&#34;math inline&#34;&gt;\(\R^n_+ \triangleq \{ x \in \R^n: \forall i,
x_i \ge 0 \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hyperplane&lt;/p&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^n, b \in R\)&lt;/span&gt;, the
hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s, c)\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: s^T x = c \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Half-space&lt;/p&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^n, b \in R\)&lt;/span&gt;, the
upper half-space &lt;span class=&#34;math inline&#34;&gt;\(H^+(s, c)\)&lt;/span&gt; is the
set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: s^T x \ge c \}\)&lt;/span&gt;;
the lower half-space &lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)\)&lt;/span&gt; is
the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n | s^T x \le c
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s, c)\)&lt;/span&gt; is
the intersection of &lt;span class=&#34;math inline&#34;&gt;\(H^+(s, c)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Euclidean ball&lt;/p&gt;
&lt;p&gt;Given the center &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \R^n\)&lt;/span&gt;
and the radius &lt;span class=&#34;math inline&#34;&gt;\(r &amp;gt; 0\)&lt;/span&gt;, the
Euclidean ball &lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r)\)&lt;/span&gt; is the
set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n: ||x - \bar x||_2 \le r
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A generalization of Euclidean ball would be to extend the norm to
other numbers that are larger than 1. For &lt;span class=&#34;math inline&#34;&gt;\(q
\ge 1\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
B_q(\bar x, r) = \{ x \in \R^n: ||x - \bar x||_q \le r \}
\]&lt;/span&gt; is also a convex set. Note that &lt;span class=&#34;math inline&#34;&gt;\(||z||_\infty = \lim_{q \to \infty} (\sum_i
z_i^q)^{1/q} = \max_i z_i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convex cone&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A set &lt;span class=&#34;math inline&#34;&gt;\(K \in \R^n\)&lt;/span&gt; is
called a &lt;strong&gt;cone&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x
\in K, \alpha &amp;gt; 0, \alpha x \in K\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Linear subspace is a cone. Affine subspace is not necessarily so.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;convex cone&lt;/strong&gt; is a cone that is
convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some examples of convex cone include &lt;span class=&#34;math inline&#34;&gt;\(\R^n_+\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^n\)&lt;/span&gt;, which is the set of
&lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; PSD matrices.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving
Operations&lt;/h3&gt;
&lt;p&gt;For any two convex sets &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;, there are some binary
operators that will preserve the convexity after being applied.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Set operations&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_1 \cup S_2\)&lt;/span&gt; is not necessarily
convex. &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2\)&lt;/span&gt; is always
convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Affine transformations&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto
\R^m\)&lt;/span&gt; is &lt;strong&gt;affine&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x,y \in \R^n, \alpha \in \R\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
A(\alpha x + (1 - \alpha) y) = \alpha A(x) + (1 - \alpha) A(y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto \R^m\)&lt;/span&gt; be
affine, &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be convex.
Then &lt;span class=&#34;math inline&#34;&gt;\(A(S) \triangleq \{ A(x): x \in S
\}\)&lt;/span&gt; is convex.&lt;/p&gt;
&lt;p&gt;There are two types of affine transformation worth noting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(A(x) = U
x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(U \in \R^{n \times
n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U U^T = U^T U=
I\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Projection&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(A(x) = P
x\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(P \in \R^{n \times
n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^2 = P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;orthogonal projection&lt;/strong&gt;, its projection matrix
further satisfies &lt;span class=&#34;math inline&#34;&gt;\(P^T = P\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an example of affine transformation, given center &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; and axes &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; which is positive definite, the
&lt;strong&gt;ellipsoid&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(E(\bar x,
Q)\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in \R^n | (x -
\bar x)^T Q (x - \bar x) \le 1 \}\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r) = E(\bar x, I/r^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: When &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is positive
semi-definite, it might occur that &lt;span class=&#34;math inline&#34;&gt;\(\{ x \in
\R^n | (x - \bar x)^T Q (x - \bar x) \le 1 \}\)&lt;/span&gt; will degenerate
into two parallel lines. Just consider the case when &lt;span class=&#34;math inline&#34;&gt;\(Q = [1,2] [1,2]^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: There always exists an affine transformation &lt;span class=&#34;math inline&#34;&gt;\(A: \R^n \mapsto \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A(B(0, 1)) = E(\bar x, Q)\)&lt;/span&gt;. Note that
&lt;span class=&#34;math inline&#34;&gt;\(B(\bar x, r) = E(\bar x, I/r^2)\)&lt;/span&gt;.
See &lt;a href=&#34;../2-cvxanal.pdf&#34;&gt;this handout&lt;/a&gt; for the construction of
such transformation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, ellipsoid is also a convex set (Problem 2 of Homework 1
proves this from the first principle).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;topological-preparation&#34;&gt;Topological Preparation&lt;/h3&gt;
&lt;h4 id=&#34;basic-topology&#34;&gt;Basic Topology&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Given a set &lt;span class=&#34;math inline&#34;&gt;\(S \in \R^n, S \ne
\emptyset\)&lt;/span&gt; and a point &lt;span class=&#34;math inline&#34;&gt;\(x \notin
S\)&lt;/span&gt;, we want to find a point in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that is closest (in terms of Euclidean
distance) to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Formally, &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \arg\min_{z \in S} ||z - x||_2\)&lt;/span&gt;
is called the &lt;strong&gt;projection&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; onto &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, denoted as &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that this projection does not necessarily exist. Neither the
projection is unique. Under what conditions can we guarantee the
existence and uniqueness of projection? Before that, some concepts are
needed. Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be a
set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is an
&lt;strong&gt;interior point&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\exists
\epsilon &amp;gt; 0, B(x, \epsilon) \subseteq S\)&lt;/span&gt;. The collection of
all interior points of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is called
the &lt;strong&gt;interior&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;,
denoted as &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathrm{int}}
S\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We say that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is
&lt;strong&gt;open&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(S =
\mathop{\mathrm{int}} S\)&lt;/span&gt;. We say that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is &lt;strong&gt;closed&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\R^n \setminus S\)&lt;/span&gt; is open. Note that it
can be the case that a set is neither open nor closed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The intersection of any family of closed sets is
closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be continuous and &lt;span class=&#34;math inline&#34;&gt;\(c \in
\R\)&lt;/span&gt; be a constant number. Then &lt;span class=&#34;math inline&#34;&gt;\(S =
\{ x \in \R^n: f(x) \le c \}\)&lt;/span&gt; is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is closed if and
only if for every convergent sequence &lt;span class=&#34;math inline&#34;&gt;\(\{ x_n
\}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, its limit is in
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
\R^n\)&lt;/span&gt; is &lt;strong&gt;compact&lt;/strong&gt; if it is closed and bounded
(&lt;span class=&#34;math inline&#34;&gt;\(\exists M &amp;gt; 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq B(0, M)\)&lt;/span&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;projection&#34;&gt;Projection&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Weierstrass theorem&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be continuous and
&lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be compact. Then
&lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \in S} f(x)\)&lt;/span&gt; always has a
solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be
non-empty, closed, and convex. Then for every &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;, there exists a unique &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Existence&lt;/p&gt;
&lt;p&gt;We may assume that &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;.
Consider any &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \in S\)&lt;/span&gt; and
define &lt;span class=&#34;math inline&#34;&gt;\(T \triangleq S \cap B(x, ||x -
x&amp;#39;||_2)\)&lt;/span&gt;. Observe that&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\arg \min_{z \in S} ||x - z||_2 = \arg
\min_{z \in T} ||x - z||_2\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is closed and bounded.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then by &lt;em&gt;Weierstrass’ theorem&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; that solves &lt;span class=&#34;math inline&#34;&gt;\(\min_{z \in T} ||x - z||_2\)&lt;/span&gt; exists. And by
Point 1 above, &lt;span class=&#34;math inline&#34;&gt;\(\hat x = \Pi_S(x)\)&lt;/span&gt;.
Note that above argument does not need convexity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uniqueness&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 =
\Pi_S(x), \hat x_2 = \Pi_S(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 \ne \hat x_2\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(z = (\hat x_1 + \hat x_2) / 2\)&lt;/span&gt;. By
convexity, &lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s distance to &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; can be calculated as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
|| x - (\hat x_1 + \hat x_2)/2 ||_2 &amp;amp;= || (x - \hat x_1)/2 + (x -
\hat x_2)/2 ||_2 \\
&amp;amp;\le || (x - \hat x_1)/2 ||_2 + || (x - \hat x_2)/2 ||_2 \\
&amp;amp;= \min_{z \in S} ||x - z||_2
\end{aligned}
\]&lt;/span&gt; If the &lt;span class=&#34;math inline&#34;&gt;\(\le\)&lt;/span&gt; is strict, the
fact that &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat x_2\)&lt;/span&gt; are the projection will be
contradicted; else &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_1)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_2)\)&lt;/span&gt; are collinear,
which means &lt;span class=&#34;math inline&#34;&gt;\(x, \hat x_1, \hat x_2\)&lt;/span&gt;
are collinear, in which case the only possible way for &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((x - \hat x_2)\)&lt;/span&gt; to be equal is &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 = \hat x_2\)&lt;/span&gt;, contradicting the
assumption &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 \ne \hat
x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As a result, &lt;span class=&#34;math inline&#34;&gt;\(\hat x_1 = \hat
x_2\)&lt;/span&gt;, which means the projection of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is unique.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be
non-empty, closed, and convex. Then for any &lt;span class=&#34;math inline&#34;&gt;\(x \in \R^n\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\hat x = \Pi_{S}(x) \iff \forall z \in S, (z - \hat x)^T(x - \hat x) \le
0
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 \le \alpha \le 1\)&lt;/span&gt;, define &lt;span class=&#34;math inline&#34;&gt;\(z(\alpha) = \alpha z + (1 - \alpha) \hat
x\)&lt;/span&gt;. By convexity, &lt;span class=&#34;math inline&#34;&gt;\(z(\alpha) \in
S\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
||\hat x - x||_2^2 \le ||z(\alpha) - x||_2^2
\]&lt;/span&gt; Note on the other hand that &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\quad ||z(\alpha) - x||_2^2 = [z(\alpha) - x]^T [z(\alpha) - x] \\
&amp;amp;= [\hat x + \alpha (z - \hat x) - x]^T [\hat x + \alpha (z - \hat
x) - x] \\
&amp;amp;= [(\hat x - x) + \alpha (z - \hat x)]^T [(\hat x - x) + \alpha (z
- \hat x)] \\
&amp;amp;= ||\hat x - x||_2^2 + 2\alpha (z - \hat x)^T (\hat x - x) +
\alpha^2 ||z - \hat x||_2^2
\end{aligned}
\]&lt;/span&gt; To make the above hold for any &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in [0,1]\)&lt;/span&gt;, it must be the case
that &lt;span class=&#34;math inline&#34;&gt;\((z - \hat x)^T (\hat x - x) \ge
0\)&lt;/span&gt; or equivalently &lt;span class=&#34;math display&#34;&gt;\[
(z - \hat x)^T (x - \hat x) \le 0
\]&lt;/span&gt; Convexity is crucial in this property. Just consider the
following case:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./convexity-is-important.png&#34; style=&#34;zoom: 50%;&#34;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that there exists some &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \ne \Pi_S(x)\)&lt;/span&gt; such that for every
&lt;span class=&#34;math inline&#34;&gt;\(z \in S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
(z - x&amp;#39;)^T (x - x&amp;#39;) \le 0
\]&lt;/span&gt; Then for &lt;span class=&#34;math inline&#34;&gt;\(\Pi_S(x) \in S\)&lt;/span&gt;,
we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
(\Pi_S(x) - x&amp;#39;)^T (x - x&amp;#39;) \le 0 \label{eq1}
\end{equation}
\]&lt;/span&gt; From the proof of sufficiency we know that for this specific
&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
(x&amp;#39; - \Pi_S(x))^T (x - \Pi_S(x)) \le 0 \label{eq2}
\end{equation}
\]&lt;/span&gt; Add up together &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq2}\)&lt;/span&gt; to give &lt;span class=&#34;math display&#34;&gt;\[
(\Pi_S(x) - x&amp;#39;)^T (\Pi_S(x) - x&amp;#39;) \le 0
\]&lt;/span&gt; This indicates that &lt;span class=&#34;math inline&#34;&gt;\(\Pi_S(x) =
x&amp;#39;\)&lt;/span&gt;, which concludes the proof.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;separation&#34;&gt;Separation&lt;/h4&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt;,
it is easy to certify that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 \ne
\emptyset\)&lt;/span&gt; so long as there is a &lt;span class=&#34;math inline&#34;&gt;\(x
\in S_1\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(x \in
S_2\)&lt;/span&gt;. But how can one certify that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 = \emptyset\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;One geometric intuition is to find a hyperplane &lt;span class=&#34;math inline&#34;&gt;\(H(s,c)\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(S_1 \subseteq H^+(s,c) \setminus H(s,c)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(S_2 \subseteq H^-(s,c) \setminus
H(s,c)\)&lt;/span&gt;. Obviously a hyperplane separation won’t always work.
But it helps in the discussion of convex set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;point-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be non-empty, closed and
convex. Let &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Then there
exists &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\left( \max_{z \in S} y^T z \right) &amp;lt; y^T x
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;p&gt;By the projection theorem, &lt;span class=&#34;math inline&#34;&gt;\(\hat x
\triangleq \Pi_S(x)\)&lt;/span&gt; exists and is unique. Take &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\((x -
\hat x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y \ne \vec 0\)&lt;/span&gt;
because &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Then according
the projection’s property, for every &lt;span class=&#34;math inline&#34;&gt;\(z \in
S\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\begin{aligned}
(z - \hat x)^T \underbrace{(x - \hat x)}_{y} &amp;amp;\le 0 \\
z^T y - \hat x^T y &amp;amp;\le 0 \\
\end{aligned} \quad \Rightarrow \quad
\begin{aligned}
y^T z &amp;amp;\le y^T (x  - y) \\
&amp;amp;\le y^T x - ||y||_2^2 \\
&amp;amp;&amp;lt; y^T x
\end{aligned}
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in S} y^T z\)&lt;/span&gt; is
obtained at &lt;span class=&#34;math inline&#34;&gt;\(z = \hat x\)&lt;/span&gt;, which
concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The theorem above is an algebraic description of point-set separation
and in essence reveals a hyperplane separation. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is exactly the normal vector of this
hyperplane. &lt;span class=&#34;math inline&#34;&gt;\((\max_{z \in S} y^T z) &amp;lt; y^T
x\)&lt;/span&gt; actually says that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is
more distant in the direction of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;
than every point &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A direct result of the theorem above is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A closed convex set &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
\R^n\)&lt;/span&gt; is the intersection of all half-spaces containing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, i.e. &lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{\substack{\text{$S \subseteq H$ and} \\ \text{$H$ is a
halfspace}}} H
\]&lt;/span&gt; Proof:&lt;/p&gt;
&lt;p&gt;Without loss of generality, due to that &lt;span class=&#34;math inline&#34;&gt;\(H^+(s,c) = H^-(-s,-c)\)&lt;/span&gt;, we claim that
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the intersection of all the
lower half-spaces containing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{H^-(s, c) \supseteq S} H^-(s,c)
\]&lt;/span&gt; We begin with two special cases: &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;. We show that &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; satisfies the above because &lt;span class=&#34;math display&#34;&gt;\[
\emptyset = H^-(0, -1) \cap \text{any lower halfspace} \\
\R^n = \bigcap_{c \ge 0} H^-(0, c)
\]&lt;/span&gt; Now consider any set &lt;span class=&#34;math inline&#34;&gt;\(\emptyset
\subsetneq S \subsetneq \R^n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x
\notin S\)&lt;/span&gt;. Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, there
exists &lt;span class=&#34;math inline&#34;&gt;\(y_x \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
c_x \triangleq \max_{z \in S} y_x^T z &amp;lt; y_x^T x
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq H^-_x
\triangleq H^-(y_x, c_x)\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(x \notin H^-_x\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq H^-_x\)&lt;/span&gt; holds for any &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S \subseteq \bigcap_{x \notin S} H^-_x
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\bigcap_{x
\notin S} H^-_x \subseteq S\)&lt;/span&gt;. Suppose on the contrary there
exists &lt;span class=&#34;math inline&#34;&gt;\(z \in \bigcap_{x \notin S}
H^-_x\)&lt;/span&gt; but &lt;span class=&#34;math inline&#34;&gt;\(z \notin S\)&lt;/span&gt;.
However, &lt;span class=&#34;math display&#34;&gt;\[
z \in \bigcap_{x \notin S} H^-_x = \bigcap_{\{ x \notin S: x \ne z \}
\cup \{ z \} } H^-_x \subseteq H^-_z
\]&lt;/span&gt; which contradicts the fact that &lt;span class=&#34;math inline&#34;&gt;\(z
\notin H^-_z\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\bigcap_{x
\notin S} H^-_x \subseteq S\)&lt;/span&gt; and consequently &lt;span class=&#34;math display&#34;&gt;\[
S = \bigcap_{x \notin S} H^-_x
\]&lt;/span&gt; Notice that &lt;span class=&#34;math inline&#34;&gt;\(\{ H^-_x: x \notin S
\}\)&lt;/span&gt; contains all the lower half-spaces that superset &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Because for every lower half-space
&lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c) \supseteq S\)&lt;/span&gt;, there exists
&lt;span class=&#34;math inline&#34;&gt;\(x \in H^+(s, c) \setminus H(s, c)\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;. For this
specific &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, there exists &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S} s^T z \le c &amp;lt; s^T x
\]&lt;/span&gt; Therefore, every &lt;span class=&#34;math inline&#34;&gt;\(H^-(s, c)
\supseteq S\)&lt;/span&gt; can be written as &lt;span class=&#34;math inline&#34;&gt;\(H^-_x\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(x \notin S\)&lt;/span&gt;, which concludes the
proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By far, the separation between a point (or a set of a single point)
and a set is settled. Now we consider the set-set separation. It is easy
to conjecture the geometric intuition derived above as the
following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conjecture: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt; be two non-empty,
closed and convex sets with &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt;. Then there exists &lt;span class=&#34;math inline&#34;&gt;\(y \in
\R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
\]&lt;/span&gt; Disproof:&lt;/p&gt;
&lt;p&gt;Just consider &lt;span class=&#34;math inline&#34;&gt;\(S_1 = \{ (u, v): u \ge 1/v,
v \ge 1 \}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_2 = \{ (u, 0): u
\ge 1 \}\)&lt;/span&gt;. The only possible separation hyperplane is the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis, with the corresponding normal
vector &lt;span class=&#34;math inline&#34;&gt;\(y = [0, -1]^T\)&lt;/span&gt;. However in
this case, &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in S_1} y^T z\)&lt;/span&gt;
does not exist at all and thus &lt;span class=&#34;math inline&#34;&gt;\(\max_{z \in
S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z\)&lt;/span&gt; does not hold.&lt;/p&gt;
&lt;p&gt;Trivially changing &lt;span class=&#34;math inline&#34;&gt;\(\max\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sup\)&lt;/span&gt; won’t help, since &lt;span class=&#34;math inline&#34;&gt;\(\sup_{z \in S_1} [0, -1] z = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lack of boundedness forbids us to take the advantage of
&lt;em&gt;Weierstrass’ theorem&lt;/em&gt; to show the existence of maxima. It would
be cheering if the predicate of the conjecture can be relaxed so that
boundedness and thus compactness applies, yielding the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2 \subseteq \R^n\)&lt;/span&gt; be two non-empty,
closed and convex sets, with &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt; and either &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt;
or &lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt; being bounded. Then there
exists &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^n\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
\]&lt;/span&gt; Hint of proof:&lt;/p&gt;
&lt;p&gt;We only show the proof when &lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;
is bounded. The case when &lt;span class=&#34;math inline&#34;&gt;\(S_1\)&lt;/span&gt; is
bounded can be handled similarly.&lt;/p&gt;
&lt;p&gt;Consider the set &lt;span class=&#34;math inline&#34;&gt;\(S \triangleq \{ x - y: x
\in S_1, y \in S_2 \}\)&lt;/span&gt; (which will be &lt;span class=&#34;math inline&#34;&gt;\(\{ x - y: x \in S_2, y \in S_1 \}\)&lt;/span&gt; in the
other case). Since &lt;span class=&#34;math inline&#34;&gt;\(S_1 \cap S_2 =
\emptyset\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0 \notin S\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is non-empty obviously. &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; can be further verified to be closed
and convex (??). Then the property of point-set separation can be
applied to proceed with the proof.&lt;/p&gt;
&lt;p&gt;That is, there exists &lt;span class=&#34;math inline&#34;&gt;\(u \in \R^n\)&lt;/span&gt;
such that &lt;span class=&#34;math display&#34;&gt;\[
\left( \max_{z \in S} u^T z \right) &amp;lt; u^T \cdot 0 = 0 \\
\Downarrow \\
\max_{x \in S_1, y \in S_2} u^T (x - y) &amp;lt; 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(y^* = \min_{y \in S_2} u^T y\)&lt;/span&gt; (such &lt;span class=&#34;math inline&#34;&gt;\(y^*\)&lt;/span&gt; exists because the compactness of
&lt;span class=&#34;math inline&#34;&gt;\(S_2\)&lt;/span&gt;) to give &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\max_{x \in S_1} u^T (x - y^*) &amp;amp;&amp;lt; 0 \\
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; u^T y^* \\
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; \min_{y \in S_2} u^T y \\
\end{aligned}
\]&lt;/span&gt; which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a richer discussion of convex set separation, please refer &lt;a href=&#34;https://www.wikiwand.com/en/Hyperplane_separation_theorem&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>3-convex-function</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</guid>
      <description>

&lt;h2 id=&#34;convex-function&#34;&gt;Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R_+\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\R_+ = \R \cup \{
+\infty \}\)&lt;/span&gt; (in convex discussion, usually only &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt; is included) be an extended
real-valued function. Note that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
shouldn’t trivially be &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;
everywhere. We say that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
&lt;strong&gt;convex&lt;/strong&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\forall x_1, x_2
\in \R^n, \alpha \in [0,1]\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
f(\alpha x_1 + (1-\alpha) x_2) \le \alpha f(x_1) + (1-\alpha) f(x_2)
\]&lt;/span&gt; Note that for &lt;span class=&#34;math inline&#34;&gt;\(x \in \R\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; +\infty, x + \infty = \infty + x =
\infty, +\infty \le +\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interestingly, convex functions on &lt;u&gt;an open domain&lt;/u&gt; are always
continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The &lt;strong&gt;epigraph&lt;/strong&gt; of a function &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\epi f \triangleq \{ (x, t) \in \R^n \times \R:
f(x) \le t \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Definition: The &lt;strong&gt;effective domain&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the set &lt;span class=&#34;math inline&#34;&gt;\(\dom f \triangleq \{ x \in \R^n: f(x) &amp;lt; \infty
\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that the real line does not contain &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. Therefore, the effective domain
of &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x\)&lt;/span&gt; is still &lt;span class=&#34;math inline&#34;&gt;\(\R\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt;
be a set. The &lt;strong&gt;indicator&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the function &lt;span class=&#34;math display&#34;&gt;\[
\mathbb 1_S (x) =
\begin{cases}
0, &amp;amp; x \in S \\
+\infty, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the indicator, we have &lt;span class=&#34;math display&#34;&gt;\[
\underset{\text{constrained}}{\inf_{x \in S} f(x)} \iff
\underset{\text{unconstrained}}{\inf_{x \in \R^n} f(x) + \mathbb 1_S(x)}
\]&lt;/span&gt; That is, we convert a constrained problem to a unconstrained
one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition (verify it): Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n
\mapsto \R_+\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is
convex (as a function) if and only if &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; is convex (as a set). Moreover, let &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq \R^n\)&lt;/span&gt; be a set. Then &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is convex (as a set) if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb 1_S\)&lt;/span&gt; is convex (as a
function).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proposition above associates the convexity of a function with
that of its epigraph; and the convexity of a set with that of its
indicator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Jensen’s inequality&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R_+\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if &lt;span class=&#34;math inline&#34;&gt;\(f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m
\alpha_i f(x_i)\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(m \in
\N^+\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_m \in
\R^n\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots,
\alpha_m \ge 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^m \alpha_i = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Sufficiency is easy to show by interpreting the definition of convex
function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,m\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\big( x_i, f(x_i) \big) \in \epi f
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\epi f\)&lt;/span&gt; is convex,
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^m \alpha_i \big( x_i, f(x_i) \big) =  \big( \sum_{i=1}^m
\alpha_i x_i, \sum_{i=1}^m \alpha_i f(x_i) \big) \in \epi f
\]&lt;/span&gt; which means &lt;span class=&#34;math display&#34;&gt;\[
f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving
Operations&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non-negative combination&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f_1, \dots, f_m\)&lt;/span&gt; be convex
functions, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1, \dots, \alpha_m \ge
0\)&lt;/span&gt; be non-negative scalars. Then, &lt;span class=&#34;math display&#34;&gt;\[
f \triangleq \sum_{i=1}^m \alpha_i f_i \text{ is convex.}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pointwise supremum&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; be an index set (either
finite or infinite) and &lt;span class=&#34;math inline&#34;&gt;\(\{ f_i: i \in I
\}\)&lt;/span&gt; be a collection of convex functions. Then &lt;span class=&#34;math display&#34;&gt;\[
f \triangleq \sup_{i \in I} f_i \text{ is convex.}
\]&lt;/span&gt; To show it, let &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2 \in
\R^n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in [0,
1]\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;f(\alpha x_1 + (1-\alpha) x_2) = \sup_{i \in I} f_i(\alpha x_1 +
(1-\alpha) x_2) \\
&amp;amp;\le \sup_{i \in I} \big( \alpha f_i(x_1) + (1-\alpha) f_i(x_2)
\big) \\
&amp;amp;\le [\alpha \sup_{i \in I} f_i(x_1)] + [(1-\alpha) \sup_{i \in I}
f_i(x_2)] \\
&amp;amp;= \alpha f(x_1) + (1-\alpha) f(x_2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Geometrically, pointwise supremum is intersecting the epigraphs of
&lt;span class=&#34;math inline&#34;&gt;\(f_i\)&lt;/span&gt;’s. &amp;gt; Example: Consider the
mapping &lt;span class=&#34;math inline&#34;&gt;\(f: \R^{m \times n} \supseteq X \to
||X||\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(||X||\)&lt;/span&gt; is the
largest singular value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Show
that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex. &amp;gt; &amp;gt; By the
&lt;em&gt;Courant-Fischer theorem&lt;/em&gt;, &amp;gt; &lt;span class=&#34;math display&#34;&gt;\[
&amp;gt; \begin{aligned}
&amp;gt; ||X|| &amp;amp;= \max_{u \in \R^m, v \in \R^n} u^T X v \\
&amp;gt; \text{s.t.} &amp;amp;\quad ||u||_2 = 1, ||v||_2 = 1
&amp;gt; \end{aligned}
&amp;gt; \]&lt;/span&gt; &amp;gt; Let &lt;span class=&#34;math inline&#34;&gt;\(f_{u,v}(X) = u^T X
v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I = \{ (u,v) \in \R^m \times
\R^n: ||u||_2 = 1, ||v||_2 = 1 \}\)&lt;/span&gt;. Then, &amp;gt; &lt;span class=&#34;math display&#34;&gt;\[
&amp;gt; f(X) = \max_{(u,v) \in I} f_{u,v}(X)
&amp;gt; \]&lt;/span&gt; &amp;gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(f_{u,v}(X)\)&lt;/span&gt; is linear and thus convex in
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. It follows directly from the
pointwise supremum principle that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
is convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Composition with increasing function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(g: \R^n \mapsto \R\)&lt;/span&gt; be
convex, &lt;span class=&#34;math inline&#34;&gt;\(h: \R \mapsto \R\)&lt;/span&gt; be convex.
&lt;span class=&#34;math inline&#34;&gt;\(h \circ g\)&lt;/span&gt; is not generally convex.
To verify it, take &lt;span class=&#34;math display&#34;&gt;\[
h(x) = -x, g(x) = x^2
\]&lt;/span&gt; But if &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is convex as
well as increasing, then &lt;span class=&#34;math inline&#34;&gt;\(h \circ g\)&lt;/span&gt;
is convex.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Restriction on lines&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a point &lt;span class=&#34;math inline&#34;&gt;\(x_0 \in \R^n\)&lt;/span&gt; and a
direction &lt;span class=&#34;math inline&#34;&gt;\(h \in \R^n \setminus \{ 0
\}\)&lt;/span&gt;, we call the set &lt;span class=&#34;math display&#34;&gt;\[
\{ x_0 + t h: t \in \R \}
\]&lt;/span&gt; a line through &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; in the
direction &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be a function.
Define&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde f_{x_0, h}(t) \triangleq f(x_0 + t h)
\]&lt;/span&gt; as the &lt;strong&gt;restriction of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; on the line&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{ x_0 + t h: t \in \R \}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\tilde f_{x_0, h}\)&lt;/span&gt; is convex for any
&lt;span class=&#34;math inline&#34;&gt;\(x_0 \in \R^n\)&lt;/span&gt; and any &lt;span class=&#34;math inline&#34;&gt;\(h \in \R^n \setminus \{ 0 \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;differentiable-convex-function&#34;&gt;Differentiable Convex
Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;
be differentiable, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla f\)&lt;/span&gt;
exists. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex if and
only if for every &lt;span class=&#34;math inline&#34;&gt;\(x, y \in \R^n\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
f(x) \ge f(y) + (\nabla f(y))^T (x - y) \tag{gradient inequality}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a fixed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, the right-hand
side of the gradient inequality is in essence an affine function of
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The graph of this affine
function is &lt;span class=&#34;math display&#34;&gt;\[
t(x) = f(y) + (\nabla f(y))^T (x - y) \\
\Updownarrow \\
\underbrace{[(\nabla f(y))^T, -1]}_{s^T} \underbrace{\begin{bmatrix}
x \\
t
\end{bmatrix}}_{z} - \underbrace{[(\nabla f(y))^T y - f(y)]}_{c} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, the normal vector &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; always points downwards (due to the
&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; be twice
continuously differentiable, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla f,
\nabla^2 f\)&lt;/span&gt; exist and &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2
f\)&lt;/span&gt; is continuous. Then, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
is convex if and only if &lt;span class=&#34;math display&#34;&gt;\[
\nabla^2 f(x) \succcurlyeq 0, \forall x \in \R^n
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \mathcal{S}_{++}^n
\mapsto \R\)&lt;/span&gt; be given by &lt;span class=&#34;math inline&#34;&gt;\(f(X) = -\ln
\det (X)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_{++}^n\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; positive definite matrix. Show
that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is convex.&lt;/p&gt;
&lt;p&gt;This problem usually emerges when calculating the capacity of channel
in information theory. To show it, we construct &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;’s restriction on all the lines. Let
&lt;span class=&#34;math inline&#34;&gt;\(X_0 \in \mathcal{S}_{++}^n\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(H \in \mathcal{S}^n \setminus \{ 0
\}\)&lt;/span&gt;. Define &lt;span class=&#34;math display&#34;&gt;\[
g(t) \triangleq \tilde f(X_0 + t H) = -\ln \det(X_0 + t H)
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(X_0 \in
\mathcal{S}_{++}^n\)&lt;/span&gt;, we can write &lt;span class=&#34;math inline&#34;&gt;\(X_0 = X_0^{1/2} X_0^{1/2}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X_0^{1/2} \in \mathcal{S}_{++}^n\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(X_0^{-1/2} = (X_0^{1/2})^{-1}\)&lt;/span&gt;.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
g(t) &amp;amp;= -\ln \det(X_0^{1/2} (I + t X_0^{-1/2} H X_0^{-1/2})
X_0^{1/2}) \\
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2})
\end{align*}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(X_0^{-1/2} H
X_0^{-1/2}\)&lt;/span&gt; is real symmetric. Let &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1, \dots, \lambda_n\)&lt;/span&gt; be its
eigenvalues. Then &lt;span class=&#34;math inline&#34;&gt;\((I + t X_0^{-1/2} H
X_0^{-1/2})\)&lt;/span&gt;’s eigenvalues are &lt;span class=&#34;math inline&#34;&gt;\(t
\lambda_1 + 1, \dots, t \lambda_n + 1\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
g(t) &amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H
X_0^{-1/2}) \\
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \sum_i \ln(t\lambda_i + 1)
\end{align*}
\]&lt;/span&gt; Now it is easy to verify that &lt;span class=&#34;math inline&#34;&gt;\(-\ln(t\lambda_i + 1)\)&lt;/span&gt;’s are convex, which
concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;differentiability-agnostic-convex-function&#34;&gt;Differentiability-agnostic
Convex Function&lt;/h2&gt;
&lt;p&gt;“Differentiability-agnostic” means that no premise on the
differentiability of the convex function is assumed. In this section, we
discuss several “differentiability-agnostic” topics.&lt;/p&gt;
&lt;h3 id=&#34;subgradient&#34;&gt;Subgradient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a function. A vector &lt;span class=&#34;math inline&#34;&gt;\(s \in
\R^n\)&lt;/span&gt; is called a &lt;strong&gt;subgradient&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; if &lt;span class=&#34;math display&#34;&gt;\[
f(x) \ge f(x_0) + s^T (x - x_0), \forall x \in \R^n
\]&lt;/span&gt; The set &lt;span class=&#34;math display&#34;&gt;\[
\partial f(x_0) \triangleq \{ s \in \R^n: \text{$s$ is a subgradient of
$f$ at $x_0$} \}
\]&lt;/span&gt; is called the &lt;strong&gt;sub-differential&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;. Sub-differential is always convex
and closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of subgradient is quite inspired from the gradient
inequality. The geometric interpretation of subgradient resembles that
of gradient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt;
be convex. Then, &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x)\)&lt;/span&gt; is
non-empty, convex and compact (non-emptiness is due to the continuity
(which in turn is due to the openness of &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;) and boundedness is due to
convexity). Moreover,&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is differentiable at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; if and only if &lt;span class=&#34;math inline&#34;&gt;\(\partial f(x_0) = \{ \nabla f(x_0)
\}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; is the global minima if and
only if &lt;span class=&#34;math inline&#34;&gt;\(0 \in \partial f(x_0)\)&lt;/span&gt;;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The topological properties that the sub-differential possesses imply
that we can do projection on the sub-differential; besides we may apply
Weierstrass theorem to solve some optimization problem w.r.t. the
sub-differential (such as what is the minimal-norm subgradient).&lt;/p&gt;
&lt;h3 id=&#34;conjugate-function&#34;&gt;Conjugate Function&lt;/h3&gt;
&lt;p&gt;If two functions have the identical epigraphs, then these two
functions are identical. That’s the geometric view of a function.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be
convex. &lt;span class=&#34;math inline&#34;&gt;\(\epi f\)&lt;/span&gt; is known to be
convex. Suppose in addition that &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; is non-empty and closed. Recall that a non-empty, closed and
convex set can be written as the intersection of lower halfspaces that
contain it: &lt;span class=&#34;math display&#34;&gt;\[
\epi f = \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T,
y_0]^T, c)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s \in \R^n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_0 \in \R\)&lt;/span&gt;. We argue that &lt;span class=&#34;math inline&#34;&gt;\(y_0 \le 0\)&lt;/span&gt; because the last entry of some
&lt;span class=&#34;math inline&#34;&gt;\(z \in \epi f\)&lt;/span&gt; can be arbitrarily
large. We further argue that &lt;span class=&#34;math inline&#34;&gt;\(y_0 &amp;lt;
0\)&lt;/span&gt; or else &lt;span class=&#34;math inline&#34;&gt;\(f(x) = -\infty\)&lt;/span&gt;
everywhere. Without loss of generality let &lt;span class=&#34;math inline&#34;&gt;\(y_0 = -1\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T, y_0]^T,c) = H^-([-s^T/y_0,
-1]^T,c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice that this lower half-space &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T, -1]^T, c)\)&lt;/span&gt; is exactly the
epigraph of the affine function &lt;span class=&#34;math display&#34;&gt;\[
h_{s, c}(x) = s^T x - c
\]&lt;/span&gt; because for any &lt;span class=&#34;math inline&#34;&gt;\((x, y) \in \R^n
\times \R\)&lt;/span&gt; that belongs to &lt;span class=&#34;math inline&#34;&gt;\(H^-([s^T,
-1]^T, c)\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
s^T x - y &amp;amp;\le c \\
y &amp;amp;\ge s^T x - c
\end{aligned}
\]&lt;/span&gt; On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\epi f
\subseteq H^-([s^T, -1]^T, c) = \epi{h_{s, c}}\)&lt;/span&gt; indicates that
&lt;span class=&#34;math inline&#34;&gt;\(\forall x \in \R^n, f(x) \ge h_{s,
c}(x)\)&lt;/span&gt;. Consequently, &lt;span class=&#34;math inline&#34;&gt;\(\epi
f\)&lt;/span&gt; can be re-written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\epi f &amp;amp;= \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T,
-1]^T,c) \\
&amp;amp;= \bigcap_{\epi{h_{s, c}} \supseteq \epi f} \epi{h_{s, c}} \\
&amp;amp;= \bigcap_{f \ge h_{s, c}} \epi{h_{s, c}}
\end{aligned}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; can be
written as the pointwise supremum of the affine functions who are less
than &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
f = \sup_{h \le f} h
\]&lt;/span&gt; The normal vectors of those affine functions that are less
than &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and that pass through &lt;span class=&#34;math inline&#34;&gt;\((x_0, f(x_0))\)&lt;/span&gt; essentially forms the
sub-differential of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider the set &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_f &amp;amp;= \{ (y, t) \in \R^n \times \R: y^T x - t \le f(x), \forall x
\in \R^n \} \\
&amp;amp;= \{ (y, t) \in \R^n \times \R: \sup_{x}(y^T x - f(x)) \le t \} \\
&amp;amp;\triangleq \epi{f^*}
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
f^*(y) \triangleq \sup_{x}(y^T x - f(x))
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(f^*(y)\)&lt;/span&gt; is called the
&lt;strong&gt;conjugate function&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>4-linear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</guid>
      <description>

&lt;h2 id=&#34;linear-programming&#34;&gt;Linear Programming&lt;/h2&gt;
&lt;p&gt;Recall that the linear programming problem is &lt;span class=&#34;math display&#34;&gt;\[
\label{lp} \begin{aligned}
\min_{x} \quad&amp;amp; c^T x \\
\text{s.t.} \quad&amp;amp; a_i^T x \le b_i, i=1,\dots,m
\end{aligned} \tag{LP}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c \in \R^n\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(a_i \in \R^{n}, b_i \in \R,
i=1,\dots,m\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;polyhedron&lt;/strong&gt; is the intersection of a
finite set of halfspaces. A bounded polyhedron is called a
&lt;strong&gt;polytope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the feasible set of LP is a polyhedron. Interestingly, all
polyhedrons are convex. That is, the feasible set of LP is convex.&lt;/p&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;The standard form of LP is &lt;span class=&#34;math display&#34;&gt;\[
\label{standard-lp} \begin{aligned}
\min_{x \in \R_+^n} \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; A x = b \\
\end{aligned} \tag{Standard LP}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c \in \R^n, A \in \R^{m
\times n}, b \in \R^m\)&lt;/span&gt;. Standard LP has standard solutions.
Thus, the next question is how to convert an ordinary LP problem to a
standard one.&lt;/p&gt;
&lt;p&gt;Now write &lt;span class=&#34;math inline&#34;&gt;\(x = x^+ - x^-\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(x^+, x^- \in \R_+^n\)&lt;/span&gt;. Introduce
another slack variable &lt;span class=&#34;math inline&#34;&gt;\(s \in
\R_+^m\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\eqref{lp}\)&lt;/span&gt; is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{x^+, x^- \ge 0, s \ge 0} \quad &amp;amp; c^T (x^+ - x^-) \\
\text{s.t.} \quad &amp;amp; A (x^+ - x^-) + s = b \\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math display&#34;&gt;\[
A&amp;#39; = \left[\begin{array}{c:c}
A &amp;amp; -A &amp;amp; I
\end{array}\right] \in \R^{m \times (2n+m)} \\
x&amp;#39; = \begin{bmatrix}
x^+ \\
\hdashline
x^- \\
\hdashline
s
\end{bmatrix} \in \R_+^{2n+m},
c&amp;#39; = \begin{bmatrix}
c \\
\hdashline
-c \\
\end{bmatrix} \in \R_+^{2n}
\]&lt;/span&gt; The problem becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min_{x&amp;#39; \in \R_+^{2n+m}} \quad &amp;amp; (c&amp;#39;)^T x&amp;#39; \\
\text{s.t.} \quad &amp;amp; A&amp;#39; x&amp;#39; = b \\
\end{aligned}
\]&lt;/span&gt; which observes the formality of standard LP. Here lays down
again the standard LP problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\label{primal} \begin{aligned}
v_p^* = \min_{x \in \R_+^n} \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; A x = b \\
\end{aligned} \tag{P}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The first question&lt;/strong&gt; to ask is whether there is a
optimal solution to it. The answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;
is feasible, then either 1) the optimal value &lt;span class=&#34;math inline&#34;&gt;\(v^* = -\infty\)&lt;/span&gt; (no optimal solution),or 2)
it has an optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas’ Lemma&lt;/h3&gt;
&lt;p&gt;But &lt;strong&gt;the second question&lt;/strong&gt; arises: how to certify that
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is infeasible? We
meet the similar dilemma to that when dealing with set-set separation:
it is easy to test the feasibility of any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;; but it is prohibitive to test the
feasibility of all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s just to show
that the original problem is infeasible.&lt;/p&gt;
&lt;p&gt;The idea is that, given the feasible polyhedron &lt;span class=&#34;math inline&#34;&gt;\(P \triangleq \{ x \in \R_+^n: Ax = b \}\)&lt;/span&gt;
of the original problem, we construct another auxiliary polyhedron &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; such that exactly one of the following
holds:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P \ne \emptyset, Q =
\emptyset\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P = \emptyset, Q \ne
\emptyset\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Farkas’ lemma&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A \in \R^{m \times n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \in \R^m\)&lt;/span&gt; be given. Then exactly one of
the following systems is solvable:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A x = b, x \ge 0\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le 0, b^T y &amp;gt; 0\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A x = b, x \ge 0\)&lt;/span&gt; means that
&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a non-negative linear
combinations of &lt;span class=&#34;math inline&#34;&gt;\(a_1, \dots, a_n\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le 0\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; forms an obtuse angle to &lt;span class=&#34;math inline&#34;&gt;\(a_1, \dots, a_n\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(b^T y &amp;gt; 0\)&lt;/span&gt; means &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; forms an acute angle with &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that the above two statements cannot be solvable at the same
time. or else this gives rise to the contradiction for some &lt;span class=&#34;math inline&#34;&gt;\(x_0 \ge 0, y_0 \in \R^m\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{x_0^T}_{\ge 0} \underbrace{A^T y_0}_{\le 0} = y_0^T A x =
\underbrace{y_0^T b}_{&amp;gt; 0}
\]&lt;/span&gt; We then claim that the above two statements cannot be
unsolvable at the same time. Suppose &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is unsolvable.&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(b \notin A^+ \triangleq \{
Ax: x \ge 0 \}\)&lt;/span&gt;. It can be verified that &lt;span class=&#34;math inline&#34;&gt;\(A^+\)&lt;/span&gt; is non-empty (&lt;span class=&#34;math inline&#34;&gt;\(0 \in A^+\)&lt;/span&gt;), closed (closeness is not
generally preserved under affine transformation; but in this case it is
&amp;lt;refer to &lt;a href=&#34;../3-lp.pdf&#34;&gt;this handout&lt;/a&gt;&amp;gt;) and convex
(because convexity is preserved under affine transformation &lt;span class=&#34;math inline&#34;&gt;\(A^+ = A \R_+^n\)&lt;/span&gt;). Then by &lt;em&gt;point-set
separation theorem&lt;/em&gt;, for any &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt;, there exists a &lt;span class=&#34;math inline&#34;&gt;\(y_0 \in
\R^m\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\max_{x \ge 0} y_0^T A x &amp;lt; y_0^T b
\]&lt;/span&gt; Take &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
y_0^T b &amp;gt; 0
\]&lt;/span&gt; We claim that &lt;span class=&#34;math inline&#34;&gt;\(A^T y_0 \le
0\)&lt;/span&gt;. Suppose on the contrary that there exists an &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\((A^T y_0)_i\)&lt;/span&gt; is positive. Then for any
&lt;span class=&#34;math inline&#34;&gt;\(\lambda \ge 0\)&lt;/span&gt;, take &lt;span class=&#34;math inline&#34;&gt;\(x = \lambda e_i\)&lt;/span&gt; and give &lt;span class=&#34;math display&#34;&gt;\[
y_0^T A x = x^T A^T y_0 = \underbrace{\lambda}_{\ge 0} \underbrace{(A^T
y_0)_i}_{&amp;gt; 0} &amp;lt; y_0^T b
\]&lt;/span&gt; which is impossible when &lt;span class=&#34;math inline&#34;&gt;\(\lambda
\to \infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; is solvable when
&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is unsolvable. &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; cannot be unsolvable at the same
time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We construct &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m: A^T y \le 0, b^T y &amp;gt; 0
\}\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; is
not a polyhedron yet, because &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m:
b^T y &amp;gt; 0 \}\)&lt;/span&gt; is open and is not a half-space. However,
observe that (verify it) &lt;span class=&#34;math display&#34;&gt;\[
A^T y \le 0, b^T y &amp;gt; 0 \text{ is solvable} \iff A^T y \le 0, b^T y =
1 \text{ is solvable}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in \R^m: b^T y = 1
\}\)&lt;/span&gt; can be rewritten as &lt;span class=&#34;math inline&#34;&gt;\(\{ y \in
\R^m: b^T y \ge 1, b^T y \le 1 \}\)&lt;/span&gt; which is an intersection of
halfspaces. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(Q \triangleq \{ y \in
\R^m: A^T y \le 0, b^T y \ge 1, b^T \le 1 \}\)&lt;/span&gt; is non-empty if
and only if &lt;span class=&#34;math inline&#34;&gt;\(Q&amp;#39;\)&lt;/span&gt; is non-empty.
Moreover, &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an intersection of
half-spaces as desired.&lt;/p&gt;
&lt;p&gt;Now, we convert the infeasibility of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; into the feasibility of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;. &lt;strong&gt;The third question&lt;/strong&gt;
is, suppose we verify that &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is
feasible, then given a solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;, how to certify its
optimality? The idea is to establish a lower bound on the optimal value
&lt;span class=&#34;math inline&#34;&gt;\(v^*_p\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;duality&#34;&gt;Duality&lt;/h3&gt;
&lt;p&gt;Consider a &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt; such that
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le c\)&lt;/span&gt;, then for any &lt;span class=&#34;math inline&#34;&gt;\(x \in \R_+^n\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(b \triangleq A x\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
b^T y = x^T A^T y \le c^T x
\]&lt;/span&gt; The above holds for &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt;
of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; as well.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
b^T y \le c^T x_p^* = v_p^*
\]&lt;/span&gt; We can try to find the largest lower bound w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\label{dual} \begin{aligned}
v_d^* = \max_y &amp;amp;\quad b^T y \\
\text{s.t.} &amp;amp;\quad A^T y \le c
\end{aligned} \tag{D}
\]&lt;/span&gt; Automatically, &lt;span class=&#34;math inline&#34;&gt;\(v_d^* \le
v_p^*\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is the also the dual of
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; in that &lt;span class=&#34;math inline&#34;&gt;\(v_p^* \ge v_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt; of LP. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
c^T \bar x \ge b^T \bar y
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = -\infty\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is infeasible.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = +\infty\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is infeasible.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; is feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;, and the &lt;strong&gt;duality
gap&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Delta(\bar x, \bar y)
\triangleq c^T \bar x - b^T \bar y\)&lt;/span&gt; is zero, then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is optimal for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; is optimal for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What about the converses of conclusions above?&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is
infeasible, then either &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
-\infty\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;
is infeasible.&lt;/p&gt;
&lt;p&gt;It is rather easy to construct a problem such that both the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; are infeasible, since the
constraints &lt;span class=&#34;math inline&#34;&gt;\(Ax = b, x \ge 0\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(A^T y \le c\)&lt;/span&gt; are quite independent.
The following will give an example: &lt;span class=&#34;math display&#34;&gt;\[
A =
\begin{bmatrix}
-1 &amp;amp; -1 \\
1 &amp;amp; 1
\end{bmatrix},
b = [1, 1]^T,
c = [-1, -1]^T
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is
infeasible, then either &lt;span class=&#34;math inline&#34;&gt;\(v_d^* =
+\infty\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is
infeasible.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality&lt;/strong&gt; for LP. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; has an optimal solution
&lt;span class=&#34;math inline&#34;&gt;\(x_p^*\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; has an optimal solution
&lt;span class=&#34;math inline&#34;&gt;\(y_d^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = c^T x_p^* = b^T y_d^* =
v_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that &lt;span class=&#34;math display&#34;&gt;\[
Ax = b, x \ge 0, c^T x - v_p^* &amp;lt; 0 \tag{I}
\]&lt;/span&gt; is unsolvable. Consider &lt;strong&gt;homogenizing&lt;/strong&gt; the
above system: &lt;span class=&#34;math display&#34;&gt;\[
Ax - bt = 0, c^T x - v_p^* t &amp;lt; 0, x \ge 0, t \ge 0 \tag{II}
\]&lt;/span&gt; We argue that &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt; is solvable if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; is solvable. If &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\((\bar x, 1)\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; too. If &lt;span class=&#34;math inline&#34;&gt;\((\bar x, \bar t)\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt;, we discuss by case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar t &amp;gt; 0\)&lt;/span&gt;. In this case,
&lt;span class=&#34;math inline&#34;&gt;\(\bar x / \bar t\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar t = 0\)&lt;/span&gt;. In this case, &lt;span class=&#34;math inline&#34;&gt;\(x^* + \bar x\)&lt;/span&gt; solves &lt;span class=&#34;math inline&#34;&gt;\(\textrm{(I)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textrm{(II)}\)&lt;/span&gt; can be rewritten
as &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A &amp;amp; -b \\
-A &amp;amp; b \\
-I &amp;amp; 0 (n \times 1) \\
0 (1 \times m) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
x \\
t
\end{bmatrix} \le 0,
[-c^T, v_p^*]
\begin{bmatrix}
x \\
t
\end{bmatrix} &amp;gt; 0
\]&lt;/span&gt; By &lt;em&gt;Farkas’ lemma&lt;/em&gt;, the following system is solvable:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A^T &amp;amp; -A^T &amp;amp; -I &amp;amp; 0 (m\times 1) \\
-b^T &amp;amp; b^T &amp;amp; 0 (1 \times n) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
z_1 \\
z_2 \\
z_3 \\
z_4
\end{bmatrix} =
\begin{bmatrix}
-c \\
v_p^*
\end{bmatrix},
\begin{bmatrix}
z_1 \\
z_2 \\
z_3 \\
z_4
\end{bmatrix}
\ge 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This means &lt;span class=&#34;math display&#34;&gt;\[
A^T (z_1 - z_2) - z_3 = -c \Rightarrow \\
A^T (z_2 - z_1) \le c \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-b^T (z_1 - z_2) - z_4 = v_p^* \Rightarrow \\
b^T (z_2 - z_1) \ge v_p^*
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(y^* \triangleq z_2 - z_1\)&lt;/span&gt;
is a feasible solution to the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Plus the weak duality which
states &lt;span class=&#34;math inline&#34;&gt;\(b^T y^* \le v_p^*\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b^T y^* = v_p^*\)&lt;/span&gt;. Therefore, &lt;span class=&#34;math inline&#34;&gt;\(y_d^* = y^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c^T x_p^* = b^T y_d^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;complementary slackness&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, they are optimal for
their respective problems if and only if &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{\bar x_i}_{\substack{\text{$i$-th} \\ \text{primal
variable}}}\ \underbrace{(c - A^T \bar y)_i}_{\substack{\text{$i$-th }
\\ \text{dual constraint}}} = 0
\]&lt;/span&gt; Proof: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
c^T \bar x - b^T \bar y &amp;amp;= c^T x - (A \bar x)^T \bar y \\
0 &amp;amp;= \bar x^T (c - A^T \bar y)
\end{aligned}
\]&lt;/span&gt; Plus that &lt;span class=&#34;math inline&#34;&gt;\(\bar x \ge 0, c - A^T
\bar y \ge 0\)&lt;/span&gt;, we can conclude with the complementary
slackness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After establishing the lower bound of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;’s optimal value with &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;’s optimal value, by strong
duality, to find optimal solutions to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is equivalent to find a
feasible solution to &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
Ax = b, x \ge 0 \tag{primal constraint} \\
A^T y \le c \tag{dual constraint} \\
c^T x = b^T y \tag{zero duality gap}
\end{gather}
\]&lt;/span&gt; Note that the zero duality gap is in essence equivalent to
&lt;span class=&#34;math display&#34;&gt;\[
x^T (c - A^T y) = 0 \tag{complementary slackness}
\]&lt;/span&gt; An optimization problem is converted to feasibility problem.
Particularly, an LP optimization problem is no harder than an LP
feasibility problem.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-lp-problem&#34;&gt;Examples of LP Problem&lt;/h2&gt;
&lt;h3 id=&#34;vertex-cover&#34;&gt;Vertex Cover&lt;/h3&gt;
&lt;p&gt;Given a graph &lt;span class=&#34;math inline&#34;&gt;\(G = (V, E)\)&lt;/span&gt; and a
cost function &lt;span class=&#34;math inline&#34;&gt;\(c: V \mapsto \R^+\)&lt;/span&gt;,
find a vertex cover that minimizes overall cost. Here we say that &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq V\)&lt;/span&gt; is vertex cover if every
edge has at least one endpoint in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. The cost of a vertex cover the sum of
costs of the vertices contained.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(x_i \in \{ 0,1 \}\)&lt;/span&gt; be an
indicator variable defined as &lt;span class=&#34;math display&#34;&gt;\[
x_i =
\begin{cases}
1, &amp;amp; \text{if vertex $i$ is in the cover} \\
0, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt; Then we have the following integer programming problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{vc} \begin{aligned}
v^* = \min_x \quad &amp;amp;\sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp;x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp;x_i \in \{ 0,1 \}, \forall i=1,\dots,|V|
\end{aligned} \tag{Vertex Cover}
\]&lt;/span&gt; The problem above is NP-hard. One typical way to tackle it is
to relax the constraint to make it easier. &lt;span class=&#34;math display&#34;&gt;\[
\label{vc-lp-I} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp; 0 \le x_i \le 1, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC I}
\]&lt;/span&gt; Note that we can further drop the &lt;span class=&#34;math inline&#34;&gt;\(x_i \le 1\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-I}\)&lt;/span&gt; because we can always let
&lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39; = 1\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x_i &amp;gt; 1\)&lt;/span&gt; without violating the
constraints but with smaller objective (because &lt;span class=&#34;math inline&#34;&gt;\(c \ge 0\)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[
\label{vc-lp-II} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \\
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \\
&amp;amp; 0 \le x_i, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC II}
\]&lt;/span&gt; Once &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; solves
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;, we need to round
&lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; to give a feasible
&lt;span class=&#34;math inline&#34;&gt;\(x_\text{rd}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd} = c^T x_\text{rd} \approx
v^*\)&lt;/span&gt;. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(v^* \ge
v_\text{lp}^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v^* \le
v_\text{rd}\)&lt;/span&gt;. The question is if we can upper-bound &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd}\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\alpha v^*\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\alpha \ge 1\)&lt;/span&gt;. Here &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is called the
&lt;strong&gt;approximation ratio&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let &lt;span class=&#34;math inline&#34;&gt;\(P = \{ x \in \R^n: a_i^T
x \le b, i=1,\dots,m \}\)&lt;/span&gt; be a polyhedron and let &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in P\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(I(\bar x) = \{ i: a_i^T x = b \}\)&lt;/span&gt; be the
active index set. We say that &lt;span class=&#34;math inline&#34;&gt;\(\bar
x\)&lt;/span&gt; is a &lt;strong&gt;vertex&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\{ a_i:
i \in I(\bar x) \}\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;
linearly-independent vectors. Alternatively, the system &lt;span class=&#34;math display&#34;&gt;\[
a_i^T x = b_i, i \in I(\bar x)
\]&lt;/span&gt; has a unique solution (which is &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The equality constraint ensures that the solution is on the boundary
of the feasible set, or rather, on a hyperplane. &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; linearly-independent vectors ensure
that the solution is the intersection of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; hyperplanes, which is why it is called
&lt;u&gt;vertex solution&lt;/u&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If an LP has a vertex feasible solution and is bounded
(which means the optimal value is finite), then it has a vertex optimal
solution (??).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let &lt;span class=&#34;math inline&#34;&gt;\(Q = \{ x \in \R^{|V|}: x_i +
x_j \ge 1, \forall (i,j) \in E; x \ge 0 \}\)&lt;/span&gt;, which is exactly
the feasible set of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; has a vertex solution (??). Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be one of such vertex solutions.
Then, &lt;span class=&#34;math display&#34;&gt;\[
\forall i, \bar x_i \in \{ 0, 1/2, 1 \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the theorem and proposition above, and because &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt; is bounded (verify it),
we can have a vertex optimal solution &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc-lp-II}\)&lt;/span&gt;. Next, we choose to
round the optimal vertex solution &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; in this way: if &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{lp}_i}^*\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{rd}_i} = 0\)&lt;/span&gt;; else &lt;span class=&#34;math inline&#34;&gt;\(x_{\text{rd}_i} = 1\)&lt;/span&gt;. Note that this
rounding method satisfies the constraint of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{vc}\)&lt;/span&gt;. The largest divergence of
&lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(v_\text{lp}^*\)&lt;/span&gt; happens when &lt;span class=&#34;math inline&#34;&gt;\(x_\text{lp}^*\)&lt;/span&gt; is full of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, in which case &lt;span class=&#34;math inline&#34;&gt;\(v_\text{rd} = 2 v_\text{lp}^*\)&lt;/span&gt;. Thus, we
can conclude that &lt;span class=&#34;math display&#34;&gt;\[
v_\text{lp}^* \le v^* \le c^T x_\text{rd} \le 2 v_\text{lp}^* \le 2 v^*
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The approximation ratio in this problem is &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;max-flow&#34;&gt;Max Flow&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&#34;&gt;cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;cooperative-game-theory&#34;&gt;Cooperative Game Theory&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N} = \{ 1,\dots,n
\}\)&lt;/span&gt; be the set of players and &lt;span class=&#34;math inline&#34;&gt;\(v: 2^N
\mapsto \R^+\)&lt;/span&gt; be the worth function, which satisfies &lt;span class=&#34;math inline&#34;&gt;\(v(\emptyset) = 0\)&lt;/span&gt;. The subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; is called
&lt;strong&gt;coalition&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt; be the &lt;strong&gt;allocation vector&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; be the payoff assigned to player
&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x(\emptyset) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x(S) \triangleq \sum_{i \in S} x_i\)&lt;/span&gt; for
non-empty coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We say that
coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; &lt;strong&gt;improves
upon&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(v(S) &amp;gt; x(S)\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The allocation vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is said to be in the
&lt;strong&gt;core&lt;/strong&gt; if &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
x(\mathcal{N}) = v(\mathcal{N}) \\
\forall S \subseteq \mathcal{N}, x(S) \ge v(S) \label{core-inequality}
\end{gather}
\]&lt;/span&gt; which means no &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; improves
upon &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is essential to understand &lt;strong&gt;the implication behind the
game&lt;/strong&gt;. This “cooperative game” is actually preventing players
from cooperation, or collusion, or called forming a coalition. The
allocation vector is part of this “evil scheme”. The fact of matter is,
the solver of this problem is trying to pay off players (in total the
amount is no more than the value he thinks the whole coalition &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; deserves) such that for every
possible coalition, there will be some member who thinks it unfair
because the share (equal share for everyone!) he gets from the coalition
is no more than the amount he would otherwise earns himself. &lt;span class=&#34;math display&#34;&gt;\[
x(S) = \sum_{i \in S} x_i \ge v(S)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(x(S)\)&lt;/span&gt; is additive w.r.t.
&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, which exactly represents the
payoff every player grabs (snout in the trough) covertly and separately.
There must be some &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(x_i \ge v(S)/|S|\)&lt;/span&gt;. The situation worsens
when the &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; relation is
strict.&lt;/p&gt;
&lt;p&gt;The question is, is the core non-empty for a given &lt;span class=&#34;math inline&#34;&gt;\((\mathcal{N}, v)\)&lt;/span&gt;, or rather, can such
scheme exist?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Treasure hunt. &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; people
discovered treasures in the mountain. 2 people are needed to bring one
piece out and thus &lt;span class=&#34;math inline&#34;&gt;\(v(S) = \lfloor
\frac{|S|}{2} \rfloor\)&lt;/span&gt;. Is the core empty?&lt;/p&gt;
&lt;p&gt;First consider the case when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;
is even. &lt;span class=&#34;math inline&#34;&gt;\(x = [1/2, \dots, 1/2]\)&lt;/span&gt; is
in the core.&lt;/p&gt;
&lt;p&gt;Now simply consider the case when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is odd. The fact is that the core is
empty in this case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;. The cooperative
game &lt;span class=&#34;math inline&#34;&gt;\((\mathcal{N}, v)\)&lt;/span&gt; has a
non-empty core if and only if for every set of numbers &lt;span class=&#34;math inline&#34;&gt;\(\{ y_S \}_{S \subseteq \mathcal{N}}\)&lt;/span&gt;,
whose elements are indexed by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt;’s subset, such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall S \subseteq \mathcal{N}, y_S \ge
0\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\forall i \in \mathcal{N}, \sum_{S: i
\in S} = 1\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{S \subseteq \mathcal{N}} y_S v(S)
\le v(\mathcal{N})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_S\)&lt;/span&gt; can be interpreted as the
amount of time each player in &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;
spent on the coalition &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;,
justifying the non-negativity constraint. The total amount of time
player &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; spent on different
coalitions is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(y_S v(S)\)&lt;/span&gt; can be understood as the
proportional outcome from partial-commitment.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Consider the following optimization problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp} \tag{Coop. Game} \begin{aligned}
\min \quad &amp;amp; [x(\mathcal{N}) \triangleq \sum_{i \in N} x_i] \\
\text{s.t.} \quad &amp;amp; x(S) \ge v(S), \forall S \subseteq \mathcal{N}
\end{aligned}
\]&lt;/span&gt; Observe that the constraints naturally imply the inequality
conditions &lt;span class=&#34;math inline&#34;&gt;\(\eqref{core-inequality}\)&lt;/span&gt;
for an allocation to be in the core. &lt;span class=&#34;math inline&#34;&gt;\(x \ge
0\)&lt;/span&gt; is automatically embedded into &lt;span class=&#34;math inline&#34;&gt;\(x(\{ i \}) \ge v(\{ i \}) \ge 0,
i=1,\dots,n\)&lt;/span&gt;. Other than that, the minimizing objective together
with the constraint &lt;span class=&#34;math inline&#34;&gt;\(x(\mathcal{N}) \ge
v(\mathcal{N})\)&lt;/span&gt; implies &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; is lower-bounded by &lt;span class=&#34;math inline&#34;&gt;\(v(\mathcal{N})\)&lt;/span&gt;. Also note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; is always feasible because we
can assign each &lt;span class=&#34;math inline&#34;&gt;\(x(S)\)&lt;/span&gt; sufficiently
large to surpass &lt;span class=&#34;math inline&#34;&gt;\(v(S)\)&lt;/span&gt;. Therefore,
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; always has an optimal
solution.&lt;/p&gt;
&lt;p&gt;To rewrite &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; to LP
form, let &lt;span class=&#34;math display&#34;&gt;\[
e = [1,\dots,1]^T,
A =
\begin{bmatrix}
\mathbb{1}_n(\emptyset)^T \\
\vdots \\
\mathbb{1}_n(S)^T \\
\vdots \\
\mathbb{1}_n(\mathcal{N})^T
\end{bmatrix},
b =
\begin{bmatrix}
v(\emptyset) \\
\vdots \\
v(S) \\
\vdots \\
v(\mathcal{N})
\end{bmatrix}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_\mathcal{N}(S)\)&lt;/span&gt; is the
indicator function that returns the vector in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt; whose &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th entry indicates if &lt;span class=&#34;math inline&#34;&gt;\(i \in S\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp}\)&lt;/span&gt; becomes &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-lp} \tag{Coop. Game LP} \begin{aligned}
v^* = \min \quad &amp;amp; e^T x \\
\text{s.t.} \quad &amp;amp; A x \ge b
\end{aligned}
\]&lt;/span&gt; We don’t rush to convert it to standard LP form yet; otherwise
we need to introduce a slack variable. On the other hand, we formulate
the following LP problem, whose optimal value is &lt;strong&gt;of the same
magnitude as but of different sign to&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-lp}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-primal} \tag{Coop. Game P} \begin{aligned}
v_p^* = \max \quad &amp;amp; -e^T x \\
\text{s.t.} \quad &amp;amp; -A x \le -b
\end{aligned}
\]&lt;/span&gt; That is, &lt;span class=&#34;math inline&#34;&gt;\(v^* = -v_p^*\)&lt;/span&gt;.
Recall that the primal and the dual are relative. The above problem has
the dual &lt;span class=&#34;math display&#34;&gt;\[
\label{cgp-dual} \tag{Coop. Game D} \begin{aligned}
v_d^* = \min_{y \ge 0} \quad &amp;amp; -b^T y \\
\text{s.t.} \quad &amp;amp; A^T y = e
\end{aligned}
\]&lt;/span&gt; which is in the standard LP form.&lt;/p&gt;
&lt;p&gt;Note that columns of &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; are
exactly those indicator vectors. Thus, &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt;’s columns and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;’s entries can both be indexed by sets.
Also, rows of &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; are indexed by
elements. Since every entry of &lt;span class=&#34;math inline&#34;&gt;\(A^T
y\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, interpreting &lt;span class=&#34;math inline&#34;&gt;\(A^T y\)&lt;/span&gt; as the row-column dot-product, the
constraint in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-dual}\)&lt;/span&gt; is
exactly &lt;span class=&#34;math display&#34;&gt;\[
\forall i \in \mathcal{N}, \sum_{S: i \in S} y_S = 1
\]&lt;/span&gt; By strong duality, we have &lt;span class=&#34;math display&#34;&gt;\[
v^* = x^*(\mathcal{N}) = \sum_{S} y_S^* v(S) = -v_d^* = -v_p^*
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\forall S \subseteq \mathcal{N},
x^*(S) \ge v(S)\)&lt;/span&gt; by the constraint of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cgp-lp}\)&lt;/span&gt;. By
&lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;’s condition, we have &lt;span class=&#34;math inline&#34;&gt;\(x^*(\mathcal{N}) = \sum_{S} y_S^* v(S) \le
v(\mathcal{N})\)&lt;/span&gt;. Therefore, we conclude that &lt;span class=&#34;math inline&#34;&gt;\(x^*(\mathcal{N}) = v(\mathcal{N})\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; is in the core.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be inferred that there are lots of duality in game theory,
given its mini-max nature.&lt;/p&gt;
&lt;h2 id=&#34;solving-lp&#34;&gt;Solving LP&lt;/h2&gt;
&lt;p&gt;The algorithm for solving LP problems generally falls into two
methods: simplex method and interior point method.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>5-conic-linear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</guid>
      <description>

&lt;p&gt;Though some problems can be relaxed to accommodate the linear
programming form, applications of LP are still limited due to its linear
constraints. To extend, a natural idea is to gradually allow for other
kinds of non-linear constraints. But that would go too far away from the
established theories for linear programming.&lt;/p&gt;
&lt;p&gt;In this post, we study the conic linear programming, which is a
phased open-up to non-linear problems.&lt;/p&gt;
&lt;h2 id=&#34;concept-preparation&#34;&gt;Concept Preparation&lt;/h2&gt;
&lt;h3 id=&#34;good-order-and-proper-cone&#34;&gt;“Good” Order and “Proper” Cone&lt;/h3&gt;
&lt;p&gt;Before exploring other non-linear problems, we first study the
properties of a “good” order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt;. between vectors. We expect it to
have the following basic three properties of a
&lt;strong&gt;partial-order&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\forall
u \in \R^n, u \succeq u\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Anti-symmetry&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v \in \R^n, u \succeq v, v \succeq u \to u = v
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transitivity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v,w \in \R^n, u \succeq v, v \succeq w \to u \succeq w
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than them, we expect &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; to further possess the following
two arithmetic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v \in \R^n, u \succeq v, \alpha \succeq 0, \alpha u \succeq
\alpha v
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Additivity&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u,v,w,z \in \R^n, u \succeq v, w \succeq z \to u + w \succeq v +
z
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say the &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; order is
“good” if it satisfies the above five properties. The above five is from
the algebraic perspective. To illustrate it geometrically, consider the
set &lt;span class=&#34;math inline&#34;&gt;\(K \triangleq \{ x \in \R^n: x \ge 0
\}\)&lt;/span&gt;. We say &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a
&lt;strong&gt;“proper” cone&lt;/strong&gt; in that it is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;non-empty&lt;/strong&gt; and &lt;strong&gt;closed under
addition&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u, v \in K, u + v \in K
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;conic&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
\forall u \in K, \alpha &amp;gt; 0 \to \alpha u \in K \\
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;pointed&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[
u, -u \in K \to u = 0
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We further claim that a proper cone is &lt;strong&gt;convex&lt;/strong&gt; and
contains the zero vector (verify it).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The pointedness plus closeness (not closeness under addition)
implies that there is no complete line in this cone, which equivalently
means there is no non-trivial subspace (i.e., except &lt;span class=&#34;math inline&#34;&gt;\(\{ 0 \}\)&lt;/span&gt; and the universe) inside this
cone. To show it, let &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; be a closed
pointed cone, suppose on the contrary there exists &lt;span class=&#34;math inline&#34;&gt;\(u, v \in K\)&lt;/span&gt; such that for every &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in \R\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(u + \alpha(v - u) \in K\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 1\)&lt;/span&gt;, consider the sequence &lt;span class=&#34;math inline&#34;&gt;\(\{w_+^t\}_{t=1}^{\infty}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{w_-^t\}_{t=1}^{\infty}\)&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
w_+^t = \frac{u + t(v-u)}{\|u + t(v-u)\|_2}, w_-^t = \frac{u -
t(v-u)}{\|u - t(v-u)\|_2}
\]&lt;/span&gt; Now &lt;span class=&#34;math inline&#34;&gt;\(w_+^t, w_-^t \in K\)&lt;/span&gt;
for all &lt;span class=&#34;math inline&#34;&gt;\(t \ge 1\)&lt;/span&gt;. But &lt;span class=&#34;math inline&#34;&gt;\(w_+^t \to w \triangleq
\frac{v-u}{\|v-u\|_2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_-^t \to
-w\)&lt;/span&gt; . Since &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is closed, we
have &lt;span class=&#34;math inline&#34;&gt;\(w, -w \in K\)&lt;/span&gt;. However, since
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; has unit norm and is not zero
vector, it follows that &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is not
pointed, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: At times, people also refer to this as the
&lt;strong&gt;salient&lt;/strong&gt; property of a cone. A &lt;u&gt;proper cone&lt;/u&gt; is
variously defined on a subset of these properties (closeness, closeness
under addition, pointed, salient, and essentially, conic) depending on
the context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, the algebraic properties and the geometric properties can
derive each other. A good order and a proper cone have a one-to-one
relationship. Now we ask, in an arbitrary &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-d universe (or a finite-dimensional
Euclidean space), is &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; the only
good order, or equivalently, is &lt;span class=&#34;math inline&#34;&gt;\(\R_+^n\)&lt;/span&gt; the only proper cone? The answer
is no.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-proper-cone&#34;&gt;Examples of Proper Cone&lt;/h3&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Lorentz cone / second-order cone / ice cream cone &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{Q}^{n+1} \triangleq \{ (t, x) \in \R \times \R^n: t \ge ||x||_2
\}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}^{n+1}\)&lt;/span&gt; is a
closed proper cone. Then what is the good order associated with this
proper cone?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Semi-definite cone &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{S}_+^n \triangleq \{ X \in \mathcal{S}^n: u^T X u \ge 0,
\forall u \in \R^n \}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^n\)&lt;/span&gt; is a
closed proper cone. Then what is the good order associated with this
proper cone?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Zero cone: &lt;span class=&#34;math inline&#34;&gt;\(\{ 0 \}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New cones from old ones by Cartesian product&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(K_1, \dots, K_m\)&lt;/span&gt; be closed
proper cones (with non-empty interior). Then &lt;span class=&#34;math display&#34;&gt;\[
K \triangleq K_1 \times \dots \times K_m = \{ (x_1, \dots, x_m): x_i \in
K_i, \forall i=1,\dots,m \}
\]&lt;/span&gt; is a closed proper cone with non-empty interior (with
non-empty interior).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: &lt;span class=&#34;math inline&#34;&gt;\(\R_+^n, \mathcal{Q}^{n+1},
\mathcal{S}_+^n\)&lt;/span&gt; have non-empty interior: &lt;span class=&#34;math display&#34;&gt;\[
\intr(\R_+^n) = \R_{++}^n \\
\intr(\mathcal{Q}^{n+1}) = \{ (t, x) \in \R \times \R^n: t &amp;gt; ||x||_2
\} \\
\intr(\mathcal{S}_+^n) = \mathcal{S}_{++}^n \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conic-linear-programming&#34;&gt;Conic Linear Programming&lt;/h2&gt;
&lt;h3 id=&#34;formulation&#34;&gt;Formulation&lt;/h3&gt;
&lt;p&gt;Recall that linear programming is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\min &amp;amp; \quad c^T x \\
\text{s.t.} &amp;amp; \quad b - A x \ge 0
\end{align*}
\]&lt;/span&gt; Now replacing &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; with
the good order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; gives the
conic linear programming. &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\tag{CLP}
\min &amp;amp; \quad c^T x \\
\text{s.t.} &amp;amp; \quad b - A x \succeq 0
\end{align*}
\]&lt;/span&gt; The next question is, can we recover Farkas’ lemma and strong
duality in this setting? The answer is yes. And the good aspect of the
good order is that we can recover the conclusions in linear programming
verbatim. Next we generalize the LP problem and show the same result
applies.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; be a finite-dimensional
Euclidean space (e.g. &lt;span class=&#34;math inline&#34;&gt;\(\R^n,
\mathcal{S}^n\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\langle \cdot,
\cdot \rangle\)&lt;/span&gt; be the inner product on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; (e.g., on &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\langle
x, y \rangle = x^T y\)&lt;/span&gt;; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\langle X, Y \rangle = \tr(X^T Y)\)&lt;/span&gt;). Let
&lt;span class=&#34;math inline&#34;&gt;\(K \subseteq E\)&lt;/span&gt; be a &lt;strong&gt;closed
proper cone&lt;/strong&gt;. Then we can have a good order &lt;span class=&#34;math inline&#34;&gt;\(\succeq\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. Consider the &lt;strong&gt;standard form of
CLP&lt;/strong&gt; as well as the primal problem: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{P} \label{primal}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; \langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall
i=1,\dots,m \\
&amp;amp; x \in K \subseteq E
\end{aligned}
\end{equation}
\]&lt;/span&gt; What is its dual? We mimic the procedure when we build the
lower bound for LP. Let &lt;span class=&#34;math inline&#34;&gt;\(y \in \R^m\)&lt;/span&gt;.
By &lt;u&gt;the linearity of inner product&lt;/u&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\langle b, y \rangle = b^T y =
\sum_{i=1}^m \langle a_i, x \rangle y_i =
\sum_{i=1}^m \langle y_i a_i, x \rangle =
\langle \sum_{i=1}^m y_i a_i, x \rangle
\]&lt;/span&gt; If we impose &lt;span class=&#34;math inline&#34;&gt;\(c \succeq
\sum_{i=1}^m y_i a_i\)&lt;/span&gt; like &lt;span class=&#34;math inline&#34;&gt;\(c \ge A^T
y\)&lt;/span&gt;, can we draw &lt;span class=&#34;math inline&#34;&gt;\(\langle c -
\sum_{i=1}^m y_i a_i, x \rangle \ge 0\)&lt;/span&gt; like &lt;span class=&#34;math inline&#34;&gt;\(x^T(c - A^T y) \ge 0\)&lt;/span&gt;? The answer is that,
this constraint is not enough in general. To reinforce the constraint,
construct the set &lt;span class=&#34;math display&#34;&gt;\[
K^* = \{ w \in E: \langle w, x \rangle \ge 0, \forall x \in K \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can draw some properties for &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is non-empty because
&lt;span class=&#34;math inline&#34;&gt;\(0 \in K^*\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is always closed and
convex (no matter what &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is,
because &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is the intersection of
hyperplanes which are closed and convex).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is conic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a closed proper cone
with non-empty interior, then so is &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; are heterogenous on the premise that
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is a closed proper cone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We have (verify it) &lt;span class=&#34;math display&#34;&gt;\[
(\R_+^n)^* = \R_+^n, (\mathcal{Q}^{n+1})^* = \mathcal{Q}^{n+1},
(\mathcal{S}_+^n)^* = \mathcal{S}_+^n
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt; is called the
&lt;strong&gt;dual cone&lt;/strong&gt;. We impose that &lt;span class=&#34;math inline&#34;&gt;\(c
- \sum_{i=1}^m y_i a_i \in K^*\)&lt;/span&gt;. Then, the dual problem can be
written as &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{D} \label{dual}
\begin{aligned}
v_d^* = \sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in K^* \\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The constraint of CLP’s dual in essence describes that &lt;u&gt;an affine
map of the variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; belongs to a
cone&lt;/u&gt;. This is a good indicator on whether we are dealing with a
CLP.&lt;/p&gt;
&lt;h4 id=&#34;second-order-cone-programming&#34;&gt;Second-order Cone
Programming&lt;/h4&gt;
&lt;p&gt;Consider the second-order cone programming: &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{SOCP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{Q}^{n+1}
\\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(a_i = [u_i,
\underbrace{a_{i,1}, \dots, a_{i, n}}_{\bar a_i^T}]^T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = [v, \underbrace{c_1, \dots,
c_n}_{d^T}]^T\)&lt;/span&gt;. The dual constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
c - \sum_{i=1}^m y_i a_i =
\begin{bmatrix}
v \\
d
\end{bmatrix} -
\sum_{i=1}^m y_i
\begin{bmatrix}
u_i \\
\bar a_i
\end{bmatrix}
=
\begin{bmatrix}
v - u^T y \\
d - A^T y
\end{bmatrix} \succeq 0, \\
\text{where }
\begin{aligned}[t]
A &amp;amp;= [\bar a_1, \dots, \bar a_n]^T, \\
u &amp;amp;= [u_1, \dots, u_n]^T, \\
\end{aligned}
\end{gathered}
\]&lt;/span&gt; That is &lt;span class=&#34;math display&#34;&gt;\[
v - u^T y \ge \|d - A^T y\|_2
\]&lt;/span&gt; The left-hand and the right-hand side of the above are both an
affine function in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The above is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \\
(d - A^T y)^T &amp;amp; v - u^T y
\end{bmatrix}
\in \mathcal{S}_+^{n+1}
\]&lt;/span&gt; To show it, firstly the case when &lt;span class=&#34;math inline&#34;&gt;\(v - u^T y = 0\)&lt;/span&gt; trivially holds. On the
other hand, when &lt;span class=&#34;math inline&#34;&gt;\(v - u^T y &amp;gt; 0\)&lt;/span&gt;,
we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(v - u^T y)^2 &amp;amp;\ge (d - A^T y)^T (d - A^T y) \\
\underbrace{(v - u^T y)}_{C} &amp;amp;\ge \underbrace{(d - A^T y)^T}_{B^T}
\underbrace{\frac{I_n}{v - u^T y}}_{A^{-1}} \underbrace{(d - A^T y)}_{B}
\end{aligned}
\]&lt;/span&gt; Consider the &lt;em&gt;Schur complement&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
A &amp;amp; B \\
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}_+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}_+^m, C \in \mathcal{S}_+^n, \\
A - B C^{-1} B^T \in \mathcal{S}_+^m \text{ or } C - B^T A^{-1} B \in
\mathcal{S}_+^n
\end{gathered}
\]&lt;/span&gt; We have, &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \\
(d - A^T y)^T &amp;amp; v - u^T y
\end{bmatrix}
\in \mathcal{S}_+^{n+1}
\]&lt;/span&gt; The converse similarly follows from &lt;em&gt;Schur complement&lt;/em&gt;.
Based on this result, we further claim that an SOCP is equivalent to an
semi-definite programming (SDP). But it is never necessary to solve SOCP
by converting it to SDP, which only increases the complexity.&lt;/p&gt;
&lt;h4 id=&#34;semi-definite-programming&#34;&gt;Semi-definite Programming&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\tag{SDP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \\
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{S}_+^n \\
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In terms of inclusion (and thus difficulty), LP =&amp;gt; QCQP =&amp;gt; SOCP
=&amp;gt; SDP =&amp;gt; CLP.&lt;/p&gt;
&lt;h3 id=&#34;weak-duality&#34;&gt;Weak Duality&lt;/h3&gt;
&lt;p&gt;Our previous derivation of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; implies the weak duality of
CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality of CLP&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(\langle c, \bar x \rangle \ge b^T \bar
y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proof: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
c - \sum_{i=1}^m y_i a_i \in K^* \Rightarrow \langle c - \sum_{i=1}^m
y_i a_i, x \rangle \ge 0 \\
\Downarrow \\
\begin{aligned}
\langle c, x \rangle &amp;amp;\ge \langle \sum_{i=1}^m y_i a_i, x \rangle \\
&amp;amp;= \sum_{i=1}^m \langle a_i, x \rangle y_i  \\
&amp;amp;= b^T y
\end{aligned}
\end{gathered}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We next investigate another method to convert between the primal and
dual in CLP. For a standard CLP problem, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}
\inf \quad &amp;amp; \langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall
i=1,\dots,m \\
&amp;amp; x \in K \subseteq E
\end{aligned} \quad
\substack{\langle a_i, x \rangle = b \iff b - \langle a_i, x \rangle \in
\{ 0 \} \\ \equiv} \quad
\begin{aligned}
-\sup \quad &amp;amp; -\langle c, x \rangle \\
\text{s.t.} \quad &amp;amp; \begin{bmatrix}
b \\
0
\end{bmatrix} -
\begin{bmatrix}
A \\
-I
\end{bmatrix} x
\in \{ 0 \} \times K
\end{aligned}
\end{gathered}
\]&lt;/span&gt; whose dual is &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation*}
\begin{aligned}
-\inf \quad &amp;amp; [b^T, 0] [y^T, \tilde y^T]^T \\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
A^T &amp;amp; -I
\end{bmatrix}
\begin{bmatrix}
y \\
\tilde y
\end{bmatrix} = -c \\
&amp;amp;
\begin{bmatrix}
y \\
\tilde y
\end{bmatrix} \in (\{ 0 \} \times K)^*
\end{aligned}
\end{equation*}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\((\{ 0 \} \times K)^* =
\{ 0 \}^* \times K^* = \R \times K^*\)&lt;/span&gt;. And interestingly, from
above, we observe that the equality constraint is in essence a cone
constraint.&lt;/p&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas’ Lemma&lt;/h3&gt;
&lt;p&gt;Recall the Farkas’ lemma for linear systems. Exactly one of the
following two systems is solvable: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
Ax = b, x \ge 0 \\
A^T y \le 0, b^T y &amp;gt; 0
\end{gather*}
\]&lt;/span&gt; Farkas’ lemma secures a strong duality for LP. We would like
to do the same to CLP. First we mimic the two systems in CLP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather*}
\langle a_i, x \rangle = b_i, \forall i=1,\dots,m; x \in K \tag{I} \\
-\sum_{i=1}^m y_i a_i \in K^*; b^T y &amp;gt; 0 \tag{II}
\end{gather*}
\]&lt;/span&gt; Is it true that exactly one of the above two systems is
solvable? We first claim that they can’t be solvable at the same time.
Suppose on the contrary they both hold. By &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
b^T y =\sum_{i=1}^m \langle a_i, x \rangle y_i =\langle
\sum_{i=1}^m  y_i a_i, x \rangle &amp;amp;&amp;gt; 0 \\
\end{aligned}
\]&lt;/span&gt; But that &lt;span class=&#34;math inline&#34;&gt;\(-\sum_{i=1}^m y_i a_i \in
K^*\)&lt;/span&gt; implies &lt;span class=&#34;math inline&#34;&gt;\(\langle \sum_{i=1}^m
y_i a_i, x \rangle = -\langle -\sum_{i=1}^m y_i a_i, x \rangle \le
0\)&lt;/span&gt;, which is a contradiction. On the other hand, for CLP, it is
possible that neither &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(I)}\)&lt;/span&gt;
nor &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt; is solvable.&lt;/p&gt;
&lt;blockquote&gt;
Example: Let &lt;span class=&#34;math inline&#34;&gt;\(E = \mathcal{S}^2, K =
\mathcal{S}_+^2\)&lt;/span&gt;. Let &lt;span class=&#34;math display&#34;&gt;\[
A_1 = \begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 0
\end{bmatrix},
A_2 = \begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix},
b = \begin{bmatrix}
0 \\
2
\end{bmatrix}
\]&lt;/span&gt; Then $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{gathered}
\begin{cases}
\langle A_1, X \rangle = 0 \\
\langle A_2, X \rangle = 2 \\
X \in \mathcal{S}_+^2
\end{cases}
\iff
\begin{cases}
X_{11} = 0 \\
2 X_{12} = 2 \\
X \in \mathcal{S}_+^2
\end{cases}
\iff
\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; X_{22}
\end{bmatrix} \in \mathcal{S}_+^2
\text{, which is unsolvable} \\

\begin{cases}
-(y_1 A_1 + y_2 A_2) \in \mathcal{S}_+^2 \\
2 y_2 &amp;gt; 0
\end{cases}
\iff
\begin{cases}
\begin{bmatrix}
-y_1 &amp;amp; -y_2 \\
-y_2 &amp;amp; 0 \\
\end{bmatrix} \in \mathcal{S}_+^2 \\
y_2 &amp;gt; 0
\end{cases}
\text{, which is unsolvable}
\end{gathered}\]&lt;/span&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is what goes wrong? Recall in the proof of the original
Farkas’ lemma, we apply the separation theorem to the setting &lt;span class=&#34;math display&#34;&gt;\[
b \notin \{ Ax: x \in \R_+^n \}
\]&lt;/span&gt; The right-hand set is always closed. However, for an arbitrary
closed proper cone &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, the set &lt;span class=&#34;math display&#34;&gt;\[
\{ (\langle a_1, x \rangle, \dots, \langle a_m, x \rangle): x \in K \}
\]&lt;/span&gt; is not always closed. Without closeness, we can’t properly
apply the &lt;em&gt;point-set separation theorem&lt;/em&gt;. For the same reason,
the optimum may not be attained in CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Continue with the &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2,
b\)&lt;/span&gt; in the previous example. Consider the set &lt;span class=&#34;math display&#34;&gt;\[
S = \{ (\langle A_1, X \rangle, \langle A_2, X \rangle): X \in
\mathcal{S}_+^2 \}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((\langle A_1, X \rangle, \langle
A_2, X \rangle) = (X_{11}, 2 X_{12})\)&lt;/span&gt; basically describes the
trajectory of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; varies in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_+^2\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
X = \begin{bmatrix}
X_{11} &amp;amp; X_{12} \\
X_{12} &amp;amp; X_{22}
\end{bmatrix}
\]&lt;/span&gt; If &lt;span class=&#34;math inline&#34;&gt;\(X_{11} = 0\)&lt;/span&gt;, the only
way to make &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; PSD is to make &lt;span class=&#34;math inline&#34;&gt;\(X_{12} = 0\)&lt;/span&gt;; if &lt;span class=&#34;math inline&#34;&gt;\(X_{11} &amp;gt; 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_{12}\)&lt;/span&gt; can be arbitrary because we can
take &lt;span class=&#34;math inline&#34;&gt;\(X_{22}\)&lt;/span&gt; sufficiently large such
that &lt;span class=&#34;math inline&#34;&gt;\(X_{11} X_{22} \ge X_{12}^2\)&lt;/span&gt;.
Therefore, &lt;span class=&#34;math display&#34;&gt;\[
S = \{ (0, 0) \} \cup \{ (x, y): x &amp;gt; 0, y \in \R \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If only we can guarantee the closeness of transformation of a proper
cone!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;conic Farkas’ lemma&lt;/strong&gt;. Suppose there exists a
&lt;span class=&#34;math inline&#34;&gt;\(\bar y \in \R^m\)&lt;/span&gt; satisfying &lt;span class=&#34;math display&#34;&gt;\[
-\sum_{i=1}^m \bar y_i a_i \in \intr(K^*) \tag{Slater condition}
\]&lt;/span&gt; Then exactly one of the &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(I)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{(II)}\)&lt;/span&gt; is solvable.&lt;/p&gt;
&lt;p&gt;The spirit of the Slater condition (and many its variants) is that
“some vector is in the interior of some set”. For this case, it
guarantees the closeness of &lt;span class=&#34;math inline&#34;&gt;\(K^*\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;Recall the LP strong duality theorem: suppose that the primal is
feasible and is bounded below (alternatively the dual is feasible and is
bounded above); then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = v_d^*\)&lt;/span&gt;
and both the primal and the dual have optimal solutions. The question is
what about &lt;strong&gt;the duality gap and the attainment&lt;/strong&gt; of
CLP?&lt;/p&gt;
&lt;blockquote&gt;
Example: &lt;strong&gt;nonzero duality gap&lt;/strong&gt;. Consider the SDP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \inf \quad &amp;amp; y_1\\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
0 &amp;amp; y_1 &amp;amp; 0 \\
y_1 &amp;amp; y_2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1+y_1
\end{bmatrix} \in \mathcal{S}_3^+
\end{aligned}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(y_1 = 0, y_2 = 0\)&lt;/span&gt; is the
only solution to it. Thus, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
0\)&lt;/span&gt;. It can be rewritten as $$
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
v_p^* = - \sup \quad &amp;amp; \underbrace{[-1, 0]}_{b^T} [y_1, y_2]^T \\
\text{s.t.} \quad &amp;amp;
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 \\
\end{bmatrix}
}_{C} -
y_1
\underbrace{
\begin{bmatrix}
0 &amp;amp; -1 &amp;amp; 0 \\
-1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; -1
\end{bmatrix}
}_{A_1} -
y_2
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; -1 \\
\end{bmatrix}
}_{A_2}

\in \mathcal{S}_3^+
\end{aligned}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Its dual is
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
v_d^* = -\inf \quad &amp;amp; X_{33} \\
\text{s.t.} \quad &amp;amp; -2 X_{12} - X_{33} = -1 \\
&amp;amp; -X_{22} = 0 \\
&amp;amp; X \in \mathcal{S}_+^3
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;$$ The dual is feasible and &lt;span class=&#34;math inline&#34;&gt;\(X_{33}\)&lt;/span&gt; can only be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = -1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that the duality gap is nonzero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;non-attainment&lt;/strong&gt;. Consider the SOCP: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \sup \quad &amp;amp; -y_1 \\
\text{s.t.} \quad &amp;amp; (y_1 + y_2, 1, y_1 - y_2) \in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; The constraint is &lt;span class=&#34;math display&#34;&gt;\[
y_1 + y_2 \ge \sqrt{1 + (y_1 - y_2)^2} \iff 4 y_1 y_2 \ge 1
\]&lt;/span&gt; This implies that &lt;span class=&#34;math inline&#34;&gt;\(y_1, y_2 &amp;gt;
0\)&lt;/span&gt;. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = 0\)&lt;/span&gt;
but cannot be attained. It can be rewritten as &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_p^* = \sup \quad &amp;amp; [-1, 0] [y_1, y_2]^T \\
\text{s.t. }\quad &amp;amp;
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix} -
y_1 \begin{bmatrix}
-1 \\
0 \\
-1
\end{bmatrix} -
y_2 \begin{bmatrix}
-1 \\
0 \\
1
\end{bmatrix}
\in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; Its dual is &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v_d^* = \inf \quad &amp;amp; z_2 \\
\text{s.t.} \quad &amp;amp; -z_1 - z_3 = -1 \\
&amp;amp; -z_1 + z_3 = 0 \\
&amp;amp; (z_1, z_2, z_3) \in \mathcal{Q}^3
\end{aligned}
\]&lt;/span&gt; The only solution is &lt;span class=&#34;math inline&#34;&gt;\((1/2, 0,
1/2)\)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\(v_d^* = 0\)&lt;/span&gt; is
attained at this point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite the above, can we draw anything about the duality gap and the
attainment of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality of CLP&lt;/strong&gt;. Suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt; is bounded and satisfies
Slater’s condition, i.e. there exists a feasible &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \intr(K)\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* = v_d^*\)&lt;/span&gt; and there exists an &lt;span class=&#34;math inline&#34;&gt;\(y^*\)&lt;/span&gt; that is optimal to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, suppose &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; is bounded and satisfies
Slater’s condition, i.e. there exists a feasible &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(c - \sum_{i=1}^m \bar y_i a_i \in
\intr(K^*)\)&lt;/span&gt;. Then, &lt;span class=&#34;math inline&#34;&gt;\(v_p^* =
v_d^*\)&lt;/span&gt; and there exists an &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; that is optimal to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With Slater’s condition on one side, we can guarantee the zero
duality gap and the attainment only on the other side. The CLP strong
duality is much “weaker” than the LP strong duality. Refer to the
&lt;u&gt;non-attainment&lt;/u&gt; example.&lt;/p&gt;
&lt;p&gt;Pay attention what CLP strong duality does not say as well. Even
though the Slater’s condition is not satisfied, zero duality gap and
attainment can hold. Refer to the &lt;u&gt;nonzero duality gap&lt;/u&gt;
example.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
    <item>
      <title>6-optimizaition-under-uncertainty</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</guid>
      <description>

&lt;p&gt;Firstly consider that in the previous (conic) linear programming
discussion, &lt;span class=&#34;math display&#34;&gt;\[
\tag{Problem} \begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
\]&lt;/span&gt; The coefficients &lt;span class=&#34;math inline&#34;&gt;\(\hat a_i, \hat b,
\hat c\)&lt;/span&gt; are given data. They correspond to the measurements in
the real world. But it is highly likely that these measurements are not
100% accurate. How do we take into account these uncertainties?&lt;/p&gt;
&lt;p&gt;Without loss of generality, we may assume that &lt;span class=&#34;math inline&#34;&gt;\(\hat c\)&lt;/span&gt; is deterministic, since the above
is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; t \\
\text{s.t.} \quad &amp;amp; \hat c^T x \le t \\
&amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
\]&lt;/span&gt; In this new problem, we bring all uncertainties into the
constraint and the coefficient &lt;span class=&#34;math inline&#34;&gt;\([0,\dots,0,1]\)&lt;/span&gt; for the variable &lt;span class=&#34;math inline&#34;&gt;\([x^T, t]\)&lt;/span&gt; is deterministic.&lt;/p&gt;
&lt;p&gt;The problem remains how to handle the uncertainty in the constraint
in &lt;span class=&#34;math inline&#34;&gt;\(\text{(Problem)}\)&lt;/span&gt;. There are
several viable methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stochastic optimization&lt;/p&gt;
&lt;p&gt;Stochastic optimization assumes that data follow a probability
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt; (unknown).
The constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta
\]&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt;.
This method is non-trivial, due to the integral of multi-dimensional
probability distribution function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Robust optimization&lt;/p&gt;
&lt;p&gt;The assumption is that data is drawn from a &lt;strong&gt;ambiguity
set&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}\)&lt;/span&gt;. We
require that &lt;span class=&#34;math display&#34;&gt;\[
\hat a_i^T x \le \hat b_i, \forall i=1,2,\dots, \forall (\hat a_i, \hat
b_i) \in \mathcal{U}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distributionally-robust optimization&lt;/p&gt;
&lt;p&gt;This is kind of the combination of the above two methods. The
assumption is that data follow a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt;, which in turn belongs to some
ambiguity set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}\)&lt;/span&gt;. The
constraint becomes &lt;span class=&#34;math display&#34;&gt;\[
\inf_{\mathbb{P} \in \mathcal{U}} \Pr(\hat a_i^T x \le \hat b_i) \ge 1 -
\delta, \forall i=1,2,\dots
\]&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt;
0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;robust-linear-programming&#34;&gt;Robust Linear Programming&lt;/h2&gt;
&lt;p&gt;Consider the problem &lt;span class=&#34;math display&#34;&gt;\[
\label{rp} \tag{R} \begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; \underbrace{[\hat \alpha_i^T, \hat b_i]}_{\hat
a_i^T} \underbrace{[x^T, x&amp;#39;]^T}_z \le 0, \\
&amp;amp; \quad \hat a_i \in \mathcal{U}_i, i=1,\dots,m \\
&amp;amp; \underbrace{x&amp;#39;}_{z_{n+1}} = -1
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{U}_i \triangleq \{
y \in \R^{n+1}: y = u_i + B_i v, B_i \in \mathcal{S}_{++}^{n+1}, \|v\|
\le 1 \}\)&lt;/span&gt; is an ellipsoid.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{rp}\)&lt;/span&gt; is not LP
actually, as it may contain infinitely-many linear constraints. This is
usually hard. Just imagine its dual problem. For a primal that has
infinitely-many constraints, its dual has infinitely-many variables.&lt;/p&gt;
&lt;p&gt;The key question now is how to tackle &lt;span class=&#34;math inline&#34;&gt;\(\hat a_i^T z \le 0, \forall \hat a_i \in
\mathcal{U}_i\)&lt;/span&gt; (ignoring &lt;span class=&#34;math inline&#34;&gt;\(z_{n+1} =
-1\)&lt;/span&gt; for now). In essence, it is equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left[ \sup_{\hat a_i \in \mathcal{U}_i} \hat a_i^T z \right] &amp;amp;\le 0
\iff \\
\left[ \sup_{\|v\| \le 1} (u_i + B_i v)^T z \right] &amp;amp;\le 0 \iff \\
u_i^T z + \left[ \sup_{\|v\| \le 1} v^T B_i z \right] &amp;amp;\le 0 \iff \\
y_i^T z + \|B_i z\|_2 &amp;amp;\le 0
\end{aligned}
\]&lt;/span&gt; This is exactly an SOCP constraint.&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>7-quadratically-constrained-quadratic-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</guid>
      <description>

&lt;h2 id=&#34;quadratically-constrained-quadratic-programming&#34;&gt;Quadratically
Constrained Quadratic Programming&lt;/h2&gt;
&lt;p&gt;Consider the quadratically constrained quadratic programming: &lt;span class=&#34;math display&#34;&gt;\[
\label{qcqp} \tag{QCQP} \begin{aligned}
\inf \quad &amp;amp; x^T Q x \\
\text{s.t.} \quad &amp;amp; x^T A_i x \ge b_i, \forall i=1,\dots,m
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Q, A_1, \dots, A_m \in
\mathcal{S}^n\)&lt;/span&gt;. By far, no convexity is not assumed. But on the
other hand, we can apply the &lt;strong&gt;semi-definite relaxation&lt;/strong&gt;
technique to transform the &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Observe that &lt;span class=&#34;math inline&#34;&gt;\(x^T Q x = \tr(x^T Q x) =
\tr(Q x x^T) = Q \bullet x x^T\)&lt;/span&gt;, which is linear in &lt;span class=&#34;math inline&#34;&gt;\(x x^T\)&lt;/span&gt;. We can apply the same trick to the
constraints so that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt; is
equivalent to &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
v^* = \inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; \exists x \in \R^n, X = x x^T \\
\text{ non-convex??}
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(\exists x \in \R^n, X =
x x^T \iff X \in \mathcal{S}_+^n, \rank(X) \le 1\)&lt;/span&gt;. That is.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; X \succeq 0 \\
&amp;amp; \rank(X) \le 1
\end{aligned}
\]&lt;/span&gt; The &lt;span class=&#34;math inline&#34;&gt;\(\rank\)&lt;/span&gt; function is not
convex. We may as well drop the rank constraint so that the
semi-definite relaxation of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{qcqp}\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\label{relaxed-qcqp} \tag{Relaxed QP} \begin{aligned}
v_R^* = \inf \quad &amp;amp; Q \bullet X \\
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \\
&amp;amp; X \succeq 0
\end{aligned}
\]&lt;/span&gt; Observe that &lt;span class=&#34;math inline&#34;&gt;\(v^* \ge
v_R^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-qcqp}\)&lt;/span&gt; is a SDP.&lt;/p&gt;
&lt;h3 id=&#34;max-cut-problem&#34;&gt;Max-cut Problem&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(G = (V,E)\)&lt;/span&gt; be an undirected
graph and &lt;span class=&#34;math inline&#34;&gt;\(w: E \mapsto \R_+\)&lt;/span&gt; be a
weight function on &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. A subset
&lt;span class=&#34;math inline&#34;&gt;\(S \subseteq V\)&lt;/span&gt; defines a cut and the
value of a cut is &lt;span class=&#34;math display&#34;&gt;\[
w(S) \triangleq \sum_{(i,j) \in E, i \in S, j \notin S} w_{ij}
\]&lt;/span&gt; The goal is to find &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq
V\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(w(S)\)&lt;/span&gt; is
maximized. The minimization problem is trivial, simply choosing &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\emptyset\)&lt;/span&gt; gives the minimum value &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(x_i
\in \{ -1, +1 \}\)&lt;/span&gt; be a binary variable indicating whether vertex
&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is in the cut (&lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt;) or not (&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;). Then, &lt;span class=&#34;math display&#34;&gt;\[
\label{mc} \tag{Max-cut} \begin{aligned}
v^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - x_i
x_j) \\
\text{s.t.} \quad &amp;amp; x_i^2 = 1, \forall i=1,\dots,n \\
\end{aligned}
\]&lt;/span&gt; Apply the SDR technique (there is a middle step to convert the
above to a QP) to get &lt;span class=&#34;math display&#34;&gt;\[
\label{relaxed-mc} \tag{Relaxed Max-cut} \begin{aligned}
v_\text{sdr}^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij}
(1 - X_{ij}) \\
\text{s.t.} \quad &amp;amp; X_{ii} = 1, \forall i=1,\dots,n \\
&amp;amp; X \succeq 0
\end{aligned}
\]&lt;/span&gt; Note that &lt;span class=&#34;math inline&#34;&gt;\(v^* \le
v_\text{sdr}^*\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-mc}\)&lt;/span&gt; is solved with a
rank-one matrix &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt;, we can
automatically decompose it to give the optimal solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{mc}\)&lt;/span&gt;. The crux is how to preserve
the optimality when &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt; is of rank
higher than one. Here is an algorithm for it:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Solve &lt;span class=&#34;math inline&#34;&gt;\(\eqref{relaxed-mc}\)&lt;/span&gt; to get
an optimal solution &lt;span class=&#34;math inline&#34;&gt;\(X^*\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(X^* = U^T U\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(U \in \R^{n \times n}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(u_i \in \R^n\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th column of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. We have &lt;span class=&#34;math inline&#34;&gt;\(\|u_i\|_2^2 = u_i^T u_i = X_{ii}^* =
1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(r \in \R^n\)&lt;/span&gt; be a random
vector uniformly distributed on the sphere &lt;span class=&#34;math inline&#34;&gt;\(S^n = \{ x \in \R^n: \|x\|_2 = 1 \}\)&lt;/span&gt; (this
can be done by normalizing the samples from standard Gaussian in &lt;span class=&#34;math inline&#34;&gt;\(\R^n\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39; = \sign(u_i^T r)\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sign(z)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(z \ge
0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; otherwise. Return
&lt;span class=&#34;math inline&#34;&gt;\(\{ x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt; as a
feasible solution to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{mc}\)&lt;/span&gt;.
This is also referred to as the &lt;strong&gt;hyperplane
rounding&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the first place, &lt;span class=&#34;math inline&#34;&gt;\(x_i&amp;#39;\)&lt;/span&gt;s
are feasible. Let &lt;span class=&#34;math inline&#34;&gt;\(v&amp;#39;\)&lt;/span&gt; be the
objective value associated with the cut &lt;span class=&#34;math inline&#34;&gt;\(\{
x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt;. Clearly, &lt;span class=&#34;math inline&#34;&gt;\(v&amp;#39; \le v^*\)&lt;/span&gt;. To analyze its
approximation bound, firstly note that &lt;span class=&#34;math inline&#34;&gt;\(\{
x_i&amp;#39;: i=1,\dots,n \}\)&lt;/span&gt; is random. We can only consider the
&lt;span class=&#34;math inline&#34;&gt;\(\E[v&amp;#39;]\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[v&amp;#39;] = \frac{1}{2} \E[\sum_{(i,j) \in E} w_{ij} (1 - x_i&amp;#39;
x_j&amp;#39;)] \\
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \E[\frac{1 - x_i&amp;#39; x_j&amp;#39;}{2}] \\
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \Pr[\sign(u_i^T r) \ne \sign(u_j^T r)]
\\
\end{aligned}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(u, v \in S^n\)&lt;/span&gt; be
arbitrary, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; be uniformly
distributed on &lt;span class=&#34;math inline&#34;&gt;\(S^{n-1}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\Pr[\sign(u^T r) \ne \sign(v^T r)] = \frac{\arccos(u^T v)}{\pi}
\]&lt;/span&gt; Under the above setting, for any &lt;span class=&#34;math inline&#34;&gt;\(z
\in [-1, 1]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(\cos \theta = z\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\frac{\arccos z}{\pi} = \frac{2 \theta}{\pi(1 - \cos \theta)}
\frac{1}{2} (1 - z) \\
\ge \alpha \cdot \frac{1}{2} (1-z)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \min_{0 \le \theta
\le \pi} \frac{2 \theta}{\pi (1 - \cos \theta)} &amp;gt; 0.878\)&lt;/span&gt;. As
a result, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\E[v&amp;#39;] = \sum_{(i,j) \in E} w_{ij} \frac{\arccos u_i^T
u_j}{\pi} \\
&amp;amp;\ge \sum_{(i,j) \in E} w_{ij} \alpha \cdot \frac{1}{2} (1 -
\underbrace{u_i^T u_j}_{X_{ij}^*}) \\
&amp;amp;= \alpha \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}^*) \\
&amp;amp;= \alpha v_\text{sdr}^* \ge \alpha v^*
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;


</description>
    </item>
    
    <item>
      <title>8-nonlinear-programming</title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</link>
      <pubDate>Fri, 07 Jan 2022 13:39:19 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</guid>
      <description>

&lt;h2 id=&#34;nonlinear-programming&#34;&gt;Nonlinear Programming&lt;/h2&gt;
&lt;p&gt;Recall the unconstrained optimization problem: &lt;span class=&#34;math display&#34;&gt;\[
\inf_{x \in \R^n} \quad f(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a continuously differentiable function, &lt;span class=&#34;math inline&#34;&gt;\(\bar x \in \R^n\)&lt;/span&gt; be an arbitrary point. If
there exists s &lt;strong&gt;direction&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(d
\in \R^n \setminus \{ 0 \}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x)^T d &amp;lt; 0\)&lt;/span&gt;, then there
exists &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0 &amp;gt; 0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
f(\bar x + \alpha d) &amp;lt; f(\bar x), \forall \alpha \in (0, \alpha_0]
\]&lt;/span&gt; Here, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is called a
&lt;strong&gt;descent direction&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\bar
x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a necessary condition for &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; to be a local minima is that &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt; (&lt;strong&gt;first-order
necessary condition&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let &lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto
\R\)&lt;/span&gt; be a convex and continuously differentiable function. Then,
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a global minima of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; if and only if &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;second-order sufficient condition&lt;/strong&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(f: \R^n \mapsto \R\)&lt;/span&gt; be a twice
continuously differentiable function. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x) = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2 f(\bar x) \succ 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a local minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;In constrained case, simple conditions does not apply, e.g. &lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \ge 1} x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\inf_{x \ge -1} x^2\)&lt;/span&gt;. Consider the
following constrained problem: &lt;span class=&#34;math display&#34;&gt;\[
\label{primal} \tag{P} \begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; g_i(x) \le 0, i=1,\dots,r \\
&amp;amp; h_j(x) = 0, j=1,\dots,s
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f, g_i, h_j\)&lt;/span&gt; are
continuously differentiable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Fritz John necessary condition&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minimum of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;. Then there exist
&lt;strong&gt;multipliers&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(u \in
\R\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_r \in
\R\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(w_1, \dots, w_s \in \R\)&lt;/span&gt;
such that &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
u \nabla f(\bar x) + \sum_{i=1}^r v_i \nabla g_i(\bar x) + \sum_{j=1}^s
w_j \nabla h_j(x) = 0 \tag{vanishing gradient} \\
[u, v_1, \dots, v_r, w_1, \dots, w_s] \ne 0 \tag{non-trivial solution}
\\
u, v_i \ge 0, i=1,\dots,r \tag{non-negativity} \\
v_i g_i(\bar x) = 0, i=1,\dots,r \tag{complementarity} \\
\end{gather}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; tells the importance
of the inequality constraint &lt;span class=&#34;math inline&#34;&gt;\(g_i(x)\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(v_i =
0\)&lt;/span&gt; implies that &lt;span class=&#34;math inline&#34;&gt;\(g_i(x) \le
0\)&lt;/span&gt; is well-fulfilled (strictly less than zero).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The implication is that, at a local minima, we should not be able to
find a direction that decreases the objective value as well as maintains
the feasibility. For simplicity of discussion, we drop the equality
constraints below.&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;Linear non-independence&lt;/p&gt;
&lt;p&gt;The vanishing gradient together with the non-trivial solution in
essence rules out the possibility of linear independence among the
gradients. This is easy to interpret. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar x), \nabla g_1(\bar x), \dots,
\nabla g_r(\bar x)\)&lt;/span&gt; are linearly independent, it is easy to find
a direction &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(\Col^\perp(\nabla g_1(\bar x), \dots, \nabla
g_r(\bar x))\)&lt;/span&gt; that is acute to &lt;span class=&#34;math inline&#34;&gt;\(-\nabla f(\bar x)\)&lt;/span&gt;. Then moving along
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; will decrease the objective
function value but maintain the inequality constraints, which
contradicts that &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a local
minima.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Non-negativity + complementarity&lt;/p&gt;
&lt;p&gt;These two components should be discussed together and are a bit
intriguing. Please refer to &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0022247X67901631&#34;&gt;this
paper&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surely, the premise is that &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is
nonzero. When the only solution to &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is zero, the corresponding solution
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will be a garbage point: it will not be
a local minima. In this case, &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(\bar
x)\)&lt;/span&gt; must have a component that is orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(\nabla g_i(\bar x)\)&lt;/span&gt;’s. We can walk along
this component (or its opposite) to decrease &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; without compromising inequality
constraints. This motivates the study of &lt;strong&gt;constraint
qualification&lt;/strong&gt;, which aims to ensure a nonzero &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that in the above discussion, we ignore the effect of &lt;span class=&#34;math inline&#34;&gt;\(\nabla h_j(\bar x)\)&lt;/span&gt;’s. They only make the
choice of direction stricter, since the direction has to be orthogonal
to them.&lt;/p&gt;
&lt;h3 id=&#34;kkt-conditions-and-constraint-qualification&#34;&gt;KKT Conditions and
Constraint Qualification&lt;/h3&gt;
&lt;p&gt;One observation is that, if the constraint gradients are linearly
independent (though required dependent), there is no way to have &lt;span class=&#34;math inline&#34;&gt;\(u = 0\)&lt;/span&gt; or otherwise &lt;span class=&#34;math inline&#34;&gt;\(v_1, \dots, v_r, u_1, \dots, u_s\)&lt;/span&gt; has to
be zero due to the linear independence, which in turn violates the
nonzero multiplier condition. How to “obtain” the contradicting linear
independence expectation and nonzero multiplier condition at the same
time?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Karush-Kuhn-Tucker conditions&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minima of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primal}\)&lt;/span&gt;. Let &lt;span class=&#34;math display&#34;&gt;\[
I(\bar x) = \{ i: g_i(\bar x) = 0 \}
\]&lt;/span&gt; be the index set on the &lt;strong&gt;active inequality
constraint&lt;/strong&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\{ \nabla
g_i(\bar x) \}_{i \in I} \cup \{ \nabla h_j(\bar x) \}_{j=1}^s\)&lt;/span&gt;
are linearly independent (&lt;strong&gt;linear-independence constraint
qualification&lt;/strong&gt;). Then, there exists &lt;span class=&#34;math inline&#34;&gt;\(v \in \R^r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w \in \R^s\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
\nabla f(\bar x) + \sum_{i=1}^r \nabla g_i(x) + \sum_{j=1}^s \nabla
h_j(x) = 0 \\
v_i \ge 0, i=1,\dots,r \\
v_i g_i(\bar x) = 0, i=1,\dots,r
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: importance of CQ. Consider the problem &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\inf \quad &amp;amp; f(x_1, x_2) = x_1 \\
\text{s.t.} \quad &amp;amp; g_1(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - 1
\le 0 \\
&amp;amp; g_2(x_1, x_2) = (x_1 - 1)^2 + (x_2 + 1)^2 - 1 \le 0 \\
\end{aligned}
\]&lt;/span&gt; The only feasible and thus optimal solution is &lt;span class=&#34;math inline&#34;&gt;\(\bar x = (1, 0)\)&lt;/span&gt;. In this case, the active
inequality constraint gradient is &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
0 \\
-2
\end{bmatrix},
\begin{bmatrix}
0 \\
2
\end{bmatrix}
\]&lt;/span&gt; They are linearly dependent. Therefore, KKT conditions doesn’t
hold. Here is the reason why we intentionally separate the equality
constraints from inequality constraints. Though &lt;span class=&#34;math inline&#34;&gt;\(h(x) = 0 \iff h(x) \le 0 \land -h(x) \le
0\)&lt;/span&gt;, if we lay down the equality constraint as inequality
constraints, the gradient of &lt;span class=&#34;math inline&#34;&gt;\(h(x)\)&lt;/span&gt;
is always linearly dependent to that of &lt;span class=&#34;math inline&#34;&gt;\(-h(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One takeaway is that, even in this convex optimization problem, KKT
may not hold.&lt;/p&gt;
&lt;p&gt;Another takeaway is that, it is very convenient to “draw circles”
when finding counter examples related to constraint qualification.
Better still, leave only one feasible point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem with LICQ is that it is tedious to check for every
solution of &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt;. We may prefer
some kind of &lt;strong&gt;“looser” constraint qualifications&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Slater constraint qualification&lt;/strong&gt;. Suppose
that &lt;span class=&#34;math inline&#34;&gt;\(g_1, \dots, g_r\)&lt;/span&gt; are convex and
&lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots, h_s\)&lt;/span&gt; are affine. Let
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be a local minima. Denote
the feasible region as &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Suppose
that there exists &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39; \in S\)&lt;/span&gt; such
that &lt;span class=&#34;math inline&#34;&gt;\(g_i(x&amp;#39;) &amp;lt; 0\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,r\)&lt;/span&gt;. Then, the KKT conditions are
necessary for optimality.&lt;/p&gt;
&lt;p&gt;This Slater condition quite resembles that in the conic Farkas’
lemma.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Suppose that &lt;span class=&#34;math inline&#34;&gt;\(g_1, \dots,
g_r\)&lt;/span&gt; are concave and &lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots,
h_s\)&lt;/span&gt; are affine. Then, the KKT conditions are necessary for
optimality.&lt;/p&gt;
&lt;p&gt;This theorem is especially useful when all the constraint functions
are affine (since affine functions are both convex and concave).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let &lt;span class=&#34;math inline&#34;&gt;\(A \in \R{m \times n}, b \in
\R^m, c \in \R^n\)&lt;/span&gt; be given. Consider &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; c^T x \\
\text{s.t.} \quad &amp;amp; Ax = b \\
&amp;amp; x \ge 0
\end{aligned}
\]&lt;/span&gt; Beware of the dimension of constraint functions when
converting this LP problem to nonlinear programming problem: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\min \quad &amp;amp; c^Tx \\
\text{s.t.} \quad &amp;amp; g_i(x) = -x_i = -e_i^T x \le 0, i=1,\dots,n \\
&amp;amp; h_i(x) = b_j - a_j^T x = 0, j=1,\dots,m
\end{aligned}
\]&lt;/span&gt; This is a linearly constrained problem; thus KKT conditions
are necessary for optimality: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
c - \sum_{i=1}^r v_i e_i - \sum_{j=1}^s w_i a_j = 0 \label{grad} \\
v_i \ge 0 \label{dual} \\
v_i x_i = 0, i=1,\dots,r \label{compl}
\end{gather}
\]&lt;/span&gt; From &lt;span class=&#34;math inline&#34;&gt;\(\eqref{grad}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
c - v - A^T w = 0
\]&lt;/span&gt; Given &lt;span class=&#34;math inline&#34;&gt;\(v \ge 0\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dual}\)&lt;/span&gt; and the complementarity from
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{compl}\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(c \ge A^T w\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((c - A^T w)_i x_i = 0\)&lt;/span&gt;. This essentially
recovers the sufficient and necessary conditions for the optimality of
LP. But KKT only tells the condition is necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is when are KKT conditions sufficient?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;KKT sufficient conditions&lt;/strong&gt;. Suppose that
&lt;span class=&#34;math inline&#34;&gt;\(f, g_1, \dots, g_r\)&lt;/span&gt; are convex and
&lt;span class=&#34;math inline&#34;&gt;\(h_1, \dots, h_s\)&lt;/span&gt; are affine. Suppose
further that there exists &lt;span class=&#34;math inline&#34;&gt;\((\bar x, \bar v,
\bar w)\)&lt;/span&gt; satisfying the KKT conditions: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
g_i(\bar x) \le 0, h_j(\bar x) = 0, i=1,\dots,r, j=1,\dots,s \tag{primal
feasibility} \\
\tag{dual feasibility} \left. \begin{gathered}
\nabla f(\bar x) + \sum_{i=1}^r \bar v_i \nabla g_i(\bar x) +
\sum_{j=1}^s \bar w_i \nabla h_i(x) = 0 \\
\bar v_i \ge 0, i=1,\dots,r
\end{gathered} \right\} \\
\bar v_i g_i(x) = 0, i=1,\dots,r \tag{complentary slackness}
\end{gather}
\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; is a global
minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h3&gt;
&lt;p&gt;For simplicity, we rewrite the constrained problem as follows: &lt;span class=&#34;math display&#34;&gt;\[
\label{primalp} \tag{P} \begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; G(x) \le 0 \\
&amp;amp; H(x) = 0
\end{aligned}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(G(x) = [g_1(x), \dots,
g_r(x)]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(x) = [h_1(x), \dots,
h_s(x)]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Observe that &lt;span class=&#34;math display&#34;&gt;\[
\eqref{primalp} \equiv \inf_{x \in \R^n} \sup_{v \in \R_+^r, w \in \R^s}
\underbrace{f(x) + v^T G(x) + w^T H(x)}_{L(x, v, w)}
\]&lt;/span&gt; The dual of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primalp}\)&lt;/span&gt; is then &lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
v_d^* = \sup_{v \in \R_+^r, w \in \R^s} \inf_{x \in \R^n} L(x, v, w)
\label{dualp} \tag{D}
\end{equation}
\]&lt;/span&gt; Observe that &lt;span class=&#34;math display&#34;&gt;\[
\underbrace{\inf_{x \in \R^n}L(x, \bar v, \bar w)}_{\theta(\bar v, \bar
w)} \le L(\bar x, \bar v, \bar w) \le \underbrace{\sup_{v \in \R_+^r, w
\in \R^s} L(\bar x, v, w)}_{\gamma(\bar x)}
\]&lt;/span&gt; This implies that &lt;span class=&#34;math display&#34;&gt;\[
v_d^* = \sup_{v \in \R_+^r, w \in \R^s} \theta(v, w) \le \inf_{x \in
\R^n} \gamma(x) = v_p^*
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{primalp}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((\bar v, \bar w)\)&lt;/span&gt; be feasible for &lt;span class=&#34;math inline&#34;&gt;\(\eqref{dualp}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
f(\bar x) = \gamma(\bar x) \ge \theta(\bar v, \bar w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: illustration of weak duality. Consider a simple case: &lt;span class=&#34;math display&#34;&gt;\[
\begin{gathered}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \\
\text{s.t.} \quad &amp;amp; g(x) \le 0
\end{aligned} \\
\rule{6cm}{0.4pt} \\
\begin{aligned}
v_d^* = \sup_{v \ge 0} &amp;amp; \inf_{x \in \R^n} [f(x) + v g(x)] \\
\end{aligned}
\end{gathered}
\]&lt;/span&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G} = \{ (y,z): y =
g(x), z = f(x), x \in \R^n \}\)&lt;/span&gt;. Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta(v) &amp;amp;= \inf_{x \in \R^n} [f(x) + v g(x)] \\
&amp;amp;= \inf_{(y, z) \in \mathcal{G}} [z + v y]
\end{aligned}
\]&lt;/span&gt; This set is quite related to the proof of the &lt;strong&gt;strong
duality&lt;/strong&gt; under KKT sufficient condition together with Slater
condition.&lt;/p&gt;
&lt;/blockquote&gt;


</description>
    </item>
    
  </channel>
</rss>
