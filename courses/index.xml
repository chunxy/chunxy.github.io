<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Courses | Chunxy&#39; Website</title>
    <link>https://chunxy.github.io/courses/</link>
      <atom:link href="https://chunxy.github.io/courses/index.xml" rel="self" type="application/rss+xml" />
    <description>Courses</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language>
    <image>
      <url>https://chunxy.github.io/media/sharing.png</url>
      <title>Courses</title>
      <link>https://chunxy.github.io/courses/</link>
    </image>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/energy-efficient-computing/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/energy-efficient-computing/notes/</guid>
      <description>&lt;h1 id=&#34;implementation-trickcomm-lower-bound&#34;&gt;Implementation Trick&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;h2 id=&#34;gemm&#34;&gt;GEMM&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ge&lt;/strong&gt;neral &lt;strong&gt;M&lt;/strong&gt;atrix &lt;strong&gt;M&lt;/strong&gt;ultiplication describes the implementation tricks that speeds up computation in neural network. Matrix multiplication is a classical, fundamental and established field in both math and computer science. And this is the reason why much effort and interest have been put into how to further speed up it and how to convert other kinds of operations into it.&lt;/p&gt;
&lt;h3 id=&#34;im2col&#34;&gt;Im2Col&lt;/h3&gt;
&lt;h4 id=&#34;single-feature-map-and-kernel&#34;&gt;Single Feature Map and Kernel&lt;/h4&gt;
&lt;p&gt;A normal convolution operation will slide a &lt;strong&gt;window&lt;/strong&gt; of the same size as &lt;strong&gt;kernel&lt;/strong&gt; ($F: k_h \times k_w$) through the &lt;strong&gt;feature map&lt;/strong&gt; ($I: i_h \times i_w$) in a row-major order (for simplicity, we will take that stride is $1$ and padding is $0$). This causes problem because numbers in a single convolution operation will span multiple columns, due to which the spatial locality cannot be exploited.&lt;/p&gt;
&lt;p&gt;Since the convolution operation is in essence doing the &amp;ldquo;sum of products&amp;rdquo;, we may just as well treat the convolution as dot product between two vectors.&lt;/p&gt;
&lt;img src=&#34;./Images/im2col.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To realize it, we can squeeze the kernel into a $k_h k_w \times 1$ column vector and each window on the feature map to a $1 \times k_h k_w$ row vector (in memory, a column vector &lt;code&gt;v[N][1]&lt;/code&gt; is no difference from a row vector &lt;code&gt;v[1][N]&lt;/code&gt;). Then we stack these row vectors vertically in the order as their original window would appear in the convolution. This newly synthesized matrix $L$ is usually called &lt;strong&gt;lowered matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As for implementation, we don&amp;rsquo;t do this &amp;ldquo;window by window&amp;rdquo;, because usually the feature map has a large width, which still introduce the same issue of not exploiting the spatial locality when accessing across rows. For each position in the feature map, we can identify all the positions that it will appear in the lowered matrix in one off. Since kernel size is usually small, accessing this lowered matrix across rows causes less cache misses than that in the input.&lt;/p&gt;
&lt;p&gt;Treating $L$ as $l_h \times l_w \times k_h \times k_w$, we fill up its entries in following way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;im2lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;normal-input-and-filter&#34;&gt;Normal Input and Filter&lt;/h4&gt;
&lt;p&gt;By far we only consider the case of a single convolution. More often that not, the real-world convolution is done in batch with multiple channels, meaning that the input $I$ is of shape $i_n \times i_c \times i_w \times i_w$ and filter $F$ is of shape $f_c \times i_c \times k_h \times k_w$. Each of $f_c$ output channels is obtained as the sum of the one-on-one convolution between each of the $i_c$ kernels and each of the $i_c$ channels (which is in accordance with PyTorch&amp;rsquo;s &lt;code&gt;Conv2d&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now we consider the case where there are multiple channels. We simplify a bit by still setting $i_n = 1$. As a result, the input $I$ is of shape $i_c \times i_h \times i_w$ and filter $F$ still remains $f_c \times i_c \times k_h \times k_w$. Suppose each feature map contains $d = o_h \times o_w$ unique kernel windows. Then each transformed channel should be of shape $d \times k_h k_w$ (the order of symbols in shortened multiplication matters too; in this case, $k_h k_w$ indicates we squeeze by row); each transformed kernel should be of shape $k_h k_w \times 1$.&lt;/p&gt;
&lt;p&gt;The input contains $i_c$ such transformed $d \times k_h k_w$ channels. Instead of doing matrix multiplication $i_c$ times, we can concatenate these $i_c$ matrices horizontally to give a single $d \times (i_c k_h k_w)$ lowered matrix $L$.&lt;/p&gt;
&lt;p&gt;The filter contains $f_c \times i_c$ such transformed $k_h k_w \times 1$ kernels. For each output channel, we can concatenate corresponding $i_c$ kernels vertically to facilitate the one-on-one convolution, which gives a single $(i_c k_h k_w) \times f_c$ transformed filter $F&amp;rsquo;$.&lt;/p&gt;
&lt;p&gt;Now $I \circledast F$ becomes $L \times F&amp;rsquo;$. The output shape is $d \times f_c$. Each input image becomes a $d \times 1$ column vector finally, which is exactly what &amp;ldquo;Im2Col&amp;rdquo; means. This $d \times 1$ vector can be transposed and then reshaped into $o_h \times o_w$ to recover the convolution result (no actual transformation has to be done, just to interpret it this way). Then by applying the Im2Col trick repeatedly, we can chain up and handle consecutive convolutional layers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// For simplicity, set s_w = s_h = 1, padding = 0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;im2lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;ker2col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f_c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mecmec&#34;&gt;MEC&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Im2Col costs much extra space because most entries in the feature map appears $k^2$ times in the transformed lowered matrix. The &lt;strong&gt;m&lt;/strong&gt;emory-&lt;strong&gt;e&lt;/strong&gt;fficient &lt;strong&gt;c&lt;/strong&gt;omputation method improves on this by putting the entries of (say vertically) adjacent windows in one row so that entries can be reused. By doing so, the transformed kernel row vector will slide through each row of obtained matrix to compute convolution result.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/mec-basic.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Again in single feature map and kernel case, the filter $F: k_h \times k_w$ is squeezed to a $k_h k_w \times 1$ column vector. The input $I: i_h \times i_w$ is converted to the lowered matrix $L: l_h \times l_w$ where
$$
l_h = o_w \triangleq (i_w - k_w) / s_w + 1 \
l_w = i_h k_w, \text{assuming that $k_w \ge s_w$}\
$$
Note that a $k_w$-width bar in $I$ (like $A$) expands to a row in $L$, and finally a row in $O$. Treating $L$ as $o_w \times i_h \times k_w$, we fill up its entries in following way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// const int l_h = o_w, l_w = i_h * k_w;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;double&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;o_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// I can be transposed first for better spatial locality.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s_w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;memory-layout&#34;&gt;Memory Layout&lt;/h4&gt;
&lt;p&gt;Memory layout is not a standalone method to speed up matrix computation, but that it cornerstones most implementation tricks. Different methods may assume different memory layouts.&lt;/p&gt;
&lt;p&gt;In convention, the input is of shape $\mathrm{N \times C \times H \times W}$, which is the assumed layout of Im2Col. However, the preferred layout for convolution is usually $\mathrm{N \times H \times W \times C}$.&lt;/p&gt;
&lt;p&gt;The justification is that, we usually would parallelize by accessing a window of pixels across all the channels. Though the window spans multiple columns in both cases, $\mathrm{N \times C \times H \times W}$ would separate these $C$ windows apart but $\mathrm{N \times H \times W \times C}$ would otherwise bring all the $c$ channel values of a pixel in a row, in which case the spatial locality can be exploited.&lt;/p&gt;
&lt;p&gt;$\mathrm{N \times H \times W \times C}$ is the assumed layout by MEC. That is, $I$ is of shape $i_n \times i_h \times i_w \times i_c$ and $F$ is of shape $k_h \times k_w \times i_c \times f_c$. Given $i_n \times i_c$ number of $o_w \times i_h k_w$ matrices, $L$ is obtained by interleaving horizontally across channels, and stacked vertically across images. As a result, $L$ is of shape $i_n o_w \times i_h k_w i_c$. Accordingly, $F$ is squeezed to $F&amp;rsquo;: k_h k_w i_c \times f_c$.&lt;/p&gt;
&lt;p&gt;The example below shows a $3 \times 7 \times 7 \times 1$ input and $3 \times 3 \times 1 \times 1$ filter.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/mec-batch.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Given the transformed $L$ and $F&amp;rsquo;$, there are two methods for the remaining dot product (convolution). One is to do $L[0 : i_n o_w, s_w i_h k_w h : s_w i_h k_w h + k_h k_w c] \times F&amp;rsquo;$ for $h = 0, \dots, o_h$, resulting in a $\mathrm{H \times N \times W \times C}$ layout (note that each bar in $L$ expands to a row in $O$), illustrated at the upper right in the above figure. If we want to chain up convolutional layers, we need to convert this layout to $\mathrm{N \times H \times W \times C}$.&lt;/p&gt;
&lt;p&gt;Another is to do $L[o_w n : o_w (n+1), s_w i_h k_w h : s_w i_h k_w h + k_h k_w c]$ for $h = 0, \dots, o_h, n = 0,\dots,i_n$, resulting in a $\mathrm{N \times H \times W \times C}$ layout, illustrated at the lower right in the above figure (the tensor is not properly drawn though; it should have been of shape $3 \times 5 \times 5$).&lt;/p&gt;
&lt;h2 id=&#34;direct-convmmdirect-convdirect-mm-1direct-mm-2direct-mm-3&#34;&gt;Direct Conv/MM&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;GEMM usually requires extra time and space to do transformation. In-place convolution/matrix multiplication also have room for improvement.&lt;/p&gt;
&lt;h3 id=&#34;loop-reordering&#34;&gt;Loop Reordering&lt;/h3&gt;
&lt;p&gt;Nested loop is very common in computation. If the effective statements only appear in the innermost loop, the nesting level of loop indices can be usually be changed without causing side effect.&lt;/p&gt;
&lt;p&gt;One reason to shuffle the loop indices is to better exploit the spatial locality. Another is to exploit the temporal locality, or called input reuse. As an example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// before reordering
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;memset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// after reordering
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matmul_ikj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;memset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before reordering, when innermost loop traverses over &lt;code&gt;k&lt;/code&gt;, there will be one output reuse (&lt;code&gt;C[i][j]&lt;/code&gt;), one cache hit (&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache miss (&lt;code&gt;B[k][j]&lt;/code&gt;. After reordering, when innermost loop traverses over &lt;code&gt;j&lt;/code&gt;, there will be one input reuse (&lt;code&gt;A[i][k]&lt;/code&gt;) and one cache hit (&lt;code&gt;B[k][j]&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;loop-unrolling&#34;&gt;Loop Unrolling&lt;/h3&gt;
&lt;p&gt;At the end of a loop, there is usually a branch checking to determine whether to exit the loop or not. Loop unrolling tries to reduce the number of branch checking in loop (see &lt;a href=&#34;https://www.wikiwand.com/en/Duff%27s_device&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duff&amp;rsquo;s device&lt;/a&gt;). If the number of loops is known in advance, as is usually the case in a &lt;code&gt;for&lt;/code&gt; loop, the number of branch checking can be reduced by repeating the loop statements several times:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;/* count &amp;gt; 0 and count % 8 == 0 assumed */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// before unrolling
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// after unrolling
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;register&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Manual loop unrolling makes little sense nowadays since modern CPU can smartly predict the correct branch and modern compiler can automatically optimize the loop code.&lt;/p&gt;
&lt;h3 id=&#34;write-caching&#34;&gt;Write Caching&lt;/h3&gt;
&lt;h3 id=&#34;tilingtiled-mmtiled-mm-multithreaded&#34;&gt;Tiling&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Tiling utilizes the matrix multiplication property that
$$
\begin{gather*}
A =
\begin{pmatrix}
A_{11} &amp;amp; A_{12} \
A_{21} &amp;amp; A_{22}
\end{pmatrix},
B =
\begin{pmatrix}
B_{11} &amp;amp; B_{12} \
B_{21} &amp;amp; B_{22}
\end{pmatrix} \ \
A B = 
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp;amp; A_{11} B_{12} + A_{12} B_{22} \
A_{21} B_{11} + A_{22} B_{21} &amp;amp; A_{21} B_{12} + A_{22} B_{22} \
\end{pmatrix}
\end{gather*}
$$&lt;/p&gt;
&lt;p&gt;Tiling divides the matrix into blocks that can better fit in the cache line, so that the temporal locality can be exploited.&lt;/p&gt;
&lt;h3 id=&#34;vectorization-simdvectorization&#34;&gt;Vectorization (SIMD)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;h3 id=&#34;array-packing&#34;&gt;Array Packing&lt;/h3&gt;
&lt;h2 id=&#34;dataflow-optimization&#34;&gt;Dataflow Optimization&lt;/h2&gt;
&lt;h3 id=&#34;systolic-arraysystolic-array&#34;&gt;Systolic Array&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Systolic array is dataflow trick implemented in hardware level to speed up compute-bound task. It is an analog to the heart beat: in systolic array setting, memory is the heart, which bumps data (blood) to (usually a regular array of) processing elements (cells) and then recycle (processing result).&lt;/p&gt;
&lt;p&gt;A whole bandwidth of data would certainly entail a bunch of PEs to digest. These PEs can have local memory and execution kernel, which means they can be any kind of computing devices. PEs also connect to each for passing data. All that&amp;rsquo;s left to do is to properly orchestrate the data flow.&lt;/p&gt;
&lt;p&gt;The crux is that, instead of bumping one piece of data to a single processing element (PE), bringing the memory bandwidth to the utmost utilization would be more efficient. Other than that, once data is brought out from memory, it and its intermediate result can be used effectively at each PE it passes through.&lt;/p&gt;
&lt;h2 id=&#34;elimination-of-multiplicationmnnfaster-mm&#34;&gt;Elimination of Multiplication&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;Multiplication is way more time-consuming than addition. Therefore generally, we are willing to trade off with more additions for less multiplications. The two algorithms below can reduce the number of multiplications in matrix computation.&lt;/p&gt;
&lt;h3 id=&#34;strassens-algorithmstrassen-implstrassen-analysis&#34;&gt;Strassen&amp;rsquo;s Algorithm&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;h1 id=&#34;endbmatrix&#34;&gt;Firstly we look at the &lt;strong&gt;Strassen&amp;rsquo;s algorithm&lt;/strong&gt; of matrix computation. Suppose we do the matrix multiplication on two square matrices $M$ and $N$ following the idea of blockwise multiplication. We split the two matrices into
$$
M = \begin{bmatrix}
A &amp;amp; B \
C &amp;amp; D
\end{bmatrix},
N = \begin{bmatrix}
E &amp;amp; F \
G &amp;amp; H
\end{bmatrix}
$$
Then we calculate the intermediate matrices
$$
\begin{align*}
S_1 &amp;amp;= (B-D)(G+H) \
S_2 &amp;amp;= (A+D)(E+H) \
S_3 &amp;amp;= (A-C)(E+F) \
S_4 &amp;amp;= (A+B)H \
S_5 &amp;amp;= A(F-H) \
S_6 &amp;amp;= D(G-E) \
S_7 &amp;amp;= (C+D)E \
\end{align*}
$$
And the final result will be
$$
\begin{bmatrix}
A &amp;amp; B \
C &amp;amp; D
\end{bmatrix}
\begin{bmatrix}
E &amp;amp; F \
G &amp;amp; H
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
S_1 + S_2 - S_4 + S_6 &amp;amp; S_4 + S_5 \
S_6 + S_7 &amp;amp; S_2 - S_3 + S_5 - S_7
\end{bmatrix}
$$
The derivation of matrix multiplication with Strassen&amp;rsquo;s algorithm can be formulated as
$$
T(n) = 
\begin{cases}
\Theta(1), &amp;amp; \text{if $n=1$;} \
7 \Theta(\frac{n}{2}) + \Theta(n^2), &amp;amp; \text{if $n&amp;gt;1$.}
\end{cases}
$$
The master theorem provides an asymptotic analysis for divide-and-conquer recurrence like this. Let $T(n)$ be a monotonically increasing function that satisfies
$$
T(n) = a T(\frac{n}{b}) + f(n) \
T(1) = c
$$
where $a \ge 1, b \ge 2, c &amp;gt; 0$. If $f(n) \in \Theta(n^d)$ where $d \ge 0$, then
$$
T(n) = \begin{cases}
\Theta(n^d) &amp;amp; \text{if $a &amp;lt; b^d$} \
\Theta(n^d \log n) &amp;amp; \text{if $a = b^d$} \
\Theta(n^{\log_b a}) &amp;amp; \text{if $a &amp;gt; b^d$} \
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;The time complexity of Strassen&amp;rsquo;s algorithm is $\Theta(n^{\log_2 7}) \approx \Theta(n^{2.8074})$. There are 7 multiplications and 18 additions (recall that subtraction is addition in computer arithmetics) in Strassen&amp;rsquo;s algorithm. In usual blockwise matrix multiplication, these numbers are 8 and 4. These extra 14 additions in Strassen&amp;rsquo;s algorithm may drag down its performance when input size is small. Other than that, the memory access pattern of Strassen&amp;rsquo;s algorithm is quite chaotic. There are many temp matrices of different shapes generated during the execution. Besides, floating-point errors will accumulate in Strassen&amp;rsquo;s large number of additions. These factors may constitute the reason why Stassen algorithm is not widely adopted.&lt;/p&gt;
&lt;h3 id=&#34;winograd-algorithmwinograd&#34;&gt;Winograd Algorithm&lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;In 1-D convolution, let $m$ be the length of the output vector and $r$ be the length of kernel. The baseline implementation would require $mr$ multiplications. But it is argued that the minimum number of required multiplications is $m + r - 1$ (denoted as $F(m,r)$ the corresponding algorithm).&lt;/p&gt;
&lt;p&gt;Similarly in 2-D convolution, let $m \times n$ be the output dimension and $r \times s$ be the kernel dimension. The baseline implementation would require $m n r s$ multiplications. But the minimum number of required multiplications is $(m + r - 1)(n + s - 1)$ (denoted as $F(m \times n, r \times s)$ the corresponding algorithm).&lt;/p&gt;
&lt;h1 id=&#34;endbmatrix-1&#34;&gt;The Winograd paper documents the following algorithm for $F(2,3)$:
$$
F(2,3) =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 \
d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}
\begin{bmatrix}
g_0 \
g_1 \
g_2
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
m_1 + m_2 + m_3 \
m_2 - m_3 - m_4
\end{bmatrix}
$$
where
$$
m_1 = (d_0 - d_2) g_0, m_2 = (d_1 + d_2) \frac{g_0 + g_1 + g_2}{2} \
m_4 = (d_1 - d_3) g_2, m_3 = (d_2 - d_1) \frac{g_0 - g_1 + g_2}{2}
$$
Actually, this can be written in matrix form as
$$
Y =A^T [(G g) \odot (B^T d)]
$$
where
$$
B^T =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; -1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; -1
\end{bmatrix},
G =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \
\frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} \
\frac{1}{2} &amp;amp; -\frac{1}{2} &amp;amp; \frac{1}{2} \
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} \
A^T =
\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \
0 &amp;amp; 1 &amp;amp; -1 &amp;amp; -1
\end{bmatrix}\
g =
\begin{bmatrix}
g_0 &amp;amp; g_1 &amp;amp; g_2
\end{bmatrix}^T \
d =
\begin{bmatrix}
d_0 &amp;amp; d_1 &amp;amp; d_2 &amp;amp; d_3
\end{bmatrix}^T
$$
$F(m \times m, r \times r)$ can be built upon $F(m, r)$. For example, $F(2 \times 2, 3 \times 3)$ is
$$
Y&amp;rsquo; = A^T \left[ [G g G^T] \odot [B^T d B] \right] A
$$
$F(m \times n, r \times s)$ can be built upon $F(m, r)$ and $F(n, s)$.&lt;/p&gt;
&lt;p&gt;Winograd algorithm is great. One problem is that the $A,B,G$ matrix are too specific. For input/kernel of different sizes, $A,B,G$ will be greatly different. For a convolutional neural network that involves inputs/kernels of varying sizes, Winograd algorithm is not suitable for acceleration on special-purpose hardware, which is usually dedicated to a fixed type of computation.&lt;/p&gt;
&lt;h2 id=&#34;sparse-computationsparse&#34;&gt;Sparse Computation[^sparse]&lt;/h2&gt;
&lt;p&gt;Structured sparsity can reduce computation when there is zero in the multiplicands. But this involves zero-check for each element of the filter, which might ruin the CPU pipeline however. To avoid zero-check in a sparse matrix, we may as well store the positions of all the nonzero elements. During computation, only involved nonzero elements will be multiplied and accumulated to obtain the final result.&lt;/p&gt;
&lt;h3 id=&#34;sparse-matrix-multiplication&#34;&gt;Sparse Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;For a sparse matrix, we either store it in &lt;strong&gt;compressed sparse row&lt;/strong&gt; (CSR) or &lt;strong&gt;compressed sparse column&lt;/strong&gt; (CSC) (or other formats&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;). That is, for every row/column, we maintain a linked list whose nodes store the nonzero value and its offset in this row/column.&lt;/p&gt;
&lt;p&gt;The compression is done after training. The choice of CSR or CSC in matrix multiplication seems arbitrary, so long as one matrix is CSR and the other is CSC. But in practice, convolution is done row-by-row. We need to access the feature map across rows more often than across columns. Thus, feature map is stored in CSR and the filter is stored in CSC.&lt;/p&gt;
&lt;h3 id=&#34;sparse-sparse-convolution&#34;&gt;Sparse-sparse Convolution&lt;/h3&gt;
&lt;p&gt;Sometimes not only the filter, but also the feature map is sparse, e.g. in point cloud case. We may as well apply the Im2Col and then apply the sparse matrix multiplication. But better still, we hope to directly apply the sparse convolution&lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;18&lt;/a&gt;&lt;/sup&gt; on the original feature map.&lt;/p&gt;
&lt;p&gt;But does sparse-sparse convolution always work? It depends. The crux is that we don&amp;rsquo;t just carry out a single sparse-sparse convolution. Modern models consist of multiple layers. We hope the output of each layer is sparse so that sparse-sparse convolution can be chained up. If the output of any intermediate layer is not sparse enough, the whole process becomes pointless.&lt;/p&gt;
&lt;h2 id=&#34;tensor-virtual-machine&#34;&gt;Tensor Virtual Machine&lt;/h2&gt;
&lt;p&gt;Current neural network frameworks translate models into a computation graph in ONNX format, which in turn is translated into hardware instructions by the compilers from different manufacturers (e.g. TensorRT for NVIDIA GPU, MNN for ARM Cortex-A CPU, OpenVINO for Intel CPU).&lt;/p&gt;
&lt;h2 id=&#34;cuda&#34;&gt;CUDA&lt;/h2&gt;
&lt;p&gt;Ideally, the computation can be parallelized for greater speedup. CUDA provides such API for parallel computation on GPU. One key concept of CUDA is its granularity of execution: grid -&amp;gt; block -&amp;gt; thread (from coarsest to finest).&lt;/p&gt;
&lt;h1 id=&#34;model-trick&#34;&gt;Model Trick&lt;/h1&gt;
&lt;p&gt;Other than implementation tricks, the matrix computation can be sped up by multiplication with zero. Block of zeros usually allows us to jump a series of block computation when using the implementation tricks mentioned before. Other than that, it saves space due to the sparse matrix storage model (a kind of &lt;strong&gt;model compression&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;It would be great if there are multiple blocks of zeros in the matrix. Better still, these zeros won&amp;rsquo;t undermine the model results much.&lt;/p&gt;
&lt;h2 id=&#34;sparsification&#34;&gt;Sparsification&lt;/h2&gt;
&lt;p&gt;Sparsification tries to zero parameters in block during training. It does so by adding special regularization term to the loss function. Typical sparsity analysis assumes the linear-regression-like problem. Denote the original loss as $\ell_w(X, Y)$ where $\ell$ is the MSE loss function, $X$ is the data, $Y$ is the target ($Y$ may be a feature map or label vector) and $w$ is the model parameters (interpreted as a vector). Then, the problem is formulated as
$$
\min_{w} \ell_w(X, Y) \triangleq ||Y - X w||&lt;em&gt;F
$$
We may force an extra $l_p$ norm term on $w$:
$$
\hat \ell_w(X, Y) \triangleq \ell_w(X, Y) + \lambda ||w||&lt;em&gt;p
$$
Note that
$$
||w||&lt;em&gt;0 = \sum&lt;/em&gt;{i} \mathbb{1}[w_i \ne 0] \
||w||&lt;em&gt;1 = \sum&lt;/em&gt;{i} |w_i| \
||w||&lt;em&gt;2 = \sqrt{\sum&lt;/em&gt;{i} x_i^2} \
||w||&lt;/em&gt;\infty = \max&lt;/em&gt;{i} |x_i|
$$
$l_0$ norm is a direct attempt to penalize nonzero parameters. However, $\lambda ||w||_0$ is not continuous at points when there is a zero entry in $w$; when $\forall i, w_i \ne 0$, $\lambda ||w||_0$ does not contribute gradient at all. There is no analytical way to determine $w_i$ should be zero or not. The only course open is to manually set each $w_i$ to zero. But this method is prohibitive in terms of the complexity: there are $2^{|w|}$ combinations to try (there are &lt;a href=&#34;https://www.wikiwand.com/en/Matching_pursuit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orthogonal matching pursuit&lt;/a&gt; &amp;lt;??&amp;gt; and &lt;a href=&#34;https://www.jmlr.org/papers/volume19/17-194/17-194.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other methods&lt;/a&gt; that try to approximate it though). Thus, the $l_0$ norm is ruled out for consideration.&lt;/p&gt;
&lt;p&gt;Then $l_1$ norm pops up. $l_1$ norm is good since there is an analytical solution to its gradient w.r.t. $w$. (why does $l_1$ norm add sparsity?)&lt;/p&gt;
&lt;h3 id=&#34;structured-sparsity&#34;&gt;Structured Sparsity&lt;/h3&gt;
&lt;p&gt;Better zero parameters is that zero parameters appear block-wise. Block-wise zero entries are the essence of sparsity. However, the Lasso term does not guarantee zero entries appear in block. To amend it, &lt;strong&gt;group Lasso&lt;/strong&gt; trick is invented and the regularized loss becomes
$$
\hat \ell_w(X, Y) = \ell_w(X, Y) + \sum_j \lambda_j ||\beta_j||_1
$$
where $\beta_j$ is the $l_2$ norm of $j$-th group of parameters that usually spatially near.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/structured-sparsity.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Structured sparsity precedes random sparsity, because random sparsity does not secure a regular memory access pattern, so that there will be a poor cache locality. The figure above illustrates from irregular structured sparsity to regular structured sparsity&lt;sup id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;. Obviously, regular structured sparsity is preferred so long as it won&amp;rsquo;t undermine the model performance much.&lt;/p&gt;
&lt;h3 id=&#34;nonlinearity-approximationnonlinear-approx&#34;&gt;Nonlinearity Approximation&lt;sup id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;As mentioned, typical sparsification analyzes the linear regression problem, which has two components: 1) the model as a linear function; 2) the MSE loss function. The MSE loss function does well, not among the best though, in most tasks. But mere linear model won&amp;rsquo;t do generally.&lt;/p&gt;
&lt;p&gt;Most nonlinearity in nonlinear model comes from the element-wise function mapping on the tensor obtained by linear transformation of input, e.g. the response on feature map obtained by convolution (in this case, $X,w,X w$ will be the transformed input, kernel and convolution result respectively, as shown in &lt;a href=&#34;#GEMM&#34;&gt;GEMM&lt;/a&gt; section). Our focus is still on sparsity of the linear component in the nonlineear model.&lt;/p&gt;
&lt;p&gt;The objective becomes
$$
\min_w ||Y - f(X w)||_F
$$
where $f$ is a nonlineear function like $\mathrm{ReLU}$.&lt;/p&gt;
&lt;h2 id=&#34;pruning&#34;&gt;Pruning&lt;/h2&gt;
&lt;p&gt;Pruning is a kind of post-processing trick, which happens after training, to make parameters more &amp;ldquo;zero&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;channel-pruning&#34;&gt;Channel Pruning&lt;/h3&gt;
&lt;p&gt;Channel pruning boils down to the following optimization problem:
$$
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i \right\Vert_F^2 \
\text{s.t.} \quad &amp;amp; ||\beta||_0 \le c&amp;rsquo;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note here that $X_i$ is not the feature map at $i$-th channel, but instead the sampled window of kernel size ($k_h \times k_h$) on $i$-th feature map. It is known that this problem is NP-hard because of the $l_0$ norm term in the constraint. In practice, it can be relaxed to (why and really?)
$$
\begin{aligned}
\min_{\beta, W} \quad &amp;amp; \left\Vert Y - \sum_{i} \beta_i X_i W_i \right\Vert_F^2 \
\text{s.t.} \quad &amp;amp; ||\beta||_1 \le c&amp;rsquo;&amp;rsquo; \and \forall i, ||W_i||_F = 1
\end{aligned}
$$
Note that kernels $W$ is also included for optimization. This is because after pruning, kernels may still be fine-tuned a bit to preserve the accuracy.&lt;/p&gt;
&lt;p&gt;The above formula can be optimized in an alternative fashion: i.e. $W$ is fixed and $\beta$ is to be optimized; then $\beta$ is fixed and $W$ is to be optimized. We also add a regularization term for each $W_i$. This is because optimizing $W$ is materially a linear regression task, which easily has infinitely many solution due to the dimension of $X_i$ unless there is an extra constraint.&lt;/p&gt;
&lt;h3 id=&#34;low-rank-decomposition&#34;&gt;Low-rank Decomposition&lt;/h3&gt;
&lt;p&gt;Take the matrix multiplication as an example to appreciate the idea of low-rank decomposition:
$$
X \times W \Rightarrow X \times U \times V
$$
We may decompose $W: m \times n$ into $U: m \times r$ and $N: r \times n$ where hopefully $r &amp;lt; n,m$ (note that when $r = \rank W$, there exists $U$ and $V$ that can fully recover $W$). In this way, storage cost can be saved and computation can be sped up.&lt;/p&gt;
&lt;p&gt;Note that our objective is not to reconstruct the matrix $W$ (called &lt;strong&gt;matrix approximation&lt;/strong&gt;) with some other lower-rank matrices. Or else simply the singular value decomposition would do the job. Instead, we are to reconstruct the model output (called &lt;strong&gt;matrix regression&lt;/strong&gt;). Therefore, the problem is formulated as
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  X W||_F \
\text{s.t.} \quad &amp;amp; W = U V \
&amp;amp; \rank U, \rank V \le r
\end{aligned}
$$&lt;/p&gt;
&lt;h4 id=&#34;combination-with-other-methods&#34;&gt;Combination with Other Methods&lt;/h4&gt;
&lt;p&gt;Low-rank decomposition can be combined with sparsity and nonlinearity approximation to give the following ultimate pruning problem:
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||_F \
\text{s.t.} \quad &amp;amp; W = A + B \
&amp;amp; ||A||_0 \le S \
&amp;amp; \rank B \le L \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $|| \cdot ||_0$ is the number of nonzero entries in the matrix.&lt;/p&gt;
&lt;p&gt;The sparsity comes from $A$ and low-rank decomposition comes from $B$. The above problem is NP-hard due to the constraints on $A$ and $B$. We can relax these constraints to other forms:
$$
\begin{aligned}
\min_{W, A, B} \quad &amp;amp; ||Y -  f(X W)||&lt;em&gt;F \
\text{s.t.} \quad &amp;amp; W = A + B \
&amp;amp; ||A||&lt;/em&gt;{21} \le S  \
&amp;amp; ||B||&lt;em&gt;* \le L \
\end{aligned}
$$
where $|| \cdot ||&lt;/em&gt;{21}$ takes the $l_1$ norm of the $l_2$ norms taken on each column of the matrix; and $|| \cdot ||_*$ is the nuclear norm that equals to the sum of singular values.&lt;/p&gt;
&lt;p&gt;To solve it, alternating direction method of multipliers (ADMM) can be adopted. The augmented Lagrangian function is (??)
$$
L(W,A,B,\Lambda) = ||Y -  f(X W)||&lt;em&gt;F + \lambda_1 ||A||&lt;/em&gt;{21} + \lambda_2 ||B||_* + \Lambda \odot (W - A - B) + \frac{\rho}{2}||W - A - B||_F^2
$$&lt;/p&gt;
&lt;h4 id=&#34;tensor-decomposition&#34;&gt;Tensor Decomposition&lt;/h4&gt;
&lt;p&gt;Tucker decomp, CP decomp, Tucker-2 (or TT) decomp&lt;/p&gt;
&lt;h2 id=&#34;quantizationquant-pytorch&#34;&gt;Quantization&lt;sup id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;21&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;Model parameters are natively floating-point numbers. During training, model parameters usually won&amp;rsquo;t be too large and won&amp;rsquo;t exceed the range of &lt;code&gt;int8&lt;/code&gt;.  One way to make the model smaller and run faster is to map model parameters to integers of smaller size (say from &lt;code&gt;fp32&lt;/code&gt; to &lt;code&gt;int16&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Other than the efficiency perspective, if the original values span a very narrow range (say from -10 to 10), by mapping them to a wider range of integers, better precision may even be obtained. This is mainly due to that most floating-point number arithmetic suffer from precision loss. Particularly, when two floating-point numbers of significant difference adds or subtracts, a great loss in precision can occur (called &lt;strong&gt;catastrophic cancellation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;After integer arithmetic, we can map these integer values back again. The process involved is called &lt;strong&gt;quantization/dequantization&lt;/strong&gt;. For a number $r$, its quantization is $Q(r) = \text{Int}(r/k) - b$ where $k$ is the scale and $b$ is the bias. To dequantize, $\hat r = k(Q(r) + b)$.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/quant_sym.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Images/quant_asym.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The first question to consider &lt;strong&gt;symmetric quantization or asymmetric quantization&lt;/strong&gt; (the figure above). That is, should the zero in the original domain mapped to zero or $-b$ (because $\text{Int}(0/k)-b = -b$). In essence, this is a question of choice of bias. Preferring to computation efficiency, we hope bias is zero. To show it, consider the dequantization process of the matrix product:
$$
A = k_A \times A_Q + b_A \
B = k_B \times B_Q + b_B \
A B = k_A k_B A_Q B_Q + k_A b_B A_Q ＋ k_B b_A B_Q + b_A b_B
$$
It would have been cleaner if $b_A, b_B$ are zero. However, if the activations in the network are mostly non-negative, like in the case where ReLU is used, symmetric quantization would waste half of the quantization range.&lt;/p&gt;
&lt;p&gt;The second question is to consider using the &lt;strong&gt;restricted range or the full range&lt;/strong&gt; of the target integer type. Take &lt;code&gt;int8&lt;/code&gt; for an example, should we map numbers to $[-127, 127]$ or $[-128, 127]$? When symmetric quantization is used, the answer is the restricted range. The reason is that, the quantization of two numbers of the same magnitude but different signs, should be of the same magnitude but different signs too. But had the full range been used, supposing the floating-point range was $[-2.2, 2.2]$, $2.2$ and $-2.2$ would have been quantized into $2.2 \times \frac{127}{2.2} = 127$ and $-2.2 \times \frac{128}{2.2} = -128$ respectively. The problem is that the scaling factors for positive number and negative number are different due to the asymmetric range, in which case a small bias would be introduced and leads to precision loss.&lt;/p&gt;
&lt;p&gt;The third question is the timing of quantization. Should the quantization happen during training (&lt;strong&gt;quantization-aware training&lt;/strong&gt;) or after training (&lt;strong&gt;post-training quantization&lt;/strong&gt;)? Accompanying this question is what the best scale should be.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Post-training (static) quantization (PTQ)&lt;/p&gt;
&lt;img src=&#34;./Images/ptq.svg&#34; style=&#34;zoom: 25%;&#34; /&gt;
&lt;p&gt;In PTQ, model are trained before quantized. After training, a small subset of training data (called &lt;strong&gt;calibration data&lt;/strong&gt;) is used to determine the scale (magnitude) and the clipping range (quantization bit number). Notice that model parameters are fixed in this process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quantization-aware training (QAT)&lt;/p&gt;
&lt;img src=&#34;./Images/qat.svg&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;In QAT, model parameters are quantized and then trained. QAT brings a great save in computation during training. But this yields another question: should the quantization happen during forward pass or during back propagation?&lt;/p&gt;
&lt;p&gt;Quantization is used in forward pass but not in back propagation. The reason not to update gradient in quantized domain is that, numbers in quantized domain are integers but gradient is fractional. Another reason not to back-propagate in quantized domain is that, the gradient might be so large as to disturb the convergence of model, compared with that in dequantized domain.&lt;/p&gt;
&lt;p&gt;The error is measured between the dequantized output and the target output. Model parameters will update in dequantized format and will be re-quantized for next round of training. There are arguments on both sides for this approach. One good aspect is that it takes the quantization error into consideration. But this causes the gradient mismatch because the gradient of the parameters are computed with quantized values but updated in dequantized format. In worst case, this may cause the model to diverge.&lt;/p&gt;
&lt;p&gt;This training method reconciles with the idea of stochastic neuron, where back propagation is done by &lt;strong&gt;straight-through estimator&lt;/strong&gt; (STE)&lt;sup id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;. There is another method based on STE and called &lt;strong&gt;parameterized clipping activation&lt;/strong&gt; (PACT)&lt;sup id=&#34;fnref:23&#34;&gt;&lt;a href=&#34;#fn:23&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;23&lt;/a&gt;&lt;/sup&gt;. The meat of PACT is a learnable ReLU function:
$$
y = \mathrm{PACT}(x, \alpha) = 0.5 (|x| - |x-\alpha| + \alpha) =
\begin{cases}
0, &amp;amp; x \le 0 \
x, &amp;amp; 0 &amp;lt; x &amp;lt; \alpha \
\alpha, &amp;amp; x \ge \alpha
\end{cases}
$$
Back-propagation is done w.r.t. the dequantized value $y_{dq} \triangleq \lfloor y \cdot \frac{2^k - 1}{\alpha} \rfloor \frac{\alpha}{2^k - 1}$. $\frac{\partial y_{dq}}{\partial y}$ is set to $\frac{\text{range before quant.}}{\text{range of quant. domain}}$ as would be in STE. $\alpha$ is learnable so that clipping range can be dynamically adjusted:
$$
\frac{\partial y}{\partial \alpha} =
\begin{cases}
0, &amp;amp; x &amp;lt; \alpha \
1, &amp;amp; x \ge \alpha
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The fourth question is the granularity of quantization. There are usually three kinds of choices, namely channel-wise, layer-wise and group-wise quantization.&lt;/p&gt;
&lt;h3 id=&#34;bnntnnbnn&#34;&gt;BNN/TNN&lt;sup id=&#34;fnref:24&#34;&gt;&lt;a href=&#34;#fn:24&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;24&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Binarized and ternary neural network bring quantization to an extreme. The reason to particularly split them out under &lt;a href=&#34;#quantization&#34;&gt;quantization&lt;/a&gt; is that, their multiplication and addition logic are quite different. They use 1 bit and 2 bits for clipping: BNN maps values to -1 and 1; TNN maps values to -1, 0 and 1. Then the multiplication and addition only involve Boolean operations, which will be much faster. Other than that, the training process of BNN/TNN resembles that of quantization.&lt;/p&gt;
&lt;p&gt;The motivation behind is that model parameters are usually within $[-1, 1]$. So why not try just using -1, 0 and 1? Given a weight matrix $W$ (squeezed into an $\R^n$ vector), there is an analytical solution to the best scaling factor $k$ and the quantized value $B$ (squeezed into an $\R^n$ vector) for BNN:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
B^* &amp;amp;= \arg \min_{B} | W - kB |&lt;em&gt;2 = \arg \min&lt;/em&gt;{B} | W - kB |&lt;em&gt;2^2 \
&amp;amp;= \arg \min&lt;/em&gt;{B} W^T W - 2 k W^T B + k^2 B^T B \
\end{aligned}
$$
Since $W$ is known and $B \in { -1, 1 }^n$, $W^T W$ is a constant $c$ and $B^T B$ is $n$. Thus,
$$
\begin{gathered}
B^* = \arg \min_{B} k^2 n + c - 2 k W^T B = \mathrm{sign}(W) \
k = \frac{W^T B^*}{n}
\end{gathered}
$$&lt;/p&gt;
&lt;h2 id=&#34;knowledge-distillation&#34;&gt;Knowledge Distillation&lt;/h2&gt;
&lt;p&gt;Knowledge distillation is a model compression method in which a smaller model is trained to mimic the pre-trained larger model. The larger and the smaller model are referred to as &amp;ldquo;teacher&amp;rdquo; and &amp;ldquo;student&amp;rdquo; respectively.&lt;/p&gt;
&lt;p&gt;There are three kinds of distillation methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Response-based knowledge&lt;/p&gt;
&lt;p&gt;This is perhaps the easiest one to think of: given the same input, the output (usually a categorical distribution) should be the same.&lt;/p&gt;
&lt;p&gt;In this case, the distillation loss is set to be the cross entropy between the distributions output by the teacher and the student. Minimizing the cross entropy between the output distributions equivalently minimizes the their KL-divergence, given that teacher&amp;rsquo;s distribution is fixed.&lt;/p&gt;
&lt;p&gt;On the other hand, the student model can further be rectified by the ground-truth loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature-based knowledge&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relation-based knowledge&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;network-architecture-search&#34;&gt;Network Architecture Search&lt;/h2&gt;
&lt;p&gt;There are mainly two indexes for a network: one is the &lt;strong&gt;latency&lt;/strong&gt; and the other is &lt;strong&gt;accuracy&lt;/strong&gt;. In NAS, latency is cheaper to check upon than accuracy, since the training is more time-consuming than a single pass of input.&lt;/p&gt;
&lt;p&gt;NAS is mostly based on heuristics. During training, we can save time by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;early stop&lt;/li&gt;
&lt;li&gt;warm restart (parameter reuse)&lt;/li&gt;
&lt;li&gt;use the arrogate target like FLOPs (which is usually proportional to latency)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for searching in the solution space, we can do with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grid search&lt;/li&gt;
&lt;li&gt;random sampling&lt;/li&gt;
&lt;li&gt;reinforcement learning&lt;/li&gt;
&lt;li&gt;evolutional algorithm&lt;/li&gt;
&lt;li&gt;Bayesian optimization like Gaussian process&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h1&gt;
&lt;p&gt;Related courses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.cs.ucsb.edu/~tyang/class/240a17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS240A - Applied Parallel Computing (ucsb.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hanlab.mit.edu/courses/2023-fall-65940&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT 6.5940 Fall 2023 TinyML and Efficient Deep Learning Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.05662&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1911.05662&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.06873&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1706.06873&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10144741&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/10144741&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs/how_to/optimize_operators/opt_gemm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tvm.apache.org/docs/how_to/optimize_operators/opt_gemm.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/6877334&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/6877334&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://spatial-lang.org/dotprod&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spatial-lang.org/dotprod&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://csapp.cs.cmu.edu/3e/waside/waside-blocking.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://csapp.cs.cmu.edu/3e/waside/waside-blocking.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://penny-xu.github.io/blog/tiled-matrix-multiplication&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://penny-xu.github.io/blog/tiled-matrix-multiplication&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.juliahub.com/LoopVectorization/4TogI/0.9.8/examples/matrix_multiplication/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.juliahub.com/LoopVectorization/4TogI/0.9.8/examples/matrix_multiplication/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8zbh4gWGa7I&amp;amp;t=424s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=8zbh4gWGa7I&amp;t=424s&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.12418&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2002.12418&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DruwS2_cVys&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=DruwS2_cVys&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://martin-thoma.com/strassen-algorithm-in-python-java-cpp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://martin-thoma.com/strassen-algorithm-in-python-java-cpp/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.diva-portal.org/smash/get/diva2:1219121/FULLTEXT01.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.diva-portal.org/smash/get/diva2:1219121/FULLTEXT01.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1509.09308&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1509.09308&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ceca.pku.edu.cn/docs/20191126112405101722.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ceca.pku.edu.cn/docs/20191126112405101722.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How does sparse convolution work? | by Zhiliang Zhou | Towards Data Science&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:18&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://rancheng.github.io/Sparse-Convolution-Explained/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Convolution explained with code – Ran Cheng – Robotics, Vision, Learning.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:18&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:19&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.08922.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1705.08922.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:19&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:20&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.4229&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1411.4229&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:20&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:21&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://pytorch.org/blog/quantization-in-practice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pytorch.org/blog/quantization-in-practice/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:21&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:22&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1308.3432&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1308.3432&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:23&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.06085&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1805.06085&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:23&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:24&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.05279&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1603.05279&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:24&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/1-optimization-problem/</guid>
      <description>&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h2&gt;
&lt;p&gt;The standard optimization problem will be in the form:
$$
\inf_{x \in X} f(x) \
$$
where $X$ is the &lt;strong&gt;feasible/constraint  region/set&lt;/strong&gt;, $f: \R^n \mapsto \R$ is the objective function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $x^&lt;em&gt;$ is an &lt;strong&gt;optimal solution&lt;/strong&gt; to the problem if $v^&lt;/em&gt; = f(x^&lt;em&gt;)$. In this case, we also say that $x^&lt;/em&gt;$ attains optimal value $v^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $x&amp;rsquo;$ is a &lt;strong&gt;local minimizer&lt;/strong&gt; if $\exists \epsilon &amp;gt; 0, \forall x \in B(x&amp;rsquo;, \epsilon), f(x) \ge f(x&amp;rsquo;)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;types-of-problems&#34;&gt;Types of Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Unconstrained: $X = \R^n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discrete programming&lt;/p&gt;
&lt;p&gt;$X$ is a discrete set, which means $\forall x \in X, \exists \epsilon &amp;gt; 0, X \cap B(x, \epsilon) = { x }$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Every feasible solution to discrete optimization problem is a local minimizer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linear programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$X = { x \in \R^n | (a^i)^T x \le c_i, i=1,2,\dots,m }$ is a set defined by a &lt;strong&gt;finite&lt;/strong&gt; number of linear inequalities.&lt;/p&gt;
&lt;p&gt;$f = b_1 x_1 + b_2 x_2 + \dots + b_n x_n = b^T x$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quadratic programming&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$X$ is the same as that in linear programming.&lt;/p&gt;
&lt;p&gt;$f(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j = x^T A x$, where $A = [a_{ij}] \in R^{n \times n}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: This form does not include any linear term. Generally, a quadratic function takes on the form $f(x) = x^T A x + b^T x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: We may assume $A$ is symmetric, since even it is not, we can have $A&amp;rsquo; = \frac{A + A^T}{2}$ and
$$
x^T A x = x^T A^T x \to x^T A x = x^T A&amp;rsquo; x
$$&lt;/p&gt;
&lt;p&gt;The right hand side is obvious because $x^T A^T x$ is a number, and it equals to its transpose $x^T A x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-definite programming&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Consider $Q \in \mathcal{S}^{n}$, where $\mathcal{S}^n$ is the set of $n \times n$ symmetric matrix. The following is equivalent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Q$ is &lt;strong&gt;positive semi-definite&lt;/strong&gt; (short as PSD, denoted as $Q \succcurlyeq 0$).&lt;/li&gt;
&lt;li&gt;$\forall x \in \R^n, x^T Q x \ge 0$.&lt;/li&gt;
&lt;li&gt;All eigenvalues of $Q$ are non-negative.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $C, A_1, \dots, A_m \in S^n$ and $b \in \R^m$ be given, the semi-definite programming is
$$
\begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; C - \underbrace{\sum_{i=1}^m x_i A_i}_{M(x)} \succcurlyeq 0
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The constraint is called a linear matrix inequality. Observe that $M: \R^m \mapsto S^n$ satisfies
$$
M(\alpha x + \beta y) = \alpha M(x) + \beta M(y)
$$
$M$ is a linear map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: Compare between linear programming and positive semi-definite programming:
$$
\begin{gathered}&lt;/p&gt;
&lt;p&gt;\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; (a^i)^T x \le c_i, \
&amp;amp; i = 1,\dots,m
\end{aligned}&lt;/p&gt;
&lt;p&gt;\quad \quad&lt;/p&gt;
&lt;p&gt;\begin{aligned}[t]
\inf_{x \in \R^n} \quad &amp;amp; b^T x \
\textrm{s.t.} \quad &amp;amp; C - \sum_{i=1}^m x_i A_i \succcurlyeq 0
\end{aligned}&lt;/p&gt;
&lt;p&gt;\end{gathered}
$$
In linear programming, construct matrices:
$$
C&amp;rsquo; =
\begin{pmatrix}
c_1 &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; c_2 &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp; \
&amp;amp; &amp;amp; &amp;amp; c_m
\end{pmatrix},&lt;/p&gt;
&lt;p&gt;A_i&amp;rsquo; =
\begin{pmatrix} 
a_1^i &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; a_2^i &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp; \
&amp;amp; &amp;amp; &amp;amp; a_m^i \
\end{pmatrix}
$$
Then
$$
C&amp;rsquo; - \sum_{i=1}^m x_i A_i&amp;rsquo; =
\begin{pmatrix}
c_1 - \sum_{j=1}^m x_j a_1^j &amp;amp; &amp;amp; &amp;amp; \
&amp;amp; c_2 - \sum_{j=1}^m x_j a_2^j &amp;amp; &amp;amp; \
&amp;amp; &amp;amp; \ddots &amp;amp;  \
&amp;amp; &amp;amp; &amp;amp; c_m - \sum_{j=1}^m x_j a_m^j &amp;amp;  \
\end{pmatrix}&lt;/p&gt;
&lt;p&gt;\succcurlyeq 0
$$
because a diagonal matrix is PSD if and only if all of its diagonal entries are non-negative.&lt;/p&gt;
&lt;p&gt;In this sense, linear programming is special case of semi-definite programming where $C$ and $A_i$&amp;rsquo;s are all diagonal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples-of-problems&#34;&gt;Examples of Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Air traffic control&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$n$ airplanes are arriving.&lt;/li&gt;
&lt;li&gt;$i$-th airplane arrives within $[a_i, b_i]$.&lt;/li&gt;
&lt;li&gt;Assume airplanes arrive and land in order.&lt;/li&gt;
&lt;li&gt;Let $t_i$ be the landing time assigned to $i$-th airplane $i$.&lt;/li&gt;
&lt;li&gt;The metering time is defined to be the time difference between two consecutive airplane landings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implicit constraints derived from above conditions is that $a_i \le t_i \le b_i$ and $ t_i \le t_{i+1}$. For safety, we want to the minimum metering time to be maximized. That is,
$$
\begin{aligned}
\max_{t} \quad &amp;amp; f(t) \triangleq \min_{1 \le i \le n-1} t_{i+1} - t_i \
\textrm{s.t.} \quad &amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
The constraints are all linear. The &lt;code&gt;min&lt;/code&gt; operation in $f$, however, doesn&amp;rsquo;t comfort us. We can introduce a new variable $z$ and convert the original problem to an equivalent one:
$$
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \
\textrm{s.t.} \quad &amp;amp; z = \min_{1 \le i \le n-1} t_{i+1} - t_i \
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
Further, since the objective is to maximize and $z$&amp;rsquo;s coefficient is positive, the problem can be converted to
$$
\begin{aligned}
\max_{t, z} \quad &amp;amp; z \
\textrm{s.t.} \quad &amp;amp; z \le t_{i+1} - t_i, i=1,\dots,n-1\
&amp;amp; a_i \le t_i \le b_i, i=1,\dots,n \
&amp;amp; t_i \le t_{i+1}, i=1,\dots,n-1
\end{aligned}
$$
Now the problem becomes a linear one and is easy to solve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data fitting problem&lt;/p&gt;
&lt;p&gt;Given data points $(a_i, b_i) \in \R^n \times \R$, a typical choice to fit those data would be an affine function $f(x) = y^T x + t$. Other than the choice of function, the choice of objective function matters too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Least squares&lt;/p&gt;
&lt;p&gt;The objective minimizes the sum of the squares of errors for all data points:
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} (y^T a_i - b_i)^2 \
\end{aligned}
$$
This is a quadratic programming problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum absolute deviation
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R} \quad &amp;amp; \sum_{i} |y^T a_i - b_i| \
\end{aligned}
$$
Using the same trick of reparameterization, the problem is equivalent to
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \
\textrm{s.t.} \quad &amp;amp; z_i = |y^T a_i - b_i|, i=1,\dots,m \
\end{aligned}
$$
Further, the trick of relaxation applies:
$$
\begin{aligned}
\min_{y \in \R^n, t \in \R, z \in \R^m} \quad &amp;amp; \sum_{i=1}^m z_i \
\textrm{s.t.} \quad &amp;amp; y^T a_i - b_i \le z_i, i=1,\dots,m \
&amp;amp; y^T a_i - b_i \ge -z_i, i=1,\dots,m \
\end{aligned}
$$
This becomes a linear programming problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key takeaway from these examples are the &lt;strong&gt;problem equivalence&lt;/strong&gt; trick, which includes introducing new variables and relaxing $\max$ or $\min$ to inequalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/2-convex-set/</guid>
      <description>&lt;h2 id=&#34;convex-set&#34;&gt;Convex Set&lt;/h2&gt;
&lt;p&gt;Given $x^1, \dots, x^k \in \R^n$, we say that $y = \sum_{i=1}^k \alpha_i x^{i}$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;linear combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\alpha_1, \dots, \alpha_k \in \R$;&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;affine combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\sum_{i=1}^k \alpha_i = 1$;&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;convex combination&lt;/strong&gt; of $x^1, \dots, x^k$ if $\sum_{i=1}^k \alpha_i = 1$ and $0 \le \alpha_1, \dots, \alpha_k$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let $S \in \R^n$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is a &lt;strong&gt;linear subspace&lt;/strong&gt; if $\forall x,y \in S, \alpha,\beta \in \R, \alpha x + \beta y \in S$;&lt;/li&gt;
&lt;li&gt;$S$ is an &lt;strong&gt;affine subspace&lt;/strong&gt; if $\forall x,y \in S, \alpha \in \R, \alpha x + (1-\alpha) y \in S$;&lt;/li&gt;
&lt;li&gt;$S$ is a &lt;strong&gt;convex set&lt;/strong&gt; if $\forall x,y \in S, 0 \le \alpha \le 1, \alpha x + (1 - \alpha) y \in S$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is affine.&lt;/li&gt;
&lt;li&gt;Any affine combination of a finite number of points in $S$ belongs to $S$.&lt;/li&gt;
&lt;li&gt;$S$ can be written as $S = {x} + V \triangleq { x + v: v \in V }$. Note that though $V$ is unique, $x$ is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The following statements are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is convex.&lt;/li&gt;
&lt;li&gt;Any convex combination of a finite number of points in $S$ belongs to $S$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;examples-of-convex-set&#34;&gt;Examples of Convex Set&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Nonnegative orthant (in 2-D, an orthant is called a quadrant): $\R^n_+ \triangleq { x \in \R^n: \forall i, x_i \ge 0 }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyperplane&lt;/p&gt;
&lt;p&gt;Given $w \in \R^n, b \in R$, the hyperplane $H(s, c)$ is the set ${ x \in \R^n: s^T x = c }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Half-space&lt;/p&gt;
&lt;p&gt;Given $w \in \R^n, b \in R$, the upper half-space $H^+(s, c)$ is the set ${ x \in \R^n: s^T x \ge c }$; the lower half-space $H^-(s, c)$ is the set ${ x \in \R^n | s^T x \le c }$.&lt;/p&gt;
&lt;p&gt;Note that hyperplane $H(s, c)$ is the intersection of $H^+(s, c)$ and $H^-(s, c)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Euclidean ball&lt;/p&gt;
&lt;p&gt;Given the center $\bar x \in \R^n$ and the radius $r &amp;gt; 0$, the Euclidean ball $B(\bar x, r)$ is the set ${ x \in \R^n: ||x - \bar x||_2 \le r }$.&lt;/p&gt;
&lt;p&gt;A generalization of Euclidean ball would be to extend the norm to other numbers that are larger than 1. For $q \ge 1$
$$
B_q(\bar x, r) = { x \in \R^n: ||x - \bar x||&lt;em&gt;q \le r }
$$
is also a convex set. Note that $||z||&lt;/em&gt;\infty = \lim_{q \to \infty} (\sum_i z_i^q)^{1/q} = \max_i z_i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convex cone&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A set $K \in \R^n$ is called a &lt;strong&gt;cone&lt;/strong&gt; if $\forall x \in K, \alpha &amp;gt; 0, \alpha x \in K$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Linear subspace is a cone. Affine subspace is not necessarily so.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;convex cone&lt;/strong&gt; is a cone that is convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some examples of convex cone include $\R^n_+$, and $\mathcal{S}_+^n$, which is the set of $n \times n$ PSD matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving Operations&lt;/h3&gt;
&lt;p&gt;For any two convex sets $S_1$ and $S_2$, there are some binary operators that will preserve the convexity after being applied.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set operations&lt;/p&gt;
&lt;p&gt;$S_1 \cup S_2$ is not necessarily convex. $S_1 \cap S_2$ is always convex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Affine transformations&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $A: \R^n \mapsto \R^m$ is &lt;strong&gt;affine&lt;/strong&gt; if $\forall x,y \in \R^n, \alpha \in \R$,
$$
A(\alpha x + (1 - \alpha) y) = \alpha A(x) + (1 - \alpha) A(y)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $A: \R^n \mapsto \R^m$ be affine, $S \subseteq \R^n$ be convex. Then $A(S) \triangleq { A(x): x \in S }$ is convex.&lt;/p&gt;
&lt;p&gt;There are two types of affine transformation worth noting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt;: $A(x) = U x$ where $U \in \R^{n \times n}$ and $U U^T = U^T U= I$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Projection&lt;/strong&gt;: $A(x) = P x$ where $P \in \R^{n \times n}$ and $P^2 = P$.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;orthogonal projection&lt;/strong&gt;, its projection matrix further satisfies $P^T = P$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an example of affine transformation, given center $\bar x$ and axes $Q$ which is positive definite, the &lt;strong&gt;ellipsoid&lt;/strong&gt; $E(\bar x, Q)$ is the set ${ x \in \R^n | (x - \bar x)^T Q (x - \bar x) \le 1 }$. Note that $B(\bar x, r) = E(\bar x, I/r^2)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: When $Q$ is positive semi-definite, it might occur that ${ x \in \R^n | (x - \bar x)^T Q (x - \bar x) \le 1 }$ will degenerate into two parallel lines. Just consider the case when $Q = [1,2] [1,2]^T$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: There always exists an affine transformation $A: \R^n \mapsto \R^n$ such that $A(B(0, 1)) = E(\bar x, Q)$. Note that $B(\bar x, r) = E(\bar x, I/r^2)$. See &lt;a href=&#34;../2-cvxanal.pdf&#34;&gt;this handout&lt;/a&gt; for the construction of such transformation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, ellipsoid is also a convex set (Problem 2 of Homework 1 proves this from the first principle).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;topological-preparation&#34;&gt;Topological Preparation&lt;/h3&gt;
&lt;h4 id=&#34;basic-topology&#34;&gt;Basic Topology&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Given a set $S \in \R^n, S \ne \emptyset$ and a point $x \notin S$, we want to find a point in $S$ that is closest (in terms of Euclidean distance) to $x$. Formally, $\hat x = \arg\min_{z \in S} ||z - x||_2$ is called the &lt;strong&gt;projection&lt;/strong&gt; of $x$ onto $S$, denoted as $\hat x = \Pi_S(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that this projection does not necessarily exist. Neither the projection is unique. Under what conditions can we guarantee the existence and uniqueness of projection? Before that, some concepts are needed. Let $S \subseteq \R^n$ be a set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: $x$ is an &lt;strong&gt;interior point&lt;/strong&gt; of $S$ if $\exists \epsilon &amp;gt; 0, B(x, \epsilon) \subseteq S$. The collection of all interior points of $S$ is called the &lt;strong&gt;interior&lt;/strong&gt; of $S$, denoted as $\mathop{\mathrm{int}} S$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We say that $S$ is &lt;strong&gt;open&lt;/strong&gt; if $S = \mathop{\mathrm{int}} S$. We say that $S$ is &lt;strong&gt;closed&lt;/strong&gt; if $\R^n \setminus S$ is open. Note that it can be the case that a set is neither open nor closed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: The intersection of any family of closed sets is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be continuous and $c \in \R$ be a constant number. Then $S = { x \in \R^n: f(x) \le c }$ is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: $S$ is closed if and only if for every convergent sequence ${ x_n }$ in $S$, its limit is in $S$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: We say that $S \subseteq \R^n$ is &lt;strong&gt;compact&lt;/strong&gt; if it is closed and bounded ($\exists M &amp;gt; 0$ such that $S \subseteq B(0, M)$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;projection&#34;&gt;Projection&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Weierstrass theorem&lt;/strong&gt;: Let $f: \R^n \mapsto \R$ be continuous and $S \subseteq \R^n$ be compact. Then $\inf_{x \in S} f(x)$ always has a solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $S \subseteq \R^n$ be non-empty, closed, and convex. Then for every $x \in \R^n$, there exists a unique $\hat x$ such that $\hat x = \Pi_S(x)$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Existence&lt;/p&gt;
&lt;p&gt;We may assume that $x \notin S$. Consider any $x&amp;rsquo; \in S$ and define $T \triangleq S \cap B(x, ||x - x&amp;rsquo;||_2)$. Observe that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\arg \min_{z \in S} ||x - z||&lt;em&gt;2 = \arg \min&lt;/em&gt;{z \in T} ||x - z||_2$.&lt;/li&gt;
&lt;li&gt;$T$ is closed and bounded.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then by &lt;em&gt;Weierstrass&amp;rsquo; theorem&lt;/em&gt;, $\hat x$ that solves $\min_{z \in T} ||x - z||_2$ exists. And by Point 1 above, $\hat x = \Pi_S(x)$. Note that above argument does not need convexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uniqueness&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that $\hat x_1 = \Pi_S(x), \hat x_2 = \Pi_S(x)$ and $\hat x_1 \ne \hat x_2$. Let $z = (\hat x_1 + \hat x_2) / 2$. By convexity, $z \in S$. Then $x$&amp;rsquo;s distance to $z$ can be calculated as
$$
\begin{aligned}
|| x - (\hat x_1 + \hat x_2)/2 ||_2 &amp;amp;= || (x - \hat x_1)/2 + (x - \hat x_2)/2 ||_2 \
&amp;amp;\le || (x - \hat x_1)/2 ||_2 + || (x - \hat x_2)/2 ||&lt;em&gt;2 \
&amp;amp;= \min&lt;/em&gt;{z \in S} ||x - z||_2
\end{aligned}
$$
If the $\le$ is strict, the fact that $\hat x_1$ and $\hat x_2$ are the projection will be contradicted; else $(x - \hat x_1)$ and $(x - \hat x_2)$ are collinear, which means $x, \hat x_1, \hat x_2$ are collinear, in which case the only possible way for $(x - \hat x_1)$ and $(x - \hat x_2)$ to be equal is $\hat x_1 = \hat x_2$, contradicting the assumption $\hat x_1 \ne \hat x_2$.&lt;/p&gt;
&lt;p&gt;As a result, $\hat x_1 = \hat x_2$, which means the projection of $x$ is unique.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $S \subseteq \R^n$ be non-empty, closed, and convex. Then for any $x \in \R^n$, we have
$$
\hat x = \Pi_{S}(x) \iff \forall z \in S, (z - \hat x)^T(x - \hat x) \le 0
$$
Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For any $z \in S$ and $0 \le \alpha \le 1$, define $z(\alpha) = \alpha z + (1 - \alpha) \hat x$. By convexity, $z(\alpha) \in S$. Then,
$$
||\hat x - x||_2^2 \le ||z(\alpha) - x||_2^2
$$
Note on the other hand that
$$
\begin{aligned}
&amp;amp;\quad ||z(\alpha) - x||_2^2 = [z(\alpha) - x]^T [z(\alpha) - x] \
&amp;amp;= [\hat x + \alpha (z - \hat x) - x]^T [\hat x + \alpha (z - \hat x) - x] \
&amp;amp;= [(\hat x - x) + \alpha (z - \hat x)]^T [(\hat x - x) + \alpha (z - \hat x)] \
&amp;amp;= ||\hat x - x||_2^2 + 2\alpha (z - \hat x)^T (\hat x - x) + \alpha^2 ||z - \hat x||_2^2
\end{aligned}
$$
To make the above hold for any $\alpha \in [0,1]$, it must be the case that $(z - \hat x)^T (\hat x - x) \ge 0$ or equivalently
$$
(z - \hat x)^T (x - \hat x) \le 0
$$
Convexity is crucial in this property. Just consider the following case:&lt;/p&gt;
&lt;img src=&#34;./convexity-is-important.png&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Suppose on the contrary that there exists some $x&amp;rsquo; \ne \Pi_S(x)$ such that for every $z \in S$,
$$
(z - x&amp;rsquo;)^T (x - x&amp;rsquo;) \le 0
$$
Then for $\Pi_S(x) \in S$, we have
$$
\begin{equation}
(\Pi_S(x) - x&amp;rsquo;)^T (x - x&amp;rsquo;) \le 0 \label{eq1}
\end{equation}
$$
From the proof of sufficiency we know that for this specific $x&amp;rsquo;$,
$$
\begin{equation}
(x&amp;rsquo; - \Pi_S(x))^T (x - \Pi_S(x)) \le 0 \label{eq2}
\end{equation}
$$
Add up together $\eqref{eq1}$ and $\eqref{eq2}$ to give
$$
(\Pi_S(x) - x&amp;rsquo;)^T (\Pi_S(x) - x&amp;rsquo;) \le 0
$$
This indicates that $\Pi_S(x) = x&amp;rsquo;$, which concludes the proof.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;separation&#34;&gt;Separation&lt;/h4&gt;
&lt;p&gt;Given $S_1, S_2 \subseteq \R^n$, it is easy to certify that $S_1 \cap S_2 \ne \emptyset$ so long as there is a $x \in S_1$ such that $x \in S_2$. But how can one certify that $S_1 \cap S_2 = \emptyset$?&lt;/p&gt;
&lt;p&gt;One geometric intuition is to find a hyperplane $H(s,c)$ such that $S_1 \subseteq H^+(s,c) \setminus H(s,c)$ and $S_2 \subseteq H^-(s,c) \setminus H(s,c)$. Obviously a hyperplane separation won&amp;rsquo;t always work. But it helps in the discussion of convex set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;point-set separation&lt;/strong&gt;. Let $S \subseteq \R^n$ be non-empty, closed and convex. Let $x \notin S$. Then there exists $y \in \R^n$ such that
$$
\left( \max_{z \in S} y^T z \right) &amp;lt; y^T x
$$
Proof:&lt;/p&gt;
&lt;p&gt;By the projection theorem, $\hat x \triangleq \Pi_S(x)$ exists and is unique. Take $y$ as $(x - \hat x)$. $y \ne \vec 0$ because $x \notin S$. Then according the projection&amp;rsquo;s property, for every $z \in S$,
$$
\begin{gather}
\begin{aligned}
(z - \hat x)^T \underbrace{(x - \hat x)}_{y} &amp;amp;\le 0 \
z^T y - \hat x^T y &amp;amp;\le 0 \
\end{aligned} \quad \Rightarrow \quad
\begin{aligned}
y^T z &amp;amp;\le y^T (x  - y) \
&amp;amp;\le y^T x - ||y||&lt;em&gt;2^2 \
&amp;amp;&amp;lt; y^T x
\end{aligned}
\end{gather}
$$
$\max&lt;/em&gt;{z \in S} y^T z$ is obtained at $z = \hat x$, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The theorem above is an algebraic description of point-set separation and in essence reveals a hyperplane separation. $y$ is exactly the normal vector of this hyperplane. $(\max_{z \in S} y^T z) &amp;lt; y^T x$ actually says that $x$ is more distant in the direction of $y$ than every point $z$ in $S$.&lt;/p&gt;
&lt;p&gt;A direct result of the theorem above is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: A closed convex set $S \subseteq \R^n$ is the intersection of all half-spaces containing $S$, i.e.
$$
S = \bigcap_{\substack{\text{$S \subseteq H$ and} \ \text{$H$ is a halfspace}}} H
$$
Proof:&lt;/p&gt;
&lt;p&gt;Without loss of generality, due to that $H^+(s,c) = H^-(-s,-c)$, we claim that $S$ is the intersection of all the lower half-spaces containing $S$:
$$
S = \bigcap_{H^-(s, c) \supseteq S} H^-(s,c)
$$
We begin with two special cases: $\emptyset$ and $\R^n$. We show that $\emptyset$ and $\R^n$ satisfies the above because
$$
\emptyset = H^-(0, -1) \cap \text{any lower halfspace} \
\R^n = \bigcap_{c \ge 0} H^-(0, c)
$$
Now consider any set $\emptyset \subsetneq S \subsetneq \R^n$. Let $x \notin S$. Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, there exists $y_x \in \R^n$ such that
$$
c_x \triangleq \max_{z \in S} y_x^T z &amp;lt; y_x^T x
$$
Therefore, $S \subseteq H^-_x \triangleq H^-(y_x, c_x)$. Note that $x \notin H^-_x$. $S \subseteq H^-&lt;em&gt;x$ holds for any $x \notin S$. Therefore,
$$
S \subseteq \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x
$$
On the other hand, $\bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x \subseteq S$. Suppose on the contrary there exists $z \in \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x$ but $z \notin S$. However,
$$
z \in \bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x = \bigcap&lt;/em&gt;{{ x \notin S: x \ne z } \cup { z } } H^-_x \subseteq H^-_z
$$
which contradicts the fact that $z \notin H^-&lt;em&gt;z$. Therefore, $\bigcap&lt;/em&gt;{x \notin S} H^-&lt;em&gt;x \subseteq S$ and consequently
$$
S = \bigcap&lt;/em&gt;{x \notin S} H^-_x
$$
Notice that ${ H^-&lt;em&gt;x: x \notin S }$ contains all the lower half-spaces that superset $S$. Because for every lower half-space $H^-(s, c) \supseteq S$, there exists $x \in H^+(s, c) \setminus H(s, c)$ such that $x \notin S$. For this specific $x$, there exists $s$ such that
$$
\max&lt;/em&gt;{z \in S} s^T z \le c &amp;lt; s^T x
$$
Therefore, every $H^-(s, c) \supseteq S$ can be written as $H^-_x$ for some $x \notin S$, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By far, the separation between a point (or a set of a single point) and a set is settled. Now we consider the set-set separation. It is easy to conjecture the geometric intuition derived above as the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conjecture: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let $S_1, S_2 \subseteq \R^n$ be two non-empty, closed and convex sets with $S_1 \cap S_2 = \emptyset$. Then there exists $y \in \R^n$ such that
$$
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
$$
Disproof:&lt;/p&gt;
&lt;p&gt;Just consider $S_1 = { (u, v): u \ge 1/v, v \ge 1 }$ and $S_2 = { (u, 0): u \ge 1 }$. The only possible separation hyperplane is the $x$-axis, with the corresponding normal vector $y = [0, -1]^T$. However in this case, $\max_{z \in S_1} y^T z$ does not exist at all and thus $\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z$ does not hold.&lt;/p&gt;
&lt;p&gt;Trivially changing $\max$ to $\sup$ won&amp;rsquo;t help, since $\sup_{z \in S_1} [0, -1] z = 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lack of boundedness forbids us to take the advantage of &lt;em&gt;Weierstrass&amp;rsquo; theorem&lt;/em&gt; to show the existence of maxima. It would be cheering if the predicate of the conjecture can be relaxed so that boundedness and thus compactness applies, yielding the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;set-set separation&lt;/strong&gt;. Let $S_1, S_2 \subseteq \R^n$ be two non-empty, closed and convex sets, with $S_1 \cap S_2 = \emptyset$ and either $S_1$ or $S_2$ being bounded. Then there exists $y \in \R^n$ such that
$$
\max_{z \in S_1} y^T z &amp;lt; \min_{z \in S_2} y^T z
$$
Hint of proof:&lt;/p&gt;
&lt;p&gt;We only show the proof when $S_2$ is bounded. The case when $S_1$ is bounded can be handled similarly.&lt;/p&gt;
&lt;p&gt;Consider the set $S \triangleq { x - y: x \in S_1, y \in S_2 }$ (which will be ${ x - y: x \in S_2, y \in S_1 }$ in the other case). Since $S_1 \cap S_2 = \emptyset$, $0 \notin S$. $S$ is non-empty obviously. $S$ can be further verified to be closed and convex (??). Then the property of point-set separation can be applied to proceed with the proof.&lt;/p&gt;
&lt;p&gt;That is, there exists $u \in \R^n$ such that
$$
\left( \max_{z \in S} u^T z \right) &amp;lt; u^T \cdot 0 = 0 \
\Downarrow \
\max_{x \in S_1, y \in S_2} u^T (x - y) &amp;lt; 0
$$&lt;/p&gt;
&lt;p&gt;Set $y$ as $y^* = \min_{y \in S_2} u^T y$ (such $y^&lt;em&gt;$ exists because the compactness of $S_2$) to give
$$
\begin{aligned}
\max_{x \in S_1} u^T (x - y^&lt;/em&gt;) &amp;amp;&amp;lt; 0 \
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; u^T y^* \
\max_{x \in S_1} u^T x &amp;amp;&amp;lt; \min_{y \in S_2} u^T y \
\end{aligned}
$$
which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a richer discussion of convex set separation, please refer &lt;a href=&#34;https://www.wikiwand.com/en/Hyperplane_separation_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/3-convex-function/</guid>
      <description>&lt;h2 id=&#34;convex-function&#34;&gt;Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $f: \R^n \mapsto \R_+$, where $\R_+ = \R \cup { +\infty }$ (in convex discussion, usually only $+\infty$ is included) be an extended real-valued function. Note that $f$ shouldn&amp;rsquo;t trivially be $+\infty$ everywhere. We say that $f$ is &lt;strong&gt;convex&lt;/strong&gt; if $\forall x_1, x_2 \in \R^n, \alpha \in [0,1]$,
$$
f(\alpha x_1 + (1-\alpha) x_2) \le \alpha f(x_1) + (1-\alpha) f(x_2)
$$
Note that for $x \in \R$, $x &amp;lt; +\infty, x + \infty = \infty + x = \infty, +\infty \le +\infty$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interestingly, convex functions on &lt;u&gt;an open domain&lt;/u&gt; are always continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The &lt;strong&gt;epigraph&lt;/strong&gt; of a function $f: \R^n \mapsto \R$ is the set $\epi f \triangleq { (x, t) \in \R^n \times \R: f(x) \le t }$.&lt;/p&gt;
&lt;p&gt;Definition: The &lt;strong&gt;effective domain&lt;/strong&gt; of $f$ is the set $\dom f \triangleq { x \in \R^n: f(x) &amp;lt; \infty }$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that the real line does not contain $\infty$. Therefore, the effective domain of $f(x) = x$ is still $\R$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $S \subseteq \R^n$ be a set. The &lt;strong&gt;indicator&lt;/strong&gt; of $S$ is the function
$$
\mathbb 1_S (x) = 
\begin{cases}
0, &amp;amp; x \in S \
+\infty, &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the indicator, we have
$$
\underset{\text{constrained}}{\inf_{x \in S} f(x)} \iff \underset{\text{unconstrained}}{\inf_{x \in \R^n} f(x) + \mathbb 1_S(x)}
$$
That is, we convert a constrained problem to a unconstrained one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition (verify it): Let $f: \R^n \mapsto \R_+$. Then, $f$ is convex (as a function) if and only if $\epi f$ is convex (as a set). Moreover, let $S \subseteq \R^n$ be a set. Then $S$ is convex (as a set) if and only if $\mathbb 1_S$ is convex (as a function).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proposition above associates the convexity of a function with that of its epigraph; and the convexity of a set with that of its indicator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;Jensen&amp;rsquo;s inequality&lt;/strong&gt;. Let $f: \R^n \mapsto \R_+$. Then $f$ is convex if and only if $f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)$ for any $m \in \N^+$ and any $x_1, \dots, x_m \in \R^n$ and any $\alpha_1, \dots, \alpha_m \ge 0$ such that $\sum_{i=1}^m \alpha_i = 1$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sufficiency&lt;/p&gt;
&lt;p&gt;Sufficiency is easy to show by interpreting the definition of convex function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Necessity&lt;/p&gt;
&lt;p&gt;For $i=1,\dots,m$, we have
$$
\big( x_i, f(x_i) \big) \in \epi f
$$
Since $\epi f$ is convex,
$$
\sum_{i=1}^m \alpha_i \big( x_i, f(x_i) \big) =  \big( \sum_{i=1}^m \alpha_i x_i, \sum_{i=1}^m \alpha_i f(x_i) \big) \in \epi f
$$
which means
$$
f(\sum_{i=1}^m \alpha_i x_i) \le \sum_{i=1}^m \alpha_i f(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convexity-preserving-operations&#34;&gt;Convexity-preserving Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-negative combination&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $f_1, \dots, f_m$ be convex functions, $\alpha_1, \dots, \alpha_m \ge 0$ be non-negative scalars. Then,
$$
f \triangleq \sum_{i=1}^m \alpha_i f_i \text{ is convex.}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pointwise supremum&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $I$ be an index set (either finite or infinite) and ${ f_i: i \in I }$ be a collection of convex functions. Then
$$
f \triangleq \sup_{i \in I} f_i \text{ is convex.}
$$
To show it, let $x_1, x_2 \in \R^n$ and $\alpha \in [0, 1]$. Then,
$$
\begin{aligned}
&amp;amp;f(\alpha x_1 + (1-\alpha) x_2) = \sup_{i \in I} f_i(\alpha x_1 + (1-\alpha) x_2) \
&amp;amp;\le \sup_{i \in I} \big( \alpha f_i(x_1) + (1-\alpha) f_i(x_2) \big) \
&amp;amp;\le [\alpha \sup_{i \in I} f_i(x_1)] + [(1-\alpha) \sup_{i \in I} f_i(x_2)] \
&amp;amp;= \alpha f(x_1) + (1-\alpha) f(x_2)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Geometrically, pointwise supremum is intersecting the epigraphs of $f_i$&amp;rsquo;s.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Consider the mapping $f: \R^{m \times n} \supseteq X \to ||X||$ where $||X||$ is the largest singular value of $X$. Show that $f$ is convex.&lt;/p&gt;
&lt;p&gt;By the &lt;em&gt;Courant-Fischer theorem&lt;/em&gt;,
$$
\begin{aligned}
||X|| &amp;amp;= \max_{u \in \R^m, v \in \R^n} u^T X v \
\text{s.t.} &amp;amp;\quad ||u||&lt;em&gt;2 = 1, ||v||&lt;em&gt;2 = 1
\end{aligned}
$$
Let $f&lt;/em&gt;{u,v}(X) = u^T X v$ and $I = { (u,v) \in \R^m \times \R^n: ||u||&lt;em&gt;2 = 1, ||v||&lt;em&gt;2 = 1 }$. Then,
$$
f(X) = \max&lt;/em&gt;{(u,v) \in I} f&lt;/em&gt;{u,v}(X)
$$
Note that $f&lt;/em&gt;{u,v}(X)$ is linear and thus convex in $X$. It follows directly from the pointwise supremum principle that $f$ is convex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Composition with increasing function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $g: \R^n \mapsto \R$ be convex, $h: \R \mapsto \R$ be convex. $h \circ g$ is not generally convex. To verify it, take
$$
h(x) = -x, g(x) = x^2
$$
But if $h$ is convex as well as increasing, then $h \circ g$ is convex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Restriction on lines&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a point $x_0 \in \R^n$ and a direction $h \in \R^n \setminus { 0 }$, we call the set
$$
{ x_0 + t h: t \in \R }
$$
a line through $x_0$ in the direction $h$. Let $f: \R^n \mapsto \R$ be a function. Define&lt;/p&gt;
&lt;p&gt;$$
\tilde f_{x_0, h}(t) \triangleq f(x_0 + t h)
$$
as the &lt;strong&gt;restriction of $f$ on the line&lt;/strong&gt; ${ x_0 + t h: t \in \R }$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, $f$ is convex if and only if $\tilde f_{x_0, h}$ is convex for any $x_0 \in \R^n$ and any $h \in \R^n \setminus { 0 }$.&lt;/p&gt;
&lt;h2 id=&#34;differentiable-convex-function&#34;&gt;Differentiable Convex Function&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f: \R^n \mapsto \R$ be differentiable, i.e. $\nabla f$ exists. Then, $f$ is convex if and only if for every $x, y \in \R^n$,
$$
f(x) \ge f(y) + (\nabla f(y))^T (x - y) \tag{gradient inequality}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a fixed $y$, the right-hand side of the gradient inequality is in essence an affine function of $x$. The graph of this affine function is
$$
t(x) = f(y) + (\nabla f(y))^T (x - y) \
\Updownarrow \
\underbrace{[(\nabla f(y))^T, -1]}&lt;em&gt;{s^T} \underbrace{\begin{bmatrix}
x \
t
\end{bmatrix}}&lt;/em&gt;{z} - \underbrace{[(\nabla f(y))^T y - f(y)]}_{c} = 0
$$&lt;/p&gt;
&lt;p&gt;Interestingly, the normal vector $s$ always points downwards (due to the $-1$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f$ be twice continuously differentiable, i.e. $\nabla f, \nabla^2 f$ exist and $\nabla^2 f$ is continuous. Then, $f$ is convex if and only if
$$
\nabla^2 f(x) \succcurlyeq 0, \forall x \in \R^n
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $f: \mathcal{S}&lt;em&gt;{++}^n \mapsto \R$ be given by $f(X) = -\ln \det (X)$ where $\mathcal{S}&lt;/em&gt;{++}^n$ is the set of $n \times n$ positive definite matrix. Show that $f$ is convex.&lt;/p&gt;
&lt;p&gt;This problem usually emerges when calculating the capacity of channel in information theory. To show it, we construct $f$&amp;rsquo;s restriction on all the lines. Let $X_0 \in \mathcal{S}&lt;em&gt;{++}^n$ and $H \in \mathcal{S}^n \setminus { 0 }$. Define
$$
g(t) \triangleq \tilde f(X_0 + t H) = -\ln \det(X_0 + t H)
$$
Since $X_0 \in \mathcal{S}&lt;/em&gt;{++}^n$, we can write $X_0 = X_0^{1/2} X_0^{1/2}$ where $X_0^{1/2} \in \mathcal{S}_{++}^n$. Let $X_0^{-1/2} = (X_0^{1/2})^{-1}$. Then,
$$
\begin{align*}
g(t) &amp;amp;= -\ln \det(X_0^{1/2} (I + t X_0^{-1/2} H X_0^{-1/2}) X_0^{1/2}) \
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2})
\end{align*}
$$
Note that $X_0^{-1/2} H X_0^{-1/2}$ is real symmetric. Let $\lambda_1, \dots, \lambda_n$ be its eigenvalues. Then $(I + t X_0^{-1/2} H X_0^{-1/2})$&amp;rsquo;s eigenvalues are $t \lambda_1 + 1, \dots, t \lambda_n + 1$.
$$
\begin{align*}
g(t) &amp;amp;= -2 \ln \det(X_0^{1/2}) - \ln \det(I + t X_0^{-1/2} H X_0^{-1/2}) \
&amp;amp;= -2 \ln \det(X_0^{1/2}) - \sum_i \ln(t\lambda_i + 1)
\end{align*}
$$
Now it is easy to verify that $-\ln(t\lambda_i + 1)$&amp;rsquo;s are convex, which concludes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;differentiability-agnostic-convex-function&#34;&gt;Differentiability-agnostic Convex Function&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;Differentiability-agnostic&amp;rdquo; means that no premise on the differentiability of the convex function is assumed. In this section, we discuss several &amp;ldquo;differentiability-agnostic&amp;rdquo; topics.&lt;/p&gt;
&lt;h3 id=&#34;subgradient&#34;&gt;Subgradient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $f: \R^n \mapsto \R$ be a function. A vector $s \in \R^n$ is called a &lt;strong&gt;subgradient&lt;/strong&gt; of $f$ at $x_0$ if
$$
f(x) \ge f(x_0) + s^T (x - x_0), \forall x \in \R^n
$$
The set
$$
\partial f(x_0) \triangleq { s \in \R^n: \text{$s$ is a subgradient of $f$ at $x_0$} }
$$
is called the &lt;strong&gt;sub-differential&lt;/strong&gt; of $f$ at $x_0$. Sub-differential is always convex and closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of subgradient is quite inspired from the gradient inequality. The geometric interpretation of subgradient resembles that of gradient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $f: \R^n \mapsto \R$ be convex. Then, $\partial f(x)$ is non-empty, convex and compact (non-emptiness is due to the continuity (which in turn is due to the openness of $\R^n$) and boundedness is due to convexity). Moreover,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is differentiable at $x_0$ if and only if $\partial f(x_0) = { \nabla f(x_0) }$;&lt;/li&gt;
&lt;li&gt;$x_0$ is the global minima if and only if $0 \in \partial f(x_0)$;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The topological properties that the sub-differential possesses imply that we can do projection on the sub-differential; besides we may apply Weierstrass theorem to solve some optimization problem w.r.t. the sub-differential (such as what is the minimal-norm subgradient).&lt;/p&gt;
&lt;h3 id=&#34;conjugate-function&#34;&gt;Conjugate Function&lt;/h3&gt;
&lt;p&gt;If two functions have the identical epigraphs, then these two functions are identical. That&amp;rsquo;s the geometric view of a function.&lt;/p&gt;
&lt;p&gt;Let $f: \R^n \mapsto \R$ be convex. $\epi f$ is known to be convex. Suppose in addition that $\epi f$ is non-empty and closed. Recall that a non-empty, closed and convex set can be written as the intersection of lower halfspaces that contain it:
$$
\epi f = \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T, y_0]^T, c)
$$&lt;/p&gt;
&lt;p&gt;where $s \in \R^n$ and $y_0 \in \R$. We argue that $y_0 \le 0$ because the last entry of some $z \in \epi f$ can be arbitrarily large. We further argue that $y_0 &amp;lt; 0$ or else $f(x) = -\infty$ everywhere. Without loss of generality let $y_0 = -1$, since $H^-([s^T, y_0]^T,c) = H^-([-s^T/y_0, -1]^T,c)$.&lt;/p&gt;
&lt;p&gt;Notice that this lower half-space $H^-([s^T, -1]^T, c)$ is exactly the epigraph of the affine function
$$
h_{s, c}(x) = s^T x - c
$$
because for any $(x, y) \in \R^n \times \R$ that belongs to $H^-([s^T, -1]^T, c)$, we have
$$
\begin{aligned}
s^T x - y &amp;amp;\le c \
y &amp;amp;\ge s^T x - c
\end{aligned}
$$
On the other hand, $\epi f \subseteq H^-([s^T, -1]^T, c) = \epi{h_{s, c}}$ indicates that $\forall x \in \R^n, f(x) \ge h_{s, c}(x)$. Consequently, $\epi f$ can be re-written as
$$
\begin{aligned}
\epi f &amp;amp;= \bigcap_{H^-([s^T, y_0]^T,c) \supseteq \epi f} H^-([s^T, -1]^T,c) \
&amp;amp;= \bigcap_{\epi{h_{s, c}} \supseteq \epi f} \epi{h_{s, c}} \
&amp;amp;= \bigcap_{f \ge h_{s, c}} \epi{h_{s, c}}
\end{aligned}
$$
Therefore, $f$ can be written as the pointwise supremum of the affine functions who are less than $f$:
$$
f = \sup_{h \le f} h
$$
The normal vectors of those affine functions that are less than $f$ and that pass through $(x_0, f(x_0))$ essentially forms the sub-differential of $f$ at $x_0$.&lt;/p&gt;
&lt;p&gt;Consider the set
$$
\begin{aligned}
S_f &amp;amp;= { (y, t) \in \R^n \times \R: y^T x - t \le f(x), \forall x \in \R^n } \
&amp;amp;= { (y, t) \in \R^n \times \R: \sup_{x}(y^T x - f(x)) \le t } \
&amp;amp;\triangleq \epi{f^&lt;em&gt;}
\end{aligned}
$$
where
$$
f^&lt;/em&gt;(y) \triangleq \sup_{x}(y^T x - f(x))
$$
$f^*(y)$ is called the &lt;strong&gt;conjugate function&lt;/strong&gt; of $f$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/4-linear-programming/</guid>
      <description>&lt;h2 id=&#34;linear-programming&#34;&gt;Linear Programming&lt;/h2&gt;
&lt;p&gt;Recall that the linear programming problem is
$$
\label{lp} \begin{aligned}
\min_{x} \quad&amp;amp; c^T x \
\text{s.t.} \quad&amp;amp; a_i^T x \le b_i, i=1,\dots,m
\end{aligned} \tag{LP}
$$
where $c \in \R^n$ and $a_i \in \R^{n}, b_i \in \R, i=1,\dots,m$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: A &lt;strong&gt;polyhedron&lt;/strong&gt; is the intersection of a finite set of halfspaces. A bounded polyhedron is called a &lt;strong&gt;polytope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the feasible set of LP is a polyhedron. Interestingly, all polyhedrons are convex. That is, the feasible set of LP is convex.&lt;/p&gt;
&lt;h3 id=&#34;standard-form&#34;&gt;Standard Form&lt;/h3&gt;
&lt;p&gt;The standard form of LP is
$$
\label{standard-lp} \begin{aligned}
\min_{x \in \R_+^n} \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; A x = b \
\end{aligned} \tag{Standard LP}
$$
where $c \in \R^n, A \in \R^{m \times n}, b \in \R^m$. Standard LP has standard solutions. Thus, the next question is how to convert an ordinary LP problem to a standard one.&lt;/p&gt;
&lt;p&gt;Now write $x = x^+ - x^-$, where $x^+, x^- \in \R_+^n$. Introduce another slack variable $s \in \R_+^m$. $\eqref{lp}$ is equivalent to
$$
\begin{aligned}
\min_{x^+, x^- \ge 0, s \ge 0} \quad &amp;amp; c^T (x^+ - x^-) \
\text{s.t.} \quad &amp;amp; A (x^+ - x^-) + s = b \
\end{aligned}
$$
Let
$$
A&amp;rsquo; = \left[\begin{array}{c:c}
A &amp;amp; -A &amp;amp; I
\end{array}\right] \in \R^{m \times (2n+m)} \
x&amp;rsquo; = \begin{bmatrix}
x^+ \
\hdashline
x^- \
\hdashline
s
\end{bmatrix} \in \R_+^{2n+m},
c&amp;rsquo; = \begin{bmatrix}
c \
\hdashline
-c \
\end{bmatrix} \in \R_+^{2n}
$$
The problem becomes
$$
\begin{aligned}
\min_{x&amp;rsquo; \in \R_+^{2n+m}} \quad &amp;amp; (c&amp;rsquo;)^T x&amp;rsquo; \
\text{s.t.} \quad &amp;amp; A&amp;rsquo; x&amp;rsquo; = b \
\end{aligned}
$$
which observes the formality of standard LP. Here lays down again the standard LP problem:&lt;/p&gt;
&lt;p&gt;$$
\label{primal} \begin{aligned}
v_p^* = \min_{x \in \R_+^n} \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; A x = b \
\end{aligned} \tag{P}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The first question&lt;/strong&gt; to ask is whether there is a optimal solution to it. The answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If $\eqref{primal}$ is feasible, then either 1) the optimal value $v^* = -\infty$ (no optimal solution),or 2) it has an optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas&amp;rsquo; Lemma&lt;/h3&gt;
&lt;p&gt;But &lt;strong&gt;the second question&lt;/strong&gt; arises: how to certify that $\eqref{primal}$ is infeasible? We meet the similar dilemma to that when dealing with set-set separation: it is easy to test the feasibility of any $x$; but it is prohibitive to test the feasibility of all $x$&amp;rsquo;s just to show that the original problem is infeasible.&lt;/p&gt;
&lt;p&gt;The idea is that, given the feasible polyhedron $P \triangleq { x \in \R_+^n: Ax = b }$ of the original problem, we construct another auxiliary polyhedron $Q$ such that exactly one of the following holds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P \ne \emptyset, Q = \emptyset$;&lt;/li&gt;
&lt;li&gt;$P = \emptyset, Q \ne \emptyset$.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Farkas&amp;rsquo; lemma&lt;/strong&gt;. Let $A \in \R^{m \times n}$ and $b \in \R^m$ be given. Then exactly one of the following systems is solvable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A x = b, x \ge 0$;&lt;/li&gt;
&lt;li&gt;$A^T y \le 0, b^T y &amp;gt; 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;$A x = b, x \ge 0$ means that $b$ is a non-negative linear combinations of $a_1, \dots, a_n$. $A^T y \le 0$ means that $y$ forms an obtuse angle to $a_1, \dots, a_n$; $b^T y &amp;gt; 0$ means $y$ forms an acute angle with $b$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that the above two statements cannot be solvable at the same time. or else this gives rise to the contradiction for some $x_0 \ge 0, y_0 \in \R^m$:
$$
\underbrace{x_0^T}&lt;em&gt;{\ge 0} \underbrace{A^T y_0}&lt;/em&gt;{\le 0} = y_0^T A x = \underbrace{y_0^T b}_{&amp;gt; 0}
$$
We then claim that the above two statements cannot be unsolvable at the same time. Suppose $1$ is unsolvable.&lt;/p&gt;
&lt;p&gt;In this case, $b \notin A^+ \triangleq { Ax: x \ge 0 }$. It can be verified that $A^+$ is non-empty ($0 \in A^+$), closed (closeness is not generally preserved under affine transformation; but in this case it is &amp;lt;refer to &lt;a href=&#34;../3-lp.pdf&#34;&gt;this handout&lt;/a&gt;&amp;gt;) and convex (because convexity is preserved under affine transformation $A^+ = A \R_+^n$). Then by &lt;em&gt;point-set separation theorem&lt;/em&gt;, for any $x \ge 0$, there exists a $y_0 \in \R^m$ such that
$$
\max_{x \ge 0} y_0^T A x &amp;lt; y_0^T b
$$
Take $x = 0$, we have
$$
y_0^T b &amp;gt; 0
$$
We claim that $A^T y_0 \le 0$. Suppose on the contrary that there exists an $i$ such that $(A^T y_0)&lt;em&gt;i$ is positive. Then for any $\lambda \ge 0$, take $x = \lambda e_i$ and give
$$
y_0^T A x = x^T A^T y_0 = \underbrace{\lambda}&lt;/em&gt;{\ge 0} \underbrace{(A^T y_0)&lt;em&gt;i}&lt;/em&gt;{&amp;gt; 0} &amp;lt; y_0^T b
$$
which is impossible when $\lambda \to \infty$.&lt;/p&gt;
&lt;p&gt;Therefore, $2$ is solvable when $1$ is unsolvable. $1$ and $2$ cannot be unsolvable at the same time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We construct $Q&amp;rsquo;$ as ${ y \in \R^m: A^T y \le 0, b^T y &amp;gt; 0 }$. Note that $Q&amp;rsquo;$ is not a polyhedron yet, because ${ y \in \R^m: b^T y &amp;gt; 0 }$ is open and is not a half-space. However, observe that (verify it)
$$
A^T y \le 0, b^T y &amp;gt; 0 \text{ is solvable} \iff A^T y \le 0, b^T y = 1 \text{ is solvable}
$$
${ y \in \R^m: b^T y = 1 }$ can be rewritten as ${ y \in \R^m: b^T y \ge 1, b^T y \le 1 }$ which is an intersection of halfspaces. Therefore, $Q \triangleq { y \in \R^m: A^T y \le 0, b^T y \ge 1, b^T \le 1 }$ is non-empty if and only if $Q&amp;rsquo;$ is non-empty. Moreover, $Q$ is an intersection of half-spaces as desired.&lt;/p&gt;
&lt;p&gt;Now, we convert the infeasibility of $P$ into the feasibility of $Q$. &lt;strong&gt;The third question&lt;/strong&gt; is, suppose we verify that $P$ is feasible, then given a solution to $\eqref{primal}$, how to certify its optimality? The idea is to establish a lower bound on the optimal value $v^*_p$.&lt;/p&gt;
&lt;h3 id=&#34;duality&#34;&gt;Duality&lt;/h3&gt;
&lt;p&gt;Consider a $y \in \R^m$ such that $A^T y \le c$, then for any $x \in \R_+^n$ such that $b \triangleq A x$, we have
$$
b^T y = x^T A^T y \le c^T x
$$
The above holds for $x^&lt;em&gt;$ of $\eqref{primal}$ as well. Therefore,
$$
b^T y \le c^T x_p^&lt;/em&gt; = v_p^*
$$
We can try to find the largest lower bound w.r.t. $y$:
$$
\label{dual} \begin{aligned}
v_d^* = \max_y &amp;amp;\quad b^T y \
\text{s.t.} &amp;amp;\quad A^T y \le c
\end{aligned} \tag{D}
$$
Automatically, $v_d^* \le v_p^&lt;em&gt;$. Note that $\eqref{primal}$ is the also the dual of $\eqref{dual}$ in that $v_p^&lt;/em&gt; \ge v_d^*$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt; of LP. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then,
$$
c^T \bar x \ge b^T \bar y
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If $v_p^* = -\infty$, then $\eqref{dual}$ is infeasible.&lt;/li&gt;
&lt;li&gt;If $v_d^* = +\infty$, then $\eqref{primal}$ is infeasible.&lt;/li&gt;
&lt;li&gt;If $\bar x$ is feasible for $\eqref{primal}$, $\bar y$ is feasible for $\eqref{dual}$, and the &lt;strong&gt;duality gap&lt;/strong&gt; $\Delta(\bar x, \bar y) \triangleq c^T \bar x - b^T \bar y$ is zero, then $\bar x$ is optimal for $\eqref{primal}$ and $\bar y$ is optimal for $\eqref{dual}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What about the converses of conclusions above?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $\eqref{dual}$ is infeasible, then either $v_p^* = -\infty$ or $\eqref{primal}$ is infeasible.&lt;/p&gt;
&lt;p&gt;It is rather easy to construct a problem such that both the $\eqref{primal}$ and $\eqref{dual}$ are infeasible, since the constraints $Ax = b, x \ge 0$ and $A^T y \le c$ are quite independent. The following will give an example:
$$
A =
\begin{bmatrix}
-1 &amp;amp; -1 \
1 &amp;amp; 1
\end{bmatrix},
b = [1, 1]^T,
c = [-1, -1]^T
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\eqref{primal}$ is infeasible, then either $v_d^* = +\infty$ or $\eqref{dual}$ is infeasible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality&lt;/strong&gt; for LP. Suppose $\eqref{primal}$ has an optimal solution $x_p^&lt;em&gt;$. Then $\eqref{dual}$ has an optimal solution $y_d^&lt;/em&gt;$ and $v_p^* = c^T x_p^* = b^T y_d^* = v_d^*$.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;We claim that
$$
Ax = b, x \ge 0, c^T x - v_p^* &amp;lt; 0 \tag{I}
$$
is unsolvable. Consider &lt;strong&gt;homogenizing&lt;/strong&gt; the above system:
$$
Ax - bt = 0, c^T x - v_p^* t &amp;lt; 0, x \ge 0, t \ge 0 \tag{II}
$$
We argue that $\textrm{(I)}$ is solvable if and only if $\textrm{(II)}$ is solvable. If $\bar x$ solves $\textrm{(I)}$, then $(\bar x, 1)$ solves $\textrm{(II)}$ too. If $(\bar x, \bar t)$ solves $\textrm{(II)}$, we discuss by case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar t &amp;gt; 0$. In this case, $\bar x / \bar t$ solves $\textrm{(I)}$.&lt;/li&gt;
&lt;li&gt;$\bar t = 0$. In this case, $x^* + \bar x$ solves $\textrm{(I)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\textrm{(II)}$ can be rewritten as
$$
\begin{bmatrix}
A &amp;amp; -b \
-A &amp;amp; b \
-I &amp;amp; 0 (n \times 1) \
0 (1 \times m) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
x \
t
\end{bmatrix} \le 0,&lt;/p&gt;
&lt;p&gt;[-c^T, v_p^&lt;em&gt;]
\begin{bmatrix}
x \
t
\end{bmatrix} &amp;gt; 0
$$
By &lt;em&gt;Farkas&amp;rsquo; lemma&lt;/em&gt;, the following system is solvable:
$$
\begin{bmatrix}
A^T &amp;amp; -A^T &amp;amp; -I &amp;amp; 0 (m\times 1) \
-b^T &amp;amp; b^T &amp;amp; 0 (1 \times n) &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
z_1 \
z_2 \
z_3 \
z_4
\end{bmatrix} =
\begin{bmatrix}
-c \
v_p^&lt;/em&gt;
\end{bmatrix},&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
z_1 \
z_2 \
z_3 \
z_4
\end{bmatrix}
\ge 0
$$
This means
$$
A^T (z_1 - z_2) - z_3 = -c \Rightarrow \
A^T (z_2 - z_1) \le c \
$$&lt;/p&gt;
&lt;p&gt;$$
-b^T (z_1 - z_2) - z_4 = v_p^* \Rightarrow \
b^T (z_2 - z_1) \ge v_p^*
$$&lt;/p&gt;
&lt;p&gt;$y^* \triangleq z_2 - z_1$ is a feasible solution to the $\eqref{dual}$. Plus the weak duality which states $b^T y^* \le v_p^&lt;em&gt;$, $b^T y^&lt;/em&gt; = v_p^&lt;em&gt;$. Therefore, $y_d^&lt;/em&gt; = y^&lt;em&gt;$ and $c^T x_p^&lt;/em&gt; = b^T y_d^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Corollary: &lt;strong&gt;complementary slackness&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then, they are optimal for their respective problems if and only if
$$
\underbrace{\bar x_i}_{\substack{\text{$i$-th} \ \text{primal variable}}}\ \underbrace{(c - A^T \bar y)&lt;em&gt;i}&lt;/em&gt;{\substack{\text{$i$-th } \ \text{dual constraint}}} = 0
$$
Proof:
$$
\begin{aligned}
c^T \bar x - b^T \bar y &amp;amp;= c^T x - (A \bar x)^T \bar y \
0 &amp;amp;= \bar x^T (c - A^T \bar y)
\end{aligned}
$$
Plus that $\bar x \ge 0, c - A^T \bar y \ge 0$, we can conclude with the complementary slackness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After establishing the lower bound of $\eqref{primal}$&amp;rsquo;s optimal value with $\eqref{dual}$&amp;rsquo;s optimal value, by strong duality, to find optimal solutions to $\eqref{primal}$ and $\eqref{dual}$ is equivalent to find a feasible solution to
$$
\begin{gather}
Ax = b, x \ge 0 \tag{primal constraint} \
A^T y \le c \tag{dual constraint} \
c^T x = b^T y \tag{zero duality gap}
\end{gather}
$$
Note that the zero duality gap is in essence equivalent to
$$
x^T (c - A^T y) = 0 \tag{complementary slackness}
$$
An optimization problem is converted to feasibility problem. Particularly, an LP optimization problem is no harder than an LP feasibility problem.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-lp-problem&#34;&gt;Examples of LP Problem&lt;/h2&gt;
&lt;h3 id=&#34;vertex-cover&#34;&gt;Vertex Cover&lt;/h3&gt;
&lt;p&gt;Given a graph $G = (V, E)$ and a cost function $c: V \mapsto \R^+$, find a vertex cover that minimizes overall cost. Here we say that $S \subseteq V$ is vertex cover if every edge has at least one endpoint in $S$. The cost of a vertex cover the sum of costs of the vertices contained.&lt;/p&gt;
&lt;p&gt;Let $x_i \in { 0,1 }$ be an indicator variable defined as
$$
x_i =
\begin{cases}
1, &amp;amp; \text{if vertex $i$ is in the cover} \
0, &amp;amp; \text{otherwise}
\end{cases}
$$
Then we have the following integer programming problem:
$$
\label{vc} \begin{aligned}
v^* = \min_x \quad &amp;amp;\sum_i x_i c_i \
\text{s.t.} \quad &amp;amp;x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp;x_i \in { 0,1 }, \forall i=1,\dots,|V|
\end{aligned} \tag{Vertex Cover}
$$
The problem above is NP-hard. One typical way to tackle it is to relax the constraint to make it easier.
$$
\label{vc-lp-I} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp; 0 \le x_i \le 1, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC I}
$$
Note that we can further drop the $x_i \le 1$ in $\eqref{vc-lp-I}$ because we can always let $x_i&amp;rsquo; = 1$ for $x_i &amp;gt; 1$ without violating the constraints but with smaller objective (because $c \ge 0$):
$$
\label{vc-lp-II} \begin{aligned}
v_\text{LP}^* = \min_x \quad &amp;amp; \sum_i x_i c_i \
\text{s.t.} \quad &amp;amp; x_i + x_j \ge 1, \forall (i,j) \in E \ 
&amp;amp; 0 \le x_i, \forall i=1,\dots,|V|
\end{aligned} \tag{Relaxed VC II}
$$
Once $x_\text{lp}^&lt;em&gt;$ solves $\eqref{vc-lp-II}$, we need to round $x_\text{lp}^&lt;/em&gt;$ to give a feasible $x_\text{rd}$ to $\eqref{vc}$ such that $v_\text{rd} = c^T x_\text{rd} \approx v^&lt;em&gt;$. Obviously, $v^&lt;/em&gt; \ge v_\text{lp}^&lt;em&gt;$ and $v^&lt;/em&gt; \le v_\text{rd}$. The question is if we can upper-bound $v_\text{rd}$ by $\alpha v^*$ for some $\alpha \ge 1$. Here $\alpha$ is called the &lt;strong&gt;approximation ratio&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Let $P = { x \in \R^n: a_i^T x \le b, i=1,\dots,m }$ be a polyhedron and let $\bar x \in P$. Let $I(\bar x) = { i: a_i^T x = b }$ be the active index set. We say that $\bar x$ is a &lt;strong&gt;vertex&lt;/strong&gt; of $P$ if ${ a_i: i \in I(\bar x) }$ has $n$ linearly-independent vectors. Alternatively, the system
$$
a_i^T x = b_i, i \in I(\bar x)
$$
has a unique solution (which is $\bar x$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The equality constraint ensures that the solution is on the boundary of the feasible set, or rather, on a hyperplane. $n$ linearly-independent vectors ensure that the solution is the intersection of $n$ hyperplanes, which is why it is called &lt;u&gt;vertex solution&lt;/u&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: If an LP has a vertex feasible solution and is bounded (which means the optimal value is finite), then it has a vertex optimal solution (??).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Let $Q = { x \in \R^{|V|}: x_i + x_j \ge 1, \forall (i,j) \in E; x \ge 0 }$, which is exactly the feasible set of $\eqref{vc-lp-II}$. Then $Q$ has a vertex solution (??). Let $\bar x$ be one of such vertex solutions. Then,
$$
\forall i, \bar x_i \in { 0, 1/2, 1 }
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the theorem and proposition above, and because $\eqref{vc-lp-II}$ is bounded (verify it), we can have a vertex optimal solution $x_\text{lp}^&lt;em&gt;$ to $\eqref{vc-lp-II}$. Next, we choose to round the optimal vertex solution $x_\text{lp}^&lt;/em&gt;$ in this way: if $x_{\text{lp}&lt;em&gt;i}^*$ is $0$, $x&lt;/em&gt;{\text{rd}&lt;em&gt;i} = 0$; else $x&lt;/em&gt;{\text{rd}&lt;em&gt;i} = 1$. Note that this rounding method satisfies the constraint of $\eqref{vc}$. The largest divergence of $v&lt;/em&gt;\text{rd}$ from $v_\text{lp}^&lt;em&gt;$ happens when $x_\text{lp}^&lt;/em&gt;$ is full of $1/2$ or $0$, in which case $v_\text{rd} = 2 v_\text{lp}^&lt;em&gt;$. Thus, we can conclude that
$$
v_\text{lp}^&lt;/em&gt; \le v^* \le c^T x_\text{rd} \le 2 v_\text{lp}^* \le 2 v^*
$$&lt;/p&gt;
&lt;p&gt;The approximation ratio in this problem is $\alpha = 2$.&lt;/p&gt;
&lt;h3 id=&#34;max-flow&#34;&gt;Max Flow&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cs.cmu.edu/~odonnell/toolkit13/lecture14.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;cooperative-game-theory&#34;&gt;Cooperative Game Theory&lt;/h3&gt;
&lt;p&gt;Let $\mathcal{N} = { 1,\dots,n }$ be the set of players and $v: 2^N \mapsto \R^+$ be the worth function, which satisfies $v(\emptyset) = 0$. The subset of $\mathcal{N}$ is called &lt;strong&gt;coalition&lt;/strong&gt;. Let $x \ge 0$ be the &lt;strong&gt;allocation vector&lt;/strong&gt; and $x_i$ be the payoff assigned to player $x_i$. Let $x(\emptyset) = 0$ and $x(S) \triangleq \sum_{i \in S} x_i$ for non-empty coalition $S$. We say that coalition $S$ &lt;strong&gt;improves upon&lt;/strong&gt; $x$ if $v(S) &amp;gt; x(S)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: The allocation vector $x$ is said to be in the &lt;strong&gt;core&lt;/strong&gt; if
$$
\begin{gather}
x(\mathcal{N}) = v(\mathcal{N}) \
\forall S \subseteq \mathcal{N}, x(S) \ge v(S) \label{core-inequality}
\end{gather}
$$
which means no $S$ improves upon $x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is essential to understand &lt;strong&gt;the implication behind the game&lt;/strong&gt;. This &amp;ldquo;cooperative game&amp;rdquo; is actually preventing players from cooperation, or collusion, or called forming a coalition. The allocation vector is part of this &amp;ldquo;evil scheme&amp;rdquo;. The fact of matter is, the solver of this problem is trying to pay off players (in total the amount is no more than the value he thinks the whole coalition $\mathcal{N}$ deserves) such that for every possible coalition, there will be some member who thinks it unfair because the share (equal share for everyone!) he gets from the coalition is no more than the amount he would otherwise earns himself.
$$
x(S) = \sum_{i \in S} x_i \ge v(S)
$$
$x(S)$ is additive w.r.t. $S$, which exactly represents the payoff every player grabs (snout in the trough) covertly and separately. There must be some $i$ in $S$ such that $x_i \ge v(S)/|S|$. The situation worsens when the $\ge$ relation is strict.&lt;/p&gt;
&lt;p&gt;The question is, is the core non-empty for a given $(\mathcal{N}, v)$, or rather, can such scheme exist?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Treasure hunt. $n$ people discovered treasures in the mountain. 2 people are needed to bring one piece out and thus $v(S) = \lfloor \frac{|S|}{2} \rfloor$. Is the core empty?&lt;/p&gt;
&lt;p&gt;First consider the case when $n$ is even. $x = [1/2, \dots, 1/2]$ is in the core.&lt;/p&gt;
&lt;p&gt;Now simply consider the case when $n$ is odd. The fact is that the core is empty in this case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;. The cooperative game $(\mathcal{N}, v)$ has a non-empty core if and only if for every set of numbers ${ y_S }_{S \subseteq \mathcal{N}}$, whose elements are indexed by $\mathcal{N}$&amp;rsquo;s subset, such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\forall S \subseteq \mathcal{N}, y_S \ge 0$ and&lt;/li&gt;
&lt;li&gt;$\forall i \in \mathcal{N}, \sum_{S: i \in S} = 1$ and&lt;/li&gt;
&lt;li&gt;$\sum_{S \subseteq \mathcal{N}} y_S v(S) \le v(\mathcal{N})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;p&gt;$y_S$ can be interpreted as the amount of time each player in $S$ spent on the coalition $S$, justifying the non-negativity constraint. The total amount of time player $i$ spent on different coalitions is $1$. $y_S v(S)$ can be understood as the proportional outcome from partial-commitment.&lt;/p&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Consider the following optimization problem:
$$
\label{cgp} \tag{Coop. Game} \begin{aligned}
\min \quad &amp;amp; [x(\mathcal{N}) \triangleq \sum_{i \in N} x_i] \
\text{s.t.} \quad &amp;amp; x(S) \ge v(S), \forall S \subseteq \mathcal{N}
\end{aligned}
$$
Observe that the constraints naturally imply the inequality conditions $\eqref{core-inequality}$ for an allocation to be in the core. $x \ge 0$ is automatically embedded into $x({ i }) \ge v({ i }) \ge 0, i=1,\dots,n$. Other than that, the minimizing objective together with the constraint $x(\mathcal{N}) \ge v(\mathcal{N})$ implies $\eqref{cgp}$ is lower-bounded by $v(\mathcal{N})$. Also note that $\eqref{cgp}$ is always feasible because we can assign each $x(S)$ sufficiently large to surpass $v(S)$. Therefore, $\eqref{cgp}$ always has an optimal solution.&lt;/p&gt;
&lt;p&gt;To rewrite $\eqref{cgp}$ to LP form, let
$$
e = [1,\dots,1]^T,
A =
\begin{bmatrix}
\mathbb{1}_n(\emptyset)^T \
\vdots \
\mathbb{1}&lt;em&gt;n(S)^T \
\vdots \
\mathbb{1}&lt;em&gt;n(\mathcal{N})^T
\end{bmatrix},
b =
\begin{bmatrix}
v(\emptyset) \
\vdots \
v(S) \
\vdots \
v(\mathcal{N})
\end{bmatrix}
$$
where $\mathbb{1}&lt;/em&gt;\mathcal{N}(S)$ is the indicator function that returns the vector in $\R^n$ whose $i$-th entry indicates if $i \in S$. Then $\eqref{cgp}$ becomes
$$
\label{cgp-lp} \tag{Coop. Game LP} \begin{aligned}
v^* = \min \quad &amp;amp; e^T x \
\text{s.t.} \quad &amp;amp; A x \ge b
\end{aligned}
$$
We don&amp;rsquo;t rush to convert it to standard LP form yet; otherwise we need to introduce a slack variable. On the other hand, we formulate the following LP problem, whose optimal value is &lt;strong&gt;of the same magnitude as but of different sign to&lt;/strong&gt; $\eqref{cgp-lp}$:
$$
\label{cgp-primal} \tag{Coop. Game P} \begin{aligned}
v_p^* = \max \quad &amp;amp; -e^T x \
\text{s.t.} \quad &amp;amp; -A x \le -b
\end{aligned}
$$
That is, $v^* = -v_p^&lt;em&gt;$. Recall that the primal and the dual are relative. The above problem has the dual
$$
\label{cgp-dual} \tag{Coop. Game D} \begin{aligned}
v_d^&lt;/em&gt; = \min&lt;/em&gt;{y \ge 0} \quad &amp;amp; -b^T y \
\text{s.t.} \quad &amp;amp; A^T y = e
\end{aligned}
$$
which is in the standard LP form.&lt;/p&gt;
&lt;p&gt;Note that columns of $A^T$ are exactly those indicator vectors. Thus, $A^T$&amp;rsquo;s columns and $y$&amp;rsquo;s entries can both be indexed by sets. Also, rows of $A^T$ are indexed by elements. Since every entry of $A^T y$ is $1$, interpreting $A^T y$ as the row-column dot-product, the constraint in $\eqref{cgp-dual}$ is exactly
$$
\forall i \in \mathcal{N}, \sum_{S: i \in S} y_S = 1
$$
By strong duality, we have
$$
v^* = x^&lt;em&gt;(\mathcal{N}) = \sum_{S} y_S^&lt;/em&gt; v(S) = -v_d^* = -v_p^*
$$
$\forall S \subseteq \mathcal{N}, x^&lt;em&gt;(S) \ge v(S)$ by the constraint of $\eqref{cgp-lp}$. By &lt;strong&gt;Bondareva-Shapeley theorem&lt;/strong&gt;&amp;rsquo;s condition, we have $x^&lt;/em&gt;(\mathcal{N}) = \sum_{S} y_S^* v(S) \le v(\mathcal{N})$. Therefore, we conclude that $x^&lt;em&gt;(\mathcal{N}) = v(\mathcal{N})$. $x^&lt;/em&gt;$ is in the core.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be inferred that there are lots of duality in game theory, given its mini-max nature.&lt;/p&gt;
&lt;h2 id=&#34;solving-lp&#34;&gt;Solving LP&lt;/h2&gt;
&lt;p&gt;The algorithm for solving LP problems generally falls into two methods: simplex method and interior point method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/5-conic-linear-programming/</guid>
      <description>&lt;p&gt;Though some problems can be relaxed to accommodate the linear programming form, applications of LP are still limited due to its linear constraints. To extend, a natural idea is to gradually allow for other kinds of non-linear constraints. But that would go too far away from the established theories for linear programming.&lt;/p&gt;
&lt;p&gt;In this post, we study the conic linear programming, which is a phased open-up to non-linear problems.&lt;/p&gt;
&lt;h2 id=&#34;concept-preparation&#34;&gt;Concept Preparation&lt;/h2&gt;
&lt;h3 id=&#34;good-order-and-proper-cone&#34;&gt;&amp;ldquo;Good&amp;rdquo; Order and &amp;ldquo;Proper&amp;rdquo; Cone&lt;/h3&gt;
&lt;p&gt;Before exploring other non-linear problems, we first study the properties of a &amp;ldquo;good&amp;rdquo; order $\succeq$. between vectors. We expect it to have the following basic three properties of a &lt;strong&gt;partial-order&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;: $\forall u \in \R^n, u \succeq u$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anti-symmetry&lt;/strong&gt;
$$
\forall u,v \in \R^n, u \succeq v, v \succeq u \to u = v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;
$$
\forall u,v,w \in \R^n, u \succeq v, v \succeq w \to u \succeq w
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than them, we expect $\succeq$ to further possess the following two arithmetic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt;
$$
\forall u,v \in \R^n, u \succeq v, \alpha \succeq 0, \alpha u \succeq \alpha v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Additivity&lt;/strong&gt;
$$
\forall u,v,w,z \in \R^n, u \succeq v, w \succeq z \to u + w \succeq v + z
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say the $\succeq$ order is &amp;ldquo;good&amp;rdquo; if it satisfies the above five properties. The above five is from the algebraic perspective. To illustrate it geometrically, consider the set $K \triangleq { x \in \R^n: x \ge 0 }$. We say $K$ is a &lt;strong&gt;&amp;ldquo;proper&amp;rdquo; cone&lt;/strong&gt; in that it is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;non-empty&lt;/strong&gt; and &lt;strong&gt;closed under addition&lt;/strong&gt;
$$
\forall u, v \in K, u + v \in K
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;conic&lt;/strong&gt;
$$
\forall u \in K, \alpha &amp;gt; 0 \to \alpha u \in K \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;pointed&lt;/strong&gt;
$$
u, -u \in K \to u = 0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We further claim that a proper cone is &lt;strong&gt;convex&lt;/strong&gt; and contains the zero vector (verify it).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: The pointedness plus closeness (not closeness under addition) implies that there is no complete line in this cone, which equivalently means there is no non-trivial subspace (i.e., except ${ 0 }$ and the universe) inside this cone. To show it, let $K$ be a closed pointed cone, suppose on the contrary there exists $u, v \in K$ such that for every $\alpha \in \R$, we have $u + \alpha(v - u) \in K$. For $t &amp;gt; 1$, consider the sequence ${w_+^t}&lt;em&gt;{t=1}^{\infty}$ and ${w&lt;/em&gt;-^t}&lt;em&gt;{t=1}^{\infty}$ where
$$
w&lt;/em&gt;+^t = \frac{u + t(v-u)}{|u + t(v-u)|&lt;em&gt;2}, w&lt;/em&gt;-^t = \frac{u - t(v-u)}{|u - t(v-u)|&lt;em&gt;2}
$$
Now $w&lt;/em&gt;+^t, w_-^t \in K$ for all $t \ge 1$. But $w_+^t \to w \triangleq \frac{v-u}{|v-u|&lt;em&gt;2}$ and $w&lt;/em&gt;-^t \to -w$ . Since $K$ is closed, we have $w, -w \in K$. However, since $w$ has unit norm and is not zero vector, it follows that $K$ is not pointed, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: At times, people also refer to this as the &lt;strong&gt;salient&lt;/strong&gt; property of a cone. A &lt;u&gt;proper cone&lt;/u&gt; is variously defined on a subset of these properties (closeness, closeness under addition, pointed, salient, and essentially, conic) depending on the context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, the algebraic properties and the geometric properties can derive each other. A good order and a proper cone have a one-to-one relationship. Now we ask, in an arbitrary $n$-d universe (or a finite-dimensional Euclidean space), is $\ge$ the only good order, or equivalently, is $\R_+^n$ the only proper cone? The answer is no.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-proper-cone&#34;&gt;Examples of Proper Cone&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lorentz cone / second-order cone / ice cream cone
$$
\mathcal{Q}^{n+1} \triangleq { (t, x) \in \R \times \R^n: t \ge ||x||_2 }
$$
$\mathcal{Q}^{n+1}$ is a closed proper cone. Then what is the good order associated with this proper cone?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Semi-definite cone
$$
\mathcal{S}&lt;em&gt;+^n \triangleq { X \in \mathcal{S}^n: u^T X u \ge 0, \forall u \in \R^n }
$$
$\mathcal{S}&lt;/em&gt;+^n$ is a closed proper cone. Then what is the good order associated with this proper cone?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zero cone: ${ 0 }$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New cones from old ones by Cartesian product&lt;/p&gt;
&lt;p&gt;Let $K_1, \dots, K_m$ be closed proper cones (with non-empty interior). Then
$$
K \triangleq K_1 \times \dots \times K_m = { (x_1, \dots, x_m): x_i \in K_i, \forall i=1,\dots,m }
$$
is a closed proper cone with non-empty interior (with non-empty interior).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark: $\R_+^n, \mathcal{Q}^{n+1}, \mathcal{S}&lt;em&gt;+^n$ have non-empty interior:
$$
\intr(\R&lt;/em&gt;+^n) = \R_{++}^n \
\intr(\mathcal{Q}^{n+1}) = { (t, x) \in \R \times \R^n: t &amp;gt; ||x||&lt;em&gt;2 } \
\intr(\mathcal{S}&lt;/em&gt;+^n) = \mathcal{S}_{++}^n \
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conic-linear-programming&#34;&gt;Conic Linear Programming&lt;/h2&gt;
&lt;h3 id=&#34;formulation&#34;&gt;Formulation&lt;/h3&gt;
&lt;p&gt;Recall that linear programming is&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\min &amp;amp; \quad c^T x \
\text{s.t.} &amp;amp; \quad b - A x \ge 0
\end{align*}
$$
Now replacing $\ge$ with the good order $\succeq$ gives the conic linear programming.
$$
\begin{align*}
\tag{CLP}
\min &amp;amp; \quad c^T x \
\text{s.t.} &amp;amp; \quad b - A x \succeq 0
\end{align*}
$$
The next question is, can we recover Farkas&amp;rsquo; lemma and strong duality in this setting? The answer is yes. And the good aspect of the good order is that we can recover the conclusions in linear programming verbatim. Next we generalize the LP problem and show the same result applies.&lt;/p&gt;
&lt;p&gt;Let $E$ be a finite-dimensional Euclidean space (e.g. $\R^n, \mathcal{S}^n$) and $\langle \cdot, \cdot \rangle$ be the inner product on $E$ (e.g., on $\R^n$, $\langle x, y \rangle = x^T y$; on $\mathcal{S}^n$, $\langle X, Y \rangle = \tr(X^T Y)$). Let $K \subseteq E$ be a &lt;strong&gt;closed proper cone&lt;/strong&gt;. Then we can have a good order $\succeq$ on $E$. Consider the &lt;strong&gt;standard form of CLP&lt;/strong&gt; as well as the primal problem:
$$
\begin{equation}
\tag{P} \label{primal}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; \langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall i=1,\dots,m \
&amp;amp; x \in K \subseteq E
\end{aligned}
\end{equation}
$$
What is its dual? We mimic the procedure when we build the lower bound for LP. Let $y \in \R^m$. By &lt;u&gt;the linearity of inner product&lt;/u&gt;,
$$
\langle b, y \rangle = b^T y = 
\sum_{i=1}^m \langle a_i, x \rangle y_i =
\sum_{i=1}^m \langle y_i a_i, x \rangle =
\langle \sum_{i=1}^m y_i a_i, x \rangle
$$
If we impose $c \succeq \sum_{i=1}^m y_i a_i$ like $c \ge A^T y$, can we draw $\langle c - \sum_{i=1}^m y_i a_i, x \rangle \ge 0$ like $x^T(c - A^T y) \ge 0$? The answer is that, this constraint is not enough in general. To reinforce the constraint, construct the set
$$
K^* = { w \in E: \langle w, x \rangle \ge 0, \forall x \in K }
$$&lt;/p&gt;
&lt;p&gt;We can draw some properties for $K^*$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$K^&lt;em&gt;$ is non-empty because $0 \in K^&lt;/em&gt;$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$K^&lt;em&gt;$ is always closed and convex (no matter what $K$ is, because $K^&lt;/em&gt;$ is the intersection of hyperplanes which are closed and convex).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$K^*$ is conic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $K$ is a closed proper cone with non-empty interior, then so is $K^&lt;em&gt;$. That is, $K$ and $K^&lt;/em&gt;$ are heterogenous on the premise that $K$ is a closed proper cone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have (verify it)
$$
(\R_+^n)^* = \R_+^n, (\mathcal{Q}^{n+1})^* = \mathcal{Q}^{n+1}, (\mathcal{S}&lt;em&gt;+^n)^* = \mathcal{S}&lt;/em&gt;+^n
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, $K^&lt;em&gt;$ is called the &lt;strong&gt;dual cone&lt;/strong&gt;. We impose that $c - \sum_{i=1}^m y_i a_i \in K^&lt;/em&gt;$. Then, the dual problem can be written as
$$
\begin{equation}
\tag{D} \label{dual}
\begin{aligned}
v_d^* = \sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in K^* \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;The constraint of CLP&amp;rsquo;s dual in essence describes that &lt;u&gt;an affine map of the variable $y$ belongs to a cone&lt;/u&gt;. This is a good indicator on whether we are dealing with a CLP.&lt;/p&gt;
&lt;h4 id=&#34;second-order-cone-programming&#34;&gt;Second-order Cone Programming&lt;/h4&gt;
&lt;h1 id=&#34;endbmatrix&#34;&gt;Consider the second-order cone programming:
$$
\begin{equation}
\tag{SOCP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{Q}^{n+1} \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$
Let $a_i = [u_i, \underbrace{a_{i,1}, \dots, a_{i, n}}&lt;em&gt;{\bar a_i^T}]^T$ and $c = [v, \underbrace{c_1, \dots, c_n}&lt;/em&gt;{d^T}]^T$. The dual constraint becomes
$$
\begin{gathered}
c - \sum_{i=1}^m y_i a_i =
\begin{bmatrix}
v \
d
\end{bmatrix} -
\sum_{i=1}^m y_i
\begin{bmatrix}
u_i \
\bar a_i
\end{bmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{bmatrix}
v - u^T y \
d - A^T y
\end{bmatrix} \succeq 0, \
\text{where }
\begin{aligned}[t]
A &amp;amp;= [\bar a_1, \dots, \bar a_n]^T, \
u &amp;amp;= [u_1, \dots, u_n]^T, \
\end{aligned}
\end{gathered}
$$
That is
$$
v - u^T y \ge |d - A^T y|&lt;em&gt;2
$$
The left-hand and the right-hand side of the above are both an affine function in $y$. The above is equivalent to
$$
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \
(d - A^T y)^T &amp;amp; v - u^T y 
\end{bmatrix}
\in \mathcal{S}&lt;/em&gt;+^{n+1}
$$
To show it, firstly the case when $v - u^T y = 0$ trivially holds. On the other hand, when $v - u^T y &amp;gt; 0$, we have
$$
\begin{aligned}
(v - u^T y)^2 &amp;amp;\ge (d - A^T y)^T (d - A^T y) \
\underbrace{(v - u^T y)}&lt;em&gt;{C} &amp;amp;\ge \underbrace{(d - A^T y)^T}&lt;/em&gt;{B^T} \underbrace{\frac{I_n}{v - u^T y}}&lt;em&gt;{A^{-1}} \underbrace{(d - A^T y)}&lt;/em&gt;{B}
\end{aligned}
$$
Consider the &lt;em&gt;Schur complement&lt;/em&gt;:
$$
\begin{bmatrix}
A &amp;amp; B \
B^T &amp;amp; C
\end{bmatrix}
\in \mathcal{S}&lt;em&gt;+^{m+n}
\iff
\begin{gathered}
A \in \mathcal{S}&lt;/em&gt;+^m, C \in \mathcal{S}&lt;em&gt;+^n, \
A - B C^{-1} B^T \in \mathcal{S}&lt;/em&gt;+^m \text{ or } C - B^T A^{-1} B \in \mathcal{S}&lt;em&gt;+^n
\end{gathered}
$$
We have,
$$
\begin{bmatrix}
(v - u^T y) I_n &amp;amp; d - A^T y \
(d - A^T y)^T &amp;amp; v - u^T y 
\end{bmatrix}
\in \mathcal{S}&lt;/em&gt;+^{n+1}
$$
The converse similarly follows from &lt;em&gt;Schur complement&lt;/em&gt;. Based on this result, we further claim that an SOCP is equivalent to an semi-definite programming (SDP). But it is never necessary to solve SOCP by converting it to SDP, which only increases the complexity.&lt;/p&gt;
&lt;h4 id=&#34;semi-definite-programming&#34;&gt;Semi-definite Programming&lt;/h4&gt;
&lt;p&gt;$$
\begin{equation}
\tag{SDP}
\begin{aligned}
\sup \quad &amp;amp; b^T y \
\text{s.t.} \quad &amp;amp; c - \sum_{i=1}^m y_i a_i \in \mathcal{S}_+^n \
&amp;amp; y \in \R^m
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;In terms of inclusion (and thus difficulty), LP =&amp;gt; QCQP =&amp;gt; SOCP =&amp;gt; SDP =&amp;gt; CLP.&lt;/p&gt;
&lt;h3 id=&#34;weak-duality&#34;&gt;Weak Duality&lt;/h3&gt;
&lt;p&gt;Our previous derivation of $\eqref{primal}$ and $\eqref{dual}$ implies the weak duality of CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality of CLP&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primal}$ and $\bar y$ be feasible for $\eqref{dual}$. Then, $\langle c, \bar x \rangle \ge b^T \bar y$.&lt;/p&gt;
&lt;p&gt;Proof:
$$
\begin{gathered}
c - \sum_{i=1}^m y_i a_i \in K^* \Rightarrow \langle c - \sum_{i=1}^m y_i a_i, x \rangle \ge 0 \
\Downarrow \
\begin{aligned}
\langle c, x \rangle &amp;amp;\ge \langle \sum_{i=1}^m y_i a_i, x \rangle \
&amp;amp;= \sum_{i=1}^m \langle a_i, x \rangle y_i  \
&amp;amp;= b^T y
\end{aligned}
\end{gathered}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We next investigate another method to convert between the primal and dual in CLP. For a standard CLP problem, we have
$$
\begin{gathered}
\begin{aligned}
\inf \quad &amp;amp; \langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \langle a_i, x \rangle = b_i, \forall i=1,\dots,m \
&amp;amp; x \in K \subseteq E
\end{aligned} \quad
\substack{\langle a_i, x \rangle = b \iff b - \langle a_i, x \rangle \in { 0 } \ \equiv} \quad
\begin{aligned}
-\sup \quad &amp;amp; -\langle c, x \rangle \
\text{s.t.} \quad &amp;amp; \begin{bmatrix}
b \
0
\end{bmatrix} -
\begin{bmatrix}
A \
-I
\end{bmatrix} x
\in { 0 } \times K
\end{aligned}
\end{gathered}
$$
whose dual is
$$
\begin{equation*}
\begin{aligned}
-\inf \quad &amp;amp; [b^T, 0] [y^T, \tilde y^T]^T \
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
A^T &amp;amp; -I
\end{bmatrix}
\begin{bmatrix}
y \
\tilde y
\end{bmatrix} = -c \
&amp;amp;
\begin{bmatrix}
y \
\tilde y
\end{bmatrix} \in ({ 0 } \times K)^*
\end{aligned}
\end{equation*}
$$
Note that $({ 0 } \times K)^* = { 0 }^* \times K^* = \R \times K^*$. And interestingly, from above, we observe that the equality constraint is in essence a cone constraint.&lt;/p&gt;
&lt;h3 id=&#34;farkas-lemma&#34;&gt;Farkas&amp;rsquo; Lemma&lt;/h3&gt;
&lt;p&gt;Recall the Farkas&amp;rsquo; lemma for linear systems. Exactly one of the following two systems is solvable:
$$
\begin{gather*}
Ax = b, x \ge 0 \
A^T y \le 0, b^T y &amp;gt; 0
\end{gather*}
$$
Farkas&amp;rsquo; lemma secures a strong duality for LP. We would like to do the same to CLP. First we mimic the two systems in CLP:
$$
\begin{gather*}
\langle a_i, x \rangle = b_i, \forall i=1,\dots,m; x \in K \tag{I} \
-\sum_{i=1}^m y_i a_i \in K^&lt;em&gt;; b^T y &amp;gt; 0 \tag{II}
\end{gather&lt;/em&gt;}
$$
Is it true that exactly one of the above two systems is solvable? We first claim that they can&amp;rsquo;t be solvable at the same time. Suppose on the contrary they both hold. By $\mathrm{(II)}$, we have
$$
\begin{aligned}
b^T y =\sum_{i=1}^m \langle a_i, x \rangle y_i =\langle \sum_{i=1}^m  y_i a_i, x \rangle &amp;amp;&amp;gt; 0 \
\end{aligned}
$$
But that $-\sum_{i=1}^m y_i a_i \in K^*$ implies $\langle \sum_{i=1}^m  y_i a_i, x \rangle = -\langle -\sum_{i=1}^m  y_i a_i, x \rangle \le 0$, which is a contradiction. On the other hand, for CLP, it is possible that neither $\mathrm{(I)}$ nor $\mathrm{(II)}$ is solvable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $E = \mathcal{S}^2, K = \mathcal{S}&lt;em&gt;+^2$. Let
$$
A_1 = \begin{bmatrix}
1 &amp;amp; 0 \
0 &amp;amp; 0
\end{bmatrix},
A_2 = \begin{bmatrix}
0 &amp;amp; 1 \
1 &amp;amp; 0
\end{bmatrix},
b = \begin{bmatrix}
0 \
2
\end{bmatrix}
$$
Then
$$
\begin{gathered}
\begin{cases}
\langle A_1, X \rangle = 0 \
\langle A_2, X \rangle = 2 \
X \in \mathcal{S}&lt;/em&gt;+^2
\end{cases}
\iff
\begin{cases}
X_{11} = 0 \
2 X_{12} = 2 \
X \in \mathcal{S}&lt;em&gt;+^2
\end{cases}
\iff
\begin{bmatrix}
0 &amp;amp; 1 \
1 &amp;amp; X&lt;/em&gt;{22}
\end{bmatrix} \in \mathcal{S}_+^2
\text{, which is unsolvable} \&lt;/p&gt;
&lt;p&gt;\begin{cases}
-(y_1 A_1 + y_2 A_2) \in \mathcal{S}&lt;em&gt;+^2 \
2 y_2 &amp;gt; 0
\end{cases}
\iff
\begin{cases}
\begin{bmatrix}
-y_1 &amp;amp; -y_2 \
-y_2 &amp;amp; 0 \
\end{bmatrix} \in \mathcal{S}&lt;/em&gt;+^2 \
y_2 &amp;gt; 0
\end{cases}
\text{, which is unsolvable}
\end{gathered}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is what goes wrong? Recall in the proof of the original Farkas&amp;rsquo; lemma, we apply the separation theorem to the setting
$$
b \notin { Ax: x \in \R_+^n }
$$
The right-hand set is always closed. However, for an arbitrary closed proper cone $K$, the set
$$
{ (\langle a_1, x \rangle, \dots, \langle a_m, x \rangle): x \in K }
$$
is not always closed. Without closeness, we can&amp;rsquo;t properly apply the &lt;em&gt;point-set separation theorem&lt;/em&gt;. For the same reason, the optimum may not be attained in CLP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Continue with the $A_1, A_2, b$ in the previous example. Consider the set
$$
S = { (\langle A_1, X \rangle, \langle A_2, X \rangle): X \in \mathcal{S}&lt;em&gt;+^2 }
$$
$(\langle A_1, X \rangle, \langle A_2, X \rangle) = (X&lt;/em&gt;{11}, 2 X_{12})$ basically describes the trajectory of $X$ as $X$ varies in $\mathcal{S}&lt;em&gt;+^2$.
$$
X = \begin{bmatrix}
X&lt;/em&gt;{11} &amp;amp; X_{12} \
X_{12} &amp;amp; X_{22}
\end{bmatrix}
$$
If $X_{11} = 0$, the only way to make $X$ PSD is to make $X_{12} = 0$; if $X_{11} &amp;gt; 0$, $X_{12}$ can be arbitrary because we can take $X_{22}$ sufficiently large such that $X_{11} X_{22} \ge X_{12}^2$. Therefore,
$$
S = { (0, 0) } \cup { (x, y): x &amp;gt; 0, y \in \R }
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If only we can guarantee the closeness of transformation of a proper cone!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;conic Farkas&amp;rsquo; lemma&lt;/strong&gt;. Suppose there exists a $\bar y \in \R^m$ satisfying
$$
-\sum_{i=1}^m \bar y_i a_i \in \intr(K^*) \tag{Slater condition}
$$
Then exactly one of the $\mathrm{(I)}$ and $\mathrm{(II)}$ is solvable.&lt;/p&gt;
&lt;p&gt;The spirit of the Slater condition (and many its variants) is that &amp;ldquo;some vector is in the interior of some set&amp;rdquo;. For this case, it guarantees the closeness of $K^*$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;strong-duality&#34;&gt;Strong Duality&lt;/h3&gt;
&lt;p&gt;Recall the LP strong duality theorem: suppose that the primal is feasible and is bounded below (alternatively the dual is feasible and is bounded above); then, $v_p^* = v_d^*$ and both the primal and the dual have optimal solutions. The question is what about &lt;strong&gt;the duality gap and the attainment&lt;/strong&gt; of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;nonzero duality gap&lt;/strong&gt;. Consider the SDP:
$$
\begin{aligned}
v_p^* = \inf \quad &amp;amp; y_1\
\text{s.t.} \quad &amp;amp;
\begin{bmatrix}
0 &amp;amp; y_1 &amp;amp; 0 \
y_1 &amp;amp; y_2 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 1+y_1
\end{bmatrix} \in \mathcal{S}&lt;em&gt;3^+
\end{aligned}
$$
$y_1 = 0, y_2 = 0$ is the only solution to it. Thus, $v_p^* = 0$. It can be rewritten as
$$
\begin{aligned}
v_p^* = - \sup \quad &amp;amp; \underbrace{[-1, 0]}&lt;/em&gt;{b^T} [y_1, y_2]^T \
\text{s.t.} \quad &amp;amp;
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 1 \
\end{bmatrix}
}&lt;em&gt;{C} -
y_1 
\underbrace{
\begin{bmatrix} 
0 &amp;amp; -1 &amp;amp; 0 \
-1 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; -1
\end{bmatrix}
}&lt;/em&gt;{A_1} -
y_2 
\underbrace{
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; 0 \
0 &amp;amp; 0 &amp;amp; -1 \
\end{bmatrix}
}_{A_2}&lt;/p&gt;
&lt;p&gt;\in \mathcal{S}&lt;em&gt;3^+
\end{aligned}
$$
Its dual is
$$
\begin{aligned}
v_d^* = -\inf \quad &amp;amp; X&lt;/em&gt;{33} \
\text{s.t.} \quad &amp;amp; -2 X_{12} - X_{33} = -1 \
&amp;amp; -X_{22} = 0 \
&amp;amp; X \in \mathcal{S}&lt;em&gt;+^3
\end{aligned}
$$
The dual is feasible and $X&lt;/em&gt;{33}$ can only be $1$. Thus, $v_d^* = -1$.&lt;/p&gt;
&lt;p&gt;Note that the duality gap is nonzero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &lt;strong&gt;non-attainment&lt;/strong&gt;. Consider the SOCP:
$$
\begin{aligned}
v_p^* = \sup \quad &amp;amp; -y_1 \
\text{s.t.} \quad &amp;amp; (y_1 + y_2, 1, y_1 - y_2) \in \mathcal{Q}^3
\end{aligned}
$$
The constraint is
$$
y_1 + y_2 \ge \sqrt{1 + (y_1 - y_2)^2} \iff 4 y_1 y_2 \ge 1
$$
This implies that $y_1, y_2 &amp;gt; 0$. Obviously, $v_p^* = 0$ but cannot be attained. It can be rewritten as
$$
\begin{aligned}
v_p^* = \sup \quad &amp;amp; [-1, 0] [y_1, y_2]^T \
\text{s.t. }\quad &amp;amp;
\begin{bmatrix}
0 \
1 \
0
\end{bmatrix} - 
y_1 \begin{bmatrix}
-1 \
0 \
-1
\end{bmatrix} -
y_2 \begin{bmatrix}
-1 \
0 \
1
\end{bmatrix}
\in \mathcal{Q}^3
\end{aligned}
$$
Its dual is
$$
\begin{aligned}
v_d^* = \inf \quad &amp;amp; z_2 \
\text{s.t.} \quad &amp;amp; -z_1 - z_3 = -1 \
&amp;amp; -z_1 + z_3 = 0 \
&amp;amp; (z_1, z_2, z_3) \in \mathcal{Q}^3
\end{aligned}
$$
The only solution is $(1/2, 0, 1/2)$ and the $v_d^* = 0$ is attained at this point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite the above, can we draw anything about the duality gap and the attainment of CLP?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;strong duality of CLP&lt;/strong&gt;. Suppose $\eqref{primal}$ is bounded and satisfies Slater&amp;rsquo;s condition, i.e. there exists a feasible $\bar x$ such that $\bar x \in \intr(K)$. Then, $v_p^* = v_d^&lt;em&gt;$ and there exists an $y^&lt;/em&gt;$ that is optimal to $\eqref{dual}$.&lt;/p&gt;
&lt;p&gt;On the other hand, suppose $\eqref{dual}$ is bounded and satisfies Slater&amp;rsquo;s condition, i.e. there exists a feasible $\bar y$ such that $c - \sum_{i=1}^m \bar y_i a_i \in \intr(K^&lt;em&gt;)$. Then, $v_p^&lt;/em&gt; = v_d^&lt;em&gt;$ and there exists an $x^&lt;/em&gt;$ that is optimal to $\eqref{primal}$.&lt;/p&gt;
&lt;p&gt;With Slater&amp;rsquo;s condition on one side, we can guarantee the zero duality gap and the attainment only on the other side. The CLP strong duality is much &amp;ldquo;weaker&amp;rdquo; than the LP strong duality. Refer to the &lt;u&gt;non-attainment&lt;/u&gt; example.&lt;/p&gt;
&lt;p&gt;Pay attention what CLP strong duality does not say as well. Even though the Slater&amp;rsquo;s condition is not satisfied, zero duality gap and attainment can hold. Refer to the &lt;u&gt;nonzero duality gap&lt;/u&gt; example.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/6-optimizaition-under-uncertainty/</guid>
      <description>&lt;p&gt;Firstly consider that in the previous (conic) linear programming discussion,
$$
\tag{Problem} \begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
$$
The coefficients $\hat a_i, \hat b, \hat c$ are given data. They correspond to the measurements in the real world. But it is highly likely that these measurements are not 100% accurate. How do we take into account these uncertainties?&lt;/p&gt;
&lt;p&gt;Without loss of generality, we may assume that $\hat c$ is deterministic, since the above is equivalent to
$$
\begin{aligned}
\min \quad &amp;amp; t \
\text{s.t.} \quad &amp;amp; \hat c^T x \le t \
&amp;amp; \hat a_i^T x \le \hat b_i, i=1,2,\dots,m
\end{aligned}
$$
In this new problem, we bring all uncertainties into the constraint and the coefficient $[0,\dots,0,1]$ for the variable $[x^T, t]$ is deterministic.&lt;/p&gt;
&lt;p&gt;The problem remains how to handle the uncertainty in the constraint in $\text{(Problem)}$. There are several viable methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stochastic optimization&lt;/p&gt;
&lt;p&gt;Stochastic optimization assumes that data follow a probability distribution $\mathbb{P}$ (unknown). The constraint becomes
$$
\Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta
$$
for some $\delta &amp;gt; 0$. This method is non-trivial, due to the integral of multi-dimensional probability distribution function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Robust optimization&lt;/p&gt;
&lt;p&gt;The assumption is that data is drawn from a &lt;strong&gt;ambiguity set&lt;/strong&gt; $\mathcal{U}$. We require that
$$
\hat a_i^T x \le \hat b_i, \forall i=1,2,\dots, \forall (\hat a_i, \hat b_i) \in \mathcal{U}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distributionally-robust optimization&lt;/p&gt;
&lt;p&gt;This is kind of the combination of the above two methods. The assumption is that data follow a probability distribution $\mathbb{P}$, which in turn belongs to some ambiguity set $\mathcal{U}$. The constraint becomes
$$
\inf_{\mathbb{P} \in \mathcal{U}} \Pr(\hat a_i^T x \le \hat b_i) \ge 1 - \delta, \forall i=1,2,\dots
$$
for some $\delta &amp;gt; 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;robust-linear-programming&#34;&gt;Robust Linear Programming&lt;/h2&gt;
&lt;p&gt;Consider the problem
$$
\label{rp} \tag{R} \begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; \underbrace{[\hat \alpha_i^T, \hat b_i]}_{\hat a_i^T} \underbrace{[x^T, x&amp;rsquo;]^T}&lt;em&gt;z \le 0, \
&amp;amp; \quad \hat a_i \in \mathcal{U}&lt;em&gt;i, i=1,\dots,m \
&amp;amp; \underbrace{x&amp;rsquo;}&lt;/em&gt;{z&lt;/em&gt;{n+1}} = -1
\end{aligned}
$$
where $\mathcal{U}&lt;em&gt;i \triangleq { y \in \R^{n+1}: y = u_i + B_i v, B_i \in \mathcal{S}&lt;/em&gt;{++}^{n+1}, |v| \le 1 }$ is an ellipsoid.&lt;/p&gt;
&lt;p&gt;Note that $\eqref{rp}$ is not LP actually, as it may contain infinitely-many linear constraints. This is usually hard. Just imagine its dual problem. For a primal that has infinitely-many constraints, its dual has infinitely-many variables.&lt;/p&gt;
&lt;p&gt;The key question now is how to tackle $\hat a_i^T z \le 0, \forall \hat a_i \in \mathcal{U}&lt;em&gt;i$ (ignoring $z&lt;/em&gt;{n+1} = -1$ for now). In essence, it is equivalent to
$$
\begin{aligned}
\left[ \sup_{\hat a_i \in \mathcal{U}&lt;em&gt;i} \hat a_i^T z \right] &amp;amp;\le 0 \iff \
\left[ \sup&lt;/em&gt;{|v| \le 1} (u_i + B_i v)^T z \right] &amp;amp;\le 0 \iff \
u_i^T z + \left[ \sup_{|v| \le 1} v^T B_i z \right] &amp;amp;\le 0 \iff \
y_i^T z + |B_i z|_2 &amp;amp;\le 0
\end{aligned}
$$
This is exactly an SOCP constraint.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/7-quadratically-constrained-quadratic-programming/</guid>
      <description>&lt;h2 id=&#34;quadratically-constrained-quadratic-programming&#34;&gt;Quadratically Constrained Quadratic Programming&lt;/h2&gt;
&lt;p&gt;Consider the quadratically constrained quadratic programming:
$$
\label{qcqp} \tag{QCQP} \begin{aligned}
\inf \quad &amp;amp; x^T Q x \
\text{s.t.} \quad &amp;amp; x^T A_i x \ge b_i, \forall i=1,\dots,m
\end{aligned}
$$
where $Q, A_1, \dots, A_m \in \mathcal{S}^n$. By far, no convexity is not assumed. But on the other hand, we can apply the &lt;strong&gt;semi-definite relaxation&lt;/strong&gt; technique to transform the $\eqref{qcqp}$.&lt;/p&gt;
&lt;p&gt;Observe that $x^T Q x = \tr(x^T Q x) = \tr(Q x x^T) = Q \bullet x x^T$, which is linear in $x x^T$. We can apply the same trick to the constraints so that $\eqref{qcqp}$ is equivalent to
$$
\begin{aligned}
v^* = \inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; \exists x \in \R^n, X = x x^T \
\text{ non-convex??}
\end{aligned}
$$
Note that $\exists x \in \R^n, X = x x^T \iff X \in \mathcal{S}_+^n, \rank(X) \le 1$. That is.
$$
\begin{aligned}
\inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; X \succeq 0 \
&amp;amp; \rank(X) \le 1
\end{aligned}
$$
The $\rank$ function is not convex. We may as well drop the rank constraint so that the semi-definite relaxation of $\eqref{qcqp}$ is
$$
\label{relaxed-qcqp} \tag{Relaxed QP} \begin{aligned}
v_R^* = \inf \quad &amp;amp; Q \bullet X \
\text{s.t.} \quad &amp;amp; A_i \bullet X \ge b_i, \forall i=1,\dots,m \
&amp;amp; X \succeq 0
\end{aligned}
$$
Observe that $v^* \ge v_R^*$ and $\eqref{relaxed-qcqp}$ is a SDP.&lt;/p&gt;
&lt;h3 id=&#34;max-cut-problem&#34;&gt;Max-cut Problem&lt;/h3&gt;
&lt;p&gt;Let $G = (V,E)$ be an undirected graph and $w: E \mapsto \R_+$ be a weight function on $E$. A subset $S \subseteq V$ defines a cut and the value of a cut is
$$
w(S) \triangleq \sum_{(i,j) \in E, i \in S, j \notin S} w_{ij}
$$
The goal is to find $S \subseteq V$ such that $w(S)$ is maximized. The minimization problem is trivial, simply choosing $S$ as $V$ or $\emptyset$ gives the minimum value $0$. Let $x_i \in { -1, +1 }$ be a binary variable indicating whether vertex $i$ is in the cut ($+1$) or not ($-1$). Then,
$$
\label{mc} \tag{Max-cut} \begin{aligned}
v^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - x_i x_j) \
\text{s.t.} \quad &amp;amp; x_i^2 = 1, \forall i=1,\dots,n \
\end{aligned}
$$
Apply the SDR technique (there is a middle step to convert the above to a QP) to get
$$
\label{relaxed-mc} \tag{Relaxed Max-cut} \begin{aligned}
v_\text{sdr}^* = \max \quad &amp;amp; \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}) \
\text{s.t.} \quad &amp;amp; X_{ii} = 1, \forall i=1,\dots,n \
&amp;amp; X \succeq 0
\end{aligned}
$$
Note that $v^* \le v_\text{sdr}^&lt;em&gt;$. If $\eqref{relaxed-mc}$ is solved with a rank-one matrix $X^&lt;/em&gt;$, we can automatically decompose it to give the optimal solution to $\eqref{mc}$. The crux is how to preserve the optimality when $X^*$ is of rank higher than one. Here is an algorithm for it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve $\eqref{relaxed-mc}$ to get an optimal solution $X^&lt;em&gt;$. Let $X^&lt;/em&gt; = U^T U$ where $U \in \R^{n \times n}$. Let $u_i \in \R^n$ be the $i$-th column of $U$. We have $|u_i|&lt;em&gt;2^2 = u_i^T u_i = X&lt;/em&gt;{ii}^* = 1$.&lt;/li&gt;
&lt;li&gt;Let $r \in \R^n$ be a random vector uniformly distributed on the sphere $S^n = { x \in \R^n: |x|_2 = 1 }$ (this can be done by normalizing the samples from standard Gaussian in $\R^n$).&lt;/li&gt;
&lt;li&gt;Let $x_i&amp;rsquo; = \sign(u_i^T r)$ where $\sign(z)$ is $+1$ if $z \ge 0$ or $-1$ otherwise. Return ${ x_i&amp;rsquo;: i=1,\dots,n }$ as a feasible solution to $\eqref{mc}$. This is also referred to as the &lt;strong&gt;hyperplane rounding&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the first place, $x_i&amp;rsquo;$s are feasible. Let $v&amp;rsquo;$ be the objective value associated with the cut ${ x_i&amp;rsquo;: i=1,\dots,n }$. Clearly, $v&amp;rsquo; \le v^&lt;em&gt;$. To analyze its approximation bound, firstly note that ${ x_i&amp;rsquo;: i=1,\dots,n }$ is random. We can only consider the $\E[v&amp;rsquo;]$.
$$
\begin{aligned}
&amp;amp;\E[v&amp;rsquo;] = \frac{1}{2} \E[\sum_{(i,j) \in E} w_{ij} (1 - x_i&amp;rsquo; x_j&amp;rsquo;)] \
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \E[\frac{1 - x_i&amp;rsquo; x_j&amp;rsquo;}{2}] \
&amp;amp;= \sum_{(i,j) \in E} w_{ij} \Pr[\sign(u_i^T r) \ne \sign(u_j^T r)] \
\end{aligned}
$$
Let $u, v \in S^n$ be arbitrary, $r$ be uniformly distributed on $S^{n-1}$. Then,
$$
\Pr[\sign(u^T r) \ne \sign(v^T r)] = \frac{\arccos(u^T v)}{\pi}
$$
Under the above setting, for any $z \in [-1, 1]$ and $\theta$ such that $\cos \theta = z$,
$$
\frac{\arccos z}{\pi} = \frac{2 \theta}{\pi(1 - \cos \theta)} \frac{1}{2} (1 - z) \
\ge \alpha \cdot \frac{1}{2} (1-z)
$$
where $\alpha = \min_{0 \le \theta \le \pi} \frac{2 \theta}{\pi (1 - \cos \theta)} &amp;gt; 0.878$. As a result,
$$
\begin{aligned}
&amp;amp;\E[v&amp;rsquo;] = \sum_{(i,j) \in E} w_{ij} \frac{\arccos u_i^T u_j}{\pi} \
&amp;amp;\ge \sum_{(i,j) \in E} w_{ij} \alpha \cdot \frac{1}{2} (1 - \underbrace{u_i^T u_j}&lt;em&gt;{X&lt;/em&gt;{ij}^&lt;/em&gt;}) \
&amp;amp;= \alpha \sum_{(i,j) \in E} \frac{1}{2} w_{ij} (1 - X_{ij}^&lt;em&gt;) \
&amp;amp;= \alpha v_\text{sdr}^&lt;/em&gt; \ge \alpha v^*
\end{aligned}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chunxy.github.io/courses/foundations-of-optimization/8-nonlinear-programming/</guid>
      <description>&lt;h2 id=&#34;nonlinear-programming&#34;&gt;Nonlinear Programming&lt;/h2&gt;
&lt;p&gt;Recall the unconstrained optimization problem:
$$
\inf_{x \in \R^n} \quad f(x)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be a continuously differentiable function, $\bar x \in \R^n$ be an arbitrary point. If there exists s &lt;strong&gt;direction&lt;/strong&gt; $d \in \R^n \setminus { 0 }$ such that $\nabla f(\bar x)^T d &amp;lt; 0$, then there exists $\alpha_0 &amp;gt; 0$ such that
$$
f(\bar x + \alpha d) &amp;lt; f(\bar x), \forall \alpha \in (0, \alpha_0]
$$
Here, $d$ is called a &lt;strong&gt;descent direction&lt;/strong&gt; of $f$ at $\bar x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a necessary condition for $\bar x$ to be a local minima is that $\nabla f(\bar x) = 0$ (&lt;strong&gt;first-order necessary condition&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: Let $f: \R^n \mapsto \R$ be a convex and continuously differentiable function. Then, $\bar x$ is a global minima of $f$ if and only if $\nabla f(\bar x) = 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Proposition: &lt;strong&gt;second-order sufficient condition&lt;/strong&gt;. Let $f: \R^n \mapsto \R$ be a twice continuously differentiable function. If $\nabla f(\bar x) = 0$ and $\nabla^2 f(\bar x) \succ 0$, then $\bar x$ is a local minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;In constrained case, simple conditions does not apply, e.g. $\inf_{x \ge 1} x^2$ and $\inf_{x \ge -1} x^2$. Consider the following constrained problem:
$$
\label{primal} \tag{P} \begin{aligned}
\inf_{x \in \R^n} \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; g_i(x) \le 0, i=1,\dots,r \
&amp;amp; h_j(x) = 0, j=1,\dots,s
\end{aligned}
$$
where $f, g_i, h_j$ are continuously differentiable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Fritz John necessary condition&lt;/strong&gt;. Let $\bar x$ be a local minimum of $\eqref{primal}$. Then there exist &lt;strong&gt;multipliers&lt;/strong&gt; $u \in \R$, $v_1, \dots, v_r \in \R$, $w_1, \dots, w_s \in \R$ such that
$$
\begin{gather}
u \nabla f(\bar x) + \sum_{i=1}^r v_i \nabla g_i(\bar x) + \sum_{j=1}^s w_j \nabla h_j(x) = 0 \tag{vanishing gradient} \
[u, v_1, \dots, v_r, w_1, \dots, w_s] \ne 0 \tag{non-trivial solution} \
u, v_i \ge 0, i=1,\dots,r \tag{non-negativity} \
v_i g_i(\bar x) = 0, i=1,\dots,r \tag{complementarity} \
\end{gather}
$$
$v_i$ tells the importance of the inequality constraint $g_i(x)$; $v_i = 0$ implies that $g_i(x) \le 0$ is well-fulfilled (strictly less than zero).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The implication is that, at a local minima, we should not be able to find a direction that decreases the objective value as well as maintains the feasibility. For simplicity of discussion, we drop the equality constraints below.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Linear non-independence&lt;/p&gt;
&lt;p&gt;The vanishing gradient together with the non-trivial solution in essence rules out the possibility of linear independence among the gradients. This is easy to interpret. If $\nabla f(\bar x), \nabla g_1(\bar x), \dots, \nabla g_r(\bar x)$ are linearly independent, it is easy to find a direction $d$ in the $\Col^\perp(\nabla g_1(\bar x), \dots, \nabla g_r(\bar x))$ that is acute to $-\nabla f(\bar x)$. Then moving along $d$ at $\bar x$ will decrease the objective function value but maintain the inequality constraints, which contradicts that $\bar x$ is a local minima.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non-negativity + complementarity&lt;/p&gt;
&lt;p&gt;These two components should be discussed together and are a bit intriguing. Please refer to &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0022247X67901631&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surely, the premise is that $u$ is nonzero. When the only solution to $u$ is zero, the corresponding solution $\bar x$ to $x$ will be a garbage point: it will not be a local minima. In this case, $\nabla f(\bar x)$ must have a component that is orthogonal to $\nabla g_i(\bar x)$&amp;rsquo;s. We can walk along this component (or its opposite) to decrease $f$ without compromising inequality constraints. This motivates the study of &lt;strong&gt;constraint qualification&lt;/strong&gt;, which aims to ensure a nonzero $u$.&lt;/p&gt;
&lt;p&gt;Note that in the above discussion, we ignore the effect of $\nabla h_j(\bar x)$&amp;rsquo;s. They only make the choice of direction stricter, since the direction has to be orthogonal to them.&lt;/p&gt;
&lt;h3 id=&#34;kkt-conditions-and-constraint-qualification&#34;&gt;KKT Conditions and Constraint Qualification&lt;/h3&gt;
&lt;p&gt;One observation is that, if the constraint gradients are linearly independent (though required dependent), there is no way to have $u = 0$ or otherwise $v_1, \dots, v_r, u_1, \dots, u_s$ has to be zero due to the linear independence, which in turn violates the nonzero multiplier condition. How to &amp;ldquo;obtain&amp;rdquo; the contradicting linear independence expectation and nonzero multiplier condition at the same time?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Karush-Kuhn-Tucker conditions&lt;/strong&gt;. Let $\bar x$ be a local minima of $\eqref{primal}$. Let
$$
I(\bar x) = { i: g_i(\bar x) = 0 }
$$
be the index set on the &lt;strong&gt;active inequality constraint&lt;/strong&gt;. Suppose that ${ \nabla g_i(\bar x) }&lt;em&gt;{i \in I} \cup { \nabla h_j(\bar x) }&lt;/em&gt;{j=1}^s$ are linearly independent (&lt;strong&gt;linear-independence constraint qualification&lt;/strong&gt;). Then, there exists $v \in \R^r$ and $w \in \R^s$ such that
$$
\nabla f(\bar x) + \sum_{i=1}^r \nabla g_i(x) + \sum_{j=1}^s \nabla h_j(x) = 0 \
v_i \ge 0, i=1,\dots,r \
v_i g_i(\bar x) = 0, i=1,\dots,r
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: importance of CQ. Consider the problem
$$
\begin{aligned}
\inf \quad &amp;amp; f(x_1, x_2) = x_1 \
\text{s.t.} \quad &amp;amp; g_1(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - 1 \le 0 \
&amp;amp; g_2(x_1, x_2) = (x_1 - 1)^2 + (x_2 + 1)^2 - 1 \le 0 \
\end{aligned}
$$
The only feasible and thus optimal solution is $\bar x = (1, 0)$. In this case, the active inequality constraint gradient is
$$
\begin{bmatrix}
0 \
-2
\end{bmatrix},
\begin{bmatrix}
0 \
2
\end{bmatrix}
$$
They are linearly dependent. Therefore, KKT conditions doesn&amp;rsquo;t hold. Here is the reason why we intentionally separate the equality constraints from inequality constraints. Though $h(x) = 0 \iff h(x) \le 0 \land -h(x) \le 0$, if we lay down the equality constraint as inequality constraints, the gradient of $h(x)$ is always linearly dependent to that of $-h(x)$.&lt;/p&gt;
&lt;p&gt;One takeaway is that, even in this convex optimization problem, KKT may not hold.&lt;/p&gt;
&lt;p&gt;Another takeaway is that, it is very convenient to &amp;ldquo;draw circles&amp;rdquo; when finding counter examples related to constraint qualification. Better still, leave only one feasible point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem with LICQ is that it is tedious to check for every solution of $\bar x$. We may prefer some kind of &lt;strong&gt;&amp;ldquo;looser&amp;rdquo; constraint qualifications&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;Slater constraint qualification&lt;/strong&gt;. Suppose that $g_1, \dots, g_r$ are convex and $h_1, \dots, h_s$ are affine. Let $\bar x$ be a local minima. Denote the feasible region as $S$. Suppose that there exists $x&amp;rsquo; \in S$ such that $g_i(x&amp;rsquo;) &amp;lt; 0$ for $i=1,\dots,r$. Then, the KKT conditions are necessary for optimality.&lt;/p&gt;
&lt;p&gt;This Slater condition quite resembles that in the conic Farkas&amp;rsquo; lemma.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: Suppose that $g_1, \dots, g_r$ are concave and $h_1, \dots, h_s$ are affine. Then, the KKT conditions are necessary for optimality.&lt;/p&gt;
&lt;p&gt;This theorem is especially useful when all the constraint functions are affine (since affine functions are both convex and concave).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Let $A \in \R{m \times n}, b \in \R^m, c \in \R^n$ be given. Consider
$$
\begin{aligned}
\min \quad &amp;amp; c^T x \
\text{s.t.} \quad &amp;amp; Ax = b \
&amp;amp; x \ge 0
\end{aligned}
$$
Beware of the dimension of constraint functions when converting this LP problem to nonlinear programming problem:
$$
\begin{aligned}
\min \quad &amp;amp; c^Tx \
\text{s.t.} \quad &amp;amp; g_i(x) = -x_i = -e_i^T x \le 0, i=1,\dots,n \
&amp;amp; h_i(x) = b_j - a_j^T x = 0, j=1,\dots,m
\end{aligned}
$$
This is a linearly constrained problem; thus KKT conditions are necessary for optimality:
$$
\begin{gather}
c - \sum_{i=1}^r v_i e_i - \sum_{j=1}^s w_i a_j = 0 \label{grad} \
v_i \ge 0 \label{dual} \
v_i x_i = 0, i=1,\dots,r \label{compl}
\end{gather}
$$
From $\eqref{grad}$,
$$
c - v - A^T w = 0
$$
Given $v \ge 0$ from $\eqref{dual}$ and the complementarity from $\eqref{compl}$, we have $c \ge A^T w$ and $(c - A^T w)_i x_i = 0$. This essentially recovers the sufficient and necessary conditions for the optimality of LP. But KKT only tells the condition is necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question is when are KKT conditions sufficient?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;KKT sufficient conditions&lt;/strong&gt;. Suppose that $f, g_1, \dots, g_r$ are convex and $h_1, \dots, h_s$ are affine. Suppose further that there exists $(\bar x, \bar v, \bar w)$ satisfying the KKT conditions:
$$
\begin{gather}
g_i(\bar x) \le 0, h_j(\bar x) = 0, i=1,\dots,r, j=1,\dots,s \tag{primal feasibility} \
\tag{dual feasibility} \left. \begin{gathered}
\nabla f(\bar x) + \sum_{i=1}^r \bar v_i \nabla g_i(\bar x) + \sum_{j=1}^s \bar w_i \nabla h_i(x) = 0 \
\bar v_i \ge 0, i=1,\dots,r
\end{gathered} \right} \
\bar v_i g_i(x) = 0, i=1,\dots,r \tag{complentary slackness}
\end{gather}
$$
Then $\bar x$ is a global minima.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h3&gt;
&lt;p&gt;For simplicity, we rewrite the constrained problem as follows:
$$
\label{primalp} \tag{P} \begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; G(x) \le 0 \
&amp;amp; H(x) = 0
\end{aligned}
$$
where $G(x) = [g_1(x), \dots, g_r(x)]$ and $H(x) = [h_1(x), \dots, h_s(x)]$.&lt;/p&gt;
&lt;p&gt;Observe that
$$
\eqref{primalp} \equiv \inf_{x \in \R^n} \sup_{v \in \R_+^r, w \in \R^s} \underbrace{f(x) + v^T G(x) + w^T H(x)}&lt;em&gt;{L(x, v, w)}
$$
The dual of $\eqref{primalp}$ is then
$$
\begin{equation}
v_d^* = \sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} \inf_{x \in \R^n} L(x, v, w) \label{dualp} \tag{D}
\end{equation}
$$
Observe that
$$
\underbrace{\inf_{x \in \R^n}L(x, \bar v, \bar w)}&lt;em&gt;{\theta(\bar v, \bar w)} \le L(\bar x, \bar v, \bar w) \le \underbrace{\sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} L(\bar x, v, w)}&lt;em&gt;{\gamma(\bar x)}
$$
This implies that
$$
v_d^* = \sup&lt;/em&gt;{v \in \R_+^r, w \in \R^s} \theta(v, w) \le \inf_{x \in \R^n} \gamma(x) = v_p^*
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem: &lt;strong&gt;weak duality&lt;/strong&gt;. Let $\bar x$ be feasible for $\eqref{primalp}$ and $(\bar v, \bar w)$ be feasible for $\eqref{dualp}$. Then,
$$
f(\bar x) = \gamma(\bar x) \ge \theta(\bar v, \bar w)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: illustration of weak duality. Consider a simple case:
$$
\begin{gathered}
\begin{aligned}
v_p^* = \inf \quad &amp;amp; f(x) \
\text{s.t.} \quad &amp;amp; g(x) \le 0
\end{aligned} \
\rule{6cm}{0.4pt} \
\begin{aligned}
v_d^* = \sup_{v \ge 0} &amp;amp; \inf_{x \in \R^n} [f(x) + v g(x)] \
\end{aligned}
\end{gathered}
$$
Let $\mathcal{G} = { (y,z): y = g(x), z = f(x), x \in \R^n }$. Then,
$$
\begin{aligned}
\theta(v) &amp;amp;= \inf_{x \in \R^n} [f(x) + v g(x)] \
&amp;amp;= \inf_{(y, z) \in \mathcal{G}} [z + v y]
\end{aligned}
$$
This set is quite related to the proof of the &lt;strong&gt;strong duality&lt;/strong&gt; under KKT sufficient condition together with Slater condition.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
